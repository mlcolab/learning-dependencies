\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[preprint]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks = true, 
		    linkcolor = blue,
		    urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}       
% hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{natbib}
\usepackage[pdftex]{graphicx}
\usepackage{siunitx} % Required for alignment
\sisetup{
  round-mode          = places, % Rounds numbers
  round-precision     = 2, % to 2 places
}
	
\bibliographystyle{unsrtnat}


\title{Extracting and evaluating educational concept dependencies from Large Language Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Dominik Glandorf\\
  Matrikelnummer 6007407\\
  \texttt{dominik.glandorf@student.uni-tuebingen.de} \\
  \And
  Anastasiia Alekseeva\\
  Matrikelnummer 5994775\\
  \texttt{anastasiia.alekseeva@student.uni-tuebingen.de} \\
  \\
  GitHub repository: \url{https://github.com/mlcolab/learning-dependencies}
}

\begin{document}
\vspace*{-5mm}
\maketitle
\vspace*{-5mm}

\begin{abstract}
% What did we do?

% Why did we to it? What did we expect?

% How did we do it?

% What did we find out?

% What now?

\end{abstract}

\section{Introduction}
% Starting broad
% Field of contribution
Large Language Models (LLMs) are trained on immense corpora of text and have proven to build on embedded factual information when performing well in downstream tasks such as question answering. Accessing this knowledge, which is represented by billions of parameters and the network's architecture, has given rise to the research field of Knowledge Extraction from LLMs (Cohen, 2023). Computer-assisted education could greatly benefit from making implicit educational knowledge in LLMs accessible. This creates new perspectives for a subfield of knowledge engineering that has rather stagnated in the previous years.

% What kind of information do we want to extract?
%% Problem of sequencing in Instructional Design:
Concept dependencies are complex but highly relevant educational knowledge. Effective and efficient instruction does not only incorporate what to teach but also how to teach. Guidelines for \textit{instructional sequencing} emphasize the order of instruction. More precisely, prerequisites of educational content should be either known to the student or taught first \citep{morrison2019designing}.
% Definition concepts
Within educational content, Merill (1983) differentiated facts, concepts, principles, rules, procedures, interpersonal skills, attitudes, and their sole recall from their application. To simplify, we will focus on concepts and their relations defined understanding. 
% Concept dependency
% Assumption: information about dependencies between concepts makes instructional sequencing more effective in terms of learning
If one concept is a prerequisite of another, we refer to this relation as a concept dependency. For example, to understand the concept of a derivative, having knowledge about the concept of a function will facilitate or even enable learning. When the dependencies are thought of as directed edges between nodes that represent concepts, a concept dependency graph emerges which is a special type of a knowledge graph \citep{wang2016using}. This graph is also called \textit{concept map} in the field of Learning Sciences.
% TODO: provide a (graphical) example of a small concept map
% Why is is important?
The graph can be used to advance curriculum planning (Yang, 2015), especially for new topics that might not be covered in textbooks or automated assessment (Wang, 2015).

% What is the gap in research that we tackle?
% What is our research question?
% How to extract educational concept dependencies that are implicit and evaluate them using existing knowledge media?
In this work, we tackled the question how to extract this particular knowledge graph and how to evaluate its quality in a scalable manner.

% What can we contribute?
% 1. No benchmark available -> we set up an evaluation framework based on textbooks and Wikipedia
First, there was no classical benchmark available to evaluate the accuracy of extracted information. Therefore, we propose a set of methods to use existing unstructured knowledge sources, namely Wikipedia and textbooks, to create baselines for evaluation. Due to the heuristic character of these methods, we conducted a manual assessment to test their suitability for our purpose. The resulting dataset can be used as a human baseline for further research.

% 2. Emerging field of prompt engineering -> we engineered a simple but effective prompt process to query a LLM
Second, there is no established manner to extract the desired knowledge from the LLM. The emerging field of prompt engineering is currently a vast collection of commands to query LLMs. We propose a method called output refeeding that sequentially queries the language generation model and transforms its answers into a knowledge graph.

\subsection{Related work}
% Field: Concept Dependency Extraction
% of interest: How are concept dependencies defined?
\cite{talukdar2012crowdsourced} defined the prerequisite relation in terms of the consumption of information about concepts. Vuong (2011) if a better graduation rate given prerequisite knowledge is fulfilled. Concepts are often equated with Wikipedia articles (Talukdar and Cohen, 2012; Wang, 2015).

% Other approaches: Learning Path Analysis and Expert Knowledge
Prerequisites can be inferred from learner behavior by testing their performance after being presented different instructional sequences (Pavlik et al., 2008, Vuong et al., 2011). However, this has the disadvantage of disengaging users with too difficult concepts before teaching easier or necessary ones. Experts usually dispose of the required knowledge about concepts to create concept maps. The high cost of expert knowledge motivates the automated extraction of concept dependencies from appropriate sources.

% Field: Textbook Knowledge Extraction
% of interest: How are relationships betweens concepts extracted?

% Field: Prompt Engineering for Knowledge Graph creation
% of interest: How can we get internal representations out of a language model?
% Cohen, 2023
% GraphGPT

\section{Method}
% Section summary
In this section, we will first detail our research design and then the characteristics of our information sources as well as the methods that we used to produce the knowledge graph. 

\subsection{Research design} 
% two baseline methods: textbooks and Wikipedia
% information extraction: one LLM method
% evaluation
% 1. manual inspection via DashBoard
% 2. manual labeling
% 3. convergence between LLM and baselines

\subsection{Baseline extraction}
\subsubsection{Wikipedia}
% Why Wikipedia?
% good coverage according to one of our papers
% 1462 unique articles for book index entries, these were covered in the textbooks (we had to select a subset of concepts to reduce computational effort)
% How did we get the information?
% describe Algorithm to extract dependencies:
%• Assumption: understanding the concept’s definition in Wikipedia requires understanding the concepts in it
%• Operationalization: links to other articles in the first sentence of article
%• Filtered out links to fields (“Mathematics”), persons (“Gabriel Cramer), and concepts that linked even earlier to the concept of interest

\subsubsection{Textbooks}
% Types of textbooks (why linear algebra?)

% Amount of data
% 10 books on Linear Algebra
% 584 pages on average (652,297 characters)
% 382 number of index entries (median)

% Steps of processing
% preprocessing
% entity recognition and disambigation
% Wikifier: identify entity mentions using link targets and PageRank
For preprocessing the textbooks we used Wikifier \citep{brank2017annotating}.
% Wikisearch: query Wikipedia search engine for keyword (and field) and rank results based on Levenshtein distance

% relation extraction
% common introductory usage: identify other concepts that are most often mentioned on the first page given in the index across all books
% order pruning:  identify unlikely dependencies based on a certain number of books that introduce concepts in a particular order

\subsection{LLM extraction}
% description of T0PP and our infrastructure
% • Fine-tuned BLOOM on several question-answering datasets
% • 11 billion parameters (44GB RAM) • ~3-10 seconds per request on our infrastructure

% description of extraction procedure
% 1. output refeeding
% first prompt: {definition} := What is the mathematical definition of {concept}?
% second prompt:  {list of dependencies} := What mathematics concepts are mentioned here: {definition}
% 2. response parsing to Wikipedia article
% {canonical list of dependencies} :=Wikisearch disambiguation on {list of dependencies}

\subsection{Manual inspection via Dashboard}
% 

\subsection{Manual baseline}
% procedure to assess the accuracy of output

\subsection{Convergence statistics}
% for which concepts did we compare what to see what

\section{Results}
% Case Study using Dashboard

%Manual baseline
% Interrater reliability
% How good are the baselines?
% How good is the LLM?
% diagram

%Convergence statistics
% diagram for similarity of outputs

\section{Discussion}
% What is our main result?
% we are able to make the implicit knowledge explicit far above chance level
% problem is very complex -> humans do not fully agree
% Wikipedia might be the best source to automatically evaluate the output

% What are potential flaws?
% disambiguation already includes noise
% we only evaluated up to two levels of the graph, not the graph as a whole. different level of abstractions in extracted dependencies
% is there even something like a gold standard? some educational reflection about different didactic approaches and more depth than just understanding the definition of a topic
% we are still using the language generator, so it is just another downstream task, maybe it is more of a philosophical question what is implicit/explicit knowledge

% Q: Outlook and Future research ideas:
% advancements in LLMs that are interesting for us
% use cases in education

\bibliography{references.bib}

\end{document}

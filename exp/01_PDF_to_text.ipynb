{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a927fd66",
   "metadata": {},
   "source": [
    "# PDF to text\n",
    "This notebook experiments how to extract text from PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a820fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"../dat/books/mml-book.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ec20a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATHEMATICS  FOR \\nMACHINE LEARNING\\nMarc Peter DeisenrothA. Aldo FaisalCheng Soon Ong\\nMATHEMATICS FOR MACHINE LEARNING DEISENROTH ET AL.\\nThe fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efﬁ  ciently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the ﬁ  rst time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book’s web site.\\nMARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine Learning at the Department of Computing, Împerial College London.\\nA. ALDO FAISAL  leads the Brain & Behaviour Lab at Imperial College London, where he is also Reader in Neurotechnology at the Department of Bioengineering and the Department of Computing.\\nCHENG SOON ONG  is Principal Research Scientist at the Machine Learning Research Group, Data61, CSIRO. He is also Adjunct Associate Professor at Australian National University.\\nCover image courtesy of Daniel Bosma / Moment / Getty ImagesCover design by Holly Johnson\\nDeisenrith et al. 9781108455145 Cover. C M Y K\\n\\nContents\\nForeword 1\\nPart I Mathematical Foundations 9\\n1 Introduction and Motivation 11\\n1.1 Finding Words for Intuitions 12\\n1.2 Two Ways to Read This Book 13\\n1.3 Exercises and Feedback 16\\n2 Linear Algebra 17\\n2.1 Systems of Linear Equations 19\\n2.2 Matrices 22\\n2.3 Solving Systems of Linear Equations 27\\n2.4 Vector Spaces 35\\n2.5 Linear Independence 40\\n2.6 Basis and Rank 44\\n2.7 Linear Mappings 48\\n2.8 Afﬁne Spaces 61\\n2.9 Further Reading 63\\nExercises 64\\n3 Analytic Geometry 70\\n3.1 Norms 71\\n3.2 Inner Products 72\\n3.3 Lengths and Distances 75\\n3.4 Angles and Orthogonality 76\\n3.5 Orthonormal Basis 78\\n3.6 Orthogonal Complement 79\\n3.7 Inner Product of Functions 80\\n3.8 Orthogonal Projections 81\\n3.9 Rotations 91\\n3.10 Further Reading 94\\nExercises 96\\n4 Matrix Decompositions 98\\n4.1 Determinant and Trace 99\\ni\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\nii Contents\\n4.2 Eigenvalues and Eigenvectors 105\\n4.3 Cholesky Decomposition 114\\n4.4 Eigendecomposition and Diagonalization 115\\n4.5 Singular Value Decomposition 119\\n4.6 Matrix Approximation 129\\n4.7 Matrix Phylogeny 134\\n4.8 Further Reading 135\\nExercises 137\\n5 Vector Calculus 139\\n5.1 Differentiation of Univariate Functions 141\\n5.2 Partial Differentiation and Gradients 146\\n5.3 Gradients of Vector-Valued Functions 149\\n5.4 Gradients of Matrices 155\\n5.5 Useful Identities for Computing Gradients 158\\n5.6 Backpropagation and Automatic Differentiation 159\\n5.7 Higher-Order Derivatives 164\\n5.8 Linearization and Multivariate Taylor Series 165\\n5.9 Further Reading 170\\nExercises 170\\n6 Probability and Distributions 172\\n6.1 Construction of a Probability Space 172\\n6.2 Discrete and Continuous Probabilities 178\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\\n6.4 Summary Statistics and Independence 186\\n6.5 Gaussian Distribution 197\\n6.6 Conjugacy and the Exponential Family 205\\n6.7 Change of Variables/Inverse Transform 214\\n6.8 Further Reading 221\\nExercises 222\\n7 Continuous Optimization 225\\n7.1 Optimization Using Gradient Descent 227\\n7.2 Constrained Optimization and Lagrange Multipliers 233\\n7.3 Convex Optimization 236\\n7.4 Further Reading 246\\nExercises 247\\nPart II Central Machine Learning Problems 249\\n8 When Models Meet Data 251\\n8.1 Data, Models, and Learning 251\\n8.2 Empirical Risk Minimization 258\\n8.3 Parameter Estimation 265\\n8.4 Probabilistic Modeling and Inference 272\\n8.5 Directed Graphical Models 278\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nContents iii\\n8.6 Model Selection 283\\n9 Linear Regression 289\\n9.1 Problem Formulation 291\\n9.2 Parameter Estimation 292\\n9.3 Bayesian Linear Regression 303\\n9.4 Maximum Likelihood as Orthogonal Projection 313\\n9.5 Further Reading 315\\n10 Dimensionality Reduction with Principal Component Analysis 317\\n10.1 Problem Setting 318\\n10.2 Maximum Variance Perspective 320\\n10.3 Projection Perspective 325\\n10.4 Eigenvector Computation and Low-Rank Approximations 333\\n10.5 PCA in High Dimensions 335\\n10.6 Key Steps of PCA in Practice 336\\n10.7 Latent Variable Perspective 339\\n10.8 Further Reading 343\\n11 Density Estimation with Gaussian Mixture Models 348\\n11.1 Gaussian Mixture Model 349\\n11.2 Parameter Learning via Maximum Likelihood 350\\n11.3 EM Algorithm 360\\n11.4 Latent-Variable Perspective 363\\n11.5 Further Reading 368\\n12 Classiﬁcation with Support Vector Machines 370\\n12.1 Separating Hyperplanes 372\\n12.2 Primal Support Vector Machine 374\\n12.3 Dual Support Vector Machine 383\\n12.4 Kernels 388\\n12.5 Numerical Solution 390\\n12.6 Further Reading 392\\nReferences 395\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n\\nForeword\\nMachine learning is the latest in a long line of attempts to distill human\\nknowledge and reasoning into a form that is suitable for constructing ma-\\nchines and engineering automated systems. As machine learning becomes\\nmore ubiquitous and its software packages become easier to use, it is nat-\\nural and desirable that the low-level technical details are abstracted away\\nand hidden from the practitioner. However, this brings with it the danger\\nthat a practitioner becomes unaware of the design decisions and, hence,\\nthe limits of machine learning algorithms.\\nThe enthusiastic practitioner who is interested to learn more about the\\nmagic behind successful machine learning algorithms currently faces a\\ndaunting set of pre-requisite knowledge:\\nProgramming languages and data analysis tools\\nLarge-scale computation and the associated frameworks\\nMathematics and statistics and how machine learning builds on it\\nAt universities, introductory courses on machine learning tend to spend\\nearly parts of the course covering some of these pre-requisites. For histori-\\ncal reasons, courses in machine learning tend to be taught in the computer\\nscience department, where students are often trained in the ﬁrst two areas\\nof knowledge, but not so much in mathematics and statistics.\\nCurrent machine learning textbooks primarily focus on machine learn-\\ning algorithms and methodologies and assume that the reader is com-\\npetent in mathematics and statistics. Therefore, these books only spend\\none or two chapters on background mathematics, either at the beginning\\nof the book or as appendices. We have found many people who want to\\ndelve into the foundations of basic machine learning methods who strug-\\ngle with the mathematical knowledge required to read a machine learning\\ntextbook. Having taught undergraduate and graduate courses at universi-\\nties, we ﬁnd that the gap between high school mathematics and the math-\\nematics level required to read a standard machine learning textbook is too\\nbig for many people.\\nThis book brings the mathematical foundations of basic machine learn-\\ning concepts to the fore and collects the information in a single place so\\nthat this skills gap is narrowed or even closed.\\n1\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n2 Foreword\\nWhy Another Book on Machine Learning?\\nMachine learning builds upon the language of mathematics to express\\nconcepts that seem intuitively obvious but that are surprisingly difﬁcult\\nto formalize. Once formalized properly, we can gain insights into the task\\nwe want to solve. One common complaint of students of mathematics\\naround the globe is that the topics covered seem to have little relevance\\nto practical problems. We believe that machine learning is an obvious and\\ndirect motivation for people to learn mathematics.\\nThis book is intended to be a guidebook to the vast mathematical lit-\\nerature that forms the foundations of modern machine learning. We mo- “Math is linked in\\nthe popular mind\\nwith phobia and\\nanxiety. You’d think\\nwe’re discussing\\nspiders.” (Strogatz,\\n2014, page 281)tivate the need for mathematical concepts by directly pointing out their\\nusefulness in the context of fundamental machine learning problems. In\\nthe interest of keeping the book short, many details and more advanced\\nconcepts have been left out. Equipped with the basic concepts presented\\nhere, and how they ﬁt into the larger context of machine learning, the\\nreader can ﬁnd numerous resources for further study, which we provide at\\nthe end of the respective chapters. For readers with a mathematical back-\\nground, this book provides a brief but precisely stated glimpse of machine\\nlearning. In contrast to other books that focus on methods and models\\nof machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar-\\nber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers\\nand Girolami, 2016) or programmatic aspects of machine learning (M ¨uller\\nand Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018),\\nwe provide only four representative examples of machine learning algo-\\nrithms. Instead, we focus on the mathematical concepts behind the models\\nthemselves. We hope that readers will be able to gain a deeper understand-\\ning of the basic questions in machine learning and connect practical ques-\\ntions arising from the use of machine learning with fundamental choices\\nin the mathematical model.\\nWe do not aim to write a classical machine learning book. Instead, our\\nintention is to provide the mathematical background, applied to four cen-\\ntral machine learning problems, to make it easier to read other machine\\nlearning textbooks.\\nWho Is the Target Audience?\\nAs applications of machine learning become widespread in society, we\\nbelieve that everybody should have some understanding of its underlying\\nprinciples. This book is written in an academic mathematical style, which\\nenables us to be precise about the concepts behind machine learning. We\\nencourage readers unfamiliar with this seemingly terse style to persevere\\nand to keep the goals of each topic in mind. We sprinkle comments and\\nremarks throughout the text, in the hope that it provides useful guidance\\nwith respect to the big picture.\\nThe book assumes the reader to have mathematical knowledge commonly\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nForeword 3\\ncovered in high school mathematics and physics. For example, the reader\\nshould have seen derivatives and integrals before, and geometric vectors\\nin two or three dimensions. Starting from there, we generalize these con-\\ncepts. Therefore, the target audience of the book includes undergraduate\\nuniversity students, evening learners and learners participating in online\\nmachine learning courses.\\nIn analogy to music, there are three types of interaction that people\\nhave with machine learning:\\nAstute Listener The democratization of machine learning by the pro-\\nvision of open-source software, online tutorials and cloud-based tools al-\\nlows users to not worry about the speciﬁcs of pipelines. Users can focus on\\nextracting insights from data using off-the-shelf tools. This enables non-\\ntech-savvy domain experts to beneﬁt from machine learning. This is sim-\\nilar to listening to music; the user is able to choose and discern between\\ndifferent types of machine learning, and beneﬁts from it. More experi-\\nenced users are like music critics, asking important questions about the\\napplication of machine learning in society such as ethics, fairness, and pri-\\nvacy of the individual. We hope that this book provides a foundation for\\nthinking about the certiﬁcation and risk management of machine learning\\nsystems, and allows them to use their domain expertise to build better\\nmachine learning systems.\\nExperienced Artist Skilled practitioners of machine learning can plug\\nand play different tools and libraries into an analysis pipeline. The stereo-\\ntypical practitioner would be a data scientist or engineer who understands\\nmachine learning interfaces and their use cases, and is able to perform\\nwonderful feats of prediction from data. This is similar to a virtuoso play-\\ning music, where highly skilled practitioners can bring existing instru-\\nments to life and bring enjoyment to their audience. Using the mathe-\\nmatics presented here as a primer, practitioners would be able to under-\\nstand the beneﬁts and limits of their favorite method, and to extend and\\ngeneralize existing machine learning algorithms. We hope that this book\\nprovides the impetus for more rigorous and principled development of\\nmachine learning methods.\\nFledgling Composer As machine learning is applied to new domains,\\ndevelopers of machine learning need to develop new methods and extend\\nexisting algorithms. They are often researchers who need to understand\\nthe mathematical basis of machine learning and uncover relationships be-\\ntween different tasks. This is similar to composers of music who, within\\nthe rules and structure of musical theory, create new and amazing pieces.\\nWe hope this book provides a high-level overview of other technical books\\nfor people who want to become composers of machine learning. There is\\na great need in society for new researchers who are able to propose and\\nexplore novel approaches for attacking the many challenges of learning\\nfrom data.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n4 Foreword\\nAcknowledgments\\nWe are grateful to many people who looked at early drafts of the book\\nand suffered through painful expositions of concepts. We tried to imple-\\nment their ideas that we did not vehemently disagree with. We would\\nlike to especially acknowledge Christfried Webers for his careful reading\\nof many parts of the book, and his detailed suggestions on structure and\\npresentation. Many friends and colleagues have also been kind enough\\nto provide their time and energy on different versions of each chapter.\\nWe have been lucky to beneﬁt from the generosity of the online commu-\\nnity, who have suggested improvements via https://github.com , which\\ngreatly improved the book.\\nThe following people have found bugs, proposed clariﬁcations and sug-\\ngested relevant literature, either via https://github.com or personal\\ncommunication. Their names are sorted alphabetically.\\nAbdul-Ganiy Usman\\nAdam Gaier\\nAdele Jackson\\nAditya Menon\\nAlasdair Tran\\nAleksandar Krnjaic\\nAlexander Makrigiorgos\\nAlfredo Canziani\\nAli Shafti\\nAmr Khalifa\\nAndrew Tanggara\\nAngus Gruen\\nAntal A. Buss\\nAntoine Toisoul Le Cann\\nAreg Sarvazyan\\nArtem Artemev\\nArtyom Stepanov\\nBill Kromydas\\nBob Williamson\\nBoon Ping Lim\\nChao Qu\\nCheng Li\\nChris Sherlock\\nChristopher Gray\\nDaniel McNamara\\nDaniel Wood\\nDarren Siegel\\nDavid Johnston\\nDawei ChenEllen Broad\\nFengkuangtian Zhu\\nFiona Condon\\nGeorgios Theodorou\\nHe Xin\\nIrene Raissa Kameni\\nJakub Nabaglo\\nJames Hensman\\nJamie Liu\\nJean Kaddour\\nJean-Paul Ebejer\\nJerry Qiang\\nJitesh Sindhare\\nJohn Lloyd\\nJonas Ngnawe\\nJon Martin\\nJustin Hsi\\nKai Arulkumaran\\nKamil Dreczkowski\\nLily Wang\\nLionel Tondji Ngoupeyou\\nLydia Kn ¨uﬁng\\nMahmoud Aslan\\nMark Hartenstein\\nMark van der Wilk\\nMarkus Hegland\\nMartin Hewing\\nMatthew Alger\\nMatthew Lee\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nForeword 5\\nMaximus McCann\\nMengyan Zhang\\nMichael Bennett\\nMichael Pedersen\\nMinjeong Shin\\nMohammad Malekzadeh\\nNaveen Kumar\\nNico Montali\\nOscar Armas\\nPatrick Henriksen\\nPatrick Wieschollek\\nPattarawat Chormai\\nPaul Kelly\\nPetros Christodoulou\\nPiotr Januszewski\\nPranav Subramani\\nQuyu Kong\\nRagib Zaman\\nRui Zhang\\nRyan-Rhys Grifﬁths\\nSalomon Kabongo\\nSamuel Ogunmola\\nSandeep Mavadia\\nSarvesh Nikumbh\\nSebastian Raschka\\nSenanayak Sesh Kumar Karri\\nSeung-Heon Baek\\nShahbaz ChaudharyShakir Mohamed\\nShawn Berry\\nSheikh Abdul Raheem Ali\\nSheng Xue\\nSridhar Thiagarajan\\nSyed Nouman Hasany\\nSzymon Brych\\nThomas B ¨uhler\\nTimur Sharapov\\nTom Melamed\\nVincent Adam\\nVincent Dutordoir\\nVu Minh\\nWasim Aftab\\nWen Zhi\\nWojciech Stokowiec\\nXiaonan Chong\\nXiaowei Zhang\\nYazhou Hao\\nYicheng Luo\\nYoung Lee\\nYu Lu\\nYun Cheng\\nYuxiao Huang\\nZac Cranko\\nZijian Cao\\nZoe Nolan\\nContributors through GitHub, whose real names were not listed on their\\nGitHub proﬁle, are:\\nSamDataMad\\nbumptiousmonkey\\nidoamihai\\ndeepakiiminsad\\nHorizonP\\ncs-maillist\\nkudo23empet\\nvictorBigand\\n17SKYE\\njessjing1995\\nWe are also very grateful to Parameswaran Raman and the many anony-\\nmous reviewers, organized by Cambridge University Press, who read one\\nor more chapters of earlier versions of the manuscript, and provided con-\\nstructive criticism that led to considerable improvements. A special men-\\ntion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt\\nadvice about LATEX-related issues. Last but not least, we are very grateful\\nto our editor Lauren Cowles, who has been patiently guiding us through\\nthe gestation process of this book.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n6 Foreword\\nTable of Symbols\\nSymbol Typical meaning\\na;b;c;\\x0b;\\x0c;\\r Scalars are lowercase\\nx;y;z Vectors are bold lowercase\\nA;B;C Matrices are bold uppercase\\nx>;A>Transpose of a vector or matrix\\nA\\x001Inverse of a matrix\\nhx;yi Inner product of xandy\\nx>y Dot product of xandy\\nB= (b1;b2;b3)(Ordered) tuple\\nB= [b1;b2;b3]Matrix of column vectors stacked horizontally\\nB=fb1;b2;b3gSet of vectors (unordered)\\nZ;N Integers and natural numbers, respectively\\nR;C Real and complex numbers, respectively\\nRnn-dimensional vector space of real numbers\\n8x Universal quantiﬁer: for all x\\n9x Existential quantiﬁer: there exists x\\na:=b a is deﬁned as b\\na=:b b is deﬁned as a\\na/b a is proportional to b, i.e.,a=constant\\x01b\\ng\\x0ef Function composition: “ gafterf”\\n() If and only if\\n=) Implies\\nA;C Sets\\na2A ais an element of set A\\n; Empty set\\nAnB A withoutB: the set of elements in Abut not inB\\nD Number of dimensions; indexed by d= 1;:::;D\\nN Number of data points; indexed by n= 1;:::;N\\nIm Identity matrix of size m\\x02m\\n0m;n Matrix of zeros of size m\\x02n\\n1m;n Matrix of ones of size m\\x02n\\nei Standard/canonical vector (where iis the component that is 1)\\ndim Dimensionality of vector space\\nrk(A) Rank of matrix A\\nIm(\\x08) Image of linear mapping \\x08\\nker(\\x08) Kernel (null space) of a linear mapping \\x08\\nspan[b1] Span (generating set) of b1\\ntr(A) Trace ofA\\ndet(A) Determinant of A\\nj\\x01j Absolute value or determinant (depending on context)\\nk\\x01k Norm; Euclidean, unless speciﬁed\\n\\x15 Eigenvalue or Lagrange multiplier\\nE\\x15 Eigenspace corresponding to eigenvalue \\x15\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nForeword 7\\nSymbol Typical meaning\\nx?y Vectorsxandyare orthogonal\\nV Vector space\\nV?Orthogonal complement of vector space VPN\\nn=1xn Sum of thexn:x1+:::+xNQN\\nn=1xn Product of the xn:x1\\x01:::\\x01xN\\n\\x12 Parameter vector\\n@f\\n@xPartial derivative of fwith respect to x\\ndf\\ndxTotal derivative of fwith respect to x\\nr Gradient\\nf\\x03= minxf(x) The smallest function value of f\\nx\\x032arg minxf(x)The valuex\\x03that minimizes f(note: arg min returns a set of values)\\nL Lagrangian\\nL Negative log-likelihood\\x00n\\nk\\x01\\nBinomial coefﬁcient, nchoosek\\nVX[x] Variance ofxwith respect to the random variable X\\nEX[x] Expectation of xwith respect to the random variable X\\nCovX;Y[x;y] Covariance between xandy.\\nX? ?YjZ X is conditionally independent of YgivenZ\\nX\\x18p Random variable Xis distributed according to p\\nN\\x00\\x16;\\x06\\x01\\nGaussian distribution with mean \\x16and covariance \\x06\\nBer(\\x16) Bernoulli distribution with parameter \\x16\\nBin(N;\\x16) Binomial distribution with parameters N;\\x16\\nBeta(\\x0b;\\x0c) Beta distribution with parameters \\x0b;\\x0c\\nTable of Abbreviations and Acronyms\\nAcronym Meaning\\ne.g. Exempli gratia (Latin: for example)\\nGMM Gaussian mixture model\\ni.e. Id est (Latin: this means)\\ni.i.d. Independent, identically distributed\\nMAP Maximum a posteriori\\nMLE Maximum likelihood estimation/estimator\\nONB Orthonormal basis\\nPCA Principal component analysis\\nPPCA Probabilistic principal component analysis\\nREF Row-echelon form\\nSPD Symmetric, positive deﬁnite\\nSVM Support vector machine\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n\\nPart I\\nMathematical Foundations\\n9\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n\\n1\\nIntroduction and Motivation\\nMachine learning is about designing algorithms that automatically extract\\nvaluable information from data. The emphasis here is on “automatic”, i.e.,\\nmachine learning is concerned about general-purpose methodologies that\\ncan be applied to many datasets, while producing something that is mean-\\ningful. There are three concepts that are at the core of machine learning:\\ndata, a model, and learning.\\nSince machine learning is inherently data driven, data is at the core data\\nof machine learning. The goal of machine learning is to design general-\\npurpose methodologies to extract valuable patterns from data, ideally\\nwithout much domain-speciﬁc expertise. For example, given a large corpus\\nof documents (e.g., books in many libraries), machine learning methods\\ncan be used to automatically ﬁnd relevant topics that are shared across\\ndocuments (Hoffman et al., 2010). To achieve this goal, we design mod-\\nelsthat are typically related to the process that generates data, similar to model\\nthe dataset we are given. For example, in a regression setting, the model\\nwould describe a function that maps inputs to real-valued outputs. To\\nparaphrase Mitchell (1997): A model is said to learn from data if its per-\\nformance on a given task improves after the data is taken into account.\\nThe goal is to ﬁnd good models that generalize well to yet unseen data,\\nwhich we may care about in the future. Learning can be understood as a learning\\nway to automatically ﬁnd patterns and structure in data by optimizing the\\nparameters of the model.\\nWhile machine learning has seen many success stories, and software is\\nreadily available to design and train rich and ﬂexible machine learning\\nsystems, we believe that the mathematical foundations of machine learn-\\ning are important in order to understand fundamental principles upon\\nwhich more complicated machine learning systems are built. Understand-\\ning these principles can facilitate creating new machine learning solutions,\\nunderstanding and debugging existing approaches, and learning about the\\ninherent assumptions and limitations of the methodologies we are work-\\ning with.\\n11\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n12 Introduction and Motivation\\n1.1 Finding Words for Intuitions\\nA challenge we face regularly in machine learning is that concepts and\\nwords are slippery, and a particular component of the machine learning\\nsystem can be abstracted to different mathematical concepts. For example,\\nthe word “algorithm” is used in at least two different senses in the con-\\ntext of machine learning. In the ﬁrst sense, we use the phrase “machine\\nlearning algorithm” to mean a system that makes predictions based on in-\\nput data. We refer to these algorithms as predictors . In the second sense, predictor\\nwe use the exact same phrase “machine learning algorithm” to mean a\\nsystem that adapts some internal parameters of the predictor so that it\\nperforms well on future unseen input data. Here we refer to this adapta-\\ntion as training a system. training\\nThis book will not resolve the issue of ambiguity, but we want to high-\\nlight upfront that, depending on the context, the same expressions can\\nmean different things. However, we attempt to make the context sufﬁ-\\nciently clear to reduce the level of ambiguity.\\nThe ﬁrst part of this book introduces the mathematical concepts and\\nfoundations needed to talk about the three main components of a machine\\nlearning system: data, models, and learning. We will brieﬂy outline these\\ncomponents here, and we will revisit them again in Chapter 8 once we\\nhave discussed the necessary mathematical concepts.\\nWhile not all data is numerical, it is often useful to consider data in\\na number format. In this book, we assume that data has already been\\nappropriately converted into a numerical representation suitable for read-\\ning into a computer program. Therefore, we think of data as vectors. As data as vectors\\nanother illustration of how subtle words are, there are (at least) three\\ndifferent ways to think about vectors: a vector as an array of numbers (a\\ncomputer science view), a vector as an arrow with a direction and magni-\\ntude (a physics view), and a vector as an object that obeys addition and\\nscaling (a mathematical view).\\nAmodel is typically used to describe a process for generating data, sim- model\\nilar to the dataset at hand. Therefore, good models can also be thought\\nof as simpliﬁed versions of the real (unknown) data-generating process,\\ncapturing aspects that are relevant for modeling the data and extracting\\nhidden patterns from it. A good model can then be used to predict what\\nwould happen in the real world without performing real-world experi-\\nments.\\nWe now come to the crux of the matter, the learning component of learning\\nmachine learning. Assume we are given a dataset and a suitable model.\\nTraining the model means to use the data available to optimize some pa-\\nrameters of the model with respect to a utility function that evaluates how\\nwell the model predicts the training data. Most training methods can be\\nthought of as an approach analogous to climbing a hill to reach its peak.\\nIn this analogy, the peak of the hill corresponds to a maximum of some\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n1.2 Two Ways to Read This Book 13\\ndesired performance measure. However, in practice, we are interested in\\nthe model to perform well on unseen data. Performing well on data that\\nwe have already seen (training data) may only mean that we found a\\ngood way to memorize the data. However, this may not generalize well to\\nunseen data, and, in practical applications, we often need to expose our\\nmachine learning system to situations that it has not encountered before.\\nLet us summarize the main concepts of machine learning that we cover\\nin this book:\\nWe represent data as vectors.\\nWe choose an appropriate model, either using the probabilistic or opti-\\nmization view.\\nWe learn from available data by using numerical optimization methods\\nwith the aim that the model performs well on data not used for training.\\n1.2 Two Ways to Read This Book\\nWe can consider two strategies for understanding the mathematics for\\nmachine learning:\\nBottom-up: Building up the concepts from foundational to more ad-\\nvanced. This is often the preferred approach in more technical ﬁelds,\\nsuch as mathematics. This strategy has the advantage that the reader\\nat all times is able to rely on their previously learned concepts. Unfor-\\ntunately, for a practitioner many of the foundational concepts are not\\nparticularly interesting by themselves, and the lack of motivation means\\nthat most foundational deﬁnitions are quickly forgotten.\\nTop-down: Drilling down from practical needs to more basic require-\\nments. This goal-driven approach has the advantage that the readers\\nknow at all times why they need to work on a particular concept, and\\nthere is a clear path of required knowledge. The downside of this strat-\\negy is that the knowledge is built on potentially shaky foundations, and\\nthe readers have to remember a set of words that they do not have any\\nway of understanding.\\nWe decided to write this book in a modular way to separate foundational\\n(mathematical) concepts from applications so that this book can be read\\nin both ways. The book is split into two parts, where Part I lays the math-\\nematical foundations and Part II applies the concepts from Part I to a set\\nof fundamental machine learning problems, which form four pillars of\\nmachine learning as illustrated in Figure 1.1: regression, dimensionality\\nreduction, density estimation, and classiﬁcation. Chapters in Part I mostly\\nbuild upon the previous ones, but it is possible to skip a chapter and work\\nbackward if necessary. Chapters in Part II are only loosely coupled and\\ncan be read in any order. There are many pointers forward and backward\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n14 Introduction and Motivation\\nFigure 1.1 The\\nfoundations and\\nfour pillars of\\nmachine learning.\\nClassiﬁcation  \\nDensity  \\nEstimation  \\nRegression  \\nDimensionality  \\nReduction  \\nMachine Learning\\nVector Calculus Probability & Distributions Optimization\\nAnalytic Geometry Matrix Decomposition Linear Algebra\\nbetween the two parts of the book to link mathematical concepts with\\nmachine learning algorithms.\\nOf course there are more than two ways to read this book. Most readers\\nlearn using a combination of top-down and bottom-up approaches, some-\\ntimes building up basic mathematical skills before attempting more com-\\nplex concepts, but also choosing topics based on applications of machine\\nlearning.\\nPart I Is about Mathematics\\nThe four pillars of machine learning we cover in this book (see Figure 1.1)\\nrequire a solid mathematical foundation, which is laid out in Part I.\\nWe represent numerical data as vectors and represent a table of such\\ndata as a matrix. The study of vectors and matrices is called linear algebra ,\\nwhich we introduce in Chapter 2. The collection of vectors as a matrix is linear algebra\\nalso described there.\\nGiven two vectors representing two objects in the real world, we want\\nto make statements about their similarity. The idea is that vectors that\\nare similar should be predicted to have similar outputs by our machine\\nlearning algorithm (our predictor). To formalize the idea of similarity be-\\ntween vectors, we need to introduce operations that take two vectors as\\ninput and return a numerical value representing their similarity. The con-\\nstruction of similarity and distances is central to analytic geometry and is analytic geometry\\ndiscussed in Chapter 3.\\nIn Chapter 4, we introduce some fundamental concepts about matri-\\nces and matrix decomposition . Some operations on matrices are extremely matrix\\ndecomposition useful in machine learning, and they allow for an intuitive interpretation\\nof the data and more efﬁcient learning.\\nWe often consider data to be noisy observations of some true underly-\\ning signal. We hope that by applying machine learning we can identify the\\nsignal from the noise. This requires us to have a language for quantify-\\ning what “noise” means. We often would also like to have predictors that\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n1.2 Two Ways to Read This Book 15\\nallow us to express some sort of uncertainty, e.g., to quantify the conﬁ-\\ndence we have about the value of the prediction at a particular test data\\npoint. Quantiﬁcation of uncertainty is the realm of probability theory and probability theory\\nis covered in Chapter 6.\\nTo train machine learning models, we typically ﬁnd parameters that\\nmaximize some performance measure. Many optimization techniques re-\\nquire the concept of a gradient, which tells us the direction in which to\\nsearch for a solution. Chapter 5 is about vector calculus and details the vector calculus\\nconcept of gradients, which we subsequently use in Chapter 7, where we\\ntalk about optimization to ﬁnd maxima/minima of functions. optimization\\nPart II Is about Machine Learning\\nThe second part of the book introduces four pillars of machine learning\\nas shown in Figure 1.1. We illustrate how the mathematical concepts in-\\ntroduced in the ﬁrst part of the book are the foundation for each pillar.\\nBroadly speaking, chapters are ordered by difﬁculty (in ascending order).\\nIn Chapter 8, we restate the three components of machine learning\\n(data, models, and parameter estimation) in a mathematical fashion. In\\naddition, we provide some guidelines for building experimental set-ups\\nthat guard against overly optimistic evaluations of machine learning sys-\\ntems. Recall that the goal is to build a predictor that performs well on\\nunseen data.\\nIn Chapter 9, we will have a close look at linear regression , where our linear regression\\nobjective is to ﬁnd functions that map inputs x2RDto corresponding ob-\\nserved function values y2R, which we can interpret as the labels of their\\nrespective inputs. We will discuss classical model ﬁtting (parameter esti-\\nmation) via maximum likelihood and maximum a posteriori estimation,\\nas well as Bayesian linear regression, where we integrate the parameters\\nout instead of optimizing them.\\nChapter 10 focuses on dimensionality reduction , the second pillar in Fig- dimensionality\\nreduction ure 1.1, using principal component analysis. The key objective of dimen-\\nsionality reduction is to ﬁnd a compact, lower-dimensional representation\\nof high-dimensional data x2RD, which is often easier to analyze than\\nthe original data. Unlike regression, dimensionality reduction is only con-\\ncerned about modeling the data – there are no labels associated with a\\ndata pointx.\\nIn Chapter 11, we will move to our third pillar: density estimation . The density estimation\\nobjective of density estimation is to ﬁnd a probability distribution that de-\\nscribes a given dataset. We will focus on Gaussian mixture models for this\\npurpose, and we will discuss an iterative scheme to ﬁnd the parameters of\\nthis model. As in dimensionality reduction, there are no labels associated\\nwith the data points x2RD. However, we do not seek a low-dimensional\\nrepresentation of the data. Instead, we are interested in a density model\\nthat describes the data.\\nChapter 12 concludes the book with an in-depth discussion of the fourth\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n16 Introduction and Motivation\\npillar: classiﬁcation . We will discuss classiﬁcation in the context of support classiﬁcation\\nvector machines. Similar to regression (Chapter 9), we have inputs xand\\ncorresponding labels y. However, unlike regression, where the labels were\\nreal-valued, the labels in classiﬁcation are integers, which requires special\\ncare.\\n1.3 Exercises and Feedback\\nWe provide some exercises in Part I, which can be done mostly by pen and\\npaper. For Part II, we provide programming tutorials (jupyter notebooks)\\nto explore some properties of the machine learning algorithms we discuss\\nin this book.\\nWe appreciate that Cambridge University Press strongly supports our\\naim to democratize education and learning by making this book freely\\navailable for download at\\nhttps://mml-book.com\\nwhere tutorials, errata, and additional materials can be found. Mistakes\\ncan be reported and feedback provided using the preceding URL.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2\\nLinear Algebra\\nWhen formalizing intuitive concepts, a common approach is to construct a\\nset of objects (symbols) and a set of rules to manipulate these objects. This\\nis known as an algebra . Linear algebra is the study of vectors and certain algebra\\nrules to manipulate vectors. The vectors many of us know from school are\\ncalled “geometric vectors”, which are usually denoted by a small arrow\\nabove the letter, e.g.,\\x00 !xand\\x00 !y. In this book, we discuss more general\\nconcepts of vectors and use a bold letter to represent them, e.g., xandy.\\nIn general, vectors are special objects that can be added together and\\nmultiplied by scalars to produce another object of the same kind. From\\nan abstract mathematical viewpoint, any object that satisﬁes these two\\nproperties can be considered a vector. Here are some examples of such\\nvector objects:\\n1. Geometric vectors. This example of a vector may be familiar from high\\nschool mathematics and physics. Geometric vectors – see Figure 2.1(a)\\n– are directed segments, which can be drawn (at least in two dimen-\\nsions). Two geometric vectors!x;!ycan be added, such that!x+!y=!z\\nis another geometric vector. Furthermore, multiplication by a scalar\\n\\x15!x,\\x152R, is also a geometric vector. In fact, it is the original vector\\nscaled by\\x15. Therefore, geometric vectors are instances of the vector\\nconcepts introduced previously. Interpreting vectors as geometric vec-\\ntors enables us to use our intuitions about direction and magnitude to\\nreason about mathematical operations.\\n2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can\\nFigure 2.1\\nDifferent types of\\nvectors. Vectors can\\nbe surprising\\nobjects, including\\n(a) geometric\\nvectors\\nand (b) polynomials.!x!y!x+!y\\n(a) Geometric vectors.\\n−2 0 2\\nx−6−4−2024y (b) Polynomials.\\n17\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n18 Linear Algebra\\nbe added together, which results in another polynomial; and they can\\nbe multiplied by a scalar \\x152R, and the result is a polynomial as\\nwell. Therefore, polynomials are (rather unusual) instances of vectors.\\nNote that polynomials are very different from geometric vectors. While\\ngeometric vectors are concrete “drawings”, polynomials are abstract\\nconcepts. However, they are both vectors in the sense previously de-\\nscribed.\\n3. Audio signals are vectors. Audio signals are represented as a series of\\nnumbers. We can add audio signals together, and their sum is a new\\naudio signal. If we scale an audio signal, we also obtain an audio signal.\\nTherefore, audio signals are a type of vector, too.\\n4. Elements of Rn(tuples ofnreal numbers) are vectors. Rnis more\\nabstract than polynomials, and it is the concept we focus on in this\\nbook. For instance,\\na=2\\n41\\n2\\n33\\n52R3(2.1)\\nis an example of a triplet of numbers. Adding two vectors a;b2Rn\\ncomponent-wise results in another vector: a+b=c2Rn. Moreover,\\nmultiplyinga2Rnby\\x152Rresults in a scaled vector \\x15a2Rn.\\nConsidering vectors as elements of Rnhas an additional beneﬁt that Be careful to check\\nwhether array\\noperations actually\\nperform vector\\noperations when\\nimplementing on a\\ncomputer.it loosely corresponds to arrays of real numbers on a computer. Many\\nprogramming languages support array operations, which allow for con-\\nvenient implementation of algorithms that involve vector operations.\\nLinear algebra focuses on the similarities between these vector concepts.\\nWe can add them together and multiply them by scalars. We will largelyPavel Grinfeld’s\\nseries on linear\\nalgebra:\\nhttp://tinyurl.\\ncom/nahclwm\\nGilbert Strang’s\\ncourse on linear\\nalgebra:\\nhttp://tinyurl.\\ncom/29p5q8j\\n3Blue1Brown series\\non linear algebra:\\nhttps://tinyurl.\\ncom/h5g4kpsfocus on vectors in Rnsince most algorithms in linear algebra are for-\\nmulated in Rn. We will see in Chapter 8 that we often consider data to\\nbe represented as vectors in Rn. In this book, we will focus on ﬁnite-\\ndimensional vector spaces, in which case there is a 1:1correspondence\\nbetween any kind of vector and Rn. When it is convenient, we will use\\nintuitions about geometric vectors and consider array-based algorithms.\\nOne major idea in mathematics is the idea of “closure”. This is the ques-\\ntion: What is the set of all things that can result from my proposed oper-\\nations? In the case of vectors: What is the set of vectors that can result by\\nstarting with a small set of vectors, and adding them to each other and\\nscaling them? This results in a vector space (Section 2.4). The concept of\\na vector space and its properties underlie much of machine learning. The\\nconcepts introduced in this chapter are summarized in Figure 2.2.\\nThis chapter is mostly based on the lecture notes and books by Drumm\\nand Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann\\n(2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.1 Systems of Linear Equations 19\\nFigure 2.2 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhere they are used\\nin other parts of the\\nbook.Vector\\nVector spaceMatrixChapter 5\\nVector calculus\\nGroup\\nSystem of\\nlinear equations\\nMatrix\\ninverse\\nGaussian\\neliminationLinear/afﬁne\\nmappingLinear\\nindependence\\nBasis\\nChapter 10\\nDimensionality\\nreductionChapter 12\\nClassiﬁcationChapter 3\\nAnalytic geometrycomposesclosure\\nAbelian\\nwith +represents\\nrepresents\\nsolved by solvesproperty ofmaximal set\\nresources are Gilbert Strang’s Linear Algebra course at MIT and the Linear\\nAlgebra Series by 3Blue1Brown.\\nLinear algebra plays an important role in machine learning and gen-\\neral mathematics. The concepts introduced in this chapter are further ex-\\npanded to include the idea of geometry in Chapter 3. In Chapter 5, we\\nwill discuss vector calculus, where a principled knowledge of matrix op-\\nerations is essential. In Chapter 10, we will use projections (to be intro-\\nduced in Section 3.8) for dimensionality reduction with principal compo-\\nnent analysis (PCA). In Chapter 9, we will discuss linear regression, where\\nlinear algebra plays a central role for solving least-squares problems.\\n2.1 Systems of Linear Equations\\nSystems of linear equations play a central part of linear algebra. Many\\nproblems can be formulated as systems of linear equations, and linear\\nalgebra gives us the tools for solving them.\\nExample 2.1\\nA company produces products N1;:::;Nnfor which resources\\nR1;:::;Rmare required. To produce a unit of product Nj,aijunits of\\nresourceRiare needed, where i= 1;:::;m andj= 1;:::;n .\\nThe objective is to ﬁnd an optimal production plan, i.e., a plan of how\\nmany units xjof productNjshould be produced if a total of biunits of\\nresourceRiare available and (ideally) no resources are left over.\\nIf we produce x1;:::;xnunits of the corresponding products, we need\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n20 Linear Algebra\\na total of\\nai1x1+\\x01\\x01\\x01+ainxn (2.2)\\nmany units of resource Ri. An optimal production plan (x1;:::;xn)2Rn,\\ntherefore, has to satisfy the following system of equations:\\na11x1+\\x01\\x01\\x01+a1nxn=b1\\n...\\nam1x1+\\x01\\x01\\x01+amnxn=bm; (2.3)\\nwhereaij2Randbi2R.\\nEquation (2.3) is the general form of a system of linear equations , and system of linear\\nequations x1;:::;xnare the unknowns of this system. Every n-tuple (x1;:::;xn)2\\nRnthat satisﬁes (2.3) is a solution of the linear equation system. solution\\nExample 2.2\\nThe system of linear equations\\nx1+x2+x3= 3 (1)\\nx1\\x00x2+ 2x3= 2 (2)\\n2x1 + 3x3= 1 (3)(2.4)\\nhasno solution: Adding the ﬁrst two equations yields 2x1+3x3= 5, which\\ncontradicts the third equation (3).\\nLet us have a look at the system of linear equations\\nx1+x2+x3= 3 (1)\\nx1\\x00x2+ 2x3= 2 (2)\\nx2+x3= 2 (3): (2.5)\\nFrom the ﬁrst and third equation, it follows that x1= 1. From (1) +(2),\\nwe get 2x1+ 3x3= 5, i.e.,x3= 1. From (3), we then get that x2= 1.\\nTherefore, (1;1;1)is the only possible and unique solution (verify that\\n(1;1;1)is a solution by plugging in).\\nAs a third example, we consider\\nx1+x2+x3= 3 (1)\\nx1\\x00x2+ 2x3= 2 (2)\\n2x1 + 3x3= 5 (3): (2.6)\\nSince (1) +(2)=(3), we can omit the third equation (redundancy). From\\n(1) and (2), we get 2x1= 5\\x003x3and2x2= 1+x3. We deﬁnex3=a2R\\nas a free variable, such that any triplet\\n\\x125\\n2\\x003\\n2a;1\\n2+1\\n2a;a\\x13\\n; a2R (2.7)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.1 Systems of Linear Equations 21\\nFigure 2.3 The\\nsolution space of a\\nsystem of two linear\\nequations with two\\nvariables can be\\ngeometrically\\ninterpreted as the\\nintersection of two\\nlines. Every linear\\nequation represents\\na line.\\n2x1\\x004x2= 1\\n4x1+ 4x2= 5\\nx1\\nx2\\nis a solution of the system of linear equations, i.e., we obtain a solution\\nset that contains inﬁnitely many solutions.\\nIn general, for a real-valued system of linear equations we obtain either\\nno, exactly one, or inﬁnitely many solutions. Linear regression (Chapter 9)\\nsolves a version of Example 2.1 when we cannot solve the system of linear\\nequations.\\nRemark (Geometric Interpretation of Systems of Linear Equations) .In a\\nsystem of linear equations with two variables x1;x2, each linear equation\\ndeﬁnes a line on the x1x2-plane. Since a solution to a system of linear\\nequations must satisfy all equations simultaneously, the solution set is the\\nintersection of these lines. This intersection set can be a line (if the linear\\nequations describe the same line), a point, or empty (when the lines are\\nparallel). An illustration is given in Figure 2.3 for the system\\n4x1+ 4x2= 5\\n2x1\\x004x2= 1(2.8)\\nwhere the solution space is the point (x1;x2) = (1;1\\n4). Similarly, for three\\nvariables, each linear equation determines a plane in three-dimensional\\nspace. When we intersect these planes, i.e., satisfy all linear equations at\\nthe same time, we can obtain a solution set that is a plane, a line, a point\\nor empty (when the planes have no common intersection). }\\nFor a systematic approach to solving systems of linear equations, we\\nwill introduce a useful compact notation. We collect the coefﬁcients aij\\ninto vectors and collect the vectors into matrices. In other words, we write\\nthe system from (2.3) in the following form:\\n2\\n64a11\\n...\\nam13\\n75x1+2\\n64a12\\n...\\nam23\\n75x2+\\x01\\x01\\x01+2\\n64a1n\\n...\\namn3\\n75xn=2\\n64b1\\n...\\nbm3\\n75 (2.9)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n22 Linear Algebra\\n()2\\n64a11\\x01\\x01\\x01a1n\\n......\\nam1\\x01\\x01\\x01amn3\\n752\\n64x1\\n...\\nxn3\\n75=2\\n64b1\\n...\\nbm3\\n75: (2.10)\\nIn the following, we will have a close look at these matrices and de-\\nﬁne computation rules. We will return to solving linear equations in Sec-\\ntion 2.3.\\n2.2 Matrices\\nMatrices play a central role in linear algebra. They can be used to com-\\npactly represent systems of linear equations, but they also represent linear\\nfunctions (linear mappings) as we will see later in Section 2.7. Before we\\ndiscuss some of these interesting topics, let us ﬁrst deﬁne what a matrix\\nis and what kind of operations we can do with matrices. We will see more\\nproperties of matrices in Chapter 4.\\nDeﬁnition 2.1 (Matrix) .Withm;n2Na real-valued (m;n)matrixAis matrix\\nanm\\x01n-tuple of elements aij,i= 1;:::;m ,j= 1;:::;n , which is ordered\\naccording to a rectangular scheme consisting of mrows andncolumns:\\nA=2\\n6664a11a12\\x01\\x01\\x01a1n\\na21a22\\x01\\x01\\x01a2n\\n.........\\nam1am2\\x01\\x01\\x01amn3\\n7775; aij2R: (2.11)\\nBy convention (1;n)-matrices are called rows and(m;1)-matrices are called row\\ncolumns . These special matrices are also called row/column vectors . column\\nrow vector\\ncolumn vector\\nFigure 2.4 By\\nstacking its\\ncolumns, a matrix A\\ncan be represented\\nas a long vector a.\\nre-shapeA2R4\\x022a2R8Rm\\x02nis the set of all real-valued (m;n)-matrices.A2Rm\\x02ncan be\\nequivalently represented as a2Rmnby stacking all ncolumns of the\\nmatrix into a long vector; see Figure 2.4.\\n2.2.1 Matrix Addition and Multiplication\\nThe sum of two matrices A2Rm\\x02n,B2Rm\\x02nis deﬁned as the element-\\nwise sum, i.e.,\\nA+B:=2\\n64a11+b11\\x01\\x01\\x01a1n+b1n\\n......\\nam1+bm1\\x01\\x01\\x01amn+bmn3\\n752Rm\\x02n: (2.12)\\nFor matricesA2Rm\\x02n,B2Rn\\x02k, the elements cijof the product Note the size of the\\nmatrices. C=AB2Rm\\x02kare computed as\\nC =\\nnp.einsum(\\'il,\\nlj\\', A, B) cij=nX\\nl=1ailblj; i = 1;:::;m; j = 1;:::;k: (2.13)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.2 Matrices 23\\nThis means, to compute element cijwe multiply the elements of the ith There arencolumns\\ninAandnrows in\\nBso that we can\\ncomputeailbljfor\\nl= 1;:::;n .\\nCommonly, the dot\\nproduct between\\ntwo vectorsa;bis\\ndenoted bya>bor\\nha;bi.row ofAwith thejth column ofBand sum them up. Later in Section 3.2,\\nwe will call this the dot product of the corresponding row and column. In\\ncases, where we need to be explicit that we are performing multiplication,\\nwe use the notation A\\x01Bto denote multiplication (explicitly showing\\n“\\x01”).\\nRemark. Matrices can only be multiplied if their “neighboring” dimensions\\nmatch. For instance, an n\\x02k-matrixAcan be multiplied with a k\\x02m-\\nmatrixB, but only from the left side:\\nA|{z}\\nn\\x02kB|{z}\\nk\\x02m=C|{z}\\nn\\x02m(2.14)\\nThe productBAis not deﬁned if m6=nsince the neighboring dimensions\\ndo not match. }\\nRemark. Matrix multiplication is notdeﬁned as an element-wise operation\\non matrix elements, i.e., cij6=aijbij(even if the size of A;Bwas cho-\\nsen appropriately). This kind of element-wise multiplication often appears\\nin programming languages when we multiply (multi-dimensional) arrays\\nwith each other, and is called a Hadamard product . } Hadamard product\\nExample 2.3\\nForA=\\x141 2 3\\n3 2 1\\x15\\n2R2\\x023,B=2\\n40 2\\n1\\x001\\n0 13\\n52R3\\x022, we obtain\\nAB=\\x141 2 3\\n3 2 1\\x152\\n40 2\\n1\\x001\\n0 13\\n5=\\x142 3\\n2 5\\x15\\n2R2\\x022; (2.15)\\nBA=2\\n40 2\\n1\\x001\\n0 13\\n5\\x141 2 3\\n3 2 1\\x15\\n=2\\n46 4 2\\n\\x002 0 2\\n3 2 13\\n52R3\\x023: (2.16)\\nFigure 2.5 Even if\\nboth matrix\\nmultiplications AB\\nandBA are\\ndeﬁned, the\\ndimensions of the\\nresults can be\\ndifferent.\\nFrom this example, we can already see that matrix multiplication is not\\ncommutative, i.e., AB6=BA; see also Figure 2.5 for an illustration.\\nDeﬁnition 2.2 (Identity Matrix) .InRn\\x02n, we deﬁne the identity matrix\\nidentity matrixIn:=2\\n6666666641 0\\x01\\x01\\x010\\x01\\x01\\x010\\n0 1\\x01\\x01\\x010\\x01\\x01\\x010\\n..................\\n0 0\\x01\\x01\\x011\\x01\\x01\\x010\\n..................\\n0 0\\x01\\x01\\x010\\x01\\x01\\x0113\\n7777777752Rn\\x02n(2.17)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n24 Linear Algebra\\nas then\\x02n-matrix containing 1on the diagonal and 0everywhere else.\\nNow that we deﬁned matrix multiplication, matrix addition and the\\nidentity matrix, let us have a look at some properties of matrices:\\nassociativity\\nAssociativity:\\n8A2Rm\\x02n;B2Rn\\x02p;C2Rp\\x02q: (AB)C=A(BC) (2.18)\\ndistributivity\\nDistributivity:\\n8A;B2Rm\\x02n;C;D2Rn\\x02p: (A+B)C=AC+BC (2.19a)\\nA(C+D) =AC+AD (2.19b)\\nMultiplication with the identity matrix:\\n8A2Rm\\x02n:ImA=AIn=A (2.20)\\nNote thatIm6=Inform6=n.\\n2.2.2 Inverse and Transpose\\nDeﬁnition 2.3 (Inverse) .Consider a square matrix A2Rn\\x02n. Let matrix A square matrix\\npossesses the same\\nnumber of columns\\nand rows.B2Rn\\x02nhave the property that AB =In=BA.Bis called the\\ninverse ofAand denoted by A\\x001.\\ninverseUnfortunately, not every matrix Apossesses an inverse A\\x001. If this\\ninverse does exist, Ais called regular /invertible /nonsingular , otherwise regular\\ninvertible\\nnonsingularsingular /noninvertible . When the matrix inverse exists, it is unique. In Sec-\\nsingular\\nnoninvertibletion 2.3, we will discuss a general way to compute the inverse of a matrix\\nby solving a system of linear equations.\\nRemark (Existence of the Inverse of a 2\\x022-matrix) .Consider a matrix\\nA:=\\x14a11a12\\na21a22\\x15\\n2R2\\x022: (2.21)\\nIf we multiply Awith\\nA0:=\\x14a22\\x00a12\\n\\x00a21a11\\x15\\n(2.22)\\nwe obtain\\nAA0=\\x14a11a22\\x00a12a21 0\\n0a11a22\\x00a12a21\\x15\\n= (a11a22\\x00a12a21)I:\\n(2.23)\\nTherefore,\\nA\\x001=1\\na11a22\\x00a12a21\\x14a22\\x00a12\\n\\x00a21a11\\x15\\n(2.24)\\nif and only if a11a22\\x00a12a216= 0. In Section 4.1, we will see that a11a22\\x00\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.2 Matrices 25\\na12a21is the determinant of a 2\\x022-matrix. Furthermore, we can generally\\nuse the determinant to check whether a matrix is invertible. }\\nExample 2.4 (Inverse Matrix)\\nThe matrices\\nA=2\\n41 2 1\\n4 4 5\\n6 7 73\\n5;B=2\\n4\\x007\\x007 6\\n2 1\\x001\\n4 5\\x0043\\n5 (2.25)\\nare inverse to each other since AB=I=BA.\\nDeﬁnition 2.4 (Transpose) .ForA2Rm\\x02nthe matrixB2Rn\\x02mwith\\nbij=ajiis called the transpose ofA. We writeB=A>. transpose\\nThe main diagonal\\n(sometimes called\\n“principal diagonal”,\\n“primary diagonal”,\\n“leading diagonal”,\\nor “major diagonal”)\\nof a matrixAis the\\ncollection of entries\\nAijwherei=j.In general,A>can be obtained by writing the columns of Aas the rows\\nofA>. The following are important properties of inverses and transposes:\\nThe scalar case of\\n(2.28) is\\n1\\n2+4=1\\n66=1\\n2+1\\n4.AA\\x001=I=A\\x001A (2.26)\\n(AB)\\x001=B\\x001A\\x001(2.27)\\n(A+B)\\x0016=A\\x001+B\\x001(2.28)\\n(A>)>=A (2.29)\\n(A+B)>=A>+B>(2.30)\\n(AB)>=B>A>(2.31)\\nDeﬁnition 2.5 (Symmetric Matrix) .A matrixA2Rn\\x02nissymmetric if symmetric matrix\\nA=A>.\\nNote that only (n;n)-matrices can be symmetric. Generally, we call\\n(n;n)-matrices also square matrices because they possess the same num- square matrix\\nber of rows and columns. Moreover, if Ais invertible, then so is A>, and\\n(A\\x001)>= (A>)\\x001=:A\\x00>.\\nRemark (Sum and Product of Symmetric Matrices) .The sum of symmet-\\nric matricesA;B2Rn\\x02nis always symmetric. However, although their\\nproduct is always deﬁned, it is generally not symmetric:\\n\\x141 0\\n0 0\\x15\\x141 1\\n1 1\\x15\\n=\\x141 1\\n0 0\\x15\\n: (2.32)\\n}\\n2.2.3 Multiplication by a Scalar\\nLet us look at what happens to matrices when they are multiplied by a\\nscalar\\x152R. LetA2Rm\\x02nand\\x152R. Then\\x15A=K,Kij=\\x15aij.\\nPractically,\\x15scales each element of A. For\\x15; 2R, the following holds:\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n26 Linear Algebra\\nassociativityAssociativity:\\n(\\x15 )C=\\x15( C);C2Rm\\x02n\\n\\x15(BC) = (\\x15B)C=B(\\x15C) = (BC)\\x15;B2Rm\\x02n;C2Rn\\x02k.\\nNote that this allows us to move scalar values around.\\n(\\x15C)>=C>\\x15>=C>\\x15=\\x15C>since\\x15=\\x15>for all\\x152R.distributivity\\nDistributivity:\\n(\\x15+ )C=\\x15C+ C;C2Rm\\x02n\\n\\x15(B+C) =\\x15B+\\x15C;B;C2Rm\\x02n\\nExample 2.5 (Distributivity)\\nIf we deﬁne\\nC:=\\x141 2\\n3 4\\x15\\n; (2.33)\\nthen for any \\x15; 2Rwe obtain\\n(\\x15+ )C=\\x14(\\x15+ )1 (\\x15+ )2\\n(\\x15+ )3 (\\x15+ )4\\x15\\n=\\x14\\x15+ 2\\x15+ 2 \\n3\\x15+ 3 4\\x15+ 4 \\x15\\n(2.34a)\\n=\\x14\\x152\\x15\\n3\\x154\\x15\\x15\\n+\\x14 2 \\n3 4 \\x15\\n=\\x15C+ C: (2.34b)\\n2.2.4 Compact Representations of Systems of Linear Equations\\nIf we consider the system of linear equations\\n2x1+ 3x2+ 5x3= 1\\n4x1\\x002x2\\x007x3= 8\\n9x1+ 5x2\\x003x3= 2(2.35)\\nand use the rules for matrix multiplication, we can write this equation\\nsystem in a more compact form as\\n2\\n42 3 5\\n4\\x002\\x007\\n9 5\\x0033\\n52\\n4x1\\nx2\\nx33\\n5=2\\n41\\n8\\n23\\n5: (2.36)\\nNote thatx1scales the ﬁrst column, x2the second one, and x3the third\\none.\\nGenerally, a system of linear equations can be compactly represented in\\ntheir matrix form as Ax=b; see (2.3), and the product Axis a (linear)\\ncombination of the columns of A. We will discuss linear combinations in\\nmore detail in Section 2.5.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.3 Solving Systems of Linear Equations 27\\n2.3 Solving Systems of Linear Equations\\nIn (2.3), we introduced the general form of an equation system, i.e.,\\na11x1+\\x01\\x01\\x01+a1nxn=b1\\n...\\nam1x1+\\x01\\x01\\x01+amnxn=bm;(2.37)\\nwhereaij2Randbi2Rare known constants and xjare unknowns,\\ni= 1;:::;m ,j= 1;:::;n . Thus far, we saw that matrices can be used as\\na compact way of formulating systems of linear equations so that we can\\nwriteAx=b, see (2.10). Moreover, we deﬁned basic matrix operations,\\nsuch as addition and multiplication of matrices. In the following, we will\\nfocus on solving systems of linear equations and provide an algorithm for\\nﬁnding the inverse of a matrix.\\n2.3.1 Particular and General Solution\\nBefore discussing how to generally solve systems of linear equations, let\\nus have a look at an example. Consider the system of equations\\n\\x141 0 8\\x004\\n0 1 2 12\\x152\\n664x1\\nx2\\nx3\\nx43\\n775=\\x1442\\n8\\x15\\n: (2.38)\\nThe system has two equations and four unknowns. Therefore, in general\\nwe would expect inﬁnitely many solutions. This system of equations is\\nin a particularly easy form, where the ﬁrst two columns consist of a 1\\nand a 0. Remember that we want to ﬁnd scalars x1;:::;x 4, such thatP4\\ni=1xici=b, where we deﬁne cito be theith column of the matrix and\\nbthe right-hand-side of (2.38). A solution to the problem in (2.38) can\\nbe found immediately by taking 42times the ﬁrst column and 8times the\\nsecond column so that\\nb=\\x1442\\n8\\x15\\n= 42\\x141\\n0\\x15\\n+ 8\\x140\\n1\\x15\\n: (2.39)\\nTherefore, a solution is [42;8;0;0]>. This solution is called a particular particular solution\\nsolution orspecial solution . However, this is not the only solution of this special solution\\nsystem of linear equations. To capture all the other solutions, we need\\nto be creative in generating 0in a non-trivial way using the columns of\\nthe matrix: Adding 0to our special solution does not change the special\\nsolution. To do so, we express the third column using the ﬁrst two columns\\n(which are of this very simple form)\\n\\x148\\n2\\x15\\n= 8\\x141\\n0\\x15\\n+ 2\\x140\\n1\\x15\\n(2.40)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n28 Linear Algebra\\nso that 0= 8c1+ 2c2\\x001c3+ 0c4and(x1;x2;x3;x4) = (8;2;\\x001;0). In\\nfact, any scaling of this solution by \\x1512Rproduces the 0vector, i.e.,\\n\\x141 0 8\\x004\\n0 1 2 12\\x150\\nBB@\\x1512\\n6648\\n2\\n\\x001\\n03\\n7751\\nCCA=\\x151(8c1+ 2c2\\x00c3) =0: (2.41)\\nFollowing the same line of reasoning, we express the fourth column of the\\nmatrix in (2.38) using the ﬁrst two columns and generate another set of\\nnon-trivial versions of 0as\\n\\x141 0 8\\x004\\n0 1 2 12\\x150\\nBB@\\x1522\\n664\\x004\\n12\\n0\\n\\x0013\\n7751\\nCCA=\\x152(\\x004c1+ 12c2\\x00c4) =0 (2.42)\\nfor any\\x1522R. Putting everything together, we obtain all solutions of the\\nequation system in (2.38), which is called the general solution , as the set general solution\\n8\\n>><\\n>>:x2R4:x=2\\n66442\\n8\\n0\\n03\\n775+\\x1512\\n6648\\n2\\n\\x001\\n03\\n775+\\x1522\\n664\\x004\\n12\\n0\\n\\x0013\\n775;\\x151;\\x1522R9\\n>>=\\n>>;:(2.43)\\nRemark. The general approach we followed consisted of the following\\nthree steps:\\n1. Find a particular solution to Ax=b.\\n2. Find all solutions to Ax=0.\\n3. Combine the solutions from steps 1. and 2. to the general solution.\\nNeither the general nor the particular solution is unique. }\\nThe system of linear equations in the preceding example was easy to\\nsolve because the matrix in (2.38) has this particularly convenient form,\\nwhich allowed us to ﬁnd the particular and the general solution by in-\\nspection. However, general equation systems are not of this simple form.\\nFortunately, there exists a constructive algorithmic way of transforming\\nany system of linear equations into this particularly simple form: Gaussian\\nelimination. Key to Gaussian elimination are elementary transformations\\nof systems of linear equations, which transform the equation system into\\na simple form. Then, we can apply the three steps to the simple form that\\nwe just discussed in the context of the example in (2.38).\\n2.3.2 Elementary Transformations\\nKey to solving a system of linear equations are elementary transformations elementary\\ntransformations that keep the solution set the same, but that transform the equation system\\ninto a simpler form:\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.3 Solving Systems of Linear Equations 29\\nExchange of two equations (rows in the matrix representing the system\\nof equations)\\nMultiplication of an equation (row) with a constant \\x152Rnf0g\\nAddition of two equations (rows)\\nExample 2.6\\nFora2R, we seek all solutions of the following system of equations:\\n\\x002x1+ 4x2\\x002x3\\x00x4+ 4x5=\\x003\\n4x1\\x008x2+ 3x3\\x003x4+x5= 2\\nx1\\x002x2+x3\\x00x4+x5= 0\\nx1\\x002x2\\x003x4+ 4x5=a: (2.44)\\nWe start by converting this system of equations into the compact matrix\\nnotationAx=b. We no longer mention the variables xexplicitly and\\nbuild the augmented matrix (in the form\\x02Ajb\\x03\\n) augmented matrix\\n2\\n664\\x002 4\\x002\\x001 4\\x003\\n4\\x008 3\\x003 1 2\\n1\\x002 1\\x001 1 0\\n1\\x002 0\\x003 4 a3\\n775Swap withR3\\nSwap withR1\\nwhere we used the vertical line to separate the left-hand side from the\\nright-hand side in (2.44). We use  to indicate a transformation of the\\naugmented matrix using elementary transformations. The augmented\\nmatrix\\x02\\nAjb\\x03\\ncompactly\\nrepresents the\\nsystem of linear\\nequationsAx=b.Swapping Rows 1and3leads to\\n2\\n6641\\x002 1\\x001 1 0\\n4\\x008 3\\x003 1 2\\n\\x002 4\\x002\\x001 4\\x003\\n1\\x002 0\\x003 4 a3\\n775\\x004R1\\n+2R1\\n\\x00R1\\nWhen we now apply the indicated transformations (e.g., subtract Row 1\\nfour times from Row 2), we obtain\\n2\\n6641\\x002 1\\x001 1 0\\n0 0\\x001 1\\x003 2\\n0 0 0 \\x003 6\\x003\\n0 0\\x001\\x002 3 a3\\n775\\n\\x00R2\\x00R3\\n 2\\n6641\\x002 1\\x001 1 0\\n0 0\\x001 1\\x003 2\\n0 0 0 \\x003 6\\x003\\n0 0 0 0 0 a+13\\n775\\x01(\\x001)\\n\\x01(\\x001\\n3)\\n 2\\n6641\\x002 1\\x001 1 0\\n0 0 1 \\x001 3\\x002\\n0 0 0 1 \\x002 1\\n0 0 0 0 0 a+13\\n775\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n30 Linear Algebra\\nThis (augmented) matrix is in a convenient form, the row-echelon form row-echelon form\\n(REF). Reverting this compact notation back into the explicit notation with\\nthe variables we seek, we obtain\\nx1\\x002x2+x3\\x00x4+x5= 0\\nx3\\x00x4+ 3x5=\\x002\\nx4\\x002x5= 1\\n0 =a+ 1: (2.45)\\nOnly fora=\\x001this system can be solved. A particular solution is particular solution\\n2\\n66664x1\\nx2\\nx3\\nx4\\nx53\\n77775=2\\n666642\\n0\\n\\x001\\n1\\n03\\n77775: (2.46)\\nThegeneral solution , which captures the set of all possible solutions, is general solution\\n8\\n>>>><\\n>>>>:x2R5:x=2\\n666642\\n0\\n\\x001\\n1\\n03\\n77775+\\x1512\\n666642\\n1\\n0\\n0\\n03\\n77775+\\x1522\\n666642\\n0\\n\\x001\\n2\\n13\\n77775; \\x15 1;\\x1522R9\\n>>>>=\\n>>>>;:(2.47)\\nIn the following, we will detail a constructive way to obtain a particular\\nand general solution of a system of linear equations.\\nRemark (Pivots and Staircase Structure) .The leading coefﬁcient of a row\\n(ﬁrst nonzero number from the left) is called the pivot and is always pivot\\nstrictly to the right of the pivot of the row above it. Therefore, any equa-\\ntion system in row-echelon form always has a “staircase” structure. }\\nDeﬁnition 2.6 (Row-Echelon Form) .A matrix is in row-echelon form if row-echelon form\\nAll rows that contain only zeros are at the bottom of the matrix; corre-\\nspondingly, all rows that contain at least one nonzero element are on\\ntop of rows that contain only zeros.\\nLooking at nonzero rows only, the ﬁrst nonzero number from the left\\n(also called the pivot or the leading coefﬁcient ) is always strictly to the pivot\\nleading coefﬁcient right of the pivot of the row above it.\\nIn other texts, it is\\nsometimes required\\nthat the pivot is 1.Remark (Basic and Free Variables) .The variables corresponding to the\\npivots in the row-echelon form are called basic variables and the other\\nbasic variable variables are free variables . For example, in (2.45), x1;x3;x4are basic\\nfree variable variables, whereas x2;x5are free variables. }\\nRemark (Obtaining a Particular Solution) .The row-echelon form makes\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.3 Solving Systems of Linear Equations 31\\nour lives easier when we need to determine a particular solution. To do\\nthis, we express the right-hand side of the equation system using the pivot\\ncolumns, such that b=PP\\ni=1\\x15ipi, wherepi; i= 1;:::;P , are the pivot\\ncolumns. The \\x15iare determined easiest if we start with the rightmost pivot\\ncolumn and work our way to the left.\\nIn the previous example, we would try to ﬁnd \\x151;\\x152;\\x153so that\\n\\x1512\\n6641\\n0\\n0\\n03\\n775+\\x1522\\n6641\\n1\\n0\\n03\\n775+\\x1532\\n664\\x001\\n\\x001\\n1\\n03\\n775=2\\n6640\\n\\x002\\n1\\n03\\n775: (2.48)\\nFrom here, we ﬁnd relatively directly that \\x153= 1;\\x152=\\x001;\\x151= 2. When\\nwe put everything together, we must not forget the non-pivot columns\\nfor which we set the coefﬁcients implicitly to 0. Therefore, we get the\\nparticular solution x= [2;0;\\x001;1;0]>. }\\nRemark (Reduced Row Echelon Form) .An equation system is in reduced reduced\\nrow-echelon form row-echelon form (also: row-reduced echelon form orrow canonical form ) if\\nIt is in row-echelon form.\\nEvery pivot is 1.\\nThe pivot is the only nonzero entry in its column.\\n}\\nThe reduced row-echelon form will play an important role later in Sec-\\ntion 2.3.3 because it allows us to determine the general solution of a sys-\\ntem of linear equations in a straightforward way.Gaussian\\nelimination Remark (Gaussian Elimination) . Gaussian elimination is an algorithm that\\nperforms elementary transformations to bring a system of linear equations\\ninto reduced row-echelon form. }\\nExample 2.7 (Reduced Row Echelon Form)\\nVerify that the following matrix is in reduced row-echelon form (the pivots\\nare in bold):\\nA=2\\n413 0 0 3\\n0 0 10 9\\n0 0 0 1\\x0043\\n5: (2.49)\\nThe key idea for ﬁnding the solutions of Ax=0is to look at the non-\\npivot columns , which we will need to express as a (linear) combination of\\nthe pivot columns. The reduced row echelon form makes this relatively\\nstraightforward, and we express the non-pivot columns in terms of sums\\nand multiples of the pivot columns that are on their left: The second col-\\numn is 3times the ﬁrst column (we can ignore the pivot columns on the\\nright of the second column). Therefore, to obtain 0, we need to subtract\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n32 Linear Algebra\\nthe second column from three times the ﬁrst column. Now, we look at the\\nﬁfth column, which is our second non-pivot column. The ﬁfth column can\\nbe expressed as 3times the ﬁrst pivot column, 9times the second pivot\\ncolumn, and\\x004times the third pivot column. We need to keep track of\\nthe indices of the pivot columns and translate this into 3times the ﬁrst col-\\numn, 0times the second column (which is a non-pivot column), 9times\\nthe third column (which is our second pivot column), and \\x004times the\\nfourth column (which is the third pivot column). Then we need to subtract\\nthe ﬁfth column to obtain 0. In the end, we are still solving a homogeneous\\nequation system.\\nTo summarize, all solutions of Ax=0;x2R5are given by\\n8\\n>>>><\\n>>>>:x2R5:x=\\x1512\\n666643\\n\\x001\\n0\\n0\\n03\\n77775+\\x1522\\n666643\\n0\\n9\\n\\x004\\n\\x0013\\n77775; \\x15 1;\\x1522R9\\n>>>>=\\n>>>>;: (2.50)\\n2.3.3 The Minus-1 Trick\\nIn the following, we introduce a practical trick for reading out the solu-\\ntionsxof a homogeneous system of linear equations Ax=0, where\\nA2Rk\\x02n;x2Rn.\\nTo start, we assume that Ais in reduced row-echelon form without any\\nrows that just contain zeros, i.e.,\\nA=2\\n666666640\\x01\\x01\\x0101\\x03 \\x01\\x01\\x01 \\x03 0\\x03 \\x01\\x01\\x01 \\x03 0\\x03 \\x01\\x01\\x01 \\x03\\n......0 0\\x01\\x01\\x0101\\x03 \\x01\\x01\\x01 \\x03.........\\n...............0...............\\n........................0......\\n0\\x01\\x01\\x010 0 0\\x01\\x01\\x010 0 0\\x01\\x01\\x0101\\x03 \\x01\\x01\\x01 \\x033\\n77777775;\\n(2.51)\\nwhere\\x03can be an arbitrary real number, with the constraints that the ﬁrst\\nnonzero entry per row must be 1and all other entries in the corresponding\\ncolumn must be 0. The columns j1;:::;jkwith the pivots (marked in\\nbold) are the standard unit vectors e1;:::;ek2Rk. We extend this matrix\\nto ann\\x02n-matrix ~Aby addingn\\x00krows of the form\\n\\x020\\x01\\x01\\x010\\x001 0\\x01\\x01\\x010\\x03\\n(2.52)\\nso that the diagonal of the augmented matrix ~Acontains either 1or\\x001.\\nThen, the columns of ~Athat contain the\\x001as pivots are solutions of\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.3 Solving Systems of Linear Equations 33\\nthe homogeneous equation system Ax=0. To be more precise, these\\ncolumns form a basis (Section 2.6.1) of the solution space of Ax=0,\\nwhich we will later call the kernel ornull space (see Section 2.7.3). kernel\\nnull space\\nExample 2.8 (Minus-1 Trick)\\nLet us revisit the matrix in (2.49), which is already in reduced REF:\\nA=2\\n41 3 0 0 3\\n0 0 1 0 9\\n0 0 0 1\\x0043\\n5: (2.53)\\nWe now augment this matrix to a 5\\x025matrix by adding rows of the\\nform (2.52) at the places where the pivots on the diagonal are missing\\nand obtain\\n~A=2\\n666641 3 0 0 3\\n0\\x0010 0 0\\n0 0 1 0 9\\n0 0 0 1\\x004\\n0 0 0 0\\x0013\\n77775: (2.54)\\nFrom this form, we can immediately read out the solutions of Ax=0by\\ntaking the columns of ~A, which contain\\x001on the diagonal:\\n8\\n>>>><\\n>>>>:x2R5:x=\\x1512\\n666643\\n\\x001\\n0\\n0\\n03\\n77775+\\x1522\\n666643\\n0\\n9\\n\\x004\\n\\x0013\\n77775; \\x15 1;\\x1522R9\\n>>>>=\\n>>>>;; (2.55)\\nwhich is identical to the solution in (2.50) that we obtained by “insight”.\\nCalculating the Inverse\\nTo compute the inverse A\\x001ofA2Rn\\x02n, we need to ﬁnd a matrix X\\nthat satisﬁesAX =In. Then,X=A\\x001. We can write this down as\\na set of simultaneous linear equations AX =In, where we solve for\\nX= [x1j\\x01\\x01\\x01jxn]. We use the augmented matrix notation for a compact\\nrepresentation of this set of systems of linear equations and obtain\\n\\x02AjIn\\x03\\n \\x01\\x01\\x01 \\x02InjA\\x001\\x03: (2.56)\\nThis means that if we bring the augmented equation system into reduced\\nrow-echelon form, we can read out the inverse on the right-hand side of\\nthe equation system. Hence, determining the inverse of a matrix is equiv-\\nalent to solving systems of linear equations.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n34 Linear Algebra\\nExample 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)\\nTo determine the inverse of\\nA=2\\n6641 0 2 0\\n1 1 0 0\\n1 2 0 1\\n1 1 1 13\\n775(2.57)\\nwe write down the augmented matrix\\n2\\n6641 0 2 0 1 0 0 0\\n1 1 0 0 0 1 0 0\\n1 2 0 1 0 0 1 0\\n1 1 1 1 0 0 0 13\\n775\\nand use Gaussian elimination to bring it into reduced row-echelon form\\n2\\n6641 0 0 0\\x001 2\\x002 2\\n0 1 0 0 1\\x001 2\\x002\\n0 0 1 0 1\\x001 1\\x001\\n0 0 0 1\\x001 0\\x001 23\\n775;\\nsuch that the desired inverse is given as its right-hand side:\\nA\\x001=2\\n664\\x001 2\\x002 2\\n1\\x001 2\\x002\\n1\\x001 1\\x001\\n\\x001 0\\x001 23\\n775: (2.58)\\nWe can verify that (2.58) is indeed the inverse by performing the multi-\\nplicationAA\\x001and observing that we recover I4.\\n2.3.4 Algorithms for Solving a System of Linear Equations\\nIn the following, we brieﬂy discuss approaches to solving a system of lin-\\near equations of the form Ax=b. We make the assumption that a solu-\\ntion exists. Should there be no solution, we need to resort to approximate\\nsolutions, which we do not cover in this chapter. One way to solve the ap-\\nproximate problem is using the approach of linear regression, which we\\ndiscuss in detail in Chapter 9.\\nIn special cases, we may be able to determine the inverse A\\x001, such\\nthat the solution of Ax=bis given asx=A\\x001b. However, this is\\nonly possible if Ais a square matrix and invertible, which is often not the\\ncase. Otherwise, under mild assumptions (i.e., Aneeds to have linearly\\nindependent columns) we can use the transformation\\nAx=b()A>Ax=A>b()x= (A>A)\\x001A>b (2.59)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.4 Vector Spaces 35\\nand use the Moore-Penrose pseudo-inverse (A>A)\\x001A>to determine the Moore-Penrose\\npseudo-inverse solution (2.59) that solves Ax=b, which also corresponds to the mini-\\nmum norm least-squares solution. A disadvantage of this approach is that\\nit requires many computations for the matrix-matrix product and comput-\\ning the inverse of A>A. Moreover, for reasons of numerical precision it\\nis generally not recommended to compute the inverse or pseudo-inverse.\\nIn the following, we therefore brieﬂy discuss alternative approaches to\\nsolving systems of linear equations.\\nGaussian elimination plays an important role when computing deter-\\nminants (Section 4.1), checking whether a set of vectors is linearly inde-\\npendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2),\\ncomputing the rank of a matrix (Section 2.6.2), and determining a basis\\nof a vector space (Section 2.6.1). Gaussian elimination is an intuitive and\\nconstructive way to solve a system of linear equations with thousands of\\nvariables. However, for systems with millions of variables, it is impracti-\\ncal as the required number of arithmetic operations scales cubically in the\\nnumber of simultaneous equations.\\nIn practice, systems of many linear equations are solved indirectly, by ei-\\nther stationary iterative methods, such as the Richardson method, the Ja-\\ncobi method, the Gauß-Seidel method, and the successive over-relaxation\\nmethod, or Krylov subspace methods, such as conjugate gradients, gener-\\nalized minimal residual, or biconjugate gradients. We refer to the books\\nby Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann\\n(2015) for further details.\\nLetx\\x03be a solution of Ax=b. The key idea of these iterative methods\\nis to set up an iteration of the form\\nx(k+1)=Cx(k)+d (2.60)\\nfor suitableCanddthat reduces the residual error kx(k+1)\\x00x\\x03kin every\\niteration and converges to x\\x03. We will introduce norms k\\x01k, which allow\\nus to compute similarities between vectors, in Section 3.1.\\n2.4 Vector Spaces\\nThus far, we have looked at systems of linear equations and how to solve\\nthem (Section 2.3). We saw that systems of linear equations can be com-\\npactly represented using matrix-vector notation (2.10). In the following,\\nwe will have a closer look at vector spaces, i.e., a structured space in which\\nvectors live.\\nIn the beginning of this chapter, we informally characterized vectors as\\nobjects that can be added together and multiplied by a scalar, and they\\nremain objects of the same type. Now, we are ready to formalize this,\\nand we will start by introducing the concept of a group, which is a set\\nof elements and an operation deﬁned on these elements that keeps some\\nstructure of the set intact.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n36 Linear Algebra\\n2.4.1 Groups\\nGroups play an important role in computer science. Besides providing a\\nfundamental framework for operations on sets, they are heavily used in\\ncryptography, coding theory, and graphics.\\nDeﬁnition 2.7 (Group) .Consider a setGand an operation\\n:G\\x02G!G\\ndeﬁned onG. ThenG:= (G;\\n)is called a group if the following hold: group\\nclosure1.Closure ofGunder\\n:8x;y2G:x\\ny2Gassociativity\\n2.Associativity:8x;y;z2G: (x\\ny)\\nz=x\\n(y\\nz)neutral element\\n3.Neutral element:9e2G8x2G:x\\ne=xande\\nx=x inverse element\\n4.Inverse element:8x2G9y2G:x\\ny=eandy\\nx=e, whereeis\\nthe neutral element. We often write x\\x001to denote the inverse element\\nofx.\\nRemark. The inverse element is deﬁned with respect to the operation \\nand does not necessarily mean1\\nx. }\\nIf additionally8x;y2G:x\\ny=y\\nx, thenG= (G;\\n)is an Abelian Abelian group\\ngroup (commutative).\\nExample 2.10 (Groups)\\nLet us have a look at some examples of sets with associated operations\\nand see whether they are groups:\\n(Z;+)is an Abelian group.\\n(N0;+)is not a group: Although (N0;+)possesses a neutral element N0:=N[f0g\\n(0), the inverse elements are missing.\\n(Z;\\x01)is not a group: Although (Z;\\x01)contains a neutral element ( 1), the\\ninverse elements for any z2Z;z6=\\x061, are missing.\\n(R;\\x01)is not a group since 0does not possess an inverse element.\\n(Rnf0g;\\x01)is Abelian.\\n(Rn;+);(Zn;+);n2Nare Abelian if +is deﬁned componentwise, i.e.,\\n(x1;\\x01\\x01\\x01;xn) + (y1;\\x01\\x01\\x01;yn) = (x1+y1;\\x01\\x01\\x01;xn+yn): (2.61)\\nThen, (x1;\\x01\\x01\\x01;xn)\\x001:= (\\x00x1;\\x01\\x01\\x01;\\x00xn)is the inverse element and\\ne= (0;\\x01\\x01\\x01;0)is the neutral element.\\n(Rm\\x02n;+), the set ofm\\x02n-matrices is Abelian (with componentwise\\naddition as deﬁned in (2.61)).\\nLet us have a closer look at (Rn\\x02n;\\x01), i.e., the set of n\\x02n-matrices with\\nmatrix multiplication as deﬁned in (2.13).\\n–Closure and associativity follow directly from the deﬁnition of matrix\\nmultiplication.\\n–Neutral element: The identity matrix Inis the neutral element with\\nrespect to matrix multiplication “ \\x01” in(Rn\\x02n;\\x01).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.4 Vector Spaces 37\\n–Inverse element: If the inverse exists ( Ais regular), then A\\x001is the\\ninverse element of A2Rn\\x02n, and in exactly this case (Rn\\x02n;\\x01)is a\\ngroup, called the general linear group .\\nDeﬁnition 2.8 (General Linear Group) .The set of regular (invertible)\\nmatricesA2Rn\\x02nis a group with respect to matrix multiplication as\\ndeﬁned in (2.13) and is called general linear group GL(n;R). However, general linear group\\nsince matrix multiplication is not commutative, the group is not Abelian.\\n2.4.2 Vector Spaces\\nWhen we discussed groups, we looked at sets Gand inner operations on\\nG, i.e., mappingsG\\x02G!G that only operate on elements in G. In the\\nfollowing, we will consider sets that in addition to an inner operation +\\nalso contain an outer operation \\x01, the multiplication of a vector x2Gby\\na scalar\\x152R. We can think of the inner operation as a form of addition,\\nand the outer operation as a form of scaling. Note that the inner/outer\\noperations have nothing to do with inner/outer products.\\nDeﬁnition 2.9 (Vector Space) .A real-valued vector space V= (V;+;\\x01)is vector space\\na setVwith two operations\\n+ :V\\x02V!V (2.62)\\n\\x01:R\\x02V!V (2.63)\\nwhere\\n1.(V;+)is an Abelian group\\n2. Distributivity:\\n1.8\\x152R;x;y2V:\\x15\\x01(x+y) =\\x15\\x01x+\\x15\\x01y\\n2.8\\x15; 2R;x2V: (\\x15+ )\\x01x=\\x15\\x01x+ \\x01x\\n3. Associativity (outer operation): 8\\x15; 2R;x2V:\\x15\\x01( \\x01x) = (\\x15 )\\x01x\\n4. Neutral element with respect to the outer operation: 8x2V: 1\\x01x=x\\nThe elements x2Vare called vectors . The neutral element of (V;+)is vector\\nthe zero vector 0= [0;:::; 0]>, and the inner operation +is called vector vector addition\\naddition . The elements \\x152Rare called scalars and the outer operation scalar\\n\\x01is amultiplication by scalars . Note that a scalar product is something multiplication by\\nscalars different, and we will get to this in Section 3.2.\\nRemark. A “vector multiplication” ab,a;b2Rn, is not deﬁned. Theoret-\\nically, we could deﬁne an element-wise multiplication, such that c=ab\\nwithcj=ajbj. This “array multiplication” is common to many program-\\nming languages but makes mathematically limited sense using the stan-\\ndard rules for matrix multiplication: By treating vectors as n\\x021matrices\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n38 Linear Algebra\\n(which we usually do), we can use the matrix multiplication as deﬁned\\nin (2.13). However, then the dimensions of the vectors do not match. Only\\nthe following multiplications for vectors are deﬁned: ab>2Rn\\x02n(outer outer product\\nproduct ),a>b2R(inner/scalar/dot product). }\\nExample 2.11 (Vector Spaces)\\nLet us have a look at some important examples:\\nV=Rn;n2Nis a vector space with operations deﬁned as follows:\\n–Addition:x+y= (x1;:::;xn)+(y1;:::;yn) = (x1+y1;:::;xn+yn)\\nfor allx;y2Rn\\n–Multiplication by scalars: \\x15x=\\x15(x1;:::;xn) = (\\x15x1;:::;\\x15xn)for\\nall\\x152R;x2Rn\\nV=Rm\\x02n;m;n2Nis a vector space with\\n–Addition:A+B=2\\n64a11+b11\\x01\\x01\\x01a1n+b1n\\n......\\nam1+bm1\\x01\\x01\\x01amn+bmn3\\n75is deﬁned ele-\\nmentwise for all A;B2V\\n–Multiplication by scalars: \\x15A=2\\n64\\x15a11\\x01\\x01\\x01\\x15a1n\\n......\\n\\x15am1\\x01\\x01\\x01\\x15amn3\\n75as deﬁned in\\nSection 2.2. Remember that Rm\\x02nis equivalent to Rmn.\\nV=C, with the standard deﬁnition of addition of complex numbers.\\nRemark. In the following, we will denote a vector space (V;+;\\x01)byV\\nwhen +and\\x01are the standard vector addition and scalar multiplication.\\nMoreover, we will use the notation x2Vfor vectors inVto simplify\\nnotation. }\\nRemark. The vector spaces Rn;Rn\\x021;R1\\x02nare only different in the way\\nwe write vectors. In the following, we will not make a distinction between\\nRnandRn\\x021, which allows us to write n-tuples as column vectors column vector\\nx=2\\n64x1\\n...\\nxn3\\n75: (2.64)\\nThis simpliﬁes the notation regarding vector space operations. However,\\nwe do distinguish between Rn\\x021andR1\\x02n(therow vectors ) to avoid con- row vector\\nfusion with matrix multiplication. By default, we write xto denote a col-\\numn vector, and a row vector is denoted by x>, the transpose ofx.} transpose\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.4 Vector Spaces 39\\n2.4.3 Vector Subspaces\\nIn the following, we will introduce vector subspaces. Intuitively, they are\\nsets contained in the original vector space with the property that when\\nwe perform vector space operations on elements within this subspace, we\\nwill never leave it. In this sense, they are “closed”. Vector subspaces are a\\nkey idea in machine learning. For example, Chapter 10 demonstrates how\\nto use vector subspaces for dimensionality reduction.\\nDeﬁnition 2.10 (Vector Subspace) .LetV= (V;+;\\x01)be a vector space\\nandU\\x12V ,U6=;. ThenU= (U;+;\\x01)is called vector subspace ofV(or vector subspace\\nlinear subspace ) ifUis a vector space with the vector space operations + linear subspace\\nand\\x01restricted toU\\x02U andR\\x02U. We writeU\\x12Vto denote a subspace\\nUofV.\\nIfU\\x12V andVis a vector space, then Unaturally inherits many prop-\\nerties directly from Vbecause they hold for all x2V, and in particular for\\nallx2U\\x12V . This includes the Abelian group properties, the distribu-\\ntivity, the associativity and the neutral element. To determine whether\\n(U;+;\\x01)is a subspace of Vwe still do need to show\\n1.U6=;, in particular: 02U\\n2. Closure of U:\\na. With respect to the outer operation: 8\\x152R8x2U:\\x15x2U.\\nb. With respect to the inner operation: 8x;y2U:x+y2U.\\nExample 2.12 (Vector Subspaces)\\nLet us have a look at some examples:\\nFor every vector space V, the trivial subspaces are Vitself andf0g.\\nOnly example Din Figure 2.6 is a subspace of R2(with the usual inner/\\nouter operations). In AandC, the closure property is violated; Bdoes\\nnot contain 0.\\nThe solution set of a homogeneous system of linear equations Ax=0\\nwithnunknownsx= [x1;:::;xn]>is a subspace of Rn.\\nThe solution of an inhomogeneous system of linear equations Ax=\\nb;b6=0is not a subspace of Rn.\\nThe intersection of arbitrarily many subspaces is a subspace itself.\\nFigure 2.6 Not all\\nsubsets of R2are\\nsubspaces. In Aand\\nC, the closure\\nproperty is violated;\\nBdoes not contain\\n0. OnlyDis a\\nsubspace.0 0 0 0AB\\nCD\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n40 Linear Algebra\\nRemark. Every subspace U\\x12(Rn;+;\\x01)is the solution space of a homo-\\ngeneous system of linear equations Ax=0forx2Rn.}\\n2.5 Linear Independence\\nIn the following, we will have a close look at what we can do with vectors\\n(elements of the vector space). In particular, we can add vectors together\\nand multiply them with scalars. The closure property guarantees that we\\nend up with another vector in the same vector space. It is possible to ﬁnd\\na set of vectors with which we can represent every vector in the vector\\nspace by adding them together and scaling them. This set of vectors is\\nabasis, and we will discuss them in Section 2.6.1. Before we get there,\\nwe will need to introduce the concepts of linear combinations and linear\\nindependence.\\nDeﬁnition 2.11 (Linear Combination) .Consider a vector space Vand a\\nﬁnite number of vectors x1;:::;xk2V. Then, every v2Vof the form\\nv=\\x151x1+\\x01\\x01\\x01+\\x15kxk=kX\\ni=1\\x15ixi2V (2.65)\\nwith\\x151;:::;\\x15k2Ris alinear combination of the vectors x1;:::;xk. linear combination\\nThe0-vector can always be written as the linear combination of kvec-\\ntorsx1;:::;xkbecause 0=Pk\\ni=10xiis always true. In the following,\\nwe are interested in non-trivial linear combinations of a set of vectors to\\nrepresent 0, i.e., linear combinations of vectors x1;:::;xk, where not all\\ncoefﬁcients \\x15iin (2.65) are 0.\\nDeﬁnition 2.12 (Linear (In)dependence) .Let us consider a vector space\\nVwithk2Nandx1;:::;xk2V. If there is a non-trivial linear com-\\nbination, such that 0=Pk\\ni=1\\x15ixiwith at least one \\x15i6= 0, the vectors\\nx1;:::;xkarelinearly dependent . If only the trivial solution exists, i.e., linearly dependent\\n\\x151=:::=\\x15k= 0the vectorsx1;:::;xkarelinearly independent . linearly\\nindependent\\nLinear independence is one of the most important concepts in linear\\nalgebra. Intuitively, a set of linearly independent vectors consists of vectors\\nthat have no redundancy, i.e., if we remove any of those vectors from\\nthe set, we will lose something. Throughout the next sections, we will\\nformalize this intuition more.\\nExample 2.13 (Linearly Dependent Vectors)\\nA geographic example may help to clarify the concept of linear indepen-\\ndence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is\\nmight say ,“You can get to Kigali by ﬁrst going 506 km Northwest to Kam-\\npala (Uganda) and then 374 km Southwest.”. This is sufﬁcient information\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.5 Linear Independence 41\\nto describe the location of Kigali because the geographic coordinate sys-\\ntem may be considered a two-dimensional vector space (ignoring altitude\\nand the Earth’s curved surface). The person may add, “It is about 751 km\\nWest of here.” Although this last statement is true, it is not necessary to\\nﬁnd Kigali given the previous information (see Figure 2.7 for an illus-\\ntration). In this example, the “ 506 km Northwest” vector (blue) and the\\n“374 km Southwest” vector (purple) are linearly independent. This means\\nthe Southwest vector cannot be described in terms of the Northwest vec-\\ntor, and vice versa. However, the third “ 751 km West” vector (black) is a\\nlinear combination of the other two vectors, and it makes the set of vec-\\ntors linearly dependent. Equivalently, given “ 751 km West” and “ 374 km\\nSouthwest” can be linearly combined to obtain “ 506 km Northwest”.\\nFigure 2.7\\nGeographic example\\n(with crude\\napproximations to\\ncardinal directions)\\nof linearly\\ndependent vectors\\nin a\\ntwo-dimensional\\nspace (plane).\\n506 km Northwest\\n751 km West\\n374 km Southwest\\n374 km Southwest\\nKampala\\nNairobi\\nKigali\\nRemark. The following properties are useful to ﬁnd out whether vectors\\nare linearly independent:\\nkvectors are either linearly dependent or linearly independent. There\\nis no third option.\\nIf at least one of the vectors x1;:::;xkis0then they are linearly de-\\npendent. The same holds if two vectors are identical.\\nThe vectorsfx1;:::;xk:xi6=0;i= 1;:::;kg,k>2, are linearly\\ndependent if and only if (at least) one of them is a linear combination\\nof the others. In particular, if one vector is a multiple of another vector,\\ni.e.,xi=\\x15xj; \\x152Rthen the setfx1;:::;xk:xi6=0;i= 1;:::;kg\\nis linearly dependent.\\nA practical way of checking whether vectors x1;:::;xk2Vare linearly\\nindependent is to use Gaussian elimination: Write all vectors as columns\\nof a matrixAand perform Gaussian elimination until the matrix is in\\nrow echelon form (the reduced row-echelon form is unnecessary here):\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n42 Linear Algebra\\n–The pivot columns indicate the vectors, which are linearly indepen-\\ndent of the vectors on the left. Note that there is an ordering of vec-\\ntors when the matrix is built.\\n–The non-pivot columns can be expressed as linear combinations of\\nthe pivot columns on their left. For instance, the row-echelon form\\n\\x141 3 0\\n0 0 2\\x15\\n(2.66)\\ntells us that the ﬁrst and third columns are pivot columns. The sec-\\nond column is a non-pivot column because it is three times the ﬁrst\\ncolumn.\\nAll column vectors are linearly independent if and only if all columns\\nare pivot columns. If there is at least one non-pivot column, the columns\\n(and, therefore, the corresponding vectors) are linearly dependent.\\n}\\nExample 2.14\\nConsider R4with\\nx1=2\\n6641\\n2\\n\\x003\\n43\\n775;x2=2\\n6641\\n1\\n0\\n23\\n775;x3=2\\n664\\x001\\n\\x002\\n1\\n13\\n775: (2.67)\\nTo check whether they are linearly dependent, we follow the general ap-\\nproach and solve\\n\\x151x1+\\x152x2+\\x153x3=\\x1512\\n6641\\n2\\n\\x003\\n43\\n775+\\x1522\\n6641\\n1\\n0\\n23\\n775+\\x1532\\n664\\x001\\n\\x002\\n1\\n13\\n775=0 (2.68)\\nfor\\x151;:::;\\x15 3. We write the vectors xi,i= 1;2;3, as the columns of a\\nmatrix and apply elementary row operations until we identify the pivot\\ncolumns:\\n2\\n6641 1\\x001\\n2 1\\x002\\n\\x003 0 1\\n4 2 13\\n775 \\x01\\x01\\x01 2\\n6641 1\\x001\\n0 1 0\\n0 0 1\\n0 0 03\\n775: (2.69)\\nHere, every column of the matrix is a pivot column. Therefore, there is no\\nnon-trivial solution, and we require \\x151= 0;\\x152= 0;\\x153= 0to solve the\\nequation system. Hence, the vectors x1;x2;x3are linearly independent.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.5 Linear Independence 43\\nRemark. Consider a vector space Vwithklinearly independent vectors\\nb1;:::;bkandmlinear combinations\\nx1=kX\\ni=1\\x15i1bi;\\n...\\nxm=kX\\ni=1\\x15imbi:(2.70)\\nDeﬁningB= [b1;:::;bk]as the matrix whose columns are the linearly\\nindependent vectors b1;:::;bk, we can write\\nxj=B\\x15j;\\x15j=2\\n64\\x151j\\n...\\n\\x15kj3\\n75; j = 1;:::;m; (2.71)\\nin a more compact form.\\nWe want to test whether x1;:::;xmare linearly independent. For this\\npurpose, we follow the general approach of testing whenPm\\nj=1 jxj=0.\\nWith (2.71), we obtain\\nmX\\nj=1 jxj=mX\\nj=1 jB\\x15j=BmX\\nj=1 j\\x15j: (2.72)\\nThis means thatfx1;:::;xmgare linearly independent if and only if the\\ncolumn vectorsf\\x151;:::;\\x15mgare linearly independent.\\n}\\nRemark. In a vector space V,mlinear combinations of kvectorsx1;:::;xk\\nare linearly dependent if m>k . }\\nExample 2.15\\nConsider a set of linearly independent vectors b1;b2;b3;b42Rnand\\nx1=b1\\x002b2+b3\\x00b4\\nx2=\\x004b1\\x002b2 + 4b4\\nx3= 2b1 + 3b2\\x00b3\\x003b4\\nx4= 17b1\\x0010b2+ 11b3+b4: (2.73)\\nAre the vectors x1;:::;x42Rnlinearly independent? To answer this\\nquestion, we investigate whether the column vectors\\n8\\n>><\\n>>:2\\n6641\\n\\x002\\n1\\n\\x0013\\n775;2\\n664\\x004\\n\\x002\\n0\\n43\\n775;2\\n6642\\n3\\n\\x001\\n\\x0033\\n775;2\\n66417\\n\\x0010\\n11\\n13\\n7759\\n>>=\\n>>;(2.74)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n44 Linear Algebra\\nare linearly independent. The reduced row-echelon form of the corre-\\nsponding linear equation system with coefﬁcient matrix\\nA=2\\n6641\\x004 2 17\\n\\x002\\x002 3\\x0010\\n1 0\\x001 11\\n\\x001 4\\x003 13\\n775(2.75)\\nis given as\\n2\\n6641 0 0\\x007\\n0 1 0\\x0015\\n0 0 1\\x0018\\n0 0 0 03\\n775: (2.76)\\nWe see that the corresponding linear equation system is non-trivially solv-\\nable: The last column is not a pivot column, and x4=\\x007x1\\x0015x2\\x0018x3.\\nTherefore,x1;:::;x4are linearly dependent as x4can be expressed as a\\nlinear combination of x1;:::;x3.\\n2.6 Basis and Rank\\nIn a vector space V, we are particularly interested in sets of vectors Athat\\npossess the property that any vector v2Vcan be obtained by a linear\\ncombination of vectors in A. These vectors are special vectors, and in the\\nfollowing, we will characterize them.\\n2.6.1 Generating Set and Basis\\nDeﬁnition 2.13 (Generating Set and Span) .Consider a vector space V=\\n(V;+;\\x01)and set of vectors A=fx1;:::;xkg\\x12V . If every vector v2\\nVcan be expressed as a linear combination of x1;:::;xk,Ais called a\\ngenerating set ofV. The set of all linear combinations of vectors in Ais generating set\\ncalled the span ofA. IfAspans the vector space V, we writeV= span[A] span\\norV= span[x1;:::;xk].\\nGenerating sets are sets of vectors that span vector (sub)spaces, i.e.,\\nevery vector can be represented as a linear combination of the vectors\\nin the generating set. Now, we will be more speciﬁc and characterize the\\nsmallest generating set that spans a vector (sub)space.\\nDeﬁnition 2.14 (Basis) .Consider a vector space V= (V;+;\\x01)andA\\x12\\nV. A generating set AofVis called minimal if there exists no smaller set minimal\\n~A(A\\x12V that spansV. Every linearly independent generating set of V\\nis minimal and is called a basis ofV. basis\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.6 Basis and Rank 45\\nLetV= (V;+;\\x01)be a vector space and B \\x12 V;B 6=;. Then, the\\nfollowing statements are equivalent: A basis is a minimal\\ngenerating set and a\\nmaximal linearly\\nindependent set of\\nvectors.Bis a basis of V.\\nBis a minimal generating set.\\nBis a maximal linearly independent set of vectors in V, i.e., adding any\\nother vector to this set will make it linearly dependent.\\nEvery vectorx2Vis a linear combination of vectors from B, and every\\nlinear combination is unique, i.e., with\\nx=kX\\ni=1\\x15ibi=kX\\ni=1 ibi (2.77)\\nand\\x15i; i2R,bi2Bit follows that \\x15i= i; i= 1;:::;k .\\nExample 2.16\\nInR3, the canonical/standard basis is canonical basis\\nB=8\\n<\\n:2\\n41\\n0\\n03\\n5;2\\n40\\n1\\n03\\n5;2\\n40\\n0\\n13\\n59\\n=\\n;: (2.78)\\nDifferent bases in R3are\\nB1=8\\n<\\n:2\\n41\\n0\\n03\\n5;2\\n41\\n1\\n03\\n5;2\\n41\\n1\\n13\\n59\\n=\\n;;B2=8\\n<\\n:2\\n40:5\\n0:8\\n0:43\\n5;2\\n41:8\\n0:3\\n0:33\\n5;2\\n4\\x002:2\\n\\x001:3\\n3:53\\n59\\n=\\n;:(2.79)\\nThe set\\nA=8\\n>><\\n>>:2\\n6641\\n2\\n3\\n43\\n775;2\\n6642\\n\\x001\\n0\\n23\\n775;2\\n6641\\n1\\n0\\n\\x0043\\n7759\\n>>=\\n>>;(2.80)\\nis linearly independent, but not a generating set (and no basis) of R4:\\nFor instance, the vector [1;0;0;0]>cannot be obtained by a linear com-\\nbination of elements in A.\\nRemark. Every vector space Vpossesses a basisB. The preceding exam-\\nples show that there can be many bases of a vector space V, i.e., there is\\nno unique basis. However, all bases possess the same number of elements,\\nthebasis vectors . } basis vector\\nWe only consider ﬁnite-dimensional vector spaces V. In this case, the\\ndimension ofVis the number of basis vectors of V, and we write dim(V).dimension\\nIfU\\x12Vis a subspace of V, then dim(U)6dim(V)anddim(U) =\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n46 Linear Algebra\\ndim(V)if and only if U=V. Intuitively, the dimension of a vector space\\ncan be thought of as the number of independent directions in this vector\\nspace. The dimension of a\\nvector space\\ncorresponds to the\\nnumber of its basis\\nvectors.Remark. The dimension of a vector space is not necessarily the number\\nof elements in a vector. For instance, the vector space V= span[\\x140\\n1\\x15\\n]is\\none-dimensional, although the basis vector possesses two elements. }\\nRemark. A basis of a subspace U= span[x1;:::;xm]\\x12Rncan be found\\nby executing the following steps:\\n1. Write the spanning vectors as columns of a matrix A\\n2. Determine the row-echelon form of A.\\n3. The spanning vectors associated with the pivot columns are a basis of\\nU.\\n}\\nExample 2.17 (Determining a Basis)\\nFor a vector subspace U\\x12R5, spanned by the vectors\\nx1=2\\n666641\\n2\\n\\x001\\n\\x001\\n\\x0013\\n77775;x2=2\\n666642\\n\\x001\\n1\\n2\\n\\x0023\\n77775;x3=2\\n666643\\n\\x004\\n3\\n5\\n\\x0033\\n77775;x4=2\\n66664\\x001\\n8\\n\\x005\\n\\x006\\n13\\n777752R5;(2.81)\\nwe are interested in ﬁnding out which vectors x1;:::;x4are a basis for U.\\nFor this, we need to check whether x1;:::;x4are linearly independent.\\nTherefore, we need to solve\\n4X\\ni=1\\x15ixi=0; (2.82)\\nwhich leads to a homogeneous system of equations with matrix\\n\\x02x1;x2;x3;x4\\x03=2\\n666641 2 3\\x001\\n2\\x001\\x004 8\\n\\x001 1 3\\x005\\n\\x001 2 5\\x006\\n\\x001\\x002\\x003 13\\n77775: (2.83)\\nWith the basic transformation rules for systems of linear equations, we\\nobtain the row-echelon form2\\n666641 2 3\\x001\\n2\\x001\\x004 8\\n\\x001 1 3\\x005\\n\\x001 2 5\\x006\\n\\x001\\x002\\x003 13\\n77775 \\x01\\x01\\x01 2\\n666641 2 3\\x001\\n0 1 2\\x002\\n0 0 0 1\\n0 0 0 0\\n0 0 0 03\\n77775:\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.6 Basis and Rank 47\\nSince the pivot columns indicate which set of vectors is linearly indepen-\\ndent, we see from the row-echelon form that x1;x2;x4are linearly inde-\\npendent (because the system of linear equations \\x151x1+\\x152x2+\\x154x4=0\\ncan only be solved with \\x151=\\x152=\\x154= 0). Therefore,fx1;x2;x4gis a\\nbasis ofU.\\n2.6.2 Rank\\nThe number of linearly independent columns of a matrix A2Rm\\x02n\\nequals the number of linearly independent rows and is called the rank rank\\nofAand is denoted by rk(A).\\nRemark. The rank of a matrix has some important properties:\\nrk(A) = rk(A>), i.e., the column rank equals the row rank.\\nThe columns of A2Rm\\x02nspan a subspace U\\x12Rmwith dim(U) =\\nrk(A). Later we will call this subspace the image orrange . A basis of\\nUcan be found by applying Gaussian elimination to Ato identify the\\npivot columns.\\nThe rows ofA2Rm\\x02nspan a subspace W\\x12Rnwith dim(W) =\\nrk(A). A basis ofWcan be found by applying Gaussian elimination to\\nA>.\\nFor allA2Rn\\x02nit holds thatAis regular (invertible) if and only if\\nrk(A) =n.\\nFor allA2Rm\\x02nand allb2Rmit holds that the linear equation\\nsystemAx=bcan be solved if and only if rk(A) = rk(Ajb), where\\nAjbdenotes the augmented system.\\nForA2Rm\\x02nthe subspace of solutions for Ax=0possesses dimen-\\nsionn\\x00rk(A). Later, we will call this subspace the kernel or the null kernel\\nnull space space .\\nA matrixA2Rm\\x02nhasfull rank if its rank equals the largest possible full rank\\nrank for a matrix of the same dimensions. This means that the rank of\\na full-rank matrix is the lesser of the number of rows and columns, i.e.,\\nrk(A) = min(m;n). A matrix is said to be rank deﬁcient if it does not rank deﬁcient\\nhave full rank.\\n}\\nExample 2.18 (Rank)\\nA=2\\n41 0 1\\n0 1 1\\n0 0 03\\n5.\\nAhas two linearly independent rows/columns so that rk(A) = 2 .\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n48 Linear Algebra\\nA=2\\n41 2 1\\n\\x002\\x003 1\\n3 5 03\\n5:\\nWe use Gaussian elimination to determine the rank:\\n2\\n41 2 1\\n\\x002\\x003 1\\n3 5 03\\n5 \\x01\\x01\\x01 2\\n41 2 1\\n0 1 3\\n0 0 03\\n5: (2.84)\\nHere, we see that the number of linearly independent rows and columns\\nis 2, such that rk(A) = 2 .\\n2.7 Linear Mappings\\nIn the following, we will study mappings on vector spaces that preserve\\ntheir structure, which will allow us to deﬁne the concept of a coordinate.\\nIn the beginning of the chapter, we said that vectors are objects that can be\\nadded together and multiplied by a scalar, and the resulting object is still\\na vector. We wish to preserve this property when applying the mapping:\\nConsider two real vector spaces V;W . A mapping \\x08 :V!Wpreserves\\nthe structure of the vector space if\\n\\x08(x+y) = \\x08(x) + \\x08(y) (2.85)\\n\\x08(\\x15x) =\\x15\\x08(x) (2.86)\\nfor allx;y2Vand\\x152R. We can summarize this in the following\\ndeﬁnition:\\nDeﬁnition 2.15 (Linear Mapping) .For vector spaces V;W , a mapping\\n\\x08 :V!Wis called a linear mapping (orvector space homomorphism / linear mapping\\nvector space\\nhomomorphismlinear transformation ) if\\nlinear\\ntransformation8x;y2V8\\x15; 2R: \\x08(\\x15x+ y) =\\x15\\x08(x) + \\x08(y): (2.87)\\nIt turns out that we can represent linear mappings as matrices (Sec-\\ntion 2.7.1). Recall that we can also collect a set of vectors as columns of a\\nmatrix. When working with matrices, we have to keep in mind what the\\nmatrix represents: a linear mapping or a collection of vectors. We will see\\nmore about linear mappings in Chapter 4. Before we continue, we will\\nbrieﬂy introduce special mappings.\\nDeﬁnition 2.16 (Injective, Surjective, Bijective) .Consider a mapping \\x08 :\\nV!W , whereV;Wcan be arbitrary sets. Then \\x08is called\\ninjective\\nInjective if8x;y2V: \\x08(x) = \\x08(y) =)x=y.surjective\\nSurjective if\\x08(V) =W.bijective\\nBijective if it is injective and surjective.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.7 Linear Mappings 49\\nIf\\x08is surjective, then every element in Wcan be “reached” from V\\nusing \\x08. A bijective \\x08can be “undone”, i.e., there exists a mapping \\t :\\nW!V so that \\t\\x0e\\x08(x) =x. This mapping \\tis then called the inverse\\nof\\x08and normally denoted by \\x08\\x001.\\nWith these deﬁnitions, we introduce the following special cases of linear\\nmappings between vector spaces VandW:\\nisomorphism\\nIsomorphism: \\x08 :V!Wlinear and bijective endomorphism\\nEndomorphism: \\x08 :V!Vlinear automorphism\\nAutomorphism: \\x08 :V!Vlinear and bijective\\nWe deﬁne idV:V!V,x7!xas the identity mapping oridentity identity mapping\\nidentity\\nautomorphismautomorphism inV.\\nExample 2.19 (Homomorphism)\\nThe mapping \\x08 :R2!C;\\x08(x) =x1+ix2, is a homomorphism:\\n\\x08\\x12\\x14x1\\nx2\\x15\\n+\\x14y1\\ny2\\x15\\x13\\n= (x1+y1) +i(x2+y2) =x1+ix2+y1+iy2\\n= \\x08\\x12\\x14x1\\nx2\\x15\\x13\\n+ \\x08\\x12\\x14y1\\ny2\\x15\\x13\\n\\x08\\x12\\n\\x15\\x14x1\\nx2\\x15\\x13\\n=\\x15x1+\\x15ix2=\\x15(x1+ix2) =\\x15\\x08\\x12\\x14x1\\nx2\\x15\\x13\\n:\\n(2.88)\\nThis also justiﬁes why complex numbers can be represented as tuples in\\nR2: There is a bijective linear mapping that converts the elementwise addi-\\ntion of tuples in R2into the set of complex numbers with the correspond-\\ning addition. Note that we only showed linearity, but not the bijection.\\nTheorem 2.17 (Theorem 3.59 in Axler (2015)) .Finite-dimensional vector\\nspacesVandWare isomorphic if and only if dim(V) = dim(W).\\nTheorem 2.17 states that there exists a linear, bijective mapping be-\\ntween two vector spaces of the same dimension. Intuitively, this means\\nthat vector spaces of the same dimension are kind of the same thing, as\\nthey can be transformed into each other without incurring any loss.\\nTheorem 2.17 also gives us the justiﬁcation to treat Rm\\x02n(the vector\\nspace ofm\\x02n-matrices) and Rmn(the vector space of vectors of length\\nmn) the same, as their dimensions are mn, and there exists a linear, bi-\\njective mapping that transforms one into the other.\\nRemark. Consider vector spaces V;W;X . Then:\\nFor linear mappings \\x08 :V!Wand\\t :W!X, the mapping\\n\\t\\x0e\\x08 :V!Xis also linear.\\nIf\\x08 :V!Wis an isomorphism, then \\x08\\x001:W!Vis an isomor-\\nphism, too.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n50 Linear Algebra\\nFigure 2.8 Two\\ndifferent coordinate\\nsystems deﬁned by\\ntwo sets of basis\\nvectors. A vector x\\nhas different\\ncoordinate\\nrepresentations\\ndepending on which\\ncoordinate system is\\nchosen.x x\\ne1e2\\nb1b2\\nIf\\x08 :V!W;\\t :V!Ware linear, then \\x08 + \\t and\\x15\\x08; \\x152R, are\\nlinear, too.\\n}\\n2.7.1 Matrix Representation of Linear Mappings\\nAnyn-dimensional vector space is isomorphic to Rn(Theorem 2.17). We\\nconsider a basisfb1;:::;bngof ann-dimensional vector space V. In the\\nfollowing, the order of the basis vectors will be important. Therefore, we\\nwrite\\nB= (b1;:::;bn) (2.89)\\nand call this n-tuple an ordered basis ofV. ordered basis\\nRemark (Notation) .We are at the point where notation gets a bit tricky.\\nTherefore, we summarize some parts here. B= (b1;:::;bn)is an ordered\\nbasis,B=fb1;:::;bngis an (unordered) basis, and B= [b1;:::;bn]is a\\nmatrix whose columns are the vectors b1;:::;bn. }\\nDeﬁnition 2.18 (Coordinates) .Consider a vector space Vand an ordered\\nbasisB= (b1;:::;bn)ofV. For anyx2Vwe obtain a unique represen-\\ntation (linear combination)\\nx=\\x0b1b1+:::+\\x0bnbn (2.90)\\nofxwith respect to B. Then\\x0b1;:::;\\x0bnare the coordinates ofxwith coordinate\\nrespect toB, and the vector\\n\\x0b=2\\n64\\x0b1\\n...\\n\\x0bn3\\n752Rn(2.91)\\nis the coordinate vector /coordinate representation ofxwith respect to the coordinate vector\\ncoordinate\\nrepresentationordered basis B.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.7 Linear Mappings 51\\nA basis effectively deﬁnes a coordinate system. We are familiar with the\\nCartesian coordinate system in two dimensions, which is spanned by the\\ncanonical basis vectors e1;e2. In this coordinate system, a vector x2R2\\nhas a representation that tells us how to linearly combine e1ande2to\\nobtainx. However, any basis of R2deﬁnes a valid coordinate system,\\nand the same vector xfrom before may have a different coordinate rep-\\nresentation in the (b1;b2)basis. In Figure 2.8, the coordinates of xwith\\nrespect to the standard basis (e1;e2)is[2;2]>. However, with respect to\\nthe basis (b1;b2)the same vector xis represented as [1:09;0:72]>, i.e.,\\nx= 1:09b1+ 0:72b2. In the following sections, we will discover how to\\nobtain this representation.\\nExample 2.20\\nLet us have a look at a geometric vector x2R2with coordinates [2;3]>Figure 2.9\\nDifferent coordinate\\nrepresentations of a\\nvectorx, depending\\non the choice of\\nbasis.\\ne1e2b2\\nb1x=\\x001\\n2b1+5\\n2b2x= 2e1+ 3e2with respect to the standard basis (e1;e2)ofR2. This means, we can write\\nx= 2e1+ 3e2. However, we do not have to choose the standard basis to\\nrepresent this vector. If we use the basis vectors b1= [1;\\x001]>;b2= [1;1]>\\nwe will obtain the coordinates1\\n2[\\x001;5]>to represent the same vector with\\nrespect to (b1;b2)(see Figure 2.9).\\nRemark. For ann-dimensional vector space Vand an ordered basis B\\nofV, the mapping \\x08 :Rn!V,\\x08(ei) =bi,i= 1;:::;n; is linear\\n(and because of Theorem 2.17 an isomorphism), where (e1;:::;en)is\\nthe standard basis of Rn.\\n}\\nNow we are ready to make an explicit connection between matrices and\\nlinear mappings between ﬁnite-dimensional vector spaces.\\nDeﬁnition 2.19 (Transformation Matrix) .Consider vector spaces V;W\\nwith corresponding (ordered) bases B= (b1;:::;bn)andC= (c1;:::;cm).\\nMoreover, we consider a linear mapping \\x08 :V!W. Forj2f1;:::;ng,\\n\\x08(bj) =\\x0b1jc1+\\x01\\x01\\x01+\\x0bmjcm=mX\\ni=1\\x0bijci (2.92)\\nis the unique representation of \\x08(bj)with respect to C. Then, we call the\\nm\\x02n-matrixA\\x08, whose elements are given by\\nA\\x08(i;j) =\\x0bij; (2.93)\\nthetransformation matrix of\\x08(with respect to the ordered bases BofV transformation\\nmatrix andCofW).\\nThe coordinates of \\x08(bj)with respect to the ordered basis CofW\\nare thej-th column of A\\x08. Consider (ﬁnite-dimensional) vector spaces\\nV;W with ordered bases B;C and a linear mapping \\x08 :V!Wwith\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n52 Linear Algebra\\ntransformation matrix A\\x08. If^xis the coordinate vector of x2Vwith\\nrespect toBand^ythe coordinate vector of y= \\x08(x)2Wwith respect\\ntoC, then\\n^y=A\\x08^x: (2.94)\\nThis means that the transformation matrix can be used to map coordinates\\nwith respect to an ordered basis in Vto coordinates with respect to an\\nordered basis in W.\\nExample 2.21 (Transformation Matrix)\\nConsider a homomorphism \\x08 :V!Wand ordered bases B=\\n(b1;:::;b3)ofVandC= (c1;:::;c4)ofW. With\\n\\x08(b1) =c1\\x00c2+ 3c3\\x00c4\\n\\x08(b2) = 2c1+c2+ 7c3+ 2c4\\n\\x08(b3) = 3c2+c3+ 4c4(2.95)\\nthe transformation matrix A\\x08with respect to BandCsatisﬁes \\x08(bk) =P4\\ni=1\\x0bikcifork= 1;:::; 3and is given as\\nA\\x08= [\\x0b1;\\x0b2;\\x0b3] =2\\n6641 2 0\\n\\x001 1 3\\n3 7 1\\n\\x001 2 43\\n775; (2.96)\\nwhere the\\x0bj; j= 1;2;3;are the coordinate vectors of \\x08(bj)with respect\\ntoC.\\nExample 2.22 (Linear Transformations of Vectors)\\nFigure 2.10 Three\\nexamples of linear\\ntransformations of\\nthe vectors shown\\nas dots in (a);\\n(b) Rotation by 45\\x0e;\\n(c) Stretching of the\\nhorizontal\\ncoordinates by 2;\\n(d) Combination of\\nreﬂection, rotation\\nand stretching.\\n(a) Original data.\\n (b) Rotation by 45\\x0e.\\n(c) Stretch along the\\nhorizontal axis.\\n(d) General linear\\nmapping.\\nWe consider three linear transformations of a set of vectors in R2with\\nthe transformation matrices\\nA1=\\x14cos(\\x19\\n4)\\x00sin(\\x19\\n4)\\nsin(\\x19\\n4) cos(\\x19\\n4)\\x15\\n;A2=\\x142 0\\n0 1\\x15\\n;A3=1\\n2\\x143\\x001\\n1\\x001\\x15\\n:(2.97)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.7 Linear Mappings 53\\nFigure 2.10 gives three examples of linear transformations of a set of vec-\\ntors. Figure 2.10(a) shows 400vectors in R2, each of which is represented\\nby a dot at the corresponding (x1;x2)-coordinates. The vectors are ar-\\nranged in a square. When we use matrix A1in (2.97) to linearly transform\\neach of these vectors, we obtain the rotated square in Figure 2.10(b). If we\\napply the linear mapping represented by A2, we obtain the rectangle in\\nFigure 2.10(c) where each x1-coordinate is stretched by 2. Figure 2.10(d)\\nshows the original square from Figure 2.10(a) when linearly transformed\\nusingA3, which is a combination of a reﬂection, a rotation, and a stretch.\\n2.7.2 Basis Change\\nIn the following, we will have a closer look at how transformation matrices\\nof a linear mapping \\x08 :V!Wchange if we change the bases in Vand\\nW. Consider two ordered bases\\nB= (b1;:::;bn);~B= (~b1;:::; ~bn) (2.98)\\nofVand two ordered bases\\nC= (c1;:::;cm);~C= (~c1;:::; ~cm) (2.99)\\nofW. Moreover,A\\x082Rm\\x02nis the transformation matrix of the linear\\nmapping \\x08 :V!Wwith respect to the bases BandC, and ~A\\x082Rm\\x02n\\nis the corresponding transformation mapping with respect to ~Band ~C.\\nIn the following, we will investigate how Aand~Aare related, i.e., how/\\nwhether we can transform A\\x08into ~A\\x08if we choose to perform a basis\\nchange from B;C to~B;~C.\\nRemark. We effectively get different coordinate representations of the\\nidentity mapping idV. In the context of Figure 2.9, this would mean to\\nmap coordinates with respect to (e1;e2)onto coordinates with respect to\\n(b1;b2)without changing the vector x. By changing the basis and corre-\\nspondingly the representation of vectors, the transformation matrix with\\nrespect to this new basis can have a particularly simple form that allows\\nfor straightforward computation. }\\nExample 2.23 (Basis Change)\\nConsider a transformation matrix\\nA=\\x142 1\\n1 2\\x15\\n(2.100)\\nwith respect to the canonical basis in R2. If we deﬁne a new basis\\nB= (\\x141\\n1\\x15\\n;\\x141\\n\\x001\\x15\\n) (2.101)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n54 Linear Algebra\\nwe obtain a diagonal transformation matrix\\n~A=\\x143 0\\n0 1\\x15\\n(2.102)\\nwith respect to B, which is easier to work with than A.\\nIn the following, we will look at mappings that transform coordinate\\nvectors with respect to one basis into coordinate vectors with respect to\\na different basis. We will state our main result ﬁrst and then provide an\\nexplanation.\\nTheorem 2.20 (Basis Change) .For a linear mapping \\x08 :V!W, ordered\\nbases\\nB= (b1;:::;bn);~B= (~b1;:::; ~bn) (2.103)\\nofVand\\nC= (c1;:::;cm);~C= (~c1;:::; ~cm) (2.104)\\nofW, and a transformation matrix A\\x08of\\x08with respect to BandC, the\\ncorresponding transformation matrix ~A\\x08with respect to the bases ~Band~C\\nis given as\\n~A\\x08=T\\x001A\\x08S: (2.105)\\nHere,S2Rn\\x02nis the transformation matrix of idVthat maps coordinates\\nwith respect to ~Bonto coordinates with respect to B, andT2Rm\\x02mis the\\ntransformation matrix of idWthat maps coordinates with respect to ~Conto\\ncoordinates with respect to C.\\nProof Following Drumm and Weil (2001), we can write the vectors of\\nthe new basis ~BofVas a linear combination of the basis vectors of B,\\nsuch that\\n~bj=s1jb1+\\x01\\x01\\x01+snjbn=nX\\ni=1sijbi; j = 1;:::;n: (2.106)\\nSimilarly, we write the new basis vectors ~CofWas a linear combination\\nof the basis vectors of C, which yields\\n~ck=t1kc1+\\x01\\x01\\x01+tmkcm=mX\\nl=1tlkcl; k = 1;:::;m: (2.107)\\nWe deﬁneS= ((sij))2Rn\\x02nas the transformation matrix that maps\\ncoordinates with respect to ~Bonto coordinates with respect to Band\\nT= ((tlk))2Rm\\x02mas the transformation matrix that maps coordinates\\nwith respect to ~Conto coordinates with respect to C. In particular, the jth\\ncolumn ofSis the coordinate representation of ~bjwith respect to Band\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.7 Linear Mappings 55\\nthekth column ofTis the coordinate representation of ~ckwith respect to\\nC. Note that both SandTare regular.\\nWe are going to look at \\x08(~bj)from two perspectives. First, applying the\\nmapping \\x08, we get that for all j= 1;:::;n\\n\\x08(~bj) =mX\\nk=1~akj~ck|{z}\\n2W(2.107)=mX\\nk=1~akjmX\\nl=1tlkcl=mX\\nl=1 mX\\nk=1tlk~akj!\\ncl;(2.108)\\nwhere we ﬁrst expressed the new basis vectors ~ck2Was linear com-\\nbinations of the basis vectors cl2Wand then swapped the order of\\nsummation.\\nAlternatively, when we express the ~bj2Vas linear combinations of\\nbj2V, we arrive at\\n\\x08(~bj)(2.106)= \\x08 nX\\ni=1sijbi!\\n=nX\\ni=1sij\\x08(bi) =nX\\ni=1sijmX\\nl=1alicl(2.109a)\\n=mX\\nl=1 nX\\ni=1alisij!\\ncl; j = 1;:::;n; (2.109b)\\nwhere we exploited the linearity of \\x08. Comparing (2.108) and (2.109b),\\nit follows for all j= 1;:::;n andl= 1;:::;m that\\nmX\\nk=1tlk~akj=nX\\ni=1alisij (2.110)\\nand, therefore,\\nT~A\\x08=A\\x08S2Rm\\x02n; (2.111)\\nsuch that\\n~A\\x08=T\\x001A\\x08S; (2.112)\\nwhich proves Theorem 2.20.\\nTheorem 2.20 tells us that with a basis change in V(Bis replaced with\\n~B) andW(Cis replaced with ~C), the transformation matrix A\\x08of a\\nlinear mapping \\x08 :V!Wis replaced by an equivalent matrix ~A\\x08with\\n~A\\x08=T\\x001A\\x08S: (2.113)\\nFigure 2.11 illustrates this relation: Consider a homomorphism \\x08 :V!\\nWand ordered bases B;~BofVandC;~CofW. The mapping \\x08CBis an\\ninstantiation of \\x08and maps basis vectors of Bonto linear combinations\\nof basis vectors of C. Assume that we know the transformation matrix A\\x08\\nof\\x08CBwith respect to the ordered bases B;C . When we perform a basis\\nchange from Bto~BinVand fromCto~CinW, we can determine the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n56 Linear Algebra\\nFigure 2.11 For a\\nhomomorphism\\n\\x08 :V!Wand\\nordered bases B;~B\\nofVandC;~CofW\\n(marked in blue),\\nwe can express the\\nmapping \\x08~C~Bwith\\nrespect to the bases\\n~B;~Cequivalently as\\na composition of the\\nhomomorphisms\\n\\x08~C~B=\\n\\x04~CC\\x0e\\x08CB\\x0e\\tB~B\\nwith respect to the\\nbases in the\\nsubscripts. The\\ncorresponding\\ntransformation\\nmatrices are in red.V W\\nB\\n~B ~CC\\x08\\n\\x08CB\\n\\x08~C~B\\tB~B \\x04C~C S T\\n~A\\x08A\\x08V W\\nB\\n~B ~CC\\x08\\n\\x08CB\\n\\x08~C~B\\tB~B \\x04~CC= \\x04\\x001\\nC~CST\\x001\\n~A\\x08A\\x08Vector spaces\\nOrdered bases\\ncorresponding transformation matrix ~A\\x08as follows: First, we ﬁnd the ma-\\ntrix representation of the linear mapping \\tB~B:V!Vthat maps coordi-\\nnates with respect to the new basis ~Bonto the (unique) coordinates with\\nrespect to the “old” basis B(inV). Then, we use the transformation ma-\\ntrixA\\x08of\\x08CB:V!Wto map these coordinates onto the coordinates\\nwith respect to CinW. Finally, we use a linear mapping \\x04~CC:W!W\\nto map the coordinates with respect to Conto coordinates with respect to\\n~C. Therefore, we can express the linear mapping \\x08~C~Bas a composition of\\nlinear mappings that involve the “old” basis:\\n\\x08~C~B= \\x04 ~CC\\x0e\\x08CB\\x0e\\tB~B= \\x04\\x001\\nC~C\\x0e\\x08CB\\x0e\\tB~B: (2.114)\\nConcretely, we use \\tB~B= idVand\\x04C~C= idW, i.e., the identity mappings\\nthat map vectors onto themselves, but with respect to a different basis.\\nDeﬁnition 2.21 (Equivalence) .Two matrices A;~A2Rm\\x02nareequivalent equivalent\\nif there exist regular matrices S2Rn\\x02nandT2Rm\\x02m, such that\\n~A=T\\x001AS.\\nDeﬁnition 2.22 (Similarity) .Two matrices A;~A2Rn\\x02naresimilar if similar\\nthere exists a regular matrix S2Rn\\x02nwith ~A=S\\x001AS\\nRemark. Similar matrices are always equivalent. However, equivalent ma-\\ntrices are not necessarily similar. }\\nRemark. Consider vector spaces V;W;X . From the remark that follows\\nTheorem 2.17, we already know that for linear mappings \\x08 :V!W\\nand\\t :W!Xthe mapping \\t\\x0e\\x08 :V!Xis also linear. With\\ntransformation matrices A\\x08andA\\tof the corresponding mappings, the\\noverall transformation matrix is A\\t\\x0e\\x08=A\\tA\\x08. }\\nIn light of this remark, we can look at basis changes from the perspec-\\ntive of composing linear mappings:\\nA\\x08is the transformation matrix of a linear mapping \\x08CB:V!W\\nwith respect to the bases B;C .\\n~A\\x08is the transformation matrix of the linear mapping \\x08~C~B:V!W\\nwith respect to the bases ~B;~C.\\nSis the transformation matrix of a linear mapping \\tB~B:V!V\\n(automorphism) that represents ~Bin terms ofB. Normally, \\t = idVis\\nthe identity mapping in V.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.7 Linear Mappings 57\\nTis the transformation matrix of a linear mapping \\x04C~C:W!W\\n(automorphism) that represents ~Cin terms ofC. Normally, \\x04 = idWis\\nthe identity mapping in W.\\nIf we (informally) write down the transformations just in terms of bases,\\nthenA\\x08:B!C,~A\\x08:~B!~C,S:~B!B,T:~C!Cand\\nT\\x001:C!~C, and\\n~B!~C=~B!B!C!~C (2.115)\\n~A\\x08=T\\x001A\\x08S: (2.116)\\nNote that the execution order in (2.116) is from right to left because vec-\\ntors are multiplied at the right-hand side so that x7!Sx7!A\\x08(Sx)7!\\nT\\x001\\x00A\\x08(Sx)\\x01=~A\\x08x.\\nExample 2.24 (Basis Change)\\nConsider a linear mapping \\x08 :R3!R4whose transformation matrix is\\nA\\x08=2\\n6641 2 0\\n\\x001 1 3\\n3 7 1\\n\\x001 2 43\\n775(2.117)\\nwith respect to the standard bases\\nB= (2\\n41\\n0\\n03\\n5;2\\n40\\n1\\n03\\n5;2\\n40\\n0\\n13\\n5); C = (2\\n6641\\n0\\n0\\n03\\n775;2\\n6640\\n1\\n0\\n03\\n775;2\\n6640\\n0\\n1\\n03\\n775;2\\n6640\\n0\\n0\\n13\\n775): (2.118)\\nWe seek the transformation matrix ~A\\x08of\\x08with respect to the new bases\\n~B= (2\\n41\\n1\\n03\\n5;2\\n40\\n1\\n13\\n5;2\\n41\\n0\\n13\\n5)2R3;~C= (2\\n6641\\n1\\n0\\n03\\n775;2\\n6641\\n0\\n1\\n03\\n775;2\\n6640\\n1\\n1\\n03\\n775;2\\n6641\\n0\\n0\\n13\\n775):(2.119)\\nThen,\\nS=2\\n41 0 1\\n1 1 0\\n0 1 13\\n5;T=2\\n6641 1 0 1\\n1 0 1 0\\n0 1 1 0\\n0 0 0 13\\n775; (2.120)\\nwhere the ith column of Sis the coordinate representation of ~biin\\nterms of the basis vectors of B. SinceBis the standard basis, the co-\\nordinate representation is straightforward to ﬁnd. For a general basis B,\\nwe would need to solve a linear equation system to ﬁnd the \\x15isuch that\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n58 Linear Algebra\\nP3\\ni=1\\x15ibi=~bj,j= 1;:::; 3. Similarly, the jth column ofTis the coordi-\\nnate representation of ~cjin terms of the basis vectors of C.\\nTherefore, we obtain\\n~A\\x08=T\\x001A\\x08S=1\\n22\\n6641 1\\x001\\x001\\n1\\x001 1\\x001\\n\\x001 1 1 1\\n0 0 0 23\\n7752\\n6643 2 1\\n0 4 2\\n10 8 4\\n1 6 33\\n775(2.121a)\\n=2\\n664\\x004\\x004\\x002\\n6 0 0\\n4 8 4\\n1 6 33\\n775: (2.121b)\\nIn Chapter 4, we will be able to exploit the concept of a basis change\\nto ﬁnd a basis with respect to which the transformation matrix of an en-\\ndomorphism has a particularly simple (diagonal) form. In Chapter 10, we\\nwill look at a data compression problem and ﬁnd a convenient basis onto\\nwhich we can project the data while minimizing the compression loss.\\n2.7.3 Image and Kernel\\nThe image and kernel of a linear mapping are vector subspaces with cer-\\ntain important properties. In the following, we will characterize them\\nmore carefully.\\nDeﬁnition 2.23 (Image and Kernel) .\\nFor\\x08 :V!W, we deﬁne the kernel /null space kernel\\nnull space\\nker(\\x08) := \\x08\\x001(0W) =fv2V: \\x08(v) =0Wg (2.122)\\nand the image /range image\\nrange\\nIm(\\x08) := \\x08( V) =fw2Wj9v2V: \\x08(v) =wg: (2.123)\\nWe also call VandWthedomain andcodomain of\\x08, respectively. domain\\ncodomain\\nIntuitively, the kernel is the set of vectors v2Vthat\\x08maps onto the\\nneutral element 0W2W. The image is the set of vectors w2Wthat\\ncan be “reached” by \\x08from any vector in V. An illustration is given in\\nFigure 2.12.\\nRemark. Consider a linear mapping \\x08 :V!W, whereV;W are vector\\nspaces.\\nIt always holds that \\x08(0V) =0Wand, therefore, 0V2ker(\\x08) . In\\nparticular, the null space is never empty.\\nIm(\\x08)\\x12Wis a subspace of W, and ker(\\x08)\\x12Vis a subspace of V.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.7 Linear Mappings 59\\nFigure 2.12 Kernel\\nand image of a\\nlinear mapping\\n\\x08 :V!W.\\nIm(\\x08)\\n0Wker(\\x08)\\n0V\\x08 :V!WV W\\n\\x08is injective (one-to-one) if and only if ker(\\x08) =f0g.\\n}\\nRemark (Null Space and Column Space) .Let us consider A2Rm\\x02nand\\na linear mapping \\x08 :Rn!Rm;x7!Ax.\\nForA= [a1;:::;an], whereaiare the columns of A, we obtain\\nIm(\\x08) =fAx:x2Rng=(nX\\ni=1xiai:x1;:::;xn2R)\\n(2.124a)\\n= span[a1;:::;an]\\x12Rm; (2.124b)\\ni.e., the image is the span of the columns of A, also called the column column space\\nspace . Therefore, the column space (image) is a subspace of Rm, where\\nmis the “height” of the matrix.\\nrk(A) = dim(Im(\\x08)) .\\nThe kernel/null space ker(\\x08) is the general solution to the homoge-\\nneous system of linear equations Ax=0and captures all possible\\nlinear combinations of the elements in Rnthat produce 02Rm.\\nThe kernel is a subspace of Rn, wherenis the “width” of the matrix.\\nThe kernel focuses on the relationship among the columns, and we can\\nuse it to determine whether/how we can express a column as a linear\\ncombination of other columns.\\n}\\nExample 2.25 (Image and Kernel of a Linear Mapping)\\nThe mapping\\n\\x08 :R4!R2;2\\n664x1\\nx2\\nx3\\nx43\\n7757!\\x141 2\\x001 0\\n1 0 0 1\\x152\\n664x1\\nx2\\nx3\\nx43\\n775=\\x14x1+ 2x2\\x00x3\\nx1+x4\\x15\\n(2.125a)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n60 Linear Algebra\\n=x1\\x141\\n1\\x15\\n+x2\\x142\\n0\\x15\\n+x3\\x14\\x001\\n0\\x15\\n+x4\\x140\\n1\\x15\\n(2.125b)\\nis linear. To determine Im(\\x08) , we can take the span of the columns of the\\ntransformation matrix and obtain\\nIm(\\x08) = span[\\x141\\n1\\x15\\n;\\x142\\n0\\x15\\n;\\x14\\x001\\n0\\x15\\n;\\x140\\n1\\x15\\n]: (2.126)\\nTo compute the kernel (null space) of \\x08, we need to solve Ax=0, i.e.,\\nwe need to solve a homogeneous equation system. To do this, we use\\nGaussian elimination to transform Ainto reduced row-echelon form:\\n\\x141 2\\x001 0\\n1 0 0 1\\x15\\n \\x01\\x01\\x01 \\x141 0 0 1\\n0 1\\x001\\n2\\x001\\n2\\x15\\n: (2.127)\\nThis matrix is in reduced row-echelon form, and we can use the Minus-\\n1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively,\\nwe can express the non-pivot columns (columns 3and4) as linear com-\\nbinations of the pivot columns (columns 1and2). The third column a3is\\nequivalent to\\x001\\n2times the second column a2. Therefore, 0=a3+1\\n2a2. In\\nthe same way, we see that a4=a1\\x001\\n2a2and, therefore, 0=a1\\x001\\n2a2\\x00a4.\\nOverall, this gives us the kernel (null space) as\\nker(\\x08) = span[2\\n6640\\n1\\n2\\n1\\n03\\n775;2\\n664\\x001\\n1\\n2\\n0\\n13\\n775]: (2.128)\\nrank-nullity\\ntheorem Theorem 2.24 (Rank-Nullity Theorem) .For vector spaces V;W and a lin-\\near mapping \\x08 :V!Wit holds that\\ndim(ker(\\x08)) + dim(Im(\\x08)) = dim( V): (2.129)\\nThe rank-nullity theorem is also referred to as the fundamental theorem fundamental\\ntheorem of linear\\nmappingsof linear mappings (Axler, 2015, theorem 3.22). The following are direct\\nconsequences of Theorem 2.24:\\nIfdim(Im(\\x08)) <dim(V), then ker(\\x08) is non-trivial, i.e., the kernel\\ncontains more than 0Vanddim(ker(\\x08))>1.\\nIfA\\x08is the transformation matrix of \\x08with respect to an ordered basis\\nanddim(Im(\\x08)) <dim(V), then the system of linear equations A\\x08x=\\n0has inﬁnitely many solutions.\\nIfdim(V) = dim(W), then the following three-way equivalence holds:\\n–\\x08is injective\\n–\\x08is surjective\\n–\\x08is bijective\\nsince Im(\\x08)\\x12W.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.8 Afﬁne Spaces 61\\n2.8 Afﬁne Spaces\\nIn the following, we will have a closer look at spaces that are offset from\\nthe origin, i.e., spaces that are no longer vector subspaces. Moreover, we\\nwill brieﬂy discuss properties of mappings between these afﬁne spaces,\\nwhich resemble linear mappings.\\nRemark. In the machine learning literature, the distinction between linear\\nand afﬁne is sometimes not clear so that we can ﬁnd references to afﬁne\\nspaces/mappings as linear spaces/mappings. }\\n2.8.1 Afﬁne Subspaces\\nDeﬁnition 2.25 (Afﬁne Subspace) .LetVbe a vector space, x02Vand\\nU\\x12Va subspace. Then the subset\\nL=x0+U:=fx0+u:u2Ug (2.130a)\\n=fv2Vj9u2U:v=x0+ug\\x12V (2.130b)\\nis called afﬁne subspace orlinear manifold ofV.Uis called direction or afﬁne subspace\\nlinear manifold\\ndirectiondirection space , andx0is called support point . In Chapter 12, we refer to\\ndirection space\\nsupport pointsuch a subspace as a hyperplane .\\nhyperplaneNote that the deﬁnition of an afﬁne subspace excludes 0ifx0=2U.\\nTherefore, an afﬁne subspace is not a (linear) subspace (vector subspace)\\nofVforx0=2U.\\nExamples of afﬁne subspaces are points, lines, and planes in R3, which\\ndo not (necessarily) go through the origin.\\nRemark. Consider two afﬁne subspaces L=x0+Uand~L=~x0+~Uof a\\nvector space V. Then,L\\x12~Lif and only if U\\x12~Uandx0\\x00~x02~U.\\nAfﬁne subspaces are often described by parameters : Consider a k-dimen-\\nsional afﬁne space L=x0+UofV. If(b1;:::;bk)is an ordered basis of\\nU, then every element x2Lcan be uniquely described as\\nx=x0+\\x151b1+:::+\\x15kbk; (2.131)\\nwhere\\x151;:::;\\x15k2R. This representation is called parametric equation parametric equation\\nofLwith directional vectors b1;:::;bkandparameters\\x151;:::;\\x15k.} parameters\\nExample 2.26 (Afﬁne Subspaces)\\nOne-dimensional afﬁne subspaces are called lines and can be written line\\nasy=x0+\\x15b1, where\\x152RandU= span[b1]\\x12Rnis a one-\\ndimensional subspace of Rn. This means that a line is deﬁned by a sup-\\nport pointx0and a vectorb1that deﬁnes the direction. See Figure 2.13\\nfor an illustration.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n62 Linear Algebra\\nTwo-dimensional afﬁne subspaces of Rnare called planes . The para- plane\\nmetric equation for planes is y=x0+\\x151b1+\\x152b2, where\\x151;\\x1522R\\nandU= span[b1;b2]\\x12Rn. This means that a plane is deﬁned by a\\nsupport point x0and two linearly independent vectors b1;b2that span\\nthe direction space.\\nInRn, the (n\\x001)-dimensional afﬁne subspaces are called hyperplanes , hyperplane\\nand the corresponding parametric equation is y=x0+Pn\\x001\\ni=1\\x15ibi,\\nwhereb1;:::;bn\\x001form a basis of an (n\\x001)-dimensional subspace\\nUofRn. This means that a hyperplane is deﬁned by a support point\\nx0and(n\\x001)linearly independent vectors b1;:::;bn\\x001that span the\\ndirection space. In R2, a line is also a hyperplane. In R3, a plane is also\\na hyperplane.\\nFigure 2.13 Lines\\nare afﬁne subspaces.\\nVectorsyon a line\\nx0+\\x15b1lie in an\\nafﬁne subspace L\\nwith support point\\nx0and direction b1.\\n0x0\\nb1yL=x0+\\x15b1\\nRemark (Inhomogeneous systems of linear equations and afﬁne subspaces) .\\nForA2Rm\\x02nandx2Rm, the solution of the system of linear equa-\\ntionsA\\x15 =xis either the empty set or an afﬁne subspace of Rnof\\ndimensionn\\x00rk(A). In particular, the solution of the linear equation\\n\\x151b1+:::+\\x15nbn=x, where (\\x151;:::;\\x15n)6= (0;:::; 0), is a hyperplane\\ninRn.\\nInRn, everyk-dimensional afﬁne subspace is the solution of an inho-\\nmogeneous system of linear equations Ax=b, whereA2Rm\\x02n;b2\\nRmandrk(A) =n\\x00k. Recall that for homogeneous equation systems\\nAx=0the solution was a vector subspace, which we can also think of\\nas a special afﬁne space with support point x0=0. }\\n2.8.2 Afﬁne Mappings\\nSimilar to linear mappings between vector spaces, which we discussed\\nin Section 2.7, we can deﬁne afﬁne mappings between two afﬁne spaces.\\nLinear and afﬁne mappings are closely related. Therefore, many properties\\nthat we already know from linear mappings, e.g., that the composition of\\nlinear mappings is a linear mapping, also hold for afﬁne mappings.\\nDeﬁnition 2.26 (Afﬁne Mapping) .For two vector spaces V;W , a linear\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n2.9 Further Reading 63\\nmapping \\x08 :V!W, anda2W, the mapping\\n\\x1e:V!W (2.132)\\nx7!a+ \\x08(x) (2.133)\\nis an afﬁne mapping fromVtoW. The vectorais called the translation afﬁne mapping\\ntranslation vector vector of\\x1e.\\nEvery afﬁne mapping \\x1e:V!Wis also the composition of a linear\\nmapping \\x08 :V!Wand a translation \\x1c:W!WinW, such that\\n\\x1e=\\x1c\\x0e\\x08. The mappings \\x08and\\x1care uniquely determined.\\nThe composition \\x1e0\\x0e\\x1eof afﬁne mappings \\x1e:V!W,\\x1e0:W!Xis\\nafﬁne.\\nAfﬁne mappings keep the geometric structure invariant. They also pre-\\nserve the dimension and parallelism.\\n2.9 Further Reading\\nThere are many resources for learning linear algebra, including the text-\\nbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and\\nMehrmann (2015). There are also several online resources that we men-\\ntioned in the introduction to this chapter. We only covered Gaussian elim-\\nination here, but there are many other approaches for solving systems of\\nlinear equations, and we refer to numerical linear algebra textbooks by\\nStoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and\\nJohnson (2013) for an in-depth discussion.\\nIn this book, we distinguish between the topics of linear algebra (e.g.,\\nvectors, matrices, linear independence, basis) and topics related to the\\ngeometry of a vector space. In Chapter 3, we will introduce the inner\\nproduct, which induces a norm. These concepts allow us to deﬁne angles,\\nlengths and distances, which we will use for orthogonal projections. Pro-\\njections turn out to be key in many machine learning algorithms, such as\\nlinear regression and principal component analysis, both of which we will\\ncover in Chapters 9 and 10, respectively.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n64 Linear Algebra\\nExercises\\n2.1 We consider (Rnf\\x001g;?), where\\na?b :=ab+a+b; a;b2Rnf\\x001g (2.134)\\na. Show that (Rnf\\x001g;?)is an Abelian group.\\nb. Solve\\n3?x?x = 15\\nin the Abelian group (Rnf\\x001g;?), where?is deﬁned in (2.134).\\n2.2 Letnbe inNnf0g. Letk;xbe inZ. We deﬁne the congruence class \\x16kof the\\nintegerkas the set\\nk=fx2Zjx\\x00k= 0 (modn)g\\n=fx2Zj9a2Z: (x\\x00k=n\\x01a)g:\\nWe now deﬁne Z=nZ(sometimes written Zn) as the set of all congruence\\nclasses modulo n. Euclidean division implies that this set is a ﬁnite set con-\\ntainingnelements:\\nZn=f0;1;:::;n\\x001g\\nFor alla;b2Zn, we deﬁne\\na\\x08b:=a+b\\na. Show that (Zn;\\x08)is a group. Is it Abelian?\\nb. We now deﬁne another operation \\nfor allaandbinZnas\\na\\nb=a\\x02b; (2.135)\\nwherea\\x02brepresents the usual multiplication in Z.\\nLetn= 5. Draw the times table of the elements of Z5nf0gunder\\n, i.e.,\\ncalculate the products a\\nbfor allaandbinZ5nf0g.\\nHence, show that Z5nf0gis closed under\\nand possesses a neutral\\nelement for\\n. Display the inverse of all elements in Z5nf0gunder\\n.\\nConclude that (Z5nf0g;\\n)is an Abelian group.\\nc. Show that (Z8nf0g;\\n)is not a group.\\nd. We recall that the B ´ezout theorem states that two integers aandbare\\nrelatively prime (i.e., gcd(a;b) = 1 ) if and only if there exist two integers\\nuandvsuch thatau+bv= 1. Show that (Znnf0g;\\n)is a group if and\\nonly ifn2Nnf0gis prime.\\n2.3 Consider the set Gof3\\x023matrices deﬁned as follows:\\nG=8\\n<\\n:2\\n41x z\\n0 1y\\n0 0 13\\n52R3\\x023\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cx;y;z2R9\\n=\\n;\\nWe deﬁne\\x01as the standard matrix multiplication.\\nIs(G;\\x01)a group? If yes, is it Abelian? Justify your answer.\\n2.4 Compute the following matrix products, if possible:\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 65\\na.\\n2\\n41 2\\n4 5\\n7 83\\n52\\n41 1 0\\n0 1 1\\n1 0 13\\n5\\nb.\\n2\\n41 2 3\\n4 5 6\\n7 8 93\\n52\\n41 1 0\\n0 1 1\\n1 0 13\\n5\\nc.\\n2\\n41 1 0\\n0 1 1\\n1 0 13\\n52\\n41 2 3\\n4 5 6\\n7 8 93\\n5\\nd.\\n\\x14\\n1 2 1 2\\n4 1\\x001\\x004\\x152\\n6640 3\\n1\\x001\\n2 1\\n5 23\\n775\\ne.\\n2\\n6640 3\\n1\\x001\\n2 1\\n5 23\\n775\\x14\\n1 2 1 2\\n4 1\\x001\\x004\\x15\\n2.5 Find the set Sof all solutions in xof the following inhomogeneous linear\\nsystemsAx=b, whereAandbare deﬁned as follows:\\na.\\nA=2\\n6641 1\\x001\\x001\\n2 5\\x007\\x005\\n2\\x001 1 3\\n5 2\\x004 23\\n775;b=2\\n6641\\n\\x002\\n4\\n63\\n775\\nb.\\nA=2\\n6641\\x001 0 0 1\\n1 1 0\\x003 0\\n2\\x001 0 1\\x001\\n\\x001 2 0\\x002\\x0013\\n775;b=2\\n6643\\n6\\n5\\n\\x0013\\n775\\n2.6 Using Gaussian elimination, ﬁnd all solutions of the inhomogeneous equa-\\ntion systemAx=bwith\\nA=2\\n40 1 0 0 1 0\\n0 0 0 1 1 0\\n0 1 0 0 0 13\\n5;b=2\\n42\\n\\x001\\n13\\n5:\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n66 Linear Algebra\\n2.7 Find all solutions in x=2\\n4x1\\nx2\\nx33\\n52R3of the equation system Ax= 12x,\\nwhere\\nA=2\\n46 4 3\\n6 0 9\\n0 8 03\\n5\\nandP3\\ni=1xi= 1.\\n2.8 Determine the inverses of the following matrices if possible:\\na.\\nA=2\\n42 3 4\\n3 4 5\\n4 5 63\\n5\\nb.\\nA=2\\n6641 0 1 0\\n0 1 1 0\\n1 1 0 1\\n1 1 1 03\\n775\\n2.9 Which of the following sets are subspaces of R3?\\na.A=f(\\x15;\\x15+\\x163;\\x15\\x00\\x163)j\\x15;\\x162Rg\\nb.B=f(\\x152;\\x00\\x152;0)j\\x152Rg\\nc. Let\\rbe inR.\\nC=f(\\x181;\\x182;\\x183)2R3j\\x181\\x002\\x182+ 3\\x183=\\rg\\nd.D=f(\\x181;\\x182;\\x183)2R3j\\x1822Zg\\n2.10 Are the following sets of vectors linearly independent?\\na.\\nx1=2\\n42\\n\\x001\\n33\\n5;x2=2\\n41\\n1\\n\\x0023\\n5;x3=2\\n43\\n\\x003\\n83\\n5\\nb.\\nx1=2\\n666641\\n2\\n1\\n0\\n03\\n77775;x2=2\\n666641\\n1\\n0\\n1\\n13\\n77775;x3=2\\n666641\\n0\\n0\\n1\\n13\\n77775\\n2.11 Write\\ny=2\\n41\\n\\x002\\n53\\n5\\nas linear combination of\\nx1=2\\n41\\n1\\n13\\n5;x2=2\\n41\\n2\\n33\\n5;x3=2\\n42\\n\\x001\\n13\\n5\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 67\\n2.12 Consider two subspaces of R4:\\nU1= span[2\\n6641\\n1\\n\\x003\\n13\\n775;2\\n6642\\n\\x001\\n0\\n\\x0013\\n775;2\\n664\\x001\\n1\\n\\x001\\n13\\n775]; U 2= span[2\\n664\\x001\\n\\x002\\n2\\n13\\n775;2\\n6642\\n\\x002\\n0\\n03\\n775;2\\n664\\x003\\n6\\n\\x002\\n\\x0013\\n775]:\\nDetermine a basis of U1\\\\U2.\\n2.13 Consider two subspaces U1andU2, whereU1is the solution space of the\\nhomogeneous equation system A1x=0andU2is the solution space of the\\nhomogeneous equation system A2x=0with\\nA1=2\\n6641 0 1\\n1\\x002\\x001\\n2 1 3\\n1 0 13\\n775;A2=2\\n6643\\x003 0\\n1 2 3\\n7\\x005 2\\n3\\x001 23\\n775:\\na. Determine the dimension of U1;U2.\\nb. Determine bases of U1andU2.\\nc. Determine a basis of U1\\\\U2.\\n2.14 Consider two subspaces U1andU2, whereU1is spanned by the columns of\\nA1andU2is spanned by the columns of A2with\\nA1=2\\n6641 0 1\\n1\\x002\\x001\\n2 1 3\\n1 0 13\\n775;A2=2\\n6643\\x003 0\\n1 2 3\\n7\\x005 2\\n3\\x001 23\\n775:\\na. Determine the dimension of U1;U2\\nb. Determine bases of U1andU2\\nc. Determine a basis of U1\\\\U2\\n2.15 LetF=f(x;y;z )2R3jx+y\\x00z= 0gandG=f(a\\x00b;a+b;a\\x003b)ja;b2Rg.\\na. Show that FandGare subspaces of R3.\\nb. Calculate F\\\\Gwithout resorting to any basis vector.\\nc. Find one basis for Fand one for G, calculateF\\\\Gusing the basis vectors\\npreviously found and check your result with the previous question.\\n2.16 Are the following mappings linear?\\na. Leta;b2R.\\n\\x08 :L1([a;b])!R\\nf7!\\x08(f) =Zb\\naf(x)dx;\\nwhereL1([a;b])denotes the set of integrable functions on [a;b].\\nb.\\n\\x08 :C1!C0\\nf7!\\x08(f) =f0;\\nwhere fork>1,Ckdenotes the set of ktimes continuously differen-\\ntiable functions, and C0denotes the set of continuous functions.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n68 Linear Algebra\\nc.\\n\\x08 :R!R\\nx7!\\x08(x) = cos(x)\\nd.\\n\\x08 :R3!R2\\nx7!\\x14\\n1 2 3\\n1 4 3\\x15\\nx\\ne. Let\\x12be in [0;2\\x19[and\\n\\x08 :R2!R2\\nx7!\\x14\\ncos(\\x12) sin(\\x12)\\n\\x00sin(\\x12) cos(\\x12)\\x15\\nx\\n2.17 Consider the linear mapping\\n\\x08 :R3!R4\\n\\x080\\n@2\\n4x1\\nx2\\nx33\\n51\\nA=2\\n6643x1+ 2x2+x3\\nx1+x2+x3\\nx1\\x003x2\\n2x1+ 3x2+x33\\n775\\nFind the transformation matrix A\\x08.\\nDetermine rk(A\\x08).\\nCompute the kernel and image of \\x08. What are dim(ker(\\x08)) anddim(Im(\\x08)) ?\\n2.18 LetEbe a vector space. Let fandgbe two automorphisms on Esuch that\\nf\\x0eg= idE(i.e.,f\\x0egis the identity mapping idE). Show that ker(f) =\\nker(g\\x0ef),Im(g) = Im(g\\x0ef)and that ker(f)\\\\Im(g) =f0Eg.\\n2.19 Consider an endomorphism \\x08 :R3!R3whose transformation matrix\\n(with respect to the standard basis in R3) is\\nA\\x08=2\\n41 1 0\\n1\\x001 0\\n1 1 13\\n5:\\na. Determine ker(\\x08) andIm(\\x08) .\\nb. Determine the transformation matrix ~A\\x08with respect to the basis\\nB= (2\\n41\\n1\\n13\\n5;2\\n41\\n2\\n13\\n5;2\\n41\\n0\\n03\\n5);\\ni.e., perform a basis change toward the new basis B.\\n2.20 Let us consider b1;b2;b0\\n1;b0\\n2,4vectors of R2expressed in the standard basis\\nofR2as\\nb1=\\x14\\n2\\n1\\x15\\n;b2=\\x14\\n\\x001\\n\\x001\\x15\\n;b0\\n1=\\x14\\n2\\n\\x002\\x15\\n;b0\\n2=\\x14\\n1\\n1\\x15\\nand let us deﬁne two ordered bases B= (b1;b2)andB0= (b0\\n1;b0\\n2)ofR2.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 69\\na. Show that BandB0are two bases of R2and draw those basis vectors.\\nb. Compute the matrix P1that performs a basis change from B0toB.\\nc. We consider c1;c2;c3, three vectors of R3deﬁned in the standard basis\\nofR3as\\nc1=2\\n41\\n2\\n\\x0013\\n5;c2=2\\n40\\n\\x001\\n23\\n5;c3=2\\n41\\n0\\n\\x0013\\n5\\nand we deﬁne C= (c1;c2;c3).\\n(i) Show that Cis a basis of R3, e.g., by using determinants (see\\nSection 4.1).\\n(ii) Let us call C0= (c0\\n1;c0\\n2;c0\\n3)the standard basis of R3. Determine\\nthe matrixP2that performs the basis change from CtoC0.\\nd. We consider a homomorphism \\x08 :R2\\x00!R3, such that\\n\\x08(b1+b2) =c2+c3\\n\\x08(b1\\x00b2) = 2c1\\x00c2+ 3c3\\nwhereB= (b1;b2)andC= (c1;c2;c3)are ordered bases of R2andR3,\\nrespectively.\\nDetermine the transformation matrix A\\x08of\\x08with respect to the or-\\ndered bases BandC.\\ne. Determine A0, the transformation matrix of \\x08with respect to the bases\\nB0andC0.\\nf. Let us consider the vector x2R2whose coordinates in B0are[2;3]>.\\nIn other words, x= 2b0\\n1+ 3b0\\n2.\\n(i) Calculate the coordinates of xinB.\\n(ii) Based on that, compute the coordinates of \\x08(x)expressed in C.\\n(iii) Then, write \\x08(x)in terms ofc0\\n1;c0\\n2;c0\\n3.\\n(iv) Use the representation of xinB0and the matrix A0to ﬁnd this\\nresult directly.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n3\\nAnalytic Geometry\\nIn Chapter 2, we studied vectors, vector spaces, and linear mappings at\\na general but abstract level. In this chapter, we will add some geomet-\\nric interpretation and intuition to all of these concepts. In particular, we\\nwill look at geometric vectors and compute their lengths and distances\\nor angles between two vectors. To be able to do this, we equip the vec-\\ntor space with an inner product that induces the geometry of the vector\\nspace. Inner products and their corresponding norms and metrics capture\\nthe intuitive notions of similarity and distances, which we use to develop\\nthe support vector machine in Chapter 12. We will then use the concepts\\nof lengths and angles between vectors to discuss orthogonal projections,\\nwhich will play a central role when we discuss principal component anal-\\nysis in Chapter 10 and regression via maximum likelihood estimation in\\nChapter 9. Figure 3.1 gives an overview of how concepts in this chapter\\nare related and how they are connected to other chapters of the book.\\nFigure 3.1 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhen they are used\\nin other parts of the\\nbook.Inner product\\nNorm\\nLengthsOrthogonal\\nprojectionAngles Rotations\\nChapter 4\\nMatrix\\ndecompositionChapter 10\\nDimensionality\\nreductionChapter 9\\nRegressionChapter 12\\nClassiﬁcationinduces\\n70\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n3.1 Norms 71\\nFigure 3.3 For\\ndifferent norms, the\\nred lines indicate\\nthe set of vectors\\nwith norm 1. Left:\\nManhattan norm;\\nRight: Euclidean\\ndistance.\\n1\\n1\\n1\\n1\\nkxk1= 1\\nkxk2= 1\\n3.1 Norms\\nWhen we think of geometric vectors, i.e., directed line segments that start\\nat the origin, then intuitively the length of a vector is the distance of the\\n“end” of this directed line segment from the origin. In the following, we\\nwill discuss the notion of the length of vectors using the concept of a norm.\\nDeﬁnition 3.1 (Norm) .Anorm on a vector space Vis a function norm\\nk\\x01k:V!R; (3.1)\\nx7!kxk; (3.2)\\nwhich assigns each vector xitslengthkxk2R, such that for all \\x152R length\\nandx;y2Vthe following hold:\\nabsolutely\\nhomogeneous Absolutely homogeneous: k\\x15xk=j\\x15jkxk\\ntriangle inequality Triangle inequality: kx+yk6kxk+kyk\\npositive deﬁnite Positive deﬁnite:kxk>0andkxk= 0()x=0\\nFigure 3.2 Triangle\\ninequality.\\na\\nb\\nc\\x14a+b In geometric terms, the triangle inequality states that for any triangle,\\nthe sum of the lengths of any two sides must be greater than or equal\\nto the length of the remaining side; see Figure 3.2 for an illustration.\\nDeﬁnition 3.1 is in terms of a general vector space V(Section 2.4), but\\nin this book we will only consider a ﬁnite-dimensional vector space Rn.\\nRecall that for a vector x2Rnwe denote the elements of the vector using\\na subscript, that is, xiis theithelement of the vector x.\\nExample 3.1 (Manhattan Norm)\\nThe Manhattan norm onRnis deﬁned for x2Rnas Manhattan norm\\nkxk1:=nX\\ni=1jxij; (3.3)\\nwherej\\x01jis the absolute value. The left panel of Figure 3.3 shows all\\nvectorsx2R2withkxk1= 1. The Manhattan norm is also called `1`1norm\\nnorm .\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n72 Analytic Geometry\\nExample 3.2 (Euclidean Norm)\\nThe Euclidean norm ofx2Rnis deﬁned as Euclidean norm\\nkxk2:=vuutnX\\ni=1x2\\ni=p\\nx>x (3.4)\\nand computes the Euclidean distance ofxfrom the origin. The right panel Euclidean distance\\nof Figure 3.3 shows all vectors x2R2withkxk2= 1. The Euclidean\\nnorm is also called `2norm . `2norm\\nRemark. Throughout this book, we will use the Euclidean norm (3.4) by\\ndefault if not stated otherwise. }\\n3.2 Inner Products\\nInner products allow for the introduction of intuitive geometrical con-\\ncepts, such as the length of a vector and the angle or distance between\\ntwo vectors. A major purpose of inner products is to determine whether\\nvectors are orthogonal to each other.\\n3.2.1 Dot Product\\nWe may already be familiar with a particular type of inner product, the\\nscalar product /dot product inRn, which is given by scalar product\\ndot product\\nx>y=nX\\ni=1xiyi: (3.5)\\nWe will refer to this particular inner product as the dot product in this\\nbook. However, inner products are more general concepts with speciﬁc\\nproperties, which we will now introduce.\\n3.2.2 General Inner Products\\nRecall the linear mapping from Section 2.7, where we can rearrange the\\nmapping with respect to addition and multiplication with a scalar. A bi- bilinear mapping\\nlinear mapping \\nis a mapping with two arguments, and it is linear in\\neach argument, i.e., when we look at a vector space Vthen it holds that\\nfor allx;y;z2V; \\x15; 2Rthat\\n\\n(\\x15x+ y;z) =\\x15\\n(x;z) + \\n(y;z) (3.6)\\n\\n(x;\\x15y+ z) =\\x15\\n(x;y) + \\n(x;z): (3.7)\\nHere, (3.6) asserts that \\nis linear in the ﬁrst argument, and (3.7) asserts\\nthat\\nis linear in the second argument (see also (2.87)).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.2 Inner Products 73\\nDeﬁnition 3.2. LetVbe a vector space and \\n :V\\x02V!Rbe a bilinear\\nmapping that takes two vectors and maps them onto a real number. Then\\n\\nis called symmetric if\\n(x;y) = \\n(y;x)for allx;y2V, i.e., the symmetric\\norder of the arguments does not matter.\\n\\nis called positive deﬁnite if positive deﬁnite\\n8x2Vnf0g: \\n(x;x)>0;\\n(0;0) = 0: (3.8)\\nDeﬁnition 3.3. LetVbe a vector space and \\n :V\\x02V!Rbe a bilinear\\nmapping that takes two vectors and maps them onto a real number. Then\\nA positive deﬁnite, symmetric bilinear mapping \\n :V\\x02V!Ris called\\naninner product onV. We typically write hx;yiinstead of \\n(x;y). inner product\\nThe pair (V;h\\x01;\\x01i)is called an inner product space or (real) vector space inner product space\\nvector space with\\ninner productwith inner product . If we use the dot product deﬁned in (3.5), we call\\n(V;h\\x01;\\x01i)aEuclidean vector space .\\nEuclidean vector\\nspace We will refer to these spaces as inner product spaces in this book.\\nExample 3.3 (Inner Product That Is Not the Dot Product)\\nConsiderV=R2. If we deﬁne\\nhx;yi:=x1y1\\x00(x1y2+x2y1) + 2x2y2 (3.9)\\nthenh\\x01;\\x01iis an inner product but different from the dot product. The proof\\nwill be an exercise.\\n3.2.3 Symmetric, Positive Deﬁnite Matrices\\nSymmetric, positive deﬁnite matrices play an important role in machine\\nlearning, and they are deﬁned via the inner product. In Section 4.3, we\\nwill return to symmetric, positive deﬁnite matrices in the context of matrix\\ndecompositions. The idea of symmetric positive semideﬁnite matrices is\\nkey in the deﬁnition of kernels (Section 12.4).\\nConsider an n-dimensional vector space Vwith an inner product h\\x01;\\x01i:\\nV\\x02V!R(see Deﬁnition 3.3) and an ordered basis B= (b1;:::;bn)of\\nV. Recall from Section 2.6.1 that any vectors x;y2Vcan be written as\\nlinear combinations of the basis vectors so that x=Pn\\ni=1 ibi2Vand\\ny=Pn\\nj=1\\x15jbj2Vfor suitable  i;\\x15j2R. Due to the bilinearity of the\\ninner product, it holds for all x;y2Vthat\\nhx;yi=*nX\\ni=1 ibi;nX\\nj=1\\x15jbj+\\n=nX\\ni=1nX\\nj=1 ihbi;bji\\x15j=^x>A^y;(3.10)\\nwhereAij:=hbi;bjiand^x;^yare the coordinates of xandywith respect\\nto the basis B. This implies that the inner product h\\x01;\\x01iis uniquely deter-\\nmined through A. The symmetry of the inner product also means that A\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n74 Analytic Geometry\\nis symmetric. Furthermore, the positive deﬁniteness of the inner product\\nimplies that\\n8x2Vnf0g:x>Ax>0: (3.11)\\nDeﬁnition 3.4 (Symmetric, Positive Deﬁnite Matrix) .A symmetric matrix\\nA2Rn\\x02nthat satisﬁes (3.11) is called symmetric, positive deﬁnite , or symmetric, positive\\ndeﬁnite justpositive deﬁnite . If only>holds in (3.11), then Ais called symmetric,\\npositive deﬁnite\\nsymmetric, positive\\nsemideﬁnitepositive semideﬁnite .\\nExample 3.4 (Symmetric, Positive Deﬁnite Matrices)\\nConsider the matrices\\nA1=\\x149 6\\n6 5\\x15\\n;A2=\\x149 6\\n6 3\\x15\\n: (3.12)\\nA1is positive deﬁnite because it is symmetric and\\nx>A1x=\\x02x1x2\\x03\\x149 6\\n6 5\\x15\\x14x1\\nx2\\x15\\n(3.13a)\\n= 9x2\\n1+ 12x1x2+ 5x2\\n2= (3x1+ 2x2)2+x2\\n2>0 (3.13b)\\nfor allx2Vnf0g. In contrast,A2is symmetric but not positive deﬁnite\\nbecausex>A2x= 9x2\\n1+ 12x1x2+ 3x2\\n2= (3x1+ 2x2)2\\x00x2\\n2can be less\\nthan 0, e.g., forx= [2;\\x003]>.\\nIfA2Rn\\x02nis symmetric, positive deﬁnite, then\\nhx;yi=^x>A^y (3.14)\\ndeﬁnes an inner product with respect to an ordered basis B, where ^xand\\n^yare the coordinate representations of x;y2Vwith respect to B.\\nTheorem 3.5. For a real-valued, ﬁnite-dimensional vector space Vand an\\nordered basis BofV, it holds thath\\x01;\\x01i:V\\x02V!Ris an inner product if\\nand only if there exists a symmetric, positive deﬁnite matrix A2Rn\\x02nwith\\nhx;yi=^x>A^y: (3.15)\\nThe following properties hold if A2Rn\\x02nis symmetric and positive\\ndeﬁnite:\\nThe null space (kernel) of Aconsists only of 0becausex>Ax>0for\\nallx6=0. This implies that Ax6=0ifx6=0.\\nThe diagonal elements aiiofAare positive because aii=e>\\niAei>0,\\nwhereeiis theith vector of the standard basis in Rn.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.3 Lengths and Distances 75\\n3.3 Lengths and Distances\\nIn Section 3.1, we already discussed norms that we can use to compute\\nthe length of a vector. Inner products and norms are closely related in the\\nsense that any inner product induces a norm Inner products\\ninduce norms.\\nkxk:=q\\nhx;xi (3.16)\\nin a natural way, such that we can compute lengths of vectors using the in-\\nner product. However, not every norm is induced by an inner product. The\\nManhattan norm (3.3) is an example of a norm without a corresponding\\ninner product. In the following, we will focus on norms that are induced\\nby inner products and introduce geometric concepts, such as lengths, dis-\\ntances, and angles.\\nRemark (Cauchy-Schwarz Inequality) .For an inner product vector space\\n(V;h\\x01;\\x01i)the induced norm k\\x01ksatisﬁes the Cauchy-Schwarz inequality Cauchy-Schwarz\\ninequality\\njhx;yij6kxkkyk: (3.17)\\n}\\nExample 3.5 (Lengths of Vectors Using Inner Products)\\nIn geometry, we are often interested in lengths of vectors. We can now use\\nan inner product to compute them using (3.16). Let us take x= [1;1]>2\\nR2. If we use the dot product as the inner product, with (3.16) we obtain\\nkxk=p\\nx>x=p\\n12+ 12=p\\n2 (3.18)\\nas the length of x. Let us now choose a different inner product:\\nhx;yi:=x>\\x141\\x001\\n2\\n\\x001\\n21\\x15\\ny=x1y1\\x001\\n2(x1y2+x2y1) +x2y2:(3.19)\\nIf we compute the norm of a vector, then this inner product returns smaller\\nvalues than the dot product if x1andx2have the same sign (and x1x2>\\n0); otherwise, it returns greater values than the dot product. With this\\ninner product, we obtain\\nhx;xi=x2\\n1\\x00x1x2+x2\\n2= 1\\x001 + 1 = 1 =) kxk=p\\n1 = 1;(3.20)\\nsuch thatxis “shorter” with this inner product than with the dot product.\\nDeﬁnition 3.6 (Distance and Metric) .Consider an inner product space\\n(V;h\\x01;\\x01i). Then\\nd(x;y) :=kx\\x00yk=q\\nhx\\x00y;x\\x00yi (3.21)\\nis called the distance betweenxandyforx;y2V. If we use the dot distance\\nproduct as the inner product, then the distance is called Euclidean distance .Euclidean distance\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n76 Analytic Geometry\\nThe mapping\\nd:V\\x02V!R (3.22)\\n(x;y)7!d(x;y) (3.23)\\nis called a metric . metric\\nRemark. Similar to the length of a vector, the distance between vectors\\ndoes not require an inner product: a norm is sufﬁcient. If we have a norm\\ninduced by an inner product, the distance may vary depending on the\\nchoice of the inner product. }\\nA metricdsatisﬁes the following:\\n1.dispositive deﬁnite , i.e.,d(x;y)>0for allx;y2Vandd(x;y) = positive deﬁnite\\n0()x=y.\\n2.dissymmetric , i.e.,d(x;y) =d(y;x)for allx;y2V. symmetric\\ntriangle inequality 3.Triangle inequality: d(x;z)6d(x;y) +d(y;z)for allx;y;z2V.\\nRemark. At ﬁrst glance, the lists of properties of inner products and met-\\nrics look very similar. However, by comparing Deﬁnition 3.3 with Deﬁni-\\ntion 3.6 we observe that hx;yiandd(x;y)behave in opposite directions.\\nVery similarxandywill result in a large value for the inner product and\\na small value for the metric. }\\n3.4 Angles and Orthogonality\\nFigure 3.4 When\\nrestricted to [0;\\x19]\\nthenf(!) = cos(!)\\nreturns a unique\\nnumber in the\\ninterval [\\x001;1].\\n0π/2π\\nω−101cos(ω)In addition to enabling the deﬁnition of lengths of vectors, as well as the\\ndistance between two vectors, inner products also capture the geometry\\nof a vector space by deﬁning the angle !between two vectors. We use\\nthe Cauchy-Schwarz inequality (3.17) to deﬁne angles !in inner prod-\\nuct spaces between two vectors x;y, and this notion coincides with our\\nintuition in R2andR3. Assume that x6=0;y6=0. Then\\n\\x0016hx;yi\\nkxkkyk61: (3.24)\\nTherefore, there exists a unique !2[0;\\x19], illustrated in Figure 3.4, with\\ncos!=hx;yi\\nkxkkyk: (3.25)\\nThe number !is the angle between the vectors xandy. Intuitively, the angle\\nangle between two vectors tells us how similar their orientations are. For\\nexample, using the dot product, the angle between xandy= 4x, i.e.,y\\nis a scaled version of x, is0: Their orientation is the same.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.4 Angles and Orthogonality 77\\nExample 3.6 (Angle between Vectors)\\nLet us compute the angle between x= [1;1]>2R2andy= [1;2]>2R2;Figure 3.5 The\\nangle!between\\ntwo vectorsx;yis\\ncomputed using the\\ninner product.\\ny\\nx\\n1 01\\n!see Figure 3.5, where we use the dot product as the inner product. Then\\nwe get\\ncos!=hx;yip\\nhx;xihy;yi=x>yp\\nx>xy>y=3p\\n10; (3.26)\\nand the angle between the two vectors is arccos(3p\\n10)\\x190:32 rad , which\\ncorresponds to about 18\\x0e.\\nA key feature of the inner product is that it also allows us to characterize\\nvectors that are orthogonal.\\nDeﬁnition 3.7 (Orthogonality) .Two vectorsxandyareorthogonal if and orthogonal\\nonly ifhx;yi= 0, and we write x?y. If additionallykxk= 1 =kyk,\\ni.e., the vectors are unit vectors, then xandyareorthonormal . orthonormal\\nAn implication of this deﬁnition is that the 0-vector is orthogonal to\\nevery vector in the vector space.\\nRemark. Orthogonality is the generalization of the concept of perpendic-\\nularity to bilinear forms that do not have to be the dot product. In our\\ncontext, geometrically, we can think of orthogonal vectors as having a\\nright angle with respect to a speciﬁc inner product. }\\nExample 3.7 (Orthogonal Vectors)\\nFigure 3.6 The\\nangle!between\\ntwo vectorsx;ycan\\nchange depending\\non the inner\\nproduct.y x\\n\\x001 1 01\\n!\\nConsider two vectors x= [1;1]>;y= [\\x001;1]>2R2; see Figure 3.6.\\nWe are interested in determining the angle !between them using two\\ndifferent inner products. Using the dot product as the inner product yields\\nan angle!betweenxandyof90\\x0e, such thatx?y. However, if we\\nchoose the inner product\\nhx;yi=x>\\x142 0\\n0 1\\x15\\ny; (3.27)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n78 Analytic Geometry\\nwe get that the angle !betweenxandyis given by\\ncos!=hx;yi\\nkxkkyk=\\x001\\n3=)!\\x191:91 rad\\x19109:5\\x0e; (3.28)\\nandxandyare not orthogonal. Therefore, vectors that are orthogonal\\nwith respect to one inner product do not have to be orthogonal with re-\\nspect to a different inner product.\\nDeﬁnition 3.8 (Orthogonal Matrix) .A square matrix A2Rn\\x02nis an\\northogonal matrix if and only if its columns are orthonormal so that orthogonal matrix\\nAA>=I=A>A; (3.29)\\nwhich implies that\\nA\\x001=A>; (3.30)\\ni.e., the inverse is obtained by simply transposing the matrix. It is convention to\\ncall these matrices\\n“orthogonal” but a\\nmore precise\\ndescription would\\nbe “orthonormal”.Transformations by orthogonal matrices are special because the length\\nof a vectorxis not changed when transforming it using an orthogonal\\nmatrixA. For the dot product, we obtain\\nTransformations\\nwith orthogonal\\nmatrices preserve\\ndistances and\\nangles.kAxk2= (Ax)>(Ax) =x>A>Ax=x>Ix=x>x=kxk2:(3.31)\\nMoreover, the angle between any two vectors x;y, as measured by their\\ninner product, is also unchanged when transforming both of them using\\nan orthogonal matrix A. Assuming the dot product as the inner product,\\nthe angle of the images AxandAyis given as\\ncos!=(Ax)>(Ay)\\nkAxkkAyk=x>A>Ayq\\nx>A>Axy>A>Ay=x>y\\nkxkkyk;(3.32)\\nwhich gives exactly the angle between xandy. This means that orthog-\\nonal matrices AwithA>=A\\x001preserve both angles and distances. It\\nturns out that orthogonal matrices deﬁne transformations that are rota-\\ntions (with the possibility of ﬂips). In Section 3.9, we will discuss more\\ndetails about rotations.\\n3.5 Orthonormal Basis\\nIn Section 2.6.1, we characterized properties of basis vectors and found\\nthat in ann-dimensional vector space, we need nbasis vectors, i.e., n\\nvectors that are linearly independent. In Sections 3.3 and 3.4, we used\\ninner products to compute the length of vectors and the angle between\\nvectors. In the following, we will discuss the special case where the basis\\nvectors are orthogonal to each other and where the length of each basis\\nvector is 1. We will call this basis then an orthonormal basis.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.6 Orthogonal Complement 79\\nLet us introduce this more formally.\\nDeﬁnition 3.9 (Orthonormal Basis) .Consider an n-dimensional vector\\nspaceVand a basisfb1;:::;bngofV. If\\nhbi;bji= 0 fori6=j (3.33)\\nhbi;bii= 1 (3.34)\\nfor alli;j= 1;:::;n then the basis is called an orthonormal basis (ONB). orthonormal basis\\nONB If only (3.33) is satisﬁed, then the basis is called an orthogonal basis . Note\\northogonal basisthat (3.34) implies that every basis vector has length/norm 1.\\nRecall from Section 2.6.1 that we can use Gaussian elimination to ﬁnd a\\nbasis for a vector space spanned by a set of vectors. Assume we are given\\na setf~b1;:::; ~bngof non-orthogonal and unnormalized basis vectors. We\\nconcatenate them into a matrix ~B= [~b1;:::; ~bn]and apply Gaussian elim-\\nination to the augmented matrix (Section 2.3.2) [~B~B>j~B]to obtain an\\northonormal basis. This constructive way to iteratively build an orthonor-\\nmal basisfb1;:::;bngis called the Gram-Schmidt process (Strang, 2003).\\nExample 3.8 (Orthonormal Basis)\\nThe canonical/standard basis for a Euclidean vector space Rnis an or-\\nthonormal basis, where the inner product is the dot product of vectors.\\nInR2, the vectors\\nb1=1p\\n2\\x141\\n1\\x15\\n;b2=1p\\n2\\x141\\n\\x001\\x15\\n(3.35)\\nform an orthonormal basis since b>\\n1b2= 0andkb1k= 1 =kb2k.\\nWe will exploit the concept of an orthonormal basis in Chapter 12 and\\nChapter 10 when we discuss support vector machines and principal com-\\nponent analysis.\\n3.6 Orthogonal Complement\\nHaving deﬁned orthogonality, we will now look at vector spaces that are\\northogonal to each other. This will play an important role in Chapter 10,\\nwhen we discuss linear dimensionality reduction from a geometric per-\\nspective.\\nConsider aD-dimensional vector space Vand anM-dimensional sub-\\nspaceU\\x12V. Then its orthogonal complement U?is a(D\\x00M)-dimensional orthogonal\\ncomplement subspace of Vand contains all vectors in Vthat are orthogonal to every\\nvector inU. Furthermore, U\\\\U?=f0gso that any vector x2Vcan be\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n80 Analytic Geometry\\nFigure 3.7 A plane\\nUin a\\nthree-dimensional\\nvector space can be\\ndescribed by its\\nnormal vector,\\nwhich spans its\\northogonal\\ncomplement U?.e3\\ne1e2w\\nU\\nuniquely decomposed into\\nx=MX\\nm=1\\x15mbm+D\\x00MX\\nj=1 jb?\\nj; \\x15m;  j2R; (3.36)\\nwhere (b1;:::;bM)is a basis of Uand(b?\\n1;:::;b?\\nD\\x00M)is a basis of U?.\\nTherefore, the orthogonal complement can also be used to describe a\\nplaneU(two-dimensional subspace) in a three-dimensional vector space.\\nMore speciﬁcally, the vector wwithkwk= 1, which is orthogonal to the\\nplaneU, is the basis vector of U?. Figure 3.7 illustrates this setting. All\\nvectors that are orthogonal to wmust (by construction) lie in the plane\\nU. The vectorwis called the normal vector ofU. normal vector\\nGenerally, orthogonal complements can be used to describe hyperplanes\\ninn-dimensional vector and afﬁne spaces.\\n3.7 Inner Product of Functions\\nThus far, we looked at properties of inner products to compute lengths,\\nangles and distances. We focused on inner products of ﬁnite-dimensional\\nvectors. In the following, we will look at an example of inner products of\\na different type of vectors: inner products of functions.\\nThe inner products we discussed so far were deﬁned for vectors with a\\nﬁnite number of entries. We can think of a vector x2Rnas a function\\nwithnfunction values. The concept of an inner product can be generalized\\nto vectors with an inﬁnite number of entries (countably inﬁnite) and also\\ncontinuous-valued functions (uncountably inﬁnite). Then the sum over\\nindividual components of vectors (see Equation (3.5) for example) turns\\ninto an integral.\\nAn inner product of two functions u:R!Randv:R!Rcan be\\ndeﬁned as the deﬁnite integral\\nhu;vi:=Zb\\nau(x)v(x)dx (3.37)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.8 Orthogonal Projections 81\\nfor lower and upper limits a;b<1, respectively. As with our usual inner\\nproduct, we can deﬁne norms and orthogonality by looking at the inner\\nproduct. If (3.37) evaluates to 0, the functions uandvare orthogonal. To\\nmake the preceding inner product mathematically precise, we need to take\\ncare of measures and the deﬁnition of integrals, leading to the deﬁnition of\\na Hilbert space. Furthermore, unlike inner products on ﬁnite-dimensional\\nvectors, inner products on functions may diverge (have inﬁnite value). All\\nthis requires diving into some more intricate details of real and functional\\nanalysis, which we do not cover in this book.\\nExample 3.9 (Inner Product of Functions)\\nIf we choose u= sin(x)andv= cos(x), the integrand f(x) =u(x)v(x)Figure 3.8f(x) =\\nsin(x) cos(x).\\n−2.5 0.0 2.5\\nx−0.50.00.5sin(x) cos(x) of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e.,\\nf(\\x00x) =\\x00f(x). Therefore, the integral with limits a=\\x00\\x19;b=\\x19of this\\nproduct evaluates to 0. Therefore, sinandcosare orthogonal functions.\\nRemark. It also holds that the collection of functions\\nf1;cos(x);cos(2x);cos(3x);:::g (3.38)\\nis orthogonal if we integrate from \\x00\\x19to\\x19, i.e., any pair of functions are\\northogonal to each other. The collection of functions in (3.38) spans a\\nlarge subspace of the functions that are even and periodic on [\\x00\\x19;\\x19), and\\nprojecting functions onto this subspace is the fundamental idea behind\\nFourier series. }\\nIn Section 6.4.6, we will have a look at a second type of unconventional\\ninner products: the inner product of random variables.\\n3.8 Orthogonal Projections\\nProjections are an important class of linear transformations (besides rota-\\ntions and reﬂections) and play an important role in graphics, coding the-\\nory, statistics and machine learning. In machine learning, we often deal\\nwith data that is high-dimensional. High-dimensional data is often hard\\nto analyze or visualize. However, high-dimensional data quite often pos-\\nsesses the property that only a few dimensions contain most information,\\nand most other dimensions are not essential to describe key properties\\nof the data. When we compress or visualize high-dimensional data, we\\nwill lose information. To minimize this compression loss, we ideally ﬁnd\\nthe most informative dimensions in the data. As discussed in Chapter 1, “Feature” is a\\ncommon expression\\nfor data\\nrepresentation.data can be represented as vectors, and in this chapter, we will discuss\\nsome of the fundamental tools for data compression. More speciﬁcally, we\\ncan project the original high-dimensional data onto a lower-dimensional\\nfeature space and work in this lower-dimensional space to learn more\\nabout the dataset and extract relevant patterns. For example, machine\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n82 Analytic Geometry\\nFigure 3.9\\nOrthogonal\\nprojection (orange\\ndots) of a\\ntwo-dimensional\\ndataset (blue dots)\\nonto a\\none-dimensional\\nsubspace (straight\\nline).\\n−4−2 0 2 4\\nx1−2−1012x2\\nlearning algorithms, such as principal component analysis (PCA) by Pear-\\nson (1901) and Hotelling (1933) and deep neural networks (e.g., deep\\nauto-encoders (Deng et al., 2010)), heavily exploit the idea of dimension-\\nality reduction. In the following, we will focus on orthogonal projections,\\nwhich we will use in Chapter 10 for linear dimensionality reduction and\\nin Chapter 12 for classiﬁcation. Even linear regression, which we discuss\\nin Chapter 9, can be interpreted using orthogonal projections. For a given\\nlower-dimensional subspace, orthogonal projections of high-dimensional\\ndata retain as much information as possible and minimize the difference/\\nerror between the original data and the corresponding projection. An il-\\nlustration of such an orthogonal projection is given in Figure 3.9. Before\\nwe detail how to obtain these projections, let us deﬁne what a projection\\nactually is.\\nDeﬁnition 3.10 (Projection) .LetVbe a vector space and U\\x12Va\\nsubspace of V. A linear mapping \\x19:V!Uis called a projection if projection\\n\\x192=\\x19\\x0e\\x19=\\x19.\\nSince linear mappings can be expressed by transformation matrices (see\\nSection 2.7), the preceding deﬁnition applies equally to a special kind\\nof transformation matrices, the projection matrices P\\x19, which exhibit the projection matrix\\nproperty that P2\\n\\x19=P\\x19.\\nIn the following, we will derive orthogonal projections of vectors in the\\ninner product space (Rn;h\\x01;\\x01i)onto subspaces. We will start with one-\\ndimensional subspaces, which are also called lines. If not mentioned oth- line\\nerwise, we assume the dot product hx;yi=x>yas the inner product.\\n3.8.1 Projection onto One-Dimensional Subspaces (Lines)\\nAssume we are given a line (one-dimensional subspace) through the ori-\\ngin with basis vector b2Rn. The line is a one-dimensional subspace\\nU\\x12Rnspanned byb. When we project x2RnontoU, we seek the\\nvector\\x19U(x)2Uthat is closest to x. Using geometric arguments, let\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.8 Orthogonal Projections 83\\nFigure 3.10\\nExamples of\\nprojections onto\\none-dimensional\\nsubspaces.\\nbx\\n\\x19U(x)\\n!\\n(a) Projection of x2R2onto a subspace U\\nwith basis vector b.cos!!sin!\\nbx\\n(b) Projection of a two-dimensional vector\\nxwithkxk= 1 onto a one-dimensional\\nsubspace spanned by b.\\nus characterize some properties of the projection \\x19U(x)(Figure 3.10(a)\\nserves as an illustration):\\nThe projection \\x19U(x)is closest tox, where “closest” implies that the\\ndistancekx\\x00\\x19U(x)kis minimal. It follows that the segment \\x19U(x)\\x00x\\nfrom\\x19U(x)toxis orthogonal to U, and therefore the basis vector bof\\nU. The orthogonality condition yields h\\x19U(x)\\x00x;bi= 0since angles\\nbetween vectors are deﬁned via the inner product.\\x15is then the\\ncoordinate of \\x19U(x)\\nwith respect to b.The projection \\x19U(x)ofxontoUmust be an element of Uand, there-\\nfore, a multiple of the basis vector bthat spansU. Hence,\\x19U(x) =\\x15b,\\nfor some\\x152R.\\nIn the following three steps, we determine the coordinate \\x15, the projection\\n\\x19U(x)2U, and the projection matrix P\\x19that maps any x2RnontoU:\\n1. Finding the coordinate \\x15. The orthogonality condition yields\\nhx\\x00\\x19U(x);bi= 0\\x19U(x)=\\x15b() hx\\x00\\x15b;bi= 0: (3.39)\\nWe can now exploit the bilinearity of the inner product and arrive at With a general inner\\nproduct, we get\\n\\x15=hx;biif\\nkbk= 1. hx;bi\\x00\\x15hb;bi= 0()\\x15=hx;bi\\nhb;bi=hb;xi\\nkbk2: (3.40)\\nIn the last step, we exploited the fact that inner products are symmet-\\nric. If we chooseh\\x01;\\x01ito be the dot product, we obtain\\n\\x15=b>x\\nb>b=b>x\\nkbk2: (3.41)\\nIfkbk= 1, then the coordinate \\x15of the projection is given by b>x.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n84 Analytic Geometry\\n2. Finding the projection point \\x19U(x)2U. Since\\x19U(x) =\\x15b, we imme-\\ndiately obtain with (3.40) that\\n\\x19U(x) =\\x15b=hx;bi\\nkbk2b=b>x\\nkbk2b; (3.42)\\nwhere the last equality holds for the dot product only. We can also\\ncompute the length of \\x19U(x)by means of Deﬁnition 3.1 as\\nk\\x19U(x)k=k\\x15bk=j\\x15jkbk: (3.43)\\nHence, our projection is of length j\\x15jtimes the length of b. This also\\nadds the intuition that \\x15is the coordinate of \\x19U(x)with respect to the\\nbasis vectorbthat spans our one-dimensional subspace U.\\nIf we use the dot product as an inner product, we get\\nk\\x19U(x)k(3.42)=jb>xj\\nkbk2kbk(3.25)=jcos!jkxkkbkkbk\\nkbk2=jcos!jkxk:\\n(3.44)\\nHere,!is the angle between xandb. This equation should be familiar\\nfrom trigonometry: If kxk= 1, thenxlies on the unit circle. It follows\\nthat the projection onto the horizontal axis spanned by bis exactly The horizontal axis\\nis a one-dimensional\\nsubspace.cos!, and the length of the corresponding vector \\x19U(x) =jcos!j. An\\nillustration is given in Figure 3.10(b).\\n3. Finding the projection matrix P\\x19. We know that a projection is a lin-\\near mapping (see Deﬁnition 3.10). Therefore, there exists a projection\\nmatrixP\\x19, such that \\x19U(x) =P\\x19x. With the dot product as inner\\nproduct and\\n\\x19U(x) =\\x15b=b\\x15=bb>x\\nkbk2=bb>\\nkbk2x; (3.45)\\nwe immediately see that\\nP\\x19=bb>\\nkbk2: (3.46)\\nNote thatbb>(and, consequently, P\\x19) is a symmetric matrix (of rank Projection matrices\\nare always\\nsymmetric.1), andkbk2=hb;biis a scalar.\\nThe projection matrix P\\x19projects any vector x2Rnonto the line through\\nthe origin with direction b(equivalently, the subspace Uspanned byb).\\nRemark. The projection \\x19U(x)2Rnis still ann-dimensional vector and\\nnot a scalar. However, we no longer require ncoordinates to represent the\\nprojection, but only a single one if we want to express it with respect to\\nthe basis vector bthat spans the subspace U:\\x15. }\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.8 Orthogonal Projections 85\\nFigure 3.11\\nProjection onto a\\ntwo-dimensional\\nsubspaceUwith\\nbasisb1;b2. The\\nprojection\\x19U(x)of\\nx2R3ontoUcan\\nbe expressed as a\\nlinear combination\\nofb1;b2and the\\ndisplacement vector\\nx\\x00\\x19U(x)is\\northogonal to both\\nb1andb2.\\n0x\\nb1b2U\\n\\x19U(x)x\\x00\\x19U(x)\\nExample 3.10 (Projection onto a Line)\\nFind the projection matrix P\\x19onto the line through the origin spanned\\nbyb=\\x021 2 2\\x03>.bis a direction and a basis of the one-dimensional\\nsubspace (line through origin).\\nWith (3.46), we obtain\\nP\\x19=bb>\\nb>b=1\\n92\\n41\\n2\\n23\\n5\\x021 2 2\\x03=1\\n92\\n41 2 2\\n2 4 4\\n2 4 43\\n5: (3.47)\\nLet us now choose a particular xand see whether it lies in the subspace\\nspanned byb. Forx=\\x021 1 1\\x03>, the projection is\\n\\x19U(x) =P\\x19x=1\\n92\\n41 2 2\\n2 4 4\\n2 4 43\\n52\\n41\\n1\\n13\\n5=1\\n92\\n45\\n10\\n103\\n52span[2\\n41\\n2\\n23\\n5]:(3.48)\\nNote that the application of P\\x19to\\x19U(x)does not change anything, i.e.,\\nP\\x19\\x19U(x) =\\x19U(x). This is expected because according to Deﬁnition 3.10,\\nwe know that a projection matrix P\\x19satisﬁesP2\\n\\x19x=P\\x19xfor allx.\\nRemark. With the results from Chapter 4, we can show that \\x19U(x)is an\\neigenvector of P\\x19, and the corresponding eigenvalue is 1.}\\n3.8.2 Projection onto General Subspaces\\nIfUis given by a set\\nof spanning vectors,\\nwhich are not a\\nbasis, make sure\\nyou determine a\\nbasisb1;:::;bm\\nbefore proceeding.In the following, we look at orthogonal projections of vectors x2Rn\\nonto lower-dimensional subspaces U\\x12Rnwith dim(U) =m>1. An\\nillustration is given in Figure 3.11.\\nAssume that (b1;:::;bm)is an ordered basis of U. Any projection \\x19U(x)\\nontoUis necessarily an element of U. Therefore, they can be represented\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n86 Analytic Geometry\\nas linear combinations of the basis vectors b1;:::;bmofU, such that\\n\\x19U(x) =Pm\\ni=1\\x15ibi. The basis vectors\\nform the columns of\\nB2Rn\\x02m, where\\nB= [b1;:::;bm].As in the 1D case, we follow a three-step procedure to ﬁnd the projec-\\ntion\\x19U(x)and the projection matrix P\\x19:\\n1. Find the coordinates \\x151;:::;\\x15mof the projection (with respect to the\\nbasis ofU), such that the linear combination\\n\\x19U(x) =mX\\ni=1\\x15ibi=B\\x15; (3.49)\\nB= [b1;:::;bm]2Rn\\x02m;\\x15= [\\x151;:::;\\x15m]>2Rm; (3.50)\\nis closest tox2Rn. As in the 1D case, “closest” means “minimum\\ndistance”, which implies that the vector connecting \\x19U(x)2Uand\\nx2Rnmust be orthogonal to all basis vectors of U. Therefore, we\\nobtainmsimultaneous conditions (assuming the dot product as the\\ninner product)\\nhb1;x\\x00\\x19U(x)i=b>\\n1(x\\x00\\x19U(x)) = 0 (3.51)\\n...\\nhbm;x\\x00\\x19U(x)i=b>\\nm(x\\x00\\x19U(x)) = 0 (3.52)\\nwhich, with \\x19U(x) =B\\x15, can be written as\\nb>\\n1(x\\x00B\\x15) = 0 (3.53)\\n...\\nb>\\nm(x\\x00B\\x15) = 0 (3.54)\\nsuch that we obtain a homogeneous linear equation system\\n2\\n64b>\\n1...\\nb>\\nm3\\n752\\n4x\\x00B\\x153\\n5=0()B>(x\\x00B\\x15) =0 (3.55)\\n()B>B\\x15=B>x: (3.56)\\nThe last expression is called normal equation . Sinceb1;:::;bmare a normal equation\\nbasis ofUand, therefore, linearly independent, B>B2Rm\\x02mis reg-\\nular and can be inverted. This allows us to solve for the coefﬁcients/\\ncoordinates\\n\\x15= (B>B)\\x001B>x: (3.57)\\nThe matrix (B>B)\\x001B>is also called the pseudo-inverse ofB, which pseudo-inverse\\ncan be computed for non-square matrices B. It only requires that B>B\\nis positive deﬁnite, which is the case if Bis full rank. In practical ap-\\nplications (e.g., linear regression), we often add a “jitter term” \\x0fIto\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.8 Orthogonal Projections 87\\nB>Bto guarantee increased numerical stability and positive deﬁnite-\\nness. This “ridge” can be rigorously derived using Bayesian inference.\\nSee Chapter 9 for details.\\n2. Find the projection \\x19U(x)2U. We already established that \\x19U(x) =\\nB\\x15. Therefore, with (3.57)\\n\\x19U(x) =B(B>B)\\x001B>x: (3.58)\\n3. Find the projection matrix P\\x19. From (3.58), we can immediately see\\nthat the projection matrix that solves P\\x19x=\\x19U(x)must be\\nP\\x19=B(B>B)\\x001B>: (3.59)\\nRemark. The solution for projecting onto general subspaces includes the\\n1D case as a special case: If dim(U) = 1 , thenB>B2Ris a scalar and\\nwe can rewrite the projection matrix in (3.59) P\\x19=B(B>B)\\x001B>as\\nP\\x19=BB>\\nB>B, which is exactly the projection matrix in (3.46). }\\nExample 3.11 (Projection onto a Two-dimensional Subspace)\\nFor a subspace U= span[2\\n41\\n1\\n13\\n5;2\\n40\\n1\\n23\\n5]\\x12R3andx=2\\n46\\n0\\n03\\n52R3ﬁnd the\\ncoordinates\\x15ofxin terms of the subspace U, the projection point \\x19U(x)\\nand the projection matrix P\\x19.\\nFirst, we see that the generating set of Uis a basis (linear indepen-\\ndence) and write the basis vectors of Uinto a matrixB=2\\n41 0\\n1 1\\n1 23\\n5.\\nSecond, we compute the matrix B>Band the vector B>xas\\nB>B=\\x141 1 1\\n0 1 2\\x152\\n41 0\\n1 1\\n1 23\\n5=\\x143 3\\n3 5\\x15\\n;B>x=\\x141 1 1\\n0 1 2\\x152\\n46\\n0\\n03\\n5=\\x146\\n0\\x15\\n:\\n(3.60)\\nThird, we solve the normal equation B>B\\x15=B>xto ﬁnd\\x15:\\n\\x143 3\\n3 5\\x15\\x14\\x151\\n\\x152\\x15\\n=\\x146\\n0\\x15\\n()\\x15=\\x145\\n\\x003\\x15\\n: (3.61)\\nFourth, the projection \\x19U(x)ofxontoU, i.e., into the column space of\\nB, can be directly computed via\\n\\x19U(x) =B\\x15=2\\n45\\n2\\n\\x0013\\n5: (3.62)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n88 Analytic Geometry\\nThe corresponding projection error is the norm of the difference vector projection error\\nbetween the original vector and its projection onto U, i.e., The projection error\\nis also called the\\nreconstruction error . kx\\x00\\x19U(x)k=\\r\\r\\r\\x021\\x002 1\\x03>\\r\\r\\r=p\\n6: (3.63)\\nFifth, the projection matrix (for any x2R3) is given by\\nP\\x19=B(B>B)\\x001B>=1\\n62\\n45 2\\x001\\n2 2 2\\n\\x001 2 53\\n5: (3.64)\\nTo verify the results, we can (a) check whether the displacement vector\\n\\x19U(x)\\x00xis orthogonal to all basis vectors of U, and (b) verify that\\nP\\x19=P2\\n\\x19(see Deﬁnition 3.10).\\nRemark. The projections \\x19U(x)are still vectors in Rnalthough they lie in\\nanm-dimensional subspace U\\x12Rn. However, to represent a projected\\nvector we only need the mcoordinates \\x151;:::;\\x15mwith respect to the\\nbasis vectorsb1;:::;bmofU. }\\nRemark. In vector spaces with general inner products, we have to pay\\nattention when computing angles and distances, which are deﬁned by\\nmeans of the inner product. }We can ﬁnd\\napproximate\\nsolutions to\\nunsolvable linear\\nequation systems\\nusing projections.Projections allow us to look at situations where we have a linear system\\nAx=bwithout a solution. Recall that this means that bdoes not lie in\\nthe span ofA, i.e., the vector bdoes not lie in the subspace spanned by\\nthe columns of A. Given that the linear equation cannot be solved exactly,\\nwe can ﬁnd an approximate solution . The idea is to ﬁnd the vector in the\\nsubspace spanned by the columns of Athat is closest to b, i.e., we compute\\nthe orthogonal projection of bonto the subspace spanned by the columns\\nofA. This problem arises often in practice, and the solution is called the\\nleast-squares solution (assuming the dot product as the inner product) of least-squares\\nsolution an overdetermined system. This is discussed further in Section 9.4. Using\\nreconstruction errors (3.63) is one possible approach to derive principal\\ncomponent analysis (Section 10.3).\\nRemark. We just looked at projections of vectors xonto a subspace Uwith\\nbasis vectorsfb1;:::;bkg. If this basis is an ONB, i.e., (3.33) and (3.34)\\nare satisﬁed, the projection equation (3.58) simpliﬁes greatly to\\n\\x19U(x) =BB>x (3.65)\\nsinceB>B=Iwith coordinates\\n\\x15=B>x: (3.66)\\nThis means that we no longer have to compute the inverse from (3.58),\\nwhich saves computation time. }\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.8 Orthogonal Projections 89\\n3.8.3 Gram-Schmidt Orthogonalization\\nProjections are at the core of the Gram-Schmidt method that allows us to\\nconstructively transform any basis (b1;:::;bn)of ann-dimensional vector\\nspaceVinto an orthogonal/orthonormal basis (u1;:::;un)ofV. This\\nbasis always exists (Liesen and Mehrmann, 2015) and span[b1;:::;bn] =\\nspan[u1;:::;un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt\\northogonalization constructs an orthogonal basis (u1;:::;un)from any basis (b1;:::;bn)of\\nVas follows:\\nu1:=b1 (3.67)\\nuk:=bk\\x00\\x19span[u1;:::;uk\\x001](bk); k = 2;:::;n: (3.68)\\nIn (3.68), the kth basis vector bkis projected onto the subspace spanned\\nby the ﬁrst k\\x001constructed orthogonal vectors u1;:::;uk\\x001; see Sec-\\ntion 3.8.2. This projection is then subtracted from bkand yields a vector\\nukthat is orthogonal to the (k\\x001)-dimensional subspace spanned by\\nu1;:::;uk\\x001. Repeating this procedure for all nbasis vectors b1;:::;bn\\nyields an orthogonal basis (u1;:::;un)ofV. If we normalize the uk, we\\nobtain an ONB where kukk= 1fork= 1;:::;n .\\nExample 3.12 (Gram-Schmidt Orthogonalization)\\nFigure 3.12\\nGram-Schmidt\\northogonalization.\\n(a) non-orthogonal\\nbasis (b1;b2)ofR2;\\n(b) ﬁrst constructed\\nbasis vectoru1and\\northogonal\\nprojection ofb2\\nonto span[u1];\\n(c) orthogonal basis\\n(u1;u2)ofR2.b1b2\\n0\\n(a) Original non-orthogonal\\nbasis vectorsb1;b2.u1b2\\n0\\x19span[u1](b2)\\n(b) First new basis vector\\nu1=b1and projection of b2\\nonto the subspace spanned by\\nu1.u1b2\\n0\\x19span[u1](b2)u2\\n(c) Orthogonal basis vectors u1\\nandu2=b2\\x00\\x19span[u1](b2).\\nConsider a basis (b1;b2)ofR2, where\\nb1=\\x142\\n0\\x15\\n;b2=\\x141\\n1\\x15\\n; (3.69)\\nsee also Figure 3.12(a). Using the Gram-Schmidt method, we construct an\\northogonal basis (u1;u2)ofR2as follows (assuming the dot product as\\nthe inner product):\\nu1:=b1=\\x142\\n0\\x15\\n; (3.70)\\nu2:=b2\\x00\\x19span[u1](b2)(3.45)=b2\\x00u1u>\\n1\\nku1k2b2=\\x141\\n1\\x15\\n\\x00\\x141 0\\n0 0\\x15\\x141\\n1\\x15\\n=\\x140\\n1\\x15\\n:\\n(3.71)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n90 Analytic Geometry\\nFigure 3.13\\nProjection onto an\\nafﬁne space.\\n(a) original setting;\\n(b) setting shifted\\nby\\x00x0so that\\nx\\x00x0can be\\nprojected onto the\\ndirection space U;\\n(c) projection is\\ntranslated back to\\nx0+\\x19U(x\\x00x0),\\nwhich gives the ﬁnal\\northogonal\\nprojection\\x19L(x).L\\nx0x\\nb2\\nb1 0\\n(a) Setting.b1 0x\\x00x0\\nU=L\\x00x0\\n\\x19U(x\\x00x0)b2\\n(b) Reduce problem to pro-\\njection\\x19Uonto vector sub-\\nspace.L\\nx0x\\nb2\\nb1 0\\x19L(x)\\n(c) Add support point back in\\nto get afﬁne projection \\x19L.\\nThese steps are illustrated in Figures 3.12(b) and (c). We immediately see\\nthatu1andu2are orthogonal, i.e., u>\\n1u2= 0.\\n3.8.4 Projection onto Afﬁne Subspaces\\nThus far, we discussed how to project a vector onto a lower-dimensional\\nsubspaceU. In the following, we provide a solution to projecting a vector\\nonto an afﬁne subspace.\\nConsider the setting in Figure 3.13(a). We are given an afﬁne space L=\\nx0+U, whereb1;b2are basis vectors of U. To determine the orthogonal\\nprojection\\x19L(x)ofxontoL, we transform the problem into a problem\\nthat we know how to solve: the projection onto a vector subspace. In\\norder to get there, we subtract the support point x0fromxand fromL,\\nso thatL\\x00x0=Uis exactly the vector subspace U. We can now use the\\northogonal projections onto a subspace we discussed in Section 3.8.2 and\\nobtain the projection \\x19U(x\\x00x0), which is illustrated in Figure 3.13(b).\\nThis projection can now be translated back into Lby addingx0, such that\\nwe obtain the orthogonal projection onto an afﬁne space Las\\n\\x19L(x) =x0+\\x19U(x\\x00x0); (3.72)\\nwhere\\x19U(\\x01)is the orthogonal projection onto the subspace U, i.e., the\\ndirection space of L; see Figure 3.13(c).\\nFrom Figure 3.13, it is also evident that the distance of xfrom the afﬁne\\nspaceLis identical to the distance of x\\x00x0fromU, i.e.,\\nd(x;L) =kx\\x00\\x19L(x)k=kx\\x00(x0+\\x19U(x\\x00x0))k (3.73a)\\n=d(x\\x00x0;\\x19U(x\\x00x0)) =d(x\\x00x0;U): (3.73b)\\nWe will use projections onto an afﬁne subspace to derive the concept of\\na separating hyperplane in Section 12.1.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.9 Rotations 91\\nFigure 3.14 A\\nrotation rotates\\nobjects in a plane\\nabout the origin. If\\nthe rotation angle is\\npositive, we rotate\\ncounterclockwise.\\nOriginal\\nRotated by 112.5◦\\nFigure 3.15 The\\nrobotic arm needs to\\nrotate its joints in\\norder to pick up\\nobjects or to place\\nthem correctly.\\nFigure taken\\nfrom (Deisenroth\\net al., 2015).\\n3.9 Rotations\\nLength and angle preservation, as discussed in Section 3.4, are the two\\ncharacteristics of linear mappings with orthogonal transformation matri-\\nces. In the following, we will have a closer look at speciﬁc orthogonal\\ntransformation matrices, which describe rotations.\\nArotation is a linear mapping (more speciﬁcally, an automorphism of rotation\\na Euclidean vector space) that rotates a plane by an angle \\x12about the\\norigin, i.e., the origin is a ﬁxed point. For a positive angle \\x12 >0, by com-\\nmon convention, we rotate in a counterclockwise direction. An example is\\nshown in Figure 3.14, where the transformation matrix is\\nR=\\x14\\x000:38\\x000:92\\n0:92\\x000:38\\x15\\n: (3.74)\\nImportant application areas of rotations include computer graphics and\\nrobotics. For example, in robotics, it is often important to know how to\\nrotate the joints of a robotic arm in order to pick up or place an object,\\nsee Figure 3.15.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n92 Analytic Geometry\\nFigure 3.16\\nRotation of the\\nstandard basis in R2\\nby an angle \\x12.\\ne1e2\\n\\x12\\x12\\x08(e2) = [\\x00sin\\x12;cos\\x12]>\\n\\x08(e1) = [cos\\x12;sin\\x12]>\\ncos\\x12sin\\x12\\n\\x00sin\\x12cos\\x12\\n3.9.1 Rotations in R2\\nConsider the standard basis\\x1a\\ne1=\\x141\\n0\\x15\\n;e2=\\x140\\n1\\x15\\x1b\\nofR2, which deﬁnes\\nthe standard coordinate system in R2. We aim to rotate this coordinate\\nsystem by an angle \\x12as illustrated in Figure 3.16. Note that the rotated\\nvectors are still linearly independent and, therefore, are a basis of R2. This\\nmeans that the rotation performs a basis change.\\nRotations \\x08are linear mappings so that we can express them by a\\nrotation matrix R(\\x12). Trigonometry (see Figure 3.16) allows us to de- rotation matrix\\ntermine the coordinates of the rotated axes (the image of \\x08) with respect\\nto the standard basis in R2. We obtain\\n\\x08(e1) =\\x14cos\\x12\\nsin\\x12\\x15\\n;\\x08(e2) =\\x14\\x00sin\\x12\\ncos\\x12\\x15\\n: (3.75)\\nTherefore, the rotation matrix that performs the basis change into the\\nrotated coordinates R(\\x12)is given as\\nR(\\x12) =\\x02\\x08(e1) \\x08(e2)\\x03=\\x14cos\\x12\\x00sin\\x12\\nsin\\x12cos\\x12\\x15\\n: (3.76)\\n3.9.2 Rotations in R3\\nIn contrast to the R2case, in R3we can rotate any two-dimensional plane\\nabout a one-dimensional axis. The easiest way to specify the general rota-\\ntion matrix is to specify how the images of the standard basis e1;e2;e3are\\nsupposed to be rotated, and making sure these images Re1;Re2;Re3are\\northonormal to each other. We can then obtain a general rotation matrix\\nRby combining the images of the standard basis.\\nTo have a meaningful rotation angle, we have to deﬁne what “coun-\\nterclockwise” means when we operate in more than two dimensions. We\\nuse the convention that a “counterclockwise” (planar) rotation about an\\naxis refers to a rotation about an axis when we look at the axis “head on,\\nfrom the end toward the origin”. In R3, there are therefore three (planar)\\nrotations about the three standard basis vectors (see Figure 3.17):\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.9 Rotations 93\\nFigure 3.17\\nRotation of a vector\\n(gray) in R3by an\\nangle\\x12about the\\ne3-axis. The rotated\\nvector is shown in\\nblue.\\ne1e2e3\\n\\x12\\nRotation about the e1-axis\\nR1(\\x12) =\\x02\\x08(e1) \\x08(e2) \\x08(e3)\\x03=2\\n41 0 0\\n0 cos\\x12\\x00sin\\x12\\n0 sin\\x12cos\\x123\\n5:(3.77)\\nHere, thee1coordinate is ﬁxed, and the counterclockwise rotation is\\nperformed in the e2e3plane.\\nRotation about the e2-axis\\nR2(\\x12) =2\\n4cos\\x120 sin\\x12\\n0 1 0\\n\\x00sin\\x120 cos\\x123\\n5: (3.78)\\nIf we rotate the e1e3plane about the e2axis, we need to look at the e2\\naxis from its “tip” toward the origin.\\nRotation about the e3-axis\\nR3(\\x12) =2\\n4cos\\x12\\x00sin\\x120\\nsin\\x12cos\\x120\\n0 0 13\\n5: (3.79)\\nFigure 3.17 illustrates this.\\n3.9.3 Rotations in nDimensions\\nThe generalization of rotations from 2D and 3D to n-dimensional Eu-\\nclidean vector spaces can be intuitively described as ﬁxing n\\x002dimen-\\nsions and restrict the rotation to a two-dimensional plane in the n-dimen-\\nsional space. As in the three-dimensional case, we can rotate any plane\\n(two-dimensional subspace of Rn).\\nDeﬁnition 3.11 (Givens Rotation) .LetVbe ann-dimensional Euclidean\\nvector space and \\x08 :V!Van automorphism with transformation ma-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n94 Analytic Geometry\\ntrix\\nRij(\\x12) :=2\\n66664Ii\\x0010\\x01\\x01\\x01 \\x01\\x01\\x01 0\\n0 cos\\x12 0\\x00sin\\x120\\n0 0Ij\\x00i\\x001 0 0\\n0 sin\\x12 0 cos\\x12 0\\n0\\x01\\x01\\x01 \\x01\\x01\\x01 0In\\x00j3\\n777752Rn\\x02n;(3.80)\\nfor16i < j6nand\\x122R. ThenRij(\\x12)is called a Givens rotation . Givens rotation\\nEssentially,Rij(\\x12)is the identity matrix Inwith\\nrii= cos\\x12; rij=\\x00sin\\x12; rji= sin\\x12; rjj= cos\\x12: (3.81)\\nIn two dimensions (i.e., n= 2), we obtain (3.76) as a special case.\\n3.9.4 Properties of Rotations\\nRotations exhibit a number of useful properties, which can be derived by\\nconsidering them as orthogonal matrices (Deﬁnition 3.8):\\nRotations preserve distances, i.e., kx\\x00yk=kR\\x12(x)\\x00R\\x12(y)k. In other\\nwords, rotations leave the distance between any two points unchanged\\nafter the transformation.\\nRotations preserve angles, i.e., the angle between R\\x12xandR\\x12yequals\\nthe angle between xandy.\\nRotations in three (or more) dimensions are generally not commuta-\\ntive. Therefore, the order in which rotations are applied is important,\\neven if they rotate about the same point. Only in two dimensions vector\\nrotations are commutative, such that R(\\x1e)R(\\x12) =R(\\x12)R(\\x1e)for all\\n\\x1e;\\x122[0;2\\x19). They form an Abelian group (with multiplication) only if\\nthey rotate about the same point (e.g., the origin).\\n3.10 Further Reading\\nIn this chapter, we gave a brief overview of some of the important concepts\\nof analytic geometry, which we will use in later chapters of the book.\\nFor a broader and more in-depth overview of some of the concepts we\\npresented, we refer to the following excellent books: Axler (2015) and\\nBoyd and Vandenberghe (2018).\\nInner products allow us to determine speciﬁc bases of vector (sub)spaces,\\nwhere each vector is orthogonal to all others (orthogonal bases) using the\\nGram-Schmidt method. These bases are important in optimization and\\nnumerical algorithms for solving linear equation systems. For instance,\\nKrylov subspace methods, such as conjugate gradients or the generalized\\nminimal residual method (GMRES), minimize residual errors that are or-\\nthogonal to each other (Stoer and Burlirsch, 2002).\\nIn machine learning, inner products are important in the context of\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n3.10 Further Reading 95\\nkernel methods (Sch ¨olkopf and Smola, 2002). Kernel methods exploit the\\nfact that many linear algorithms can be expressed purely by inner prod-\\nuct computations. Then, the “kernel trick” allows us to compute these\\ninner products implicitly in a (potentially inﬁnite-dimensional) feature\\nspace, without even knowing this feature space explicitly. This allowed the\\n“non-linearization” of many algorithms used in machine learning, such as\\nkernel-PCA (Sch ¨olkopf et al., 1997) for dimensionality reduction. Gaus-\\nsian processes (Rasmussen and Williams, 2006) also fall into the category\\nof kernel methods and are the current state of the art in probabilistic re-\\ngression (ﬁtting curves to data points). The idea of kernels is explored\\nfurther in Chapter 12.\\nProjections are often used in computer graphics, e.g., to generate shad-\\nows. In optimization, orthogonal projections are often used to (iteratively)\\nminimize residual errors. This also has applications in machine learning,\\ne.g., in linear regression where we want to ﬁnd a (linear) function that\\nminimizes the residual errors, i.e., the lengths of the orthogonal projec-\\ntions of the data onto the linear function (Bishop, 2006). We will investi-\\ngate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also\\nuses projections to reduce the dimensionality of high-dimensional data.\\nWe will discuss this in more detail in Chapter 10.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n96 Analytic Geometry\\nExercises\\n3.1 Show thath\\x01;\\x01ideﬁned for all x= [x1;x2]>2R2andy= [y1;y2]>2R2by\\nhx;yi:=x1y1\\x00(x1y2+x2y1) + 2(x2y2)\\nis an inner product.\\n3.2 Consider R2withh\\x01;\\x01ideﬁned for all xandyinR2as\\nhx;yi:=x>\\x14\\n2 0\\n1 2\\x15\\n|{z}\\n=:Ay:\\nIsh\\x01;\\x01ian inner product?\\n3.3 Compute the distance between\\nx=2\\n41\\n2\\n33\\n5;y=2\\n4\\x001\\n\\x001\\n03\\n5\\nusing\\na.hx;yi:=x>y\\nb.hx;yi:=x>Ay;A:=2\\n42 1 0\\n1 3\\x001\\n0\\x001 23\\n5\\n3.4 Compute the angle between\\nx=\\x14\\n1\\n2\\x15\\n;y=\\x14\\n\\x001\\n\\x001\\x15\\nusing\\na.hx;yi:=x>y\\nb.hx;yi:=x>By;B:=\\x14\\n2 1\\n1 3\\x15\\n3.5 Consider the Euclidean vector space R5with the dot product. A subspace\\nU\\x12R5andx2R5are given by\\nU= span[2\\n666640\\n\\x001\\n2\\n0\\n23\\n77775;2\\n666641\\n\\x003\\n1\\n\\x001\\n23\\n77775;2\\n66664\\x003\\n4\\n1\\n2\\n13\\n77775;2\\n66664\\x001\\n\\x003\\n5\\n0\\n73\\n77775];x=2\\n66664\\x001\\n\\x009\\n\\x001\\n4\\n13\\n77775:\\na. Determine the orthogonal projection \\x19U(x)ofxontoU\\nb. Determine the distance d(x;U)\\n3.6 Consider R3with the inner product\\nhx;yi:=x>2\\n42 1 0\\n1 2\\x001\\n0\\x001 23\\n5y:\\nFurthermore, we deﬁne e1;e2;e3as the standard/canonical basis in R3.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 97\\na. Determine the orthogonal projection \\x19U(e2)ofe2onto\\nU= span[e1;e3]:\\nHint: Orthogonality is deﬁned through the inner product.\\nb. Compute the distance d(e2;U).\\nc. Draw the scenario: standard basis vectors and \\x19U(e2)\\n3.7 LetVbe a vector space and \\x19an endomorphism of V.\\na. Prove that \\x19is a projection if and only if idV\\x00\\x19is a projection, where\\nidVis the identity endomorphism on V.\\nb. Assume now that \\x19is a projection. Calculate Im(idV\\x00\\x19)andker(idV\\x00\\x19)\\nas a function of Im(\\x19)andker(\\x19).\\n3.8 Using the Gram-Schmidt method, turn the basis B= (b1;b2)of a two-\\ndimensional subspace U\\x12R3into an ONB C= (c1;c2)ofU, where\\nb1:=2\\n41\\n1\\n13\\n5;b2:=2\\n4\\x001\\n2\\n03\\n5:\\n3.9 Letn2Nand letx1;:::;xn>0benpositive real numbers so that x1+\\n:::+xn= 1. Use the Cauchy-Schwarz inequality and show that\\na.Pn\\ni=1x2\\ni>1\\nn\\nb.Pn\\ni=11\\nxi>n2\\nHint: Think about the dot product on Rn. Then, choose speciﬁc vectors\\nx;y2Rnand apply the Cauchy-Schwarz inequality.\\n3.10 Rotate the vectors\\nx1:=\\x14\\n2\\n3\\x15\\n;x2:=\\x14\\n0\\n\\x001\\x15\\nby30\\x0e.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n4\\nMatrix Decompositions\\nIn Chapters 2 and 3, we studied ways to manipulate and measure vectors,\\nprojections of vectors, and linear mappings. Mappings and transforma-\\ntions of vectors can be conveniently described as operations performed by\\nmatrices. Moreover, data is often represented in matrix form as well, e.g.,\\nwhere the rows of the matrix represent different people and the columns\\ndescribe different features of the people, such as weight, height, and socio-\\neconomic status. In this chapter, we present three aspects of matrices: how\\nto summarize matrices, how matrices can be decomposed, and how these\\ndecompositions can be used for matrix approximations.\\nWe ﬁrst consider methods that allow us to describe matrices with just\\na few numbers that characterize the overall properties of matrices. We\\nwill do this in the sections on determinants (Section 4.1) and eigenval-\\nues (Section 4.2) for the important special case of square matrices. These\\ncharacteristic numbers have important mathematical consequences and\\nallow us to quickly grasp what useful properties a matrix has. From here\\nwe will proceed to matrix decomposition methods: An analogy for ma-\\ntrix decomposition is the factoring of numbers, such as the factoring of\\n21into prime numbers 7\\x013. For this reason matrix decomposition is also\\noften referred to as matrix factorization . Matrix decompositions are used matrix factorization\\nto describe a matrix by means of a different representation using factors\\nof interpretable matrices.\\nWe will ﬁrst cover a square-root-like operation for symmetric, positive\\ndeﬁnite matrices, the Cholesky decomposition (Section 4.3). From here\\nwe will look at two related methods for factorizing matrices into canoni-\\ncal forms. The ﬁrst one is known as matrix diagonalization (Section 4.4),\\nwhich allows us to represent the linear mapping using a diagonal trans-\\nformation matrix if we choose an appropriate basis. The second method,\\nsingular value decomposition (Section 4.5), extends this factorization to\\nnon-square matrices, and it is considered one of the fundamental concepts\\nin linear algebra. These decompositions are helpful, as matrices represent-\\ning numerical data are often very large and hard to analyze. We conclude\\nthe chapter with a systematic overview of the types of matrices and the\\ncharacteristic properties that distinguish them in the form of a matrix tax-\\nonomy (Section 4.7).\\nThe methods that we cover in this chapter will become important in\\n98\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n4.1 Determinant and Trace 99\\nFigure 4.1 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhere they are used\\nin other parts of the\\nbook.Determinant Invertibility Cholesky\\nEigenvalues\\nEigenvectors Orthogonal matrix Diagonalization\\nSVDChapter 6\\nProbability\\n& distributions\\nChapter 10\\nDimensionality\\nreductiontests used inused in\\nused in determines\\nused in\\nused in\\nused inconstructs used in\\nused inused in\\nboth subsequent mathematical chapters, such as Chapter 6, but also in\\napplied chapters, such as dimensionality reduction in Chapters 10 or den-\\nsity estimation in Chapter 11. This chapter’s overall structure is depicted\\nin the mind map of Figure 4.1.\\n4.1 Determinant and TraceThe determinant\\nnotationjAjmust\\nnot be confused\\nwith the absolute\\nvalue.Determinants are important concepts in linear algebra. A determinant is\\na mathematical object in the analysis and solution of systems of linear\\nequations. Determinants are only deﬁned for square matrices A2Rn\\x02n,\\ni.e., matrices with the same number of rows and columns. In this book,\\nwe write the determinant as det(A)or sometimes asjAjso that\\ndet(A) =\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ca11a12::: a 1n\\na21a22::: a 2n\\n.........\\nan1an2::: ann\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c: (4.1)\\nThedeterminant of a square matrix A2Rn\\x02nis a function that maps A determinant\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n100 Matrix Decompositions\\nonto a real number. Before providing a deﬁnition of the determinant for\\ngeneraln\\x02nmatrices, let us have a look at some motivating examples,\\nand deﬁne determinants for some special matrices.\\nExample 4.1 (Testing for Matrix Invertibility)\\nLet us begin with exploring if a square matrix Ais invertible (see Sec-\\ntion 2.2.2). For the smallest cases, we already know when a matrix\\nis invertible. If Ais a 1\\x021matrix, i.e., it is a scalar number, then\\nA=a=)A\\x001=1\\na. Thusa1\\na= 1holds, if and only if a6= 0.\\nFor2\\x022matrices, by the deﬁnition of the inverse (Deﬁnition 2.3), we\\nknow thatAA\\x001=I. Then, with (2.24), the inverse of Ais\\nA\\x001=1\\na11a22\\x00a12a21\\x14a22\\x00a12\\n\\x00a21a11\\x15\\n: (4.2)\\nHence,Ais invertible if and only if\\na11a22\\x00a12a216= 0: (4.3)\\nThis quantity is the determinant of A2R2\\x022, i.e.,\\ndet(A) =\\x0c\\x0c\\x0c\\x0c\\x0ca11a12\\na21a22\\x0c\\x0c\\x0c\\x0c\\x0c=a11a22\\x00a12a21: (4.4)\\nExample 4.1 points already at the relationship between determinants\\nand the existence of inverse matrices. The next theorem states the same\\nresult forn\\x02nmatrices.\\nTheorem 4.1. For any square matrix A2Rn\\x02nit holds thatAis invertible\\nif and only if det(A)6= 0.\\nWe have explicit (closed-form) expressions for determinants of small\\nmatrices in terms of the elements of the matrix. For n= 1,\\ndet(A) = det(a11) =a11: (4.5)\\nForn= 2,\\ndet(A) =\\x0c\\x0c\\x0c\\x0ca11a12\\na21a22\\x0c\\x0c\\x0c\\x0c=a11a22\\x00a12a21; (4.6)\\nwhich we have observed in the preceding example.\\nForn= 3(known as Sarrus’ rule),\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ca11a12a13\\na21a22a23\\na31a32a33\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c=a11a22a33+a21a32a13+a31a12a23 (4.7)\\n\\x00a31a22a13\\x00a11a32a23\\x00a21a12a33:\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.1 Determinant and Trace 101\\nFor a memory aid of the product terms in Sarrus’ rule, try tracing the\\nelements of the triple products in the matrix.\\nWe call a square matrix Tanupper-triangular matrix ifTij= 0 for upper-triangular\\nmatrix i>j , i.e., the matrix is zero below its diagonal. Analogously, we deﬁne a\\nlower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular\\nmatrix angular matrix T2Rn\\x02n, the determinant is the product of the diagonal\\nelements, i.e.,\\ndet(T) =nY\\ni=1Tii: (4.8)\\nThe determinant is\\nthe signed volume\\nof the parallelepiped\\nformed by the\\ncolumns of the\\nmatrix.\\nFigure 4.2 The area\\nof the parallelogram\\n(shaded region)\\nspanned by the\\nvectorsbandgis\\njdet([b;g])j.\\nb\\ng\\nFigure 4.3 The\\nvolume of the\\nparallelepiped\\n(shaded volume)\\nspanned by vectors\\nr;b;gis\\njdet([r;b;g])j.\\nb\\ngrExample 4.2 (Determinants as Measures of Volume)\\nThe notion of a determinant is natural when we consider it as a mapping\\nfrom a set of nvectors spanning an object in Rn. It turns out that the de-\\nterminant det(A)is the signed volume of an n-dimensional parallelepiped\\nformed by columns of the matrix A.\\nForn= 2, the columns of the matrix form a parallelogram; see Fig-\\nure 4.2. As the angle between vectors gets smaller, the area of a parallel-\\nogram shrinks, too. Consider two vectors b;gthat form the columns of a\\nmatrixA= [b;g]. Then, the absolute value of the determinant of Ais the\\narea of the parallelogram with vertices 0;b;g;b+g. In particular, if b;g\\nare linearly dependent so that b=\\x15gfor some\\x152R, they no longer\\nform a two-dimensional parallelogram. Therefore, the corresponding area\\nis0. On the contrary, if b;gare linearly independent and are multiples of\\nthe canonical basis vectors e1;e2then they can be written as b=\\x14b\\n0\\x15\\nand\\ng=\\x140\\ng\\x15\\n, and the determinant is\\x0c\\x0c\\x0c\\x0cb0\\n0g\\x0c\\x0c\\x0c\\x0c=bg\\x000 =bg.\\nThe sign of the determinant indicates the orientation of the spanning\\nvectorsb;gwith respect to the standard basis (e1;e2). In our ﬁgure, ﬂip-\\nping the order to g;bswaps the columns of Aand reverses the orientation\\nof the shaded area. This becomes the familiar formula: area =height\\x02\\nlength. This intuition extends to higher dimensions. In R3, we consider\\nthree vectors r;b;g2R3spanning the edges of a parallelepiped, i.e., a\\nsolid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the\\ndeterminant\\nindicates the\\norientation of the\\nspanning vectors.solute value of the determinant of the 3\\x023matrix [r;b;g]is the volume\\nof the solid. Thus, the determinant acts as a function that measures the\\nsigned volume formed by column vectors composed in a matrix.\\nConsider the three linearly independent vectors r;g;b2R3given as\\nr=2\\n42\\n0\\n\\x0083\\n5;g=2\\n46\\n1\\n03\\n5;b=2\\n41\\n4\\n\\x0013\\n5: (4.9)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n102 Matrix Decompositions\\nWriting these vectors as the columns of a matrix\\nA= [r;g;b] =2\\n42 6 1\\n0 1 4\\n\\x008 0\\x0013\\n5 (4.10)\\nallows us to compute the desired volume as\\nV=jdet(A)j= 186: (4.11)\\nComputing the determinant of an n\\x02nmatrix requires a general algo-\\nrithm to solve the cases for n>3, which we are going to explore in the fol-\\nlowing. Theorem 4.2 below reduces the problem of computing the deter-\\nminant of an n\\x02nmatrix to computing the determinant of (n\\x001)\\x02(n\\x001)\\nmatrices. By recursively applying the Laplace expansion (Theorem 4.2),\\nwe can therefore compute determinants of n\\x02nmatrices by ultimately\\ncomputing determinants of 2\\x022matrices.\\nLaplace expansion\\nTheorem 4.2 (Laplace Expansion) .Consider a matrix A2Rn\\x02n. Then,\\nfor allj= 1;:::;n :\\n1. Expansion along column j det(Ak;j)is called\\naminor and\\n(\\x001)k+jdet(Ak;j)\\nacofactor .det(A) =nX\\nk=1(\\x001)k+jakjdet(Ak;j): (4.12)\\n2. Expansion along row j\\ndet(A) =nX\\nk=1(\\x001)k+jajkdet(Aj;k): (4.13)\\nHereAk;j2R(n\\x001)\\x02(n\\x001)is the submatrix of Athat we obtain when delet-\\ning rowkand column j.\\nExample 4.3 (Laplace Expansion)\\nLet us compute the determinant of\\nA=2\\n41 2 3\\n3 1 2\\n0 0 13\\n5 (4.14)\\nusing the Laplace expansion along the ﬁrst row. Applying (4.13) yields\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c1 2 3\\n3 1 2\\n0 0 1\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c= (\\x001)1+1\\x011\\x0c\\x0c\\x0c\\x0c1 2\\n0 1\\x0c\\x0c\\x0c\\x0c\\n+ (\\x001)1+2\\x012\\x0c\\x0c\\x0c\\x0c3 2\\n0 1\\x0c\\x0c\\x0c\\x0c+ (\\x001)1+3\\x013\\x0c\\x0c\\x0c\\x0c3 1\\n0 0\\x0c\\x0c\\x0c\\x0c:(4.15)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.1 Determinant and Trace 103\\nWe use (4.6) to compute the determinants of all 2\\x022matrices and obtain\\ndet(A) = 1(1\\x000)\\x002(3\\x000) + 3(0\\x000) =\\x005: (4.16)\\nFor completeness we can compare this result to computing the determi-\\nnant using Sarrus’ rule (4.7):\\ndet(A) = 1\\x011\\x011+3\\x010\\x013+0\\x012\\x012\\x000\\x011\\x013\\x001\\x010\\x012\\x003\\x012\\x011 = 1\\x006 =\\x005:(4.17)\\nForA2Rn\\x02nthe determinant exhibits the following properties:\\nThe determinant of a matrix product is the product of the corresponding\\ndeterminants, det(AB) = det(A)det(B).\\nDeterminants are invariant to transposition, i.e., det(A) = det(A>).\\nIfAis regular (invertible), then det(A\\x001) =1\\ndet(A).\\nSimilar matrices (Deﬁnition 2.22) possess the same determinant. There-\\nfore, for a linear mapping \\x08 :V!Vall transformation matrices A\\x08\\nof\\x08have the same determinant. Thus, the determinant is invariant to\\nthe choice of basis of a linear mapping.\\nAdding a multiple of a column/row to another one does not change\\ndet(A).\\nMultiplication of a column/row with \\x152Rscales det(A)by\\x15. In\\nparticular, det(\\x15A) =\\x15ndet(A).\\nSwapping two rows/columns changes the sign of det(A).\\nBecause of the last three properties, we can use Gaussian elimination (see\\nSection 2.1) to compute det(A)by bringingAinto row-echelon form.\\nWe can stop Gaussian elimination when we have Ain a triangular form\\nwhere the elements below the diagonal are all 0. Recall from (4.8) that the\\ndeterminant of a triangular matrix is the product of the diagonal elements.\\nTheorem 4.3. A square matrix A2Rn\\x02nhasdet(A)6= 0if and only if\\nrk(A) =n. In other words, Ais invertible if and only if it is full rank.\\nWhen mathematics was mainly performed by hand, the determinant\\ncalculation was considered an essential way to analyze matrix invertibil-\\nity. However, contemporary approaches in machine learning use direct\\nnumerical methods that superseded the explicit calculation of the deter-\\nminant. For example, in Chapter 2, we learned that inverse matrices can\\nbe computed by Gaussian elimination. Gaussian elimination can thus be\\nused to compute the determinant of a matrix.\\nDeterminants will play an important theoretical role for the following\\nsections, especially when we learn about eigenvalues and eigenvectors\\n(Section 4.2) through the characteristic polynomial.\\nDeﬁnition 4.4. Thetrace of a square matrix A2Rn\\x02nis deﬁned as trace\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n104 Matrix Decompositions\\ntr(A) :=nX\\ni=1aii; (4.18)\\ni.e. , the trace is the sum of the diagonal elements of A.\\nThe trace satisﬁes the following properties:\\ntr(A+B) =tr(A) +tr(B)forA;B2Rn\\x02n\\ntr(\\x0bA) =\\x0btr(A);\\x0b2RforA2Rn\\x02n\\ntr(In) =n\\ntr(AB) =tr(BA)forA2Rn\\x02k;B2Rk\\x02n\\nIt can be shown that only one function satisﬁes these four properties to-\\ngether – the trace (Gohberg et al., 2012).\\nThe properties of the trace of matrix products are more general. Specif-\\nically, the trace is invariant under cyclic permutations, i.e., The trace is\\ninvariant under\\ncyclic permutations. tr(AKL ) =tr(KLA ) (4.19)\\nfor matricesA2Ra\\x02k;K2Rk\\x02l;L2Rl\\x02a. This property generalizes to\\nproducts of an arbitrary number of matrices. As a special case of (4.19), it\\nfollows that for two vectors x;y2Rn\\ntr(xy>) =tr(y>x) =y>x2R: (4.20)\\nGiven a linear mapping \\x08 :V!V, whereVis a vector space, we\\ndeﬁne the trace of this map by using the trace of matrix representation\\nof\\x08. For a given basis of V, we can describe \\x08by means of the transfor-\\nmation matrix A. Then the trace of \\x08is the trace of A. For a different\\nbasis ofV, it holds that the corresponding transformation matrix Bof\\x08\\ncan be obtained by a basis change of the form S\\x001ASfor suitableS(see\\nSection 2.7.2). For the corresponding trace of \\x08, this means\\ntr(B) =tr(S\\x001AS)(4.19)=tr(ASS\\x001) =tr(A): (4.21)\\nHence, while matrix representations of linear mappings are basis depen-\\ndent the trace of a linear mapping \\x08is independent of the basis.\\nIn this section, we covered determinants and traces as functions char-\\nacterizing a square matrix. Taking together our understanding of determi-\\nnants and traces we can now deﬁne an important equation describing a\\nmatrixAin terms of a polynomial, which we will use extensively in the\\nfollowing sections.\\nDeﬁnition 4.5 (Characteristic Polynomial) .For\\x152Rand a square ma-\\ntrixA2Rn\\x02n\\npA(\\x15) := det(A\\x00\\x15I) (4.22a)\\n=c0+c1\\x15+c2\\x152+\\x01\\x01\\x01+cn\\x001\\x15n\\x001+ (\\x001)n\\x15n; (4.22b)\\nc0;:::;cn\\x0012R, is the characteristic polynomial ofA. In particular, characteristic\\npolynomial\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.2 Eigenvalues and Eigenvectors 105\\nc0= det(A); (4.23)\\ncn\\x001= (\\x001)n\\x001tr(A): (4.24)\\nThe characteristic polynomial (4.22a) will allow us to compute eigen-\\nvalues and eigenvectors, covered in the next section.\\n4.2 Eigenvalues and Eigenvectors\\nWe will now get to know a new way to characterize a matrix and its associ-\\nated linear mapping. Recall from Section 2.7.1 that every linear mapping\\nhas a unique transformation matrix given an ordered basis. We can in-\\nterpret linear mappings and their associated transformation matrices by\\nperforming an “eigen” analysis. As we will see, the eigenvalues of a lin- Eigen is a German\\nword meaning\\n“characteristic”,\\n“self”, or “own”.ear mapping will tell us how a special set of vectors, the eigenvectors, is\\ntransformed by the linear mapping.\\nDeﬁnition 4.6. LetA2Rn\\x02nbe a square matrix. Then \\x152Ris an\\neigenvalue ofAandx2Rnnf0gis the corresponding eigenvector ofAif eigenvalue\\neigenvectorAx=\\x15x: (4.25)\\nWe call (4.25) the eigenvalue equation . eigenvalue equation\\nRemark. In the linear algebra literature and software, it is often a conven-\\ntion that eigenvalues are sorted in descending order, so that the largest\\neigenvalue and associated eigenvector are called the ﬁrst eigenvalue and\\nits associated eigenvector, and the second largest called the second eigen-\\nvalue and its associated eigenvector, and so on. However, textbooks and\\npublications may have different or no notion of orderings. We do not want\\nto presume an ordering in this book if not stated explicitly. }\\nThe following statements are equivalent:\\n\\x15is an eigenvalue of A2Rn\\x02n.\\nThere exists an x2Rnnf0gwithAx=\\x15x, or equivalently, (A\\x00\\n\\x15In)x=0can be solved non-trivially, i.e., x6=0.\\nrk(A\\x00\\x15In)<n.\\ndet(A\\x00\\x15In) = 0 .\\nDeﬁnition 4.7 (Collinearity and Codirection) .Two vectors that point in\\nthe same direction are called codirected . Two vectors are collinear if they codirected\\ncollinear point in the same or the opposite direction.\\nRemark (Non-uniqueness of eigenvectors) .Ifxis an eigenvector of A\\nassociated with eigenvalue \\x15, then for any c2Rnf0git holds that cxis\\nan eigenvector of Awith the same eigenvalue since\\nA(cx) =cAx=c\\x15x=\\x15(cx): (4.26)\\nThus, all vectors that are collinear to xare also eigenvectors of A.\\n}\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n106 Matrix Decompositions\\nTheorem 4.8. \\x152Ris an eigenvalue of A2Rn\\x02nif and only if \\x15is a\\nroot of the characteristic polynomial pA(\\x15)ofA.\\nDeﬁnition 4.9. Let a square matrix Ahave an eigenvalue \\x15i. The algebraic algebraic\\nmultiplicity multiplicity of\\x15iis the number of times the root appears in the character-\\nistic polynomial.\\nDeﬁnition 4.10 (Eigenspace and Eigenspectrum) .ForA2Rn\\x02n, the set\\nof all eigenvectors of Aassociated with an eigenvalue \\x15spans a subspace\\nofRn, which is called the eigenspace ofAwith respect to \\x15and is denoted eigenspace\\nbyE\\x15. The set of all eigenvalues of Ais called the eigenspectrum , or just eigenspectrum\\nspectrum , ofA. spectrum\\nIf\\x15is an eigenvalue of A2Rn\\x02n, then the corresponding eigenspace\\nE\\x15is the solution space of the homogeneous system of linear equations\\n(A\\x00\\x15I)x=0. Geometrically, the eigenvector corresponding to a nonzero\\neigenvalue points in a direction that is stretched by the linear mapping.\\nThe eigenvalue is the factor by which it is stretched. If the eigenvalue is\\nnegative, the direction of the stretching is ﬂipped.\\nExample 4.4 (The Case of the Identity Matrix)\\nThe identity matrix I2Rn\\x02nhas characteristic polynomial pI(\\x15) =\\ndet(I\\x00\\x15I) = (1\\x00\\x15)n= 0, which has only one eigenvalue \\x15= 1that oc-\\ncursntimes. Moreover, Ix=\\x15x= 1xholds for all vectors x2Rnnf0g.\\nBecause of this, the sole eigenspace E1of the identity matrix spans ndi-\\nmensions, and all nstandard basis vectors of Rnare eigenvectors of I.\\nUseful properties regarding eigenvalues and eigenvectors include the\\nfollowing:\\nA matrixAand its transpose A>possess the same eigenvalues, but not\\nnecessarily the same eigenvectors.\\nThe eigenspace E\\x15is the null space of A\\x00\\x15Isince\\nAx=\\x15x()Ax\\x00\\x15x=0 (4.27a)\\n() (A\\x00\\x15I)x=0()x2ker(A\\x00\\x15I):(4.27b)\\nSimilar matrices (see Deﬁnition 2.22) possess the same eigenvalues.\\nTherefore, a linear mapping \\x08has eigenvalues that are independent of\\nthe choice of basis of its transformation matrix. This makes eigenvalues,\\ntogether with the determinant and the trace, key characteristic param-\\neters of a linear mapping as they are all invariant under basis change.\\nSymmetric, positive deﬁnite matrices always have positive, real eigen-\\nvalues.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.2 Eigenvalues and Eigenvectors 107\\nExample 4.5 (Computing Eigenvalues, Eigenvectors, and\\nEigenspaces)\\nLet us ﬁnd the eigenvalues and eigenvectors of the 2\\x022matrix\\nA=\\x144 2\\n1 3\\x15\\n: (4.28)\\nStep 1: Characteristic Polynomial. From our deﬁnition of the eigen-\\nvectorx6=0and eigenvalue \\x15ofA, there will be a vector such that\\nAx=\\x15x, i.e., (A\\x00\\x15I)x=0. Sincex6=0, this requires that the kernel\\n(null space) of A\\x00\\x15Icontains more elements than just 0. This means\\nthatA\\x00\\x15Iis not invertible and therefore det(A\\x00\\x15I) = 0 . Hence, we\\nneed to compute the roots of the characteristic polynomial (4.22a) to ﬁnd\\nthe eigenvalues.\\nStep 2: Eigenvalues. The characteristic polynomial is\\npA(\\x15) = det(A\\x00\\x15I) (4.29a)\\n= det\\x12\\x144 2\\n1 3\\x15\\n\\x00\\x14\\x150\\n0\\x15\\x15\\x13\\n=\\x0c\\x0c\\x0c\\x0c4\\x00\\x15 2\\n1 3\\x00\\x15\\x0c\\x0c\\x0c\\x0c(4.29b)\\n= (4\\x00\\x15)(3\\x00\\x15)\\x002\\x011: (4.29c)\\nWe factorize the characteristic polynomial and obtain\\np(\\x15) = (4\\x00\\x15)(3\\x00\\x15)\\x002\\x011 = 10\\x007\\x15+\\x152= (2\\x00\\x15)(5\\x00\\x15)(4.30)\\ngiving the roots \\x151= 2and\\x152= 5.\\nStep 3: Eigenvectors and Eigenspaces. We ﬁnd the eigenvectors that\\ncorrespond to these eigenvalues by looking at vectors xsuch that\\n\\x144\\x00\\x15 2\\n1 3\\x00\\x15\\x15\\nx=0: (4.31)\\nFor\\x15= 5we obtain\\n\\x144\\x005 2\\n1 3\\x005\\x15\\x14x1\\nx2\\x15\\n=\\x14\\x001 2\\n1\\x002\\x15\\x14x1\\nx2\\x15\\n=0: (4.32)\\nWe solve this homogeneous system and obtain a solution space\\nE5= span[\\x142\\n1\\x15\\n]: (4.33)\\nThis eigenspace is one-dimensional as it possesses a single basis vector.\\nAnalogously, we ﬁnd the eigenvector for \\x15= 2by solving the homoge-\\nneous system of equations\\n\\x144\\x002 2\\n1 3\\x002\\x15\\nx=\\x142 2\\n1 1\\x15\\nx=0: (4.34)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n108 Matrix Decompositions\\nThis means any vector x=\\x14x1\\nx2\\x15\\n, wherex2=\\x00x1, such as\\x141\\n\\x001\\x15\\n, is an\\neigenvector with eigenvalue 2. The corresponding eigenspace is given as\\nE2= span[\\x141\\n\\x001\\x15\\n]: (4.35)\\nThe two eigenspaces E5andE2in Example 4.5 are one-dimensional\\nas they are each spanned by a single vector. However, in other cases\\nwe may have multiple identical eigenvalues (see Deﬁnition 4.9) and the\\neigenspace may have more than one dimension.\\nDeﬁnition 4.11. Let\\x15ibe an eigenvalue of a square matrix A. Then the\\ngeometric multiplicity of\\x15iis the number of linearly independent eigen- geometric\\nmultiplicity vectors associated with \\x15i. In other words, it is the dimensionality of the\\neigenspace spanned by the eigenvectors associated with \\x15i.\\nRemark. A speciﬁc eigenvalue’s geometric multiplicity must be at least\\none because every eigenvalue has at least one associated eigenvector. An\\neigenvalue’s geometric multiplicity cannot exceed its algebraic multiplic-\\nity, but it may be lower. }\\nExample 4.6\\nThe matrixA=\\x142 1\\n0 2\\x15\\nhas two repeated eigenvalues \\x151=\\x152= 2and an\\nalgebraic multiplicity of 2. The eigenvalue has, however, only one distinct\\nunit eigenvector x1=\\x141\\n0\\x15\\nand, thus, geometric multiplicity 1.\\nGraphical Intuition in Two Dimensions\\nLet us gain some intuition for determinants, eigenvectors, and eigenval-\\nues using different linear mappings. Figure 4.4 depicts ﬁve transformation\\nmatricesA1;:::;A5and their impact on a square grid of points, centered\\nat the origin: In geometry, the\\narea-preserving\\nproperties of this\\ntype of shearing\\nparallel to an axis is\\nalso known as\\nCavalieri’s principle\\nof equal areas for\\nparallelograms\\n(Katz, 2004).A1=\\x141\\n20\\n0 2\\x15\\n. The direction of the two eigenvectors correspond to the\\ncanonical basis vectors in R2, i.e., to two cardinal axes. The vertical axis\\nis extended by a factor of 2(eigenvalue \\x151= 2), and the horizontal axis\\nis compressed by factor1\\n2(eigenvalue \\x152=1\\n2). The mapping is area\\npreserving ( det(A1) = 1 = 2\\x011\\n2).\\nA2=\\x1411\\n2\\n0 1\\x15\\ncorresponds to a shearing mapping , i.e., it shears the\\npoints along the horizontal axis to the right if they are on the positive\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.2 Eigenvalues and Eigenvectors 109\\nFigure 4.4\\nDeterminants and\\neigenspaces.\\nOverview of ﬁve\\nlinear mappings and\\ntheir associated\\ntransformation\\nmatrices\\nAi2R2\\x022\\nprojecting 400\\ncolor-coded points\\nx2R2(left\\ncolumn) onto target\\npointsAix(right\\ncolumn). The\\ncentral column\\ndepicts the ﬁrst\\neigenvector,\\nstretched by its\\nassociated\\neigenvalue\\x151, and\\nthe second\\neigenvector\\nstretched by its\\neigenvalue\\x152. Each\\nrow depicts the\\neffect of one of ﬁve\\ntransformation\\nmatricesAiwith\\nrespect to the\\nstandard basis.\\ndet(A) = 1.0\\x151= 2.0\\n\\x152= 0.5\\ndet(A) = 1.0\\x151= 1.0\\n\\x152= 1.0\\ndet(A) = 1.0\\x151= (0.87-0.5j)\\n\\x152= (0.87+0.5j)\\ndet(A) = 0.0\\x151= 0.0\\n\\x152= 2.0\\ndet(A) = 0.75\\x151= 0.5\\n\\x152= 1.5\\nhalf of the vertical axis, and to the left vice versa. This mapping is area\\npreserving ( det(A2) = 1 ). The eigenvalue \\x151= 1 =\\x152is repeated\\nand the eigenvectors are collinear (drawn here for emphasis in two\\nopposite directions). This indicates that the mapping acts only along\\none direction (the horizontal axis).\\nA3=\\x14cos(\\x19\\n6)\\x00sin(\\x19\\n6)\\nsin(\\x19\\n6) cos(\\x19\\n6)\\x15\\n=1\\n2\\x14p\\n3\\x001\\n1p\\n3\\x15\\nThe matrixA3rotates the\\npoints by\\x19\\n6rad = 30\\x0ecounter-clockwise and has only complex eigen-\\nvalues, reﬂecting that the mapping is a rotation (hence, no eigenvectors\\nare drawn). A rotation has to be volume preserving, and so the deter-\\nminant is 1. For more details on rotations, we refer to Section 3.9.\\nA4=\\x141\\x001\\n\\x001 1\\x15\\nrepresents a mapping in the standard basis that col-\\nlapses a two-dimensional domain onto one dimension. Since one eigen-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n110 Matrix Decompositions\\nvalue is 0, the space in direction of the (blue) eigenvector corresponding\\nto\\x151= 0 collapses, while the orthogonal (red) eigenvector stretches\\nspace by a factor \\x152= 2. Therefore, the area of the image is 0.\\nA5=\\x1411\\n21\\n21\\x15\\nis a shear-and-stretch mapping that scales space by 75%\\nsincejdet(A5)j=3\\n4. It stretches space along the (red) eigenvector\\nof\\x152by a factor 1:5and compresses it along the orthogonal (blue)\\neigenvector by a factor 0:5.\\nExample 4.7 (Eigenspectrum of a Biological Neural Network)\\nFigure 4.5\\nCaenorhabditis\\nelegans neural\\nnetwork (Kaiser and\\nHilgetag,\\n2006).(a) Sym-\\nmetrized\\nconnectivity matrix;\\n(b) Eigenspectrum.\\n0 50 100 150 200 250\\nneuron index0\\n50\\n100\\n150\\n200\\n250neuron index\\n(a) Connectivity matrix.\\n0 100 200\\nindex of sorted eigenvalue\\x0010\\x0050510152025eigenvalue\\n (b) Eigenspectrum.\\nMethods to analyze and learn from network data are an essential com-\\nponent of machine learning methods. The key to understanding networks\\nis the connectivity between network nodes, especially if two nodes are\\nconnected to each other or not. In data science applications, it is often\\nuseful to study the matrix that captures this connectivity data.\\nWe build a connectivity/adjacency matrix A2R277\\x02277of the complete\\nneural network of the worm C.Elegans . Each row/column represents one\\nof the 277neurons of this worm’s brain. The connectivity matrix Ahas\\na value ofaij= 1 if neuronitalks to neuron jthrough a synapse, and\\naij= 0 otherwise. The connectivity matrix is not symmetric, which im-\\nplies that eigenvalues may not be real valued. Therefore, we compute a\\nsymmetrized version of the connectivity matrix as Asym:=A+A>. This\\nnew matrixAsymis shown in Figure 4.5(a) and has a nonzero value aijif\\nand only if two neurons are connected (white pixels), irrespective of the\\ndirection of the connection. In Figure 4.5(b), we show the correspond-\\ning eigenspectrum of Asym. The horizontal axis shows the index of the\\neigenvalues, sorted in descending order. The vertical axis shows the corre-\\nsponding eigenvalue. The S-like shape of this eigenspectrum is typical for\\nmany biological neural networks. The underlying mechanism responsible\\nfor this is an area of active neuroscience research.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.2 Eigenvalues and Eigenvectors 111\\nTheorem 4.12. The eigenvectors x1;:::;xnof a matrixA2Rn\\x02nwithn\\ndistinct eigenvalues \\x151;:::;\\x15nare linearly independent.\\nThis theorem states that eigenvectors of a matrix with ndistinct eigen-\\nvalues form a basis of Rn.\\nDeﬁnition 4.13. A square matrix A2Rn\\x02nisdefective if it possesses defective\\nfewer thannlinearly independent eigenvectors.\\nA non-defective matrix A2Rn\\x02ndoes not necessarily require ndis-\\ntinct eigenvalues, but it does require that the eigenvectors form a basis of\\nRn. Looking at the eigenspaces of a defective matrix, it follows that the\\nsum of the dimensions of the eigenspaces is less than n. Speciﬁcally, a de-\\nfective matrix has at least one eigenvalue \\x15iwith an algebraic multiplicity\\nm> 1and a geometric multiplicity of less than m.\\nRemark. A defective matrix cannot have ndistinct eigenvalues, as distinct\\neigenvalues have linearly independent eigenvectors (Theorem 4.12). }\\nTheorem 4.14. Given a matrix A2Rm\\x02n, we can always obtain a sym-\\nmetric, positive semideﬁnite matrix S2Rn\\x02nby deﬁning\\nS:=A>A: (4.36)\\nRemark. Ifrk(A) =n, thenS:=A>Ais symmetric, positive deﬁnite.\\n}\\nUnderstanding why Theorem 4.14 holds is insightful for how we can\\nuse symmetrized matrices: Symmetry requires S=S>, and by insert-\\ning (4.36) we obtain S=A>A=A>(A>)>= (A>A)>=S>. More-\\nover, positive semideﬁniteness (Section 3.2.3) requires that x>Sx>0\\nand inserting (4.36) we obtain x>Sx=x>A>Ax= (x>A>)(Ax) =\\n(Ax)>(Ax)>0, because the dot product computes a sum of squares\\n(which are themselves non-negative).\\nspectral theorem\\nTheorem 4.15 (Spectral Theorem) .IfA2Rn\\x02nis symmetric, there ex-\\nists an orthonormal basis of the corresponding vector space Vconsisting of\\neigenvectors of A, and each eigenvalue is real.\\nA direct implication of the spectral theorem is that the eigendecompo-\\nsition of a symmetric matrix Aexists (with real eigenvalues), and that\\nwe can ﬁnd an ONB of eigenvectors so that A=PDP>, whereDis\\ndiagonal and the columns of Pcontain the eigenvectors.\\nExample 4.8\\nConsider the matrix\\nA=2\\n43 2 2\\n2 3 2\\n2 2 33\\n5: (4.37)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n112 Matrix Decompositions\\nThe characteristic polynomial of Ais\\npA(\\x15) =\\x00(\\x15\\x001)2(\\x15\\x007); (4.38)\\nso that we obtain the eigenvalues \\x151= 1 and\\x152= 7, where\\x151is a\\nrepeated eigenvalue. Following our standard procedure for computing\\neigenvectors, we obtain the eigenspaces\\nE1= span[2\\n4\\x001\\n1\\n03\\n5\\n|{z}\\n=:x1;2\\n4\\x001\\n0\\n13\\n5\\n|{z}\\n=:x2]; E 7= span[2\\n41\\n1\\n13\\n5\\n|{z}\\n=:x3]: (4.39)\\nWe see thatx3is orthogonal to both x1andx2. However, since x>\\n1x2=\\n16= 0, they are not orthogonal. The spectral theorem (Theorem 4.15)\\nstates that there exists an orthogonal basis, but the one we have is not\\northogonal. However, we can construct one.\\nTo construct such a basis, we exploit the fact that x1;x2are eigenvec-\\ntors associated with the same eigenvalue \\x15. Therefore, for any \\x0b;\\x0c2Rit\\nholds that\\nA(\\x0bx1+\\x0cx2) =Ax1\\x0b+Ax2\\x0c=\\x15(\\x0bx1+\\x0cx2); (4.40)\\ni.e., any linear combination of x1andx2is also an eigenvector of Aas-\\nsociated with \\x15. The Gram-Schmidt algorithm (Section 3.8.3) is a method\\nfor iteratively constructing an orthogonal/orthonormal basis from a set of\\nbasis vectors using such linear combinations. Therefore, even if x1andx2\\nare not orthogonal, we can apply the Gram-Schmidt algorithm and ﬁnd\\neigenvectors associated with \\x151= 1 that are orthogonal to each other\\n(and tox3). In our example, we will obtain\\nx0\\n1=2\\n4\\x001\\n1\\n03\\n5;x0\\n2=1\\n22\\n4\\x001\\n\\x001\\n23\\n5; (4.41)\\nwhich are orthogonal to each other, orthogonal to x3, and eigenvectors of\\nAassociated with \\x151= 1.\\nBefore we conclude our considerations of eigenvalues and eigenvectors\\nit is useful to tie these matrix characteristics together with the concepts of\\nthe determinant and the trace.\\nTheorem 4.16. The determinant of a matrix A2Rn\\x02nis the product of\\nits eigenvalues, i.e.,\\ndet(A) =nY\\ni=1\\x15i; (4.42)\\nwhere\\x15i2Care (possibly repeated) eigenvalues of A.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.2 Eigenvalues and Eigenvectors 113\\nFigure 4.6\\nGeometric\\ninterpretation of\\neigenvalues. The\\neigenvectors of A\\nget stretched by the\\ncorresponding\\neigenvalues. The\\narea of the unit\\nsquare changes by\\nj\\x151\\x152j, the\\nperimeter changes\\nby a factor of\\n1\\n2(j\\x151j+j\\x152j).x1x2\\nv1v2A\\nTheorem 4.17. The trace of a matrix A2Rn\\x02nis the sum of its eigenval-\\nues, i.e.,\\ntr(A) =nX\\ni=1\\x15i; (4.43)\\nwhere\\x15i2Care (possibly repeated) eigenvalues of A.\\nLet us provide a geometric intuition of these two theorems. Consider\\na matrixA2R2\\x022that possesses two linearly independent eigenvectors\\nx1;x2. For this example, we assume (x1;x2)are an ONB of R2so that they\\nare orthogonal and the area of the square they span is 1; see Figure 4.6.\\nFrom Section 4.1, we know that the determinant computes the change of\\narea of unit square under the transformation A. In this example, we can\\ncompute the change of area explicitly: Mapping the eigenvectors using\\nAgives us vectors v1=Ax1=\\x151x1andv2=Ax2=\\x152x2, i.e., the\\nnew vectorsviare scaled versions of the eigenvectors xi, and the scaling\\nfactors are the corresponding eigenvalues \\x15i.v1;v2are still orthogonal,\\nand the area of the rectangle they span is j\\x151\\x152j.\\nGiven thatx1;x2(in our example) are orthonormal, we can directly\\ncompute the perimeter of the unit square as 2(1 + 1) . Mapping the eigen-\\nvectors using Acreates a rectangle whose perimeter is 2(j\\x151j+j\\x152j).\\nTherefore, the sum of the absolute values of the eigenvalues tells us how\\nthe perimeter of the unit square changes under the transformation matrix\\nA.\\nExample 4.9 (Google’s PageRank – Webpages as Eigenvectors)\\nGoogle uses the eigenvector corresponding to the maximal eigenvalue of\\na matrixAto determine the rank of a page for search. The idea for the\\nPageRank algorithm, developed at Stanford University by Larry Page and\\nSergey Brin in 1996, was that the importance of any web page can be ap-\\nproximated by the importance of pages that link to it. For this, they write\\ndown all web sites as a huge directed graph that shows which page links\\nto which. PageRank computes the weight (importance) xi>0of a web\\nsiteaiby counting the number of pages pointing to ai. Moreover, PageR-\\nank takes into account the importance of the web sites that link to ai. The\\nnavigation behavior of a user is then modeled by a transition matrix Aof\\nthis graph that tells us with what (click) probability somebody will end up\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n114 Matrix Decompositions\\non a different web site. The matrix Ahas the property that for any ini-\\ntial rank/importance vector xof a web site the sequence x;Ax;A2x;:::\\nconverges to a vector x\\x03. This vector is called the PageRank and satisﬁes PageRank\\nAx\\x03=x\\x03, i.e., it is an eigenvector (with corresponding eigenvalue 1) of\\nA. After normalizing x\\x03, such thatkx\\x03k= 1, we can interpret the entries\\nas probabilities. More details and different perspectives on PageRank can\\nbe found in the original technical report (Page et al., 1999).\\n4.3 Cholesky Decomposition\\nThere are many ways to factorize special types of matrices that we en-\\ncounter often in machine learning. In the positive real numbers, we have\\nthe square-root operation that gives us a decomposition of the number\\ninto identical components, e.g., 9 = 3\\x013. For matrices, we need to be\\ncareful that we compute a square-root-like operation on positive quanti-\\nties. For symmetric, positive deﬁnite matrices (see Section 3.2.3), we can\\nchoose from a number of square-root equivalent operations. The Cholesky Cholesky\\ndecomposition decomposition /Cholesky factorization provides a square-root equivalent op-\\nCholesky\\nfactorizationeration on symmetric, positive deﬁnite matrices that is useful in practice.\\nTheorem 4.18 (Cholesky Decomposition) .A symmetric, positive deﬁnite\\nmatrixAcan be factorized into a product A=LL>, whereLis a lower-\\ntriangular matrix with positive diagonal elements:\\n2\\n64a11\\x01\\x01\\x01a1n\\n.........\\nan1\\x01\\x01\\x01ann3\\n75=2\\n64l11\\x01\\x01\\x01 0\\n.........\\nln1\\x01\\x01\\x01lnn3\\n752\\n64l11\\x01\\x01\\x01ln1\\n.........\\n0\\x01\\x01\\x01lnn3\\n75: (4.44)\\nLis called the Cholesky factor of A, andLis unique. Cholesky factor\\nExample 4.10 (Cholesky Factorization)\\nConsider a symmetric, positive deﬁnite matrix A2R3\\x023. We are inter-\\nested in ﬁnding its Cholesky factorization A=LL>, i.e.,\\nA=2\\n4a11a21a31\\na21a22a32\\na31a32a333\\n5=LL>=2\\n4l110 0\\nl21l220\\nl31l32l333\\n52\\n4l11l21l31\\n0l22l32\\n0 0l333\\n5:(4.45)\\nMultiplying out the right-hand side yields\\nA=2\\n4l2\\n11l21l11 l31l11\\nl21l11l2\\n21+l2\\n22l31l21+l32l22\\nl31l11l31l21+l32l22l2\\n31+l2\\n32+l2\\n333\\n5: (4.46)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.4 Eigendecomposition and Diagonalization 115\\nComparing the left-hand side of (4.45) and the right-hand side of (4.46)\\nshows that there is a simple pattern in the diagonal elements lii:\\nl11=pa11; l 22=q\\na22\\x00l2\\n21; l 33=q\\na33\\x00(l2\\n31+l2\\n32):(4.47)\\nSimilarly for the elements below the diagonal ( lij, wherei > j ), there is\\nalso a repeating pattern:\\nl21=1\\nl11a21; l 31=1\\nl11a31; l 32=1\\nl22(a32\\x00l31l21): (4.48)\\nThus, we constructed the Cholesky decomposition for any symmetric, pos-\\nitive deﬁnite 3\\x023matrix. The key realization is that we can backward\\ncalculate what the components lijfor theLshould be, given the values\\naijforAand previously computed values of lij.\\nThe Cholesky decomposition is an important tool for the numerical\\ncomputations underlying machine learning. Here, symmetric positive def-\\ninite matrices require frequent manipulation, e.g., the covariance matrix\\nof a multivariate Gaussian variable (see Section 6.5) is symmetric, positive\\ndeﬁnite. The Cholesky factorization of this covariance matrix allows us to\\ngenerate samples from a Gaussian distribution. It also allows us to perform\\na linear transformation of random variables, which is heavily exploited\\nwhen computing gradients in deep stochastic models, such as the varia-\\ntional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\\n2014). The Cholesky decomposition also allows us to compute determi-\\nnants very efﬁciently. Given the Cholesky decomposition A=LL>, we\\nknow that det(A) = det(L) det(L>) = det(L)2. SinceLis a triangular\\nmatrix, the determinant is simply the product of its diagonal entries so\\nthatdet(A) =Q\\nil2\\nii. Thus, many numerical software packages use the\\nCholesky decomposition to make computations more efﬁcient.\\n4.4 Eigendecomposition and Diagonalization\\nAdiagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\\nments, i.e., they are of the form\\nD=2\\n64c1\\x01\\x01\\x01 0\\n.........\\n0\\x01\\x01\\x01cn3\\n75: (4.49)\\nThey allow fast computation of determinants, powers, and inverses. The\\ndeterminant is the product of its diagonal entries, a matrix power Dkis\\ngiven by each diagonal element raised to the power k, and the inverse\\nD\\x001is the reciprocal of its diagonal elements if all of them are nonzero.\\nIn this section, we will discuss how to transform matrices into diagonal\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n116 Matrix Decompositions\\nform. This is an important application of the basis change we discussed in\\nSection 2.7.2 and eigenvalues from Section 4.2.\\nRecall that two matrices A;Dare similar (Deﬁnition 2.22) if there ex-\\nists an invertible matrix P, such thatD=P\\x001AP. More speciﬁcally, we\\nwill look at matrices Athat are similar to diagonal matrices Dthat con-\\ntain the eigenvalues of Aon the diagonal.\\nDeﬁnition 4.19 (Diagonalizable) .A matrixA2Rn\\x02nisdiagonalizable diagonalizable\\nif it is similar to a diagonal matrix, i.e., if there exists an invertible matrix\\nP2Rn\\x02nsuch thatD=P\\x001AP.\\nIn the following, we will see that diagonalizing a matrix A2Rn\\x02nis\\na way of expressing the same linear mapping but in another basis (see\\nSection 2.6.1), which will turn out to be a basis that consists of the eigen-\\nvectors ofA.\\nLetA2Rn\\x02n, let\\x151;:::;\\x15nbe a set of scalars, and let p1;:::;pnbe a\\nset of vectors in Rn. We deﬁneP:= [p1;:::;pn]and letD2Rn\\x02nbe a\\ndiagonal matrix with diagonal entries \\x151;:::;\\x15n. Then we can show that\\nAP=PD (4.50)\\nif and only if \\x151;:::;\\x15nare the eigenvalues of Aandp1;:::;pnare cor-\\nresponding eigenvectors of A.\\nWe can see that this statement holds because\\nAP=A[p1;:::;pn] = [Ap1;:::;Apn]; (4.51)\\nPD = [p1;:::;pn]2\\n64\\x151 0\\n...\\n0\\x15n3\\n75= [\\x151p1;:::;\\x15npn]: (4.52)\\nThus, (4.50) implies that\\nAp1=\\x151p1 (4.53)\\n...\\nApn=\\x15npn: (4.54)\\nTherefore, the columns of Pmust be eigenvectors of A.\\nOur deﬁnition of diagonalization requires that P2Rn\\x02nis invertible,\\ni.e.,Phas full rank (Theorem 4.3). This requires us to have nlinearly\\nindependent eigenvectors p1;:::;pn, i.e., thepiform a basis of Rn.\\nTheorem 4.20 (Eigendecomposition) .A square matrix A2Rn\\x02ncan be\\nfactored into\\nA=PDP\\x001; (4.55)\\nwhereP2Rn\\x02nandDis a diagonal matrix whose diagonal entries are\\nthe eigenvalues of A, if and only if the eigenvectors of Aform a basis of Rn.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.4 Eigendecomposition and Diagonalization 117\\nFigure 4.7 Intuition\\nbehind the\\neigendecomposition\\nas sequential\\ntransformations.\\nTop-left to\\nbottom-left:P\\x001\\nperforms a basis\\nchange (here drawn\\ninR2and depicted\\nas a rotation-like\\noperation) from the\\nstandard basis into\\nthe eigenbasis.\\nBottom-left to\\nbottom-right: D\\nperforms a scaling\\nalong the remapped\\northogonal\\neigenvectors,\\ndepicted here by a\\ncircle being\\nstretched to an\\nellipse. Bottom-right\\nto top-right:P\\nundoes the basis\\nchange (depicted as\\na reverse rotation)\\nand restores the\\noriginal coordinate\\nframe.\\ne1e2\\np1p2\\np1p2e1e2\\np1p2\\n\\x151p1\\x152p2\\ne1e2\\nAe 1Ae 2P\\x001\\nDPA\\nTheorem 4.20 implies that only non-defective matrices can be diagonal-\\nized and that the columns of Pare theneigenvectors of A. For symmetric\\nmatrices we can obtain even stronger outcomes for the eigenvalue decom-\\nposition.\\nTheorem 4.21. A symmetric matrix S2Rn\\x02ncan always be diagonalized.\\nTheorem 4.21 follows directly from the spectral theorem 4.15. More-\\nover, the spectral theorem states that we can ﬁnd an ONB of eigenvectors\\nofRn. This makesPan orthogonal matrix so that D=P>AP.\\nRemark. The Jordan normal form of a matrix offers a decomposition that\\nworks for defective matrices (Lang, 1987) but is beyond the scope of this\\nbook. }\\nGeometric Intuition for the Eigendecomposition\\nWe can interpret the eigendecomposition of a matrix as follows (see also\\nFigure 4.7): Let Abe the transformation matrix of a linear mapping with\\nrespect to the standard basis ei(blue arrows). P\\x001performs a basis\\nchange from the standard basis into the eigenbasis. Then, the diagonal\\nDscales the vectors along these axes by the eigenvalues \\x15i. Finally,P\\ntransforms these scaled vectors back into the standard/canonical coordi-\\nnates yielding \\x15ipi.\\nExample 4.11 (Eigendecomposition)\\nLet us compute the eigendecomposition of A=1\\n2\\x145\\x002\\n\\x002 5\\x15\\n.\\nStep 1: Compute eigenvalues and eigenvectors. The characteristic\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n118 Matrix Decompositions\\npolynomial of Ais\\ndet(A\\x00\\x15I) = det\\x12\\x145\\n2\\x00\\x15\\x001\\n\\x0015\\n2\\x00\\x15\\x15\\x13\\n(4.56a)\\n= (5\\n2\\x00\\x15)2\\x001 =\\x152\\x005\\x15+21\\n4= (\\x15\\x007\\n2)(\\x15\\x003\\n2): (4.56b)\\nTherefore, the eigenvalues of Aare\\x151=7\\n2and\\x152=3\\n2(the roots of the\\ncharacteristic polynomial), and the associated (normalized) eigenvectors\\nare obtained via\\nAp1=7\\n2p1;Ap2=3\\n2p2: (4.57)\\nThis yields\\np1=1p\\n2\\x141\\n\\x001\\x15\\n;p2=1p\\n2\\x141\\n1\\x15\\n: (4.58)\\nStep 2: Check for existence. The eigenvectors p1;p2form a basis of R2.\\nTherefore,Acan be diagonalized.\\nStep 3: Construct the matrix Pto diagonalize A.We collect the eigen-\\nvectors ofAinPso that\\nP= [p1;p2] =1p\\n2\\x141 1\\n\\x001 1\\x15\\n: (4.59)\\nWe then obtain\\nP\\x001AP=\\x147\\n20\\n03\\n2\\x15\\n=D: (4.60)\\nEquivalently, we get (exploiting that P\\x001=P>since the eigenvectors Figure 4.7 visualizes\\nthe\\neigendecomposition\\nofA=\\x145\\x002\\n\\x002 5\\x15\\nas a sequence of\\nlinear\\ntransformations.p1andp2in this example form an ONB)\\n1\\n2\\x145\\x002\\n\\x002 5\\x15\\n|{z}\\nA=1p\\n2\\x141 1\\n\\x001 1\\x15\\n|{z}\\nP\\x147\\n20\\n03\\n2\\x15\\n|{z}\\nD1p\\n2\\x141\\x001\\n1 1\\x15\\n|{z}\\nP\\x001: (4.61)\\nDiagonal matrices Dcan efﬁciently be raised to a power. Therefore,\\nwe can ﬁnd a matrix power for a matrix A2Rn\\x02nvia the eigenvalue\\ndecomposition (if it exists) so that\\nAk= (PDP\\x001)k=PDkP\\x001: (4.62)\\nComputingDkis efﬁcient because we apply this operation individually\\nto any diagonal element.\\nAssume that the eigendecomposition A=PDP\\x001exists. Then,\\ndet(A) = det(PDP\\x001) = det(P) det(D) det(P\\x001) (4.63a)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.5 Singular Value Decomposition 119\\n= det(D) =Y\\nidii (4.63b)\\nallows for an efﬁcient computation of the determinant of A.\\nThe eigenvalue decomposition requires square matrices. It would be\\nuseful to perform a decomposition on general matrices. In the next sec-\\ntion, we introduce a more general matrix decomposition technique, the\\nsingular value decomposition.\\n4.5 Singular Value Decomposition\\nThe singular value decomposition (SVD) of a matrix is a central matrix\\ndecomposition method in linear algebra. It has been referred to as the\\n“fundamental theorem of linear algebra” (Strang, 1993) because it can be\\napplied to all matrices, not only to square matrices, and it always exists.\\nMoreover, as we will explore in the following, the SVD of a matrix A,\\nwhich represents a linear mapping \\x08 :V!W, quantiﬁes the change\\nbetween the underlying geometry of these two vector spaces. We recom-\\nmend the work by Kalman (1996) and Roy and Banerjee (2014) for a\\ndeeper overview of the mathematics of the SVD.\\nSVD theorem\\nTheorem 4.22 (SVD Theorem) .LetA2Rm\\x02nbe a rectangular matrix of\\nrankr2[0;min(m;n)]. The SVD ofAis a decomposition of the form SVD\\nsingular value\\ndecomposition\\n=UA V>\\x06mn\\nmm\\nmn\\nnn\\n(4.64)\\nwith an orthogonal matrix U2Rm\\x02mwith column vectors ui,i= 1;:::;m ,\\nand an orthogonal matrix V2Rn\\x02nwith column vectors vj,j= 1;:::;n .\\nMoreover, \\x06is anm\\x02nmatrix with \\x06ii=\\x1bi>0and\\x06ij= 0; i6=j.\\nThe diagonal entries \\x1bi,i= 1;:::;r , of\\x06are called the singular values ,singular values\\nuiare called the left-singular vectors , andvjare called the right-singular left-singular vectors\\nright-singular\\nvectorsvectors . By convention, the singular values are ordered, i.e., \\x1b1>\\x1b2>\\n\\x1br>0.\\nThesingular value matrix \\x06is unique, but it requires some attention. singular value\\nmatrix Observe that the \\x062Rm\\x02nis rectangular. In particular, \\x06is of the same\\nsize asA. This means that \\x06has a diagonal submatrix that contains the\\nsingular values and needs additional zero padding. Speciﬁcally, if m>n ,\\nthen the matrix \\x06has diagonal structure up to row nand then consists of\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n120 Matrix Decompositions\\nFigure 4.8 Intuition\\nbehind the SVD of a\\nmatrixA2R3\\x022\\nas sequential\\ntransformations.\\nTop-left to\\nbottom-left:V>\\nperforms a basis\\nchange in R2.\\nBottom-left to\\nbottom-right: \\x06\\nscales and maps\\nfromR2toR3. The\\nellipse in the\\nbottom-right lives in\\nR3. The third\\ndimension is\\northogonal to the\\nsurface of the\\nelliptical disk.\\nBottom-right to\\ntop-right:U\\nperforms a basis\\nchange within R3.v2\\nv1\\n \\x1b2u2\\n\\x1b1u1\\ne2\\ne1\\x1b2e2\\n\\x1b1e1A\\nV>\\n\\x06U\\n0>row vectors from n+ 1tombelow so that\\n\\x06=2\\n666666664\\x1b10 0\\n0...0\\n0 0\\x1bn\\n0::: 0\\n......\\n0::: 03\\n777777775: (4.65)\\nIfm < n , the matrix \\x06has a diagonal structure up to column mand\\ncolumns that consist of 0fromm+ 1ton:\\n\\x06=2\\n64\\x1b10 0 0 :::0\\n0...0......\\n0 0\\x1bm0:::03\\n75: (4.66)\\nRemark. The SVD exists for any matrix A2Rm\\x02n. }\\n4.5.1 Geometric Intuitions for the SVD\\nThe SVD offers geometric intuitions to describe a transformation matrix\\nA. In the following, we will discuss the SVD as sequential linear trans-\\nformations performed on the bases. In Example 4.12, we will then apply\\ntransformation matrices of the SVD to a set of vectors in R2, which allows\\nus to visualize the effect of each transformation more clearly.\\nThe SVD of a matrix can be interpreted as a decomposition of a corre-\\nsponding linear mapping (recall Section 2.7.1) \\x08 :Rn!Rminto three\\noperations; see Figure 4.8. The SVD intuition follows superﬁcially a simi-\\nlar structure to our eigendecomposition intuition, see Figure 4.7: Broadly\\nspeaking, the SVD performs a basis change via V>followed by a scal-\\ning and augmentation (or reduction) in dimensionality via the singular\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.5 Singular Value Decomposition 121\\nvalue matrix \\x06. Finally, it performs a second basis change via U. The SVD\\nentails a number of important details and caveats, which is why we will\\nreview our intuition in more detail. It is useful to review\\nbasis changes\\n(Section 2.7.2),\\northogonal matrices\\n(Deﬁnition 3.8) and\\northonormal bases\\n(Section 3.5).Assume we are given a transformation matrix of a linear mapping \\x08 :\\nRn!Rmwith respect to the standard bases BandCofRnandRm,\\nrespectively. Moreover, assume a second basis ~BofRnand~CofRm. Then\\n1. The matrix Vperforms a basis change in the domain Rnfrom ~B(rep-\\nresented by the red and orange vectors v1andv2in the top-left of Fig-\\nure 4.8) to the standard basis B.V>=V\\x001performs a basis change\\nfromBto~B. The red and orange vectors are now aligned with the\\ncanonical basis in the bottom-left of Figure 4.8.\\n2. Having changed the coordinate system to ~B,\\x06scales the new coordi-\\nnates by the singular values \\x1bi(and adds or deletes dimensions), i.e.,\\n\\x06is the transformation matrix of \\x08with respect to ~Band ~C, rep-\\nresented by the red and orange vectors being stretched and lying in\\nthee1-e2plane, which is now embedded in a third dimension in the\\nbottom-right of Figure 4.8.\\n3.Uperforms a basis change in the codomain Rmfrom ~Cinto the canoni-\\ncal basis of Rm, represented by a rotation of the red and orange vectors\\nout of thee1-e2plane. This is shown in the top-right of Figure 4.8.\\nThe SVD expresses a change of basis in both the domain and codomain.\\nThis is in contrast with the eigendecomposition that operates within the\\nsame vector space, where the same basis change is applied and then un-\\ndone. What makes the SVD special is that these two different bases are\\nsimultaneously linked by the singular value matrix \\x06.\\nExample 4.12 (Vectors and the SVD)\\nConsider a mapping of a square grid of vectors X2R2that ﬁt in a box of\\nsize2\\x022centered at the origin. Using the standard basis, we map these\\nvectors using\\nA=2\\n41\\x000:8\\n0 1\\n1 03\\n5=U\\x06V>(4.67a)\\n=2\\n4\\x000:79 0\\x000:62\\n0:38\\x000:78\\x000:49\\n\\x000:48\\x000:62 0:623\\n52\\n41:62 0\\n0 1:0\\n0 03\\n5\\x14\\x000:78 0:62\\n\\x000:62\\x000:78\\x15\\n:(4.67b)\\nWe start with a set of vectors X(colored dots; see top-left panel of Fig-\\nure 4.9) arranged in a grid. We then apply V>2R2\\x022, which rotatesX.\\nThe rotated vectors are shown in the bottom-left panel of Figure 4.9. We\\nnow map these vectors using the singular value matrix \\x06to the codomain\\nR3(see the bottom-right panel in Figure 4.9). Note that all vectors lie in\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n122 Matrix Decompositions\\nthex1-x2plane. The third coordinate is always 0. The vectors in the x1-x2\\nplane have been stretched by the singular values.\\nThe direct mapping of the vectors XbyAto the codomain R3equals\\nthe transformation of XbyU\\x06V>, whereUperforms a rotation within\\nthe codomain R3so that the mapped vectors are no longer restricted to\\nthex1-x2plane; they still are on a plane as shown in the top-right panel\\nof Figure 4.9.\\nFigure 4.9 SVD and\\nmapping of vectors\\n(represented by\\ndiscs). The panels\\nfollow the same\\nanti-clockwise\\nstructure of\\nFigure 4.8.\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx1−1.5−1.0−0.50.00.51.01.5x2\\nx1-1.5-0.5\\n0.5\\n1.5x2\\n-1.5-0.50.51.5x3\\n-1.0-0.50.00.51.0\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx1−1.5−1.0−0.50.00.51.01.5x2\\nx1-1.5-0.50.51.5x2\\n-1.5-0.50.51.5x3\\n0\\n4.5.2 Construction of the SVD\\nWe will next discuss why the SVD exists and show how to compute it\\nin detail. The SVD of a general matrix shares some similarities with the\\neigendecomposition of a square matrix.\\nRemark. Compare the eigendecomposition of an SPD matrix\\nS=S>=PDP>(4.68)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.5 Singular Value Decomposition 123\\nwith the corresponding SVD\\nS=U\\x06V>: (4.69)\\nIf we set\\nU=P=V;D=\\x06; (4.70)\\nwe see that the SVD of SPD matrices is their eigendecomposition. }\\nIn the following, we will explore why Theorem 4.22 holds and how\\nthe SVD is constructed. Computing the SVD of A2Rm\\x02nis equivalent\\nto ﬁnding two sets of orthonormal bases U= (u1;:::;um)andV=\\n(v1;:::;vn)of the codomain Rmand the domain Rn, respectively. From\\nthese ordered bases, we will construct the matrices UandV.\\nOur plan is to start with constructing the orthonormal set of right-\\nsingular vectors v1;:::;vn2Rn. We then construct the orthonormal set\\nof left-singular vectors u1;:::;um2Rm. Thereafter, we will link the two\\nand require that the orthogonality of the viis preserved under the trans-\\nformation ofA. This is important because we know that the images Avi\\nform a set of orthogonal vectors. We will then normalize these images by\\nscalar factors, which will turn out to be the singular values.\\nLet us begin with constructing the right-singular vectors. The spectral\\ntheorem (Theorem 4.15) tells us that the eigenvectors of a symmetric\\nmatrix form an ONB, which also means it can be diagonalized. More-\\nover, from Theorem 4.14 we can always construct a symmetric, positive\\nsemideﬁnite matrix A>A2Rn\\x02nfrom any rectangular matrix A2\\nRm\\x02n. Thus, we can always diagonalize A>Aand obtain\\nA>A=PDP>=P2\\n64\\x151\\x01\\x01\\x01 0\\n.........\\n0\\x01\\x01\\x01\\x15n3\\n75P>; (4.71)\\nwherePis an orthogonal matrix, which is composed of the orthonormal\\neigenbasis. The \\x15i>0are the eigenvalues of A>A. Let us assume the\\nSVD ofAexists and inject (4.64) into (4.71). This yields\\nA>A= (U\\x06V>)>(U\\x06V>) =V\\x06>U>U\\x06V>; (4.72)\\nwhereU;Vare orthogonal matrices. Therefore, with U>U=Iwe ob-\\ntain\\nA>A=V\\x06>\\x06V>=V2\\n64\\x1b2\\n10 0\\n0...0\\n0 0\\x1b2\\nn3\\n75V>: (4.73)\\nComparing now (4.71) and (4.73), we identify\\nV>=P>; (4.74)\\n\\x1b2\\ni=\\x15i: (4.75)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n124 Matrix Decompositions\\nTherefore, the eigenvectors of A>Athat compose Pare the right-singular\\nvectorsVofA(see (4.74)). The eigenvalues of A>Aare the squared\\nsingular values of \\x06(see (4.75)).\\nTo obtain the left-singular vectors U, we follow a similar procedure.\\nWe start by computing the SVD of the symmetric matrix AA>2Rm\\x02m\\n(instead of the previous A>A2Rn\\x02n). The SVD of Ayields\\nAA>= (U\\x06V>)(U\\x06V>)>=U\\x06V>V\\x06>U>(4.76a)\\n=U2\\n64\\x1b2\\n10 0\\n0...0\\n0 0\\x1b2\\nm3\\n75U>: (4.76b)\\nThe spectral theorem tells us that AA>=SDS>can be diagonalized\\nand we can ﬁnd an ONB of eigenvectors of AA>, which are collected in\\nS. The orthonormal eigenvectors of AA>are the left-singular vectors U\\nand form an orthonormal basis in the codomain of the SVD.\\nThis leaves the question of the structure of the matrix \\x06. SinceAA>\\nandA>Ahave the same nonzero eigenvalues (see page 106), the nonzero\\nentries of the \\x06matrices in the SVD for both cases have to be the same.\\nThe last step is to link up all the parts we touched upon so far. We have\\nan orthonormal set of right-singular vectors in V. To ﬁnish the construc-\\ntion of the SVD, we connect them with the orthonormal vectors U. To\\nreach this goal, we use the fact the images of the viunderAhave to be\\northogonal, too. We can show this by using the results from Section 3.4.\\nWe require that the inner product between AviandAvjmust be 0for\\ni6=j. For any two orthogonal eigenvectors vi;vj,i6=j, it holds that\\n(Avi)>(Avj) =v>\\ni(A>A)vj=v>\\ni(\\x15jvj) =\\x15jv>\\nivj= 0: (4.77)\\nFor the case m>r, it holds thatfAv1;:::;Avrgis a basis of an r-\\ndimensional subspace of Rm.\\nTo complete the SVD construction, we need left-singular vectors that\\nare ortho normal : We normalize the images of the right-singular vectors\\nAviand obtain\\nui:=Avi\\nkAvik=1p\\x15iAvi=1\\n\\x1biAvi; (4.78)\\nwhere the last equality was obtained from (4.75) and (4.76b), showing\\nus that the eigenvalues of AA>are such that \\x1b2\\ni=\\x15i.\\nTherefore, the eigenvectors of A>A, which we know are the right-\\nsingular vectors vi, and their normalized images under A, the left-singular\\nvectorsui, form two self-consistent ONBs that are connected through the\\nsingular value matrix \\x06.\\nLet us rearrange (4.78) to obtain the singular value equation singular value\\nequation\\nAvi=\\x1biui; i= 1;:::;r: (4.79)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.5 Singular Value Decomposition 125\\nThis equation closely resembles the eigenvalue equation (4.25), but the\\nvectors on the left- and the right-hand sides are not the same.\\nForn < m , (4.79) holds only for i6n, but (4.79) says nothing about\\ntheuifori > n . However, we know by construction that they are or-\\nthonormal. Conversely, for m<n , (4.79) holds only for i6m. Fori>m ,\\nwe haveAvi=0and we still know that the viform an orthonormal set.\\nThis means that the SVD also supplies an orthonormal basis of the kernel\\n(null space) of A, the set of vectors xwithAx=0(see Section 2.7.3).\\nConcatenating the vias the columns of Vand theuias the columns of\\nUyields\\nAV=U\\x06; (4.80)\\nwhere \\x06has the same dimensions as Aand a diagonal structure for rows\\n1;:::;r . Hence, right-multiplying with V>yieldsA=U\\x06V>, which is\\nthe SVD ofA.\\nExample 4.13 (Computing the SVD)\\nLet us ﬁnd the singular value decomposition of\\nA=\\x141 0 1\\n\\x002 1 0\\x15\\n: (4.81)\\nThe SVD requires us to compute the right-singular vectors vj, the singular\\nvalues\\x1bk, and the left-singular vectors ui.\\nStep 1: Right-singular vectors as the eigenbasis of A>A.\\nWe start by computing\\nA>A=2\\n41\\x002\\n0 1\\n1 03\\n5\\x141 0 1\\n\\x002 1 0\\x15\\n=2\\n45\\x002 1\\n\\x002 1 0\\n1 0 13\\n5: (4.82)\\nWe compute the singular values and right-singular vectors vjthrough\\nthe eigenvalue decomposition of A>A, which is given as\\nA>A=2\\n645p\\n300\\x001p\\n6\\n\\x002p\\n301p\\n5\\x002p\\n6\\n1p\\n302p\\n51p\\n63\\n752\\n46 0 0\\n0 1 0\\n0 0 03\\n52\\n645p\\n30\\x002p\\n301p\\n30\\n01p\\n52p\\n5\\n\\x001p\\n6\\x002p\\n61p\\n63\\n75=PDP>;\\n(4.83)\\nand we obtain the right-singular vectors as the columns of Pso that\\nV=P=2\\n645p\\n300\\x001p\\n6\\n\\x002p\\n301p\\n5\\x002p\\n6\\n1p\\n302p\\n51p\\n63\\n75: (4.84)\\nStep 2: Singular-value matrix.\\nAs the singular values \\x1biare the square roots of the eigenvalues of\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n126 Matrix Decompositions\\nA>Awe obtain them straight from D. Since rk(A) = 2 , there are only\\ntwo nonzero singular values: \\x1b1=p\\n6and\\x1b2= 1. The singular value\\nmatrix must be the same size as A, and we obtain\\n\\x06=\\x14p\\n6 0 0\\n0 1 0\\x15\\n: (4.85)\\nStep 3: Left-singular vectors as the normalized image of the right-\\nsingular vectors.\\nWe ﬁnd the left-singular vectors by computing the image of the right-\\nsingular vectors under Aand normalizing them by dividing them by their\\ncorresponding singular value. We obtain\\nu1=1\\n\\x1b1Av1=1p\\n6\\x141 0 1\\n\\x002 1 0\\x152\\n645p\\n30\\n\\x002p\\n30\\n1p\\n303\\n75=\"\\n1p\\n5\\n\\x002p\\n5#\\n; (4.86)\\nu2=1\\n\\x1b2Av2=1\\n1\\x141 0 1\\n\\x002 1 0\\x152\\n40\\n1p\\n5\\n2p\\n53\\n5=\"\\n2p\\n5\\n1p\\n5#\\n; (4.87)\\nU= [u1;u2] =1p\\n5\\x141 2\\n\\x002 1\\x15\\n: (4.88)\\nNote that on a computer the approach illustrated here has poor numerical\\nbehavior, and the SVD of Ais normally computed without resorting to the\\neigenvalue decomposition of A>A.\\n4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition\\nLet us consider the eigendecomposition A=PDP\\x001and the SVDA=\\nU\\x06V>and review the core elements of the past sections.\\nThe SVD always exists for any matrix Rm\\x02n. The eigendecomposition is\\nonly deﬁned for square matrices Rn\\x02nand only exists if we can ﬁnd a\\nbasis of eigenvectors of Rn.\\nThe vectors in the eigendecomposition matrix Pare not necessarily\\northogonal, i.e., the change of basis is not a simple rotation and scaling.\\nOn the other hand, the vectors in the matrices UandVin the SVD are\\northonormal, so they do represent rotations.\\nBoth the eigendecomposition and the SVD are compositions of three\\nlinear mappings:\\n1. Change of basis in the domain\\n2. Independent scaling of each new basis vector and mapping from do-\\nmain to codomain\\n3. Change of basis in the codomain\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.5 Singular Value Decomposition 127\\nFigure 4.10 Movie\\nratings of three\\npeople for four\\nmovies and its SVD\\ndecomposition. 5 4 1\\n5 5 0\\n0 0 5\\n1 0 42\\n6643\\n775Ali\\nBeatrix\\nChandra\\nStar Wars\\nBlade Runner\\nAmelie\\nDelicatessen=\\x000:6710 0:0236 0:4647\\x000:5774\\n\\x000:7197 0:2054\\x000:4759 0:4619\\n\\x000:0939\\x000:7705\\x000:5268\\x000:3464\\n\\x000:1515\\x000:6030 0:5293\\x000:57742\\n66643\\n7775\\n9:6438 0 0\\n06:3639 0\\n0 00:7056\\n0 0 02\\n6643\\n775\\n\\x000:7367\\x000:6515\\x000:1811\\n0:0852 0:1762\\x000:9807\\n0:6708\\x000:7379\\x000:07432\\n643\\n75\\nA key difference between the eigendecomposition and the SVD is that\\nin the SVD, domain and codomain can be vector spaces of different\\ndimensions.\\nIn the SVD, the left- and right-singular vector matrices UandVare\\ngenerally not inverse of each other (they perform basis changes in dif-\\nferent vector spaces). In the eigendecomposition, the basis change ma-\\ntricesPandP\\x001are inverses of each other.\\nIn the SVD, the entries in the diagonal matrix \\x06are all real and non-\\nnegative, which is not generally true for the diagonal matrix in the\\neigendecomposition.\\nThe SVD and the eigendecomposition are closely related through their\\nprojections\\n–The left-singular vectors of Aare eigenvectors of AA>\\n–The right-singular vectors of Aare eigenvectors of A>A.\\n–The nonzero singular values of Aare the square roots of the nonzero\\neigenvalues of both AA>andA>A.\\nFor symmetric matrices A2Rn\\x02n, the eigenvalue decomposition and\\nthe SVD are one and the same, which follows from the spectral theo-\\nrem 4.15.\\nExample 4.14 (Finding Structure in Movie Ratings and Consumers)\\nLet us add a practical interpretation of the SVD by analyzing data on\\npeople and their preferred movies. Consider three viewers (Ali, Beatrix,\\nChandra) rating four different movies ( Star Wars ,Blade Runner ,Amelie ,\\nDelicatessen ). Their ratings are values between 0(worst) and 5(best) and\\nencoded in a data matrix A2R4\\x023as shown in Figure 4.10. Each row\\nrepresents a movie and each column a user. Thus, the column vectors of\\nmovie ratings, one for each viewer, are xAli,xBeatrix ,xChandra .\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n128 Matrix Decompositions\\nFactoringAusing the SVD offers us a way to capture the relationships\\nof how people rate movies, and especially if there is a structure linking\\nwhich people like which movies. Applying the SVD to our data matrix A\\nmakes a number of assumptions:\\n1. All viewers rate movies consistently using the same linear mapping.\\n2. There are no errors or noise in the ratings.\\n3. We interpret the left-singular vectors uias stereotypical movies and\\nthe right-singular vectors vjas stereotypical viewers.\\nWe then make the assumption that any viewer’s speciﬁc movie preferences\\ncan be expressed as a linear combination of the vj. Similarly, any movie’s\\nlike-ability can be expressed as a linear combination of the ui. Therefore,\\na vector in the domain of the SVD can be interpreted as a viewer in the\\n“space” of stereotypical viewers, and a vector in the codomain of the SVD\\ncorrespondingly as a movie in the “space” of stereotypical movies. Let us These two “spaces”\\nare only\\nmeaningfully\\nspanned by the\\nrespective viewer\\nand movie data if\\nthe data itself covers\\na sufﬁcient diversity\\nof viewers and\\nmovies.inspect the SVD of our movie-user matrix. The ﬁrst left-singular vector u1\\nhas large absolute values for the two science ﬁction movies and a large\\nﬁrst singular value (red shading in Figure 4.10). Thus, this groups a type\\nof users with a speciﬁc set of movies (science ﬁction theme). Similarly, the\\nﬁrst right-singular v1shows large absolute values for Ali and Beatrix, who\\ngive high ratings to science ﬁction movies (green shading in Figure 4.10).\\nThis suggests that v1reﬂects the notion of a science ﬁction lover.\\nSimilarly,u2, seems to capture a French art house ﬁlm theme, and v2in-\\ndicates that Chandra is close to an idealized lover of such movies. An ide-\\nalized science ﬁction lover is a purist and only loves science ﬁction movies,\\nso a science ﬁction lover v1gives a rating of zero to everything but science\\nﬁction themed—this logic is implied by the diagonal substructure for the\\nsingular value matrix \\x06. A speciﬁc movie is therefore represented by how\\nit decomposes (linearly) into its stereotypical movies. Likewise, a person\\nwould be represented by how they decompose (via linear combination)\\ninto movie themes.\\nIt is worth to brieﬂy discuss SVD terminology and conventions, as there\\nare different versions used in the literature. While these differences can\\nbe confusing, the mathematics remains invariant to them.\\nFor convenience in notation and abstraction, we use an SVD notation\\nwhere the SVD is described as having two square left- and right-singular\\nvector matrices, but a non-square singular value matrix. Our deﬁni-\\ntion (4.64) for the SVD is sometimes called the full SVD . full SVD\\nSome authors deﬁne the SVD a bit differently and focus on square sin-\\ngular matrices. Then, for A2Rm\\x02nandm>n,\\nA\\nm\\x02n=U\\nm\\x02n\\x06\\nn\\x02nV>\\nn\\x02n: (4.89)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.6 Matrix Approximation 129\\nSometimes this formulation is called the reduced SVD (e.g., Datta (2010)) reduced SVD\\northeSVD (e.g., Press et al. (2007)). This alternative format changes\\nmerely how the matrices are constructed but leaves the mathematical\\nstructure of the SVD unchanged. The convenience of this alternative\\nformulation is that \\x06is diagonal, as in the eigenvalue decomposition.\\nIn Section 4.6, we will learn about matrix approximation techniques\\nusing the SVD, which is also called the truncated SVD . truncated SVD\\nIt is possible to deﬁne the SVD of a rank- rmatrixAso thatUis an\\nm\\x02rmatrix, \\x06a diagonal matrix of size r\\x02r, andVann\\x02rmatrix.\\nThis construction is very similar to our deﬁnition and ensures that the\\ndiagonal matrix \\x06has only nonzero entries along the diagonal. The\\nmain convenience of this alternative notation is that \\x06is diagonal, as\\nin the eigenvalue decomposition.\\nA restriction that the SVD for Aonly applies to m\\x02nmatrices with\\nm>n is practically unnecessary. When m<n , the SVD decomposition\\nwill yield \\x06with more zero columns than rows and, consequently, the\\nsingular values \\x1bm+1;:::;\\x1bnare0.\\nThe SVD is used in a variety of applications in machine learning from\\nleast-squares problems in curve ﬁtting to solving systems of linear equa-\\ntions. These applications harness various important properties of the SVD,\\nits relation to the rank of a matrix, and its ability to approximate matrices\\nof a given rank with lower-rank matrices. Substituting a matrix with its\\nSVD has often the advantage of making calculation more robust to nu-\\nmerical rounding errors. As we will explore in the next section, the SVD’s\\nability to approximate matrices with “simpler” matrices in a principled\\nmanner opens up machine learning applications ranging from dimension-\\nality reduction and topic modeling to data compression and clustering.\\n4.6 Matrix Approximation\\nWe considered the SVD as a way to factorize A=U\\x06V>2Rm\\x02ninto\\nthe product of three matrices, where U2Rm\\x02mandV2Rn\\x02nare or-\\nthogonal and \\x06contains the singular values on its main diagonal. Instead\\nof doing the full SVD factorization, we will now investigate how the SVD\\nallows us to represent a matrix Aas a sum of simpler (low-rank) matrices\\nAi, which lends itself to a matrix approximation scheme that is cheaper\\nto compute than the full SVD.\\nWe construct a rank- 1matrixAi2Rm\\x02nas\\nAi:=uiv>\\ni; (4.90)\\nwhich is formed by the outer product of the ith orthogonal column vector\\nofUandV. Figure 4.11 shows an image of Stonehenge, which can be\\nrepresented by a matrix A2R1432\\x021910, and some outer products Ai, as\\ndeﬁned in (4.90).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n130 Matrix Decompositions\\nFigure 4.11 Image\\nprocessing with the\\nSVD. (a) The\\noriginal grayscale\\nimage is a\\n1;432\\x021;910\\nmatrix of values\\nbetween 0(black)\\nand1(white).\\n(b)–(f) Rank- 1\\nmatrices\\nA1;:::;A5and\\ntheir corresponding\\nsingular values\\n\\x1b1;:::;\\x1b 5. The\\ngrid-like structure of\\neach rank- 1matrix\\nis imposed by the\\nouter-product of the\\nleft and\\nright-singular\\nvectors.\\n(a) Original image A.\\n (b)A1; \\x1b1\\x19228;052.\\n (c)A2; \\x1b2\\x1940;647.\\n(d)A3; \\x1b3\\x1926;125.\\n (e)A4; \\x1b4\\x1920;232.\\n (f)A5; \\x1b5\\x1915;436.\\nA matrixA2Rm\\x02nof rankrcan be written as a sum of rank-1 matrices\\nAiso that\\nA=rX\\ni=1\\x1biuiv>\\ni=rX\\ni=1\\x1biAi; (4.91)\\nwhere the outer-product matrices Aiare weighted by the ith singular\\nvalue\\x1bi. We can see why (4.91) holds: The diagonal structure of the\\nsingular value matrix \\x06multiplies only matching left- and right-singular\\nvectorsuiv>\\niand scales them by the corresponding singular value \\x1bi. All\\nterms \\x06ijuiv>\\njvanish fori6=jbecause \\x06is a diagonal matrix. Any terms\\ni>r vanish because the corresponding singular values are 0.\\nIn (4.90), we introduced rank- 1matricesAi. We summed up the rin-\\ndividual rank- 1matrices to obtain a rank- rmatrixA; see (4.91). If the\\nsum does not run over all matrices Ai,i= 1;:::;r , but only up to an\\nintermediate value k<r , we obtain a rank-kapproximation rank-k\\napproximation\\nbA(k) :=kX\\ni=1\\x1biuiv>\\ni=kX\\ni=1\\x1biAi (4.92)\\nofAwith rk(bA(k)) =k. Figure 4.12 shows low-rank approximations\\nbA(k)of an original image Aof Stonehenge. The shape of the rocks be-\\ncomes increasingly visible and clearly recognizable in the rank- 5approx-\\nimation. While the original image requires 1;432\\x011;910 = 2;735;120\\nnumbers, the rank- 5approximation requires us only to store the ﬁve sin-\\ngular values and the ﬁve left- and right-singular vectors ( 1;432and1;910-\\ndimensional each) for a total of 5\\x01(1;432+1;910+1) = 16 ;715numbers\\n– just above 0:6%of the original.\\nTo measure the difference (error) between Aand its rank- kapproxima-\\ntionbA(k), we need the notion of a norm. In Section 3.1, we already used\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.6 Matrix Approximation 131\\nFigure 4.12 Image\\nreconstruction with\\nthe SVD. (a)\\nOriginal image.\\n(b)–(f) Image\\nreconstruction using\\nthe low-rank\\napproximation of\\nthe SVD, where the\\nrank-k\\napproximation is\\ngiven bybA(k) =Pk\\ni=1\\x1biAi.\\n(a) Original image A.\\n (b) Rank-1 approximation bA(1).\\n(c) Rank-2 approximation bA(2).\\n(d) Rank-3 approximation bA(3).\\n(e) Rank-4 approximation bA(4).\\n(f) Rank-5 approximation bA(5).\\nnorms on vectors that measure the length of a vector. By analogy we can\\nalso deﬁne norms on matrices.\\nDeﬁnition 4.23 (Spectral Norm of a Matrix) .Forx2Rnnf0g, the spectral spectral norm\\nnorm of a matrixA2Rm\\x02nis deﬁned as\\nkAk2:= max\\nxkAxk2\\nkxk2: (4.93)\\nWe introduce the notation of a subscript in the matrix norm (left-hand\\nside), similar to the Euclidean norm for vectors (right-hand side), which\\nhas subscript 2. The spectral norm (4.93) determines how long any vector\\nxcan at most become when multiplied by A.\\nTheorem 4.24. The spectral norm of Ais its largest singular value \\x1b1.\\nWe leave the proof of this theorem as an exercise.\\nEckart-Young\\ntheorem Theorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)) .Con-\\nsider a matrix A2Rm\\x02nof rankrand letB2Rm\\x02nbe a matrix of rank\\nk. For anyk6rwithbA(k) =Pk\\ni=1\\x1biuiv>\\niit holds that\\nbA(k) = argminrk(B)=kkA\\x00Bk2; (4.94)\\r\\r\\rA\\x00bA(k)\\r\\r\\r\\n2=\\x1bk+1: (4.95)\\nThe Eckart-Young theorem states explicitly how much error we intro-\\nduce by approximating Ausing a rank- kapproximation. We can inter-\\npret the rank- kapproximation obtained with the SVD as a projection of\\nthe full-rank matrix Aonto a lower-dimensional space of rank-at-most- k\\nmatrices. Of all possible projections, the SVD minimizes the error (with\\nrespect to the spectral norm) between Aand any rank- kapproximation.\\nWe can retrace some of the steps to understand why (4.95) should hold.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n132 Matrix Decompositions\\nWe observe that the difference between A\\x00bA(k)is a matrix containing\\nthe sum of the remaining rank- 1matrices\\nA\\x00bA(k) =rX\\ni=k+1\\x1biuiv>\\ni: (4.96)\\nBy Theorem 4.24, we immediately obtain \\x1bk+1as the spectral norm of the\\ndifference matrix. Let us have a closer look at (4.94). If we assume that\\nthere is another matrix Bwith rk(B)6k, such that\\nkA\\x00Bk2<\\r\\r\\rA\\x00bA(k)\\r\\r\\r\\n2; (4.97)\\nthen there exists an at least ( n\\x00k)-dimensional null space Z\\x12Rn, such\\nthatx2Zimplies thatBx=0. Then it follows that\\nkAxk2=k(A\\x00B)xk2; (4.98)\\nand by using a version of the Cauchy-Schwartz inequality (3.17) that en-\\ncompasses norms of matrices, we obtain\\nkAxk26kA\\x00Bk2kxk2<\\x1bk+1kxk2: (4.99)\\nHowever, there exists a (k+ 1)-dimensional subspace where kAxk2>\\n\\x1bk+1kxk2, which is spanned by the right-singular vectors vj;j6k+ 1of\\nA. Adding up dimensions of these two spaces yields a number greater than\\nn, as there must be a nonzero vector in both spaces. This is a contradiction\\nof the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\\nThe Eckart-Young theorem implies that we can use SVD to reduce a\\nrank-rmatrixAto a rank-kmatrixbAin a principled, optimal (in the\\nspectral norm sense) manner. We can interpret the approximation of Aby\\na rank-kmatrix as a form of lossy compression. Therefore, the low-rank\\napproximation of a matrix appears in many machine learning applications,\\ne.g., image processing, noise ﬁltering, and regularization of ill-posed prob-\\nlems. Furthermore, it plays a key role in dimensionality reduction and\\nprincipal component analysis, as we will see in Chapter 10.\\nExample 4.15 (Finding Structure in Movie Ratings and Consumers\\n(continued))\\nComing back to our movie-rating example, we can now apply the con-\\ncept of low-rank approximations to approximate the original data matrix.\\nRecall that our ﬁrst singular value captures the notion of science ﬁction\\ntheme in movies and science ﬁction lovers. Thus, by using only the ﬁrst\\nsingular value term in a rank- 1decomposition of the movie-rating matrix,\\nwe obtain the predicted ratings\\nA1=u1v>\\n1=2\\n664\\x000:6710\\n\\x000:7197\\n\\x000:0939\\n\\x000:15153\\n775\\x02\\x000:7367\\x000:6515\\x000:1811\\x03\\n(4.100a)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.7 Matrix Phylogeny 133\\n=2\\n6640:4943 0:4372 0:1215\\n0:5302 0:4689 0:1303\\n0:0692 0:0612 0:0170\\n0:1116 0:0987 0:02743\\n775: (4.100b)\\nThis ﬁrst rank- 1approximation A1is insightful: it tells us that Ali and\\nBeatrix like science ﬁction movies, such as Star Wars and Bladerunner\\n(entries have values >0:4), but fails to capture the ratings of the other\\nmovies by Chandra. This is not surprising, as Chandra’s type of movies is\\nnot captured by the ﬁrst singular value. The second singular value gives\\nus a better rank- 1approximation for those movie-theme lovers:\\nA2=u2v>\\n2=2\\n6640:0236\\n0:2054\\n\\x000:7705\\n\\x000:60303\\n775\\x020:0852 0:1762\\x000:9807\\x03\\n(4.101a)\\n=2\\n6640:0020 0:0042\\x000:0231\\n0:0175 0:0362\\x000:2014\\n\\x000:0656\\x000:1358 0:7556\\n\\x000:0514\\x000:1063 0:59143\\n775: (4.101b)\\nIn this second rank- 1approximation A2, we capture Chandra’s ratings\\nand movie types well, but not the science ﬁction movies. This leads us to\\nconsider the rank- 2approximation bA(2), where we combine the ﬁrst two\\nrank- 1approximations\\nbA(2) =\\x1b1A1+\\x1b2A2=2\\n6644:7801 4:2419 1:0244\\n5:2252 4:7522\\x000:0250\\n0:2493\\x000:2743 4:9724\\n0:7495 0:2756 4:02783\\n775:(4.102)\\nbA(2)is similar to the original movie ratings table\\nA=2\\n6645 4 1\\n5 5 0\\n0 0 5\\n1 0 43\\n775; (4.103)\\nand this suggests that we can ignore the contribution of A3. We can in-\\nterpret this so that in the data table there is no evidence of a third movie-\\ntheme/movie-lovers category. This also means that the entire space of\\nmovie-themes/movie-lovers in our example is a two-dimensional space\\nspanned by science ﬁction and French art house movies and lovers.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n134 Matrix Decompositions\\nFigure 4.13 A\\nfunctional\\nphylogeny of\\nmatrices\\nencountered in\\nmachine learning.Real matrices\\n9Pseudo-inverse\\n9SVD\\nSquare\\n9Determinant\\n9TraceNonsquare\\nDefectiveSingular\\nNon-defective\\n(diagonalizable)Singular\\nNormal Non-normal\\nSymmetric\\neigenvalues2R\\nPositive deﬁnite\\nCholesky\\neigenvalues >0Diagonal\\nIdentity\\nmatrix9Inverse Matrix\\nRegular\\n(invertible)\\nOrthogonal RotationRn\\x02nRn\\x02m\\nNo basis of\\neigenvectors\\nBasis of\\neigenvectors\\nA>A=AA>A>A6=AA>\\nColumns are\\northogonal\\neigenvectorsA>A=AA\\n>\\n=Idet\\n6= 0det\\n6= 0det = 0\\n4.7 Matrix Phylogeny\\nThe word\\n“phylogenetic”\\ndescribes how we\\ncapture the\\nrelationships among\\nindividuals or\\ngroups and derived\\nfrom the Greek\\nwords for “tribe”\\nand “source”.In Chapters 2 and 3, we covered the basics of linear algebra and analytic\\ngeometry. In this chapter, we looked at fundamental characteristics of ma-\\ntrices and linear mappings. Figure 4.13 depicts the phylogenetic tree of\\nrelationships between different types of matrices (black arrows indicating\\n“is a subset of”) and the covered operations we can perform on them (in\\nblue). We consider all real matricesA2Rn\\x02m. For non-square matrices\\n(wheren6=m), the SVD always exists, as we saw in this chapter. Focus-\\ning on square matrices A2Rn\\x02n, the determinant informs us whether a\\nsquare matrix possesses an inverse matrix , i.e., whether it belongs to the\\nclass of regular, invertible matrices. If the square n\\x02nmatrix possesses n\\nlinearly independent eigenvectors, then the matrix is non-defective and an\\neigendecomposition exists (Theorem 4.12). We know that repeated eigen-\\nvalues may result in defective matrices, which cannot be diagonalized.\\nNon-singular and non-defective matrices are not the same. For exam-\\nple, a rotation matrix will be invertible (determinant is nonzero) but not\\ndiagonalizable in the real numbers (eigenvalues are not guaranteed to be\\nreal numbers).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n4.8 Further Reading 135\\nWe dive further into the branch of non-defective square n\\x02nmatrices.\\nAisnormal if the condition A>A=AA>holds. Moreover, if the more\\nrestrictive condition holds that A>A=AA>=I, thenAis called or-\\nthogona l (see Deﬁnition 3.8). The set of orthogonal matrices is a subset of\\nthe regular (invertible) matrices and satisﬁes A>=A\\x001.\\nNormal matrices have a frequently encountered subset, the symmetric\\nmatricesS2Rn\\x02n, which satisfy S=S>. Symmetric matrices have only\\nreal eigenvalues. A subset of the symmetric matrices consists of the pos-\\nitive deﬁnite matrices Pthat satisfy the condition of x>Px>0for all\\nx2Rnnf0g. In this case, a unique Cholesky decomposition exists (Theo-\\nrem 4.18). Positive deﬁnite matrices have only positive eigenvalues and\\nare always invertible (i.e., have a nonzero determinant).\\nAnother subset of symmetric matrices consists of the diagonal matrices\\nD. Diagonal matrices are closed under multiplication and addition, but do\\nnot necessarily form a group (this is only the case if all diagonal entries\\nare nonzero so that the matrix is invertible). A special diagonal matrix is\\nthe identity matrix I.\\n4.8 Further Reading\\nMost of the content in this chapter establishes underlying mathematics\\nand connects them to methods for studying mappings, many of which are\\nat the heart of machine learning at the level of underpinning software so-\\nlutions and building blocks for almost all machine learning theory. Matrix\\ncharacterization using determinants, eigenspectra, and eigenspaces pro-\\nvides fundamental features and conditions for categorizing and analyzing\\nmatrices. This extends to all forms of representations of data and map-\\npings involving data, as well as judging the numerical stability of compu-\\ntational operations on such matrices (Press et al., 2007).\\nDeterminants are fundamental tools in order to invert matrices and\\ncompute eigenvalues “by hand”. However, for almost all but the smallest\\ninstances, numerical computation by Gaussian elimination outperforms\\ndeterminants (Press et al., 2007). Determinants remain nevertheless a\\npowerful theoretical concept, e.g., to gain intuition about the orientation\\nof a basis based on the sign of the determinant. Eigenvectors can be used\\nto perform basis changes to transform data into the coordinates of mean-\\ningful orthogonal, feature vectors. Similarly, matrix decomposition meth-\\nods, such as the Cholesky decomposition, reappear often when we com-\\npute or simulate random events (Rubinstein and Kroese, 2016). Therefore,\\nthe Cholesky decomposition enables us to compute the reparametrization\\ntrick where we want to perform continuous differentiation over random\\nvariables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014;\\nKingma and Welling, 2014).\\nEigendecomposition is fundamental in enabling us to extract mean-\\ningful and interpretable information that characterizes linear mappings.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n136 Matrix Decompositions\\nTherefore, the eigendecomposition underlies a general class of machine\\nlearning algorithms called spectral methods that perform eigendecomposi-\\ntion of a positive-deﬁnite kernel. These spectral decomposition methods\\nencompass classical approaches to statistical data analysis, such as the\\nfollowing:\\nprincipal component\\nanalysis Principal component analysis (PCA (Pearson, 1901), see also Chapter 10),\\nin which a low-dimensional subspace, which explains most of the vari-\\nability in the data, is sought. Fisher discriminant\\nanalysis Fisher discriminant analysis , which aims to determine a separating hy-\\nperplane for data classiﬁcation (Mika et al., 1999). multidimensional\\nscaling Multidimensional scaling (MDS) (Carroll and Chang, 1970).\\nThe computational efﬁciency of these methods typically comes from ﬁnd-\\ning the best rank- kapproximation to a symmetric, positive semideﬁnite\\nmatrix. More contemporary examples of spectral methods have different\\norigins, but each of them requires the computation of the eigenvectors\\nand eigenvalues of a positive-deﬁnite kernel, such as Isomap (Tenenbaum Isomap\\net al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), Hessian Laplacian\\neigenmaps\\nHessian eigenmapseigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and\\nspectral clusteringMalik, 2000). The core computations of these are generally underpinned\\nby low-rank matrix approximation techniques (Belabbas and Wolfe, 2009)\\nas we encountered here via the SVD.\\nThe SVD allows us to discover some of the same kind of information as\\nthe eigendecomposition. However, the SVD is more generally applicable\\nto non-square matrices and data tables. These matrix factorization meth-\\nods become relevant whenever we want to identify heterogeneity in data\\nwhen we want to perform data compression by approximation, e.g., in-\\nstead of storing n\\x02mvalues just storing (n+m)kvalues, or when we want\\nto perform data pre-processing, e.g., to decorrelate predictor variables of\\na design matrix (Ormoneit et al., 2001). The SVD operates on matrices,\\nwhich we can interpret as rectangular arrays with two indices (rows and\\ncolumns). The extension of matrix-like structure to higher-dimensional\\narrays are called tensors. It turns out that the SVD is the special case of\\na more general family of decompositions that operate on such tensors\\n(Kolda and Bader, 2009). SVD-like operations and low-rank approxima-\\ntions on tensors are, for example, the Tucker decomposition (Tucker, 1966) Tucker\\ndecomposition or the CP decomposition (Carroll and Chang, 1970).\\nCP decomposition The SVD low-rank approximation is frequently used in machine learn-\\ning for computational efﬁciency reasons. This is because it reduces the\\namount of memory and operations with nonzero multiplications we need\\nto perform on potentially very large matrices of data (Trefethen and Bau III,\\n1997). Moreover, low-rank approximations are used to operate on ma-\\ntrices that may contain missing values as well as for purposes of lossy\\ncompression and dimensionality reduction (Moonen and De Moor, 1995;\\nMarkovsky, 2011).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 137\\nExercises\\n4.1 Compute the determinant using the Laplace expansion (using the ﬁrst row)\\nand the Sarrus rule for\\nA=2\\n41 3 5\\n2 4 6\\n0 2 43\\n5:\\n4.2 Compute the following determinant efﬁciently:\\n2\\n666642 0 1 2 0\\n2\\x001 0 1 1\\n0 1 2 1 2\\n\\x002 0 2\\x001 2\\n2 0 0 1 13\\n77775:\\n4.3 Compute the eigenspaces of\\na.\\nA:=\\x14\\n1 0\\n1 1\\x15\\nb.\\nB:=\\x14\\n\\x002 2\\n2 1\\x15\\n4.4 Compute all eigenspaces of\\nA=2\\n6640\\x001 1 1\\n\\x001 1\\x002 3\\n2\\x001 0 0\\n1\\x001 1 03\\n775:\\n4.5 Diagonalizability of a matrix is unrelated to its invertibility. Determine for\\nthe following four matrices whether they are diagonalizable and/or invert-\\nible\\n\\x14\\n1 0\\n0 1\\x15\\n;\\x14\\n1 0\\n0 0\\x15\\n;\\x14\\n1 1\\n0 1\\x15\\n;\\x14\\n0 1\\n0 0\\x15\\n:\\n4.6 Compute the eigenspaces of the following transformation matrices. Are they\\ndiagonalizable?\\na. For\\nA=2\\n42 3 0\\n1 4 3\\n0 0 13\\n5\\nb. For\\nA=2\\n6641 1 0 0\\n0 0 0 0\\n0 0 0 0\\n0 0 0 03\\n775\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n138 Matrix Decompositions\\n4.7 Are the following matrices diagonalizable? If yes, determine their diagonal\\nform and a basis with respect to which the transformation matrices are di-\\nagonal. If no, give reasons why they are not diagonalizable.\\na.\\nA=\\x14\\n0 1\\n\\x008 4\\x15\\nb.\\nA=2\\n41 1 1\\n1 1 1\\n1 1 13\\n5\\nc.\\nA=2\\n6645 4 2 1\\n0 1\\x001\\x001\\n\\x001\\x001 3 0\\n1 1\\x001 23\\n775\\nd.\\nA=2\\n45\\x006\\x006\\n\\x001 4 2\\n3\\x006\\x0043\\n5\\n4.8 Find the SVD of the matrix\\nA=\\x14\\n3 2 2\\n2 3\\x002\\x15\\n:\\n4.9 Find the singular value decomposition of\\nA=\\x14\\n2 2\\n\\x001 1\\x15\\n:\\n4.10 Find the rank-1 approximation of\\nA=\\x14\\n3 2 2\\n2 3\\x002\\x15\\n4.11 Show that for any A2Rm\\x02nthe matrices A>AandAA>possess the\\nsame nonzero eigenvalues.\\n4.12 Show that for x6=0Theorem 4.24 holds, i.e., show that\\nmax\\nxkAxk2\\nkxk2=\\x1b1;\\nwhere\\x1b1is the largest singular value of A2Rm\\x02n.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5\\nVector Calculus\\nMany algorithms in machine learning optimize an objective function with\\nrespect to a set of desired model parameters that control how well a model\\nexplains the data: Finding good parameters can be phrased as an opti-\\nmization problem (see Sections 8.2 and 8.3). Examples include: (i) lin-\\near regression (see Chapter 9), where we look at curve-ﬁtting problems\\nand optimize linear weight parameters to maximize the likelihood; (ii)\\nneural-network auto-encoders for dimensionality reduction and data com-\\npression, where the parameters are the weights and biases of each layer,\\nand where we minimize a reconstruction error by repeated application of\\nthe chain rule; and (iii) Gaussian mixture models (see Chapter 11) for\\nmodeling data distributions, where we optimize the location and shape\\nparameters of each mixture component to maximize the likelihood of the\\nmodel. Figure 5.1 illustrates some of these problems, which we typically\\nsolve by using optimization algorithms that exploit gradient information\\n(Section 7.1). Figure 5.2 gives an overview of how concepts in this chap-\\nter are related and how they are connected to other chapters of the book.\\nCentral to this chapter is the concept of a function. A function fis\\na quantity that relates two quantities to each other. In this book, these\\nquantities are typically inputs x2RDand targets (function values) f(x),\\nwhich we assume are real-valued if not stated otherwise. Here RDis the\\ndomain off, and the function values f(x)are the image/codomain off.domain\\nimage/codomain\\nFigure 5.1 Vector\\ncalculus plays a\\ncentral role in (a)\\nregression (curve\\nﬁtting) and (b)\\ndensity estimation,\\ni.e., modeling data\\ndistributions.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(a) Regression problem: Find parameters,\\nsuch that the curve explains the observations\\n(crosses) well.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(b) Density estimation with a Gaussian mixture\\nmodel: Find means and covariances, such that\\nthe data (dots) can be explained well.\\n139\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n140 Vector Calculus\\nFigure 5.2 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhen they are used\\nin other parts of the\\nbook.Difference quotient\\nPartial derivatives\\nJacobian\\nHessian\\nTaylor seriesChapter 7\\nOptimization\\nChapter 6\\nProbabilityChapter 9\\nRegression\\nChapter 10\\nDimensionality\\nreduction\\nChapter 11\\nDensity estimation\\nChapter 12\\nClassiﬁcation\\ndeﬁnes collected in used inused inused in\\nused inused in\\nused inused in\\nSection 2.7.3 provides much more detailed discussion in the context of\\nlinear functions. We often write\\nf:RD!R (5.1a)\\nx7!f(x) (5.1b)\\nto specify a function, where (5.1a) speciﬁes that fis a mapping from\\nRDtoRand (5.1b) speciﬁes the explicit assignment of an input xto\\na function value f(x). A function fassigns every input xexactly one\\nfunction value f(x).\\nExample 5.1\\nRecall the dot product as a special case of an inner product (Section 3.2).\\nIn the previous notation, the function f(x) =x>x;x2R2, would be\\nspeciﬁed as\\nf:R2!R (5.2a)\\nx7!x2\\n1+x2\\n2: (5.2b)\\nIn this chapter, we will discuss how to compute gradients of functions,\\nwhich is often essential to facilitate learning in machine learning models\\nsince the gradient points in the direction of steepest ascent. Therefore,\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.1 Differentiation of Univariate Functions 141\\nFigure 5.3 The\\naverage incline of a\\nfunctionfbetween\\nx0andx0+\\x0exis\\nthe incline of the\\nsecant (blue)\\nthroughf(x0)and\\nf(x0+\\x0ex)and\\ngiven by\\x0ey=\\x0ex .\\n\\x0ey\\n\\x0exf(x)\\nxy\\nf(x0)f(x0+\\x0ex)\\nvector calculus is one of the fundamental mathematical tools we need in\\nmachine learning. Throughout this book, we assume that functions are\\ndifferentiable. With some additional technical deﬁnitions, which we do\\nnot cover here, many of the approaches presented can be extended to\\nsub-differentials (functions that are continuous but not differentiable at\\ncertain points). We will look at an extension to the case of functions with\\nconstraints in Chapter 7.\\n5.1 Differentiation of Univariate Functions\\nIn the following, we brieﬂy revisit differentiation of a univariate function,\\nwhich may be familiar from high school mathematics. We start with the\\ndifference quotient of a univariate function y=f(x); x;y2R, which we\\nwill subsequently use to deﬁne derivatives.\\nDeﬁnition 5.1 (Difference Quotient) .Thedifference quotient difference quotient\\n\\x0ey\\n\\x0ex:=f(x+\\x0ex)\\x00f(x)\\n\\x0ex(5.3)\\ncomputes the slope of the secant line through two points on the graph of\\nf. In Figure 5.3, these are the points with x-coordinates x0andx0+\\x0ex.\\nThe difference quotient can also be considered the average slope of f\\nbetweenxandx+\\x0exif we assume fto be a linear function. In the limit\\nfor\\x0ex!0, we obtain the tangent of fatx, iffis differentiable. The\\ntangent is then the derivative of fatx.\\nDeﬁnition 5.2 (Derivative) .More formally, for h >0thederivative off derivative\\natxis deﬁned as the limit\\ndf\\ndx:= lim\\nh!0f(x+h)\\x00f(x)\\nh; (5.4)\\nand the secant in Figure 5.3 becomes a tangent.\\nThe derivative of fpoints in the direction of steepest ascent of f.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n142 Vector Calculus\\nExample 5.2 (Derivative of a Polynomial)\\nWe want to compute the derivative of f(x) =xn;n2N. We may already\\nknow that the answer will be nxn\\x001, but we want to derive this result\\nusing the deﬁnition of the derivative as the limit of the difference quotient.\\nUsing the deﬁnition of the derivative in (5.4), we obtain\\ndf\\ndx= lim\\nh!0f(x+h)\\x00f(x)\\nh(5.5a)\\n= lim\\nh!0(x+h)n\\x00xn\\nh(5.5b)\\n= lim\\nh!0Pn\\ni=0\\x00n\\ni\\x01xn\\x00ihi\\x00xn\\nh: (5.5c)\\nWe see that xn=\\x00n\\n0\\x01xn\\x000h0. By starting the sum at 1, thexn-term cancels,\\nand we obtain\\ndf\\ndx= lim\\nh!0Pn\\ni=1\\x00n\\ni\\x01xn\\x00ihi\\nh(5.6a)\\n= lim\\nh!0nX\\ni=1 \\nn\\ni!\\nxn\\x00ihi\\x001(5.6b)\\n= lim\\nh!0  \\nn\\n1!\\nxn\\x001+nX\\ni=2 \\nn\\ni!\\nxn\\x00ihi\\x001\\n|{z}\\n!0ash!0!\\n(5.6c)\\n=n!\\n1!(n\\x001)!xn\\x001=nxn\\x001: (5.6d)\\n5.1.1 Taylor Series\\nThe Taylor series is a representation of a function fas an inﬁnite sum of\\nterms. These terms are determined using derivatives of fevaluated at x0.\\nDeﬁnition 5.3 (Taylor Polynomial) .TheTaylor polynomial of degreenof Taylor polynomial\\nf:R!Ratx0is deﬁned as We deﬁnet0:= 1\\nfor allt2R.\\nTn(x) :=nX\\nk=0f(k)(x0)\\nk!(x\\x00x0)k; (5.7)\\nwheref(k)(x0)is thekth derivative of fatx0(which we assume exists)\\nandf(k)(x0)\\nk!are the coefﬁcients of the polynomial.\\nDeﬁnition 5.4 (Taylor Series) .For a smooth function f2C1,f:R!R,\\ntheTaylor series offatx0is deﬁned as Taylor series\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.1 Differentiation of Univariate Functions 143\\nT1(x) =1X\\nk=0f(k)(x0)\\nk!(x\\x00x0)k: (5.8)\\nForx0= 0, we obtain the Maclaurin series as a special instance of the f2C1means that\\nfis continuously\\ndifferentiable\\ninﬁnitely many\\ntimes.\\nMaclaurin seriesTaylor series. If f(x) =T1(x), thenfis called analytic .\\nanalyticRemark. In general, a Taylor polynomial of degree nis an approximation\\nof a function, which does not need to be a polynomial. The Taylor poly-\\nnomial is similar to fin a neighborhood around x0. However, a Taylor\\npolynomial of degree nis an exact representation of a polynomial fof\\ndegreek6nsince all derivatives f(i),i>k vanish. }\\nExample 5.3 (Taylor Polynomial)\\nWe consider the polynomial\\nf(x) =x4(5.9)\\nand seek the Taylor polynomial T6, evaluated at x0= 1. We start by com-\\nputing the coefﬁcients f(k)(1)fork= 0;:::; 6:\\nf(1) = 1 (5.10)\\nf0(1) = 4 (5.11)\\nf00(1) = 12 (5.12)\\nf(3)(1) = 24 (5.13)\\nf(4)(1) = 24 (5.14)\\nf(5)(1) = 0 (5.15)\\nf(6)(1) = 0 (5.16)\\nTherefore, the desired Taylor polynomial is\\nT6(x) =6X\\nk=0f(k)(x0)\\nk!(x\\x00x0)k(5.17a)\\n= 1 + 4(x\\x001) + 6(x\\x001)2+ 4(x\\x001)3+ (x\\x001)4+ 0:(5.17b)\\nMultiplying out and re-arranging yields\\nT6(x) = (1\\x004 + 6\\x004 + 1) +x(4\\x0012 + 12\\x004)\\n+x2(6\\x0012 + 6) +x3(4\\x004) +x4(5.18a)\\n=x4=f(x); (5.18b)\\ni.e., we obtain an exact representation of the original function.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n144 Vector Calculus\\nFigure 5.4 Taylor\\npolynomials. The\\noriginal function\\nf(x) =\\nsin(x) + cos(x)\\n(black, solid) is\\napproximated by\\nTaylor polynomials\\n(dashed) around\\nx0= 0.\\nHigher-order Taylor\\npolynomials\\napproximate the\\nfunctionfbetter\\nand more globally.\\nT10is already\\nsimilar tofin\\n[\\x004;4].\\n−4−2 0 2 4\\nx−2024yf\\nT0\\nT1\\nT5\\nT10\\nExample 5.4 (Taylor Series)\\nConsider the function in Figure 5.4 given by\\nf(x) = sin(x) + cos(x)2C1: (5.19)\\nWe seek a Taylor series expansion of fatx0= 0, which is the Maclaurin\\nseries expansion of f. We obtain the following derivatives:\\nf(0) = sin(0) + cos(0) = 1 (5.20)\\nf0(0) = cos(0)\\x00sin(0) = 1 (5.21)\\nf00(0) =\\x00sin(0)\\x00cos(0) =\\x001 (5.22)\\nf(3)(0) =\\x00cos(0) + sin(0) = \\x001 (5.23)\\nf(4)(0) = sin(0) + cos(0) = f(0) = 1 (5.24)\\n...\\nWe can see a pattern here: The coefﬁcients in our Taylor series are only\\n\\x061(since sin(0) = 0 ), each of which occurs twice before switching to the\\nother one. Furthermore, f(k+4)(0) =f(k)(0).\\nTherefore, the full Taylor series expansion of fatx0= 0is given by\\nT1(x) =1X\\nk=0f(k)(x0)\\nk!(x\\x00x0)k(5.25a)\\n= 1 +x\\x001\\n2!x2\\x001\\n3!x3+1\\n4!x4+1\\n5!x5\\x00\\x01\\x01\\x01 (5.25b)\\n= 1\\x001\\n2!x2+1\\n4!x4\\x07\\x01\\x01\\x01 +x\\x001\\n3!x3+1\\n5!x5\\x07\\x01\\x01\\x01 (5.25c)\\n=1X\\nk=0(\\x001)k1\\n(2k)!x2k+1X\\nk=0(\\x001)k 1\\n(2k+ 1)!x2k+1(5.25d)\\n= cos(x) + sin(x); (5.25e)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.1 Differentiation of Univariate Functions 145\\nwhere we used the power series representations power series\\nrepresentation\\ncos(x) =1X\\nk=0(\\x001)k1\\n(2k)!x2k; (5.26)\\nsin(x) =1X\\nk=0(\\x001)k 1\\n(2k+ 1)!x2k+1: (5.27)\\nFigure 5.4 shows the corresponding ﬁrst Taylor polynomials Tnforn=\\n0;1;5;10.\\nRemark. A Taylor series is a special case of a power series\\nf(x) =1X\\nk=0ak(x\\x00c)k(5.28)\\nwhereakare coefﬁcients and cis a constant, which has the special form\\nin Deﬁnition 5.4. }\\n5.1.2 Differentiation Rules\\nIn the following, we brieﬂy state basic differentiation rules, where we\\ndenote the derivative of fbyf0.\\nProduct rule: (f(x)g(x))0=f0(x)g(x) +f(x)g0(x) (5.29)\\nQuotient rule:\\x12f(x)\\ng(x)\\x130\\n=f0(x)g(x)\\x00f(x)g0(x)\\n(g(x))2(5.30)\\nSum rule: (f(x) +g(x))0=f0(x) +g0(x) (5.31)\\nChain rule:\\x00g(f(x))\\x010= (g\\x0ef)0(x) =g0(f(x))f0(x) (5.32)\\nHere,g\\x0efdenotes function composition x7!f(x)7!g(f(x)).\\nExample 5.5 (Chain Rule)\\nLet us compute the derivative of the function h(x) = (2x+ 1)4using the\\nchain rule. With\\nh(x) = (2x+ 1)4=g(f(x)); (5.33)\\nf(x) = 2x+ 1; (5.34)\\ng(f) =f4; (5.35)\\nwe obtain the derivatives of fandgas\\nf0(x) = 2; (5.36)\\ng0(f) = 4f3; (5.37)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n146 Vector Calculus\\nsuch that the derivative of his given as\\nh0(x) =g0(f)f0(x) = (4f3)\\x012(5.34)= 4(2x+ 1)3\\x012 = 8(2x+ 1)3;(5.38)\\nwhere we used the chain rule (5.32) and substituted the deﬁnition of f\\nin (5.34) in g0(f).\\n5.2 Partial Differentiation and Gradients\\nDifferentiation as discussed in Section 5.1 applies to functions fof a\\nscalar variable x2R. In the following, we consider the general case\\nwhere the function fdepends on one or more variables x2Rn, e.g.,\\nf(x) =f(x1;x2). The generalization of the derivative to functions of sev-\\neral variables is the gradient .\\nWe ﬁnd the gradient of the function fwith respect to xbyvarying one\\nvariable at a time and keeping the others constant. The gradient is then\\nthe collection of these partial derivatives .\\nDeﬁnition 5.5 (Partial Derivative) .For a function f:Rn!R;x7!\\nf(x);x2Rnofnvariablesx1;:::;xnwe deﬁne the partial derivatives as partial derivative\\n@f\\n@x1= lim\\nh!0f(x1+h;x 2;:::;xn)\\x00f(x)\\nh\\n...\\n@f\\n@xn= lim\\nh!0f(x1;:::;xn\\x001;xn+h)\\x00f(x)\\nh(5.39)\\nand collect them in the row vector\\nrxf= gradf=df\\ndx=\\x14@f(x)\\n@x1@f(x)\\n@x2\\x01\\x01\\x01@f(x)\\n@xn\\x15\\n2R1\\x02n;(5.40)\\nwherenis the number of variables and 1is the dimension of the image/\\nrange/codomain of f. Here, we deﬁned the column vector x= [x1;:::;xn]>\\n2Rn. The row vector in (5.40) is called the gradient offor the Jacobian gradient\\nJacobian and is the generalization of the derivative from Section 5.1.\\nRemark. This deﬁnition of the Jacobian is a special case of the general\\ndeﬁnition of the Jacobian for vector-valued functions as the collection of\\npartial derivatives. We will get back to this in Section 5.3. }We can use results\\nfrom scalar\\ndifferentiation: Each\\npartial derivative is\\na derivative with\\nrespect to a scalar.Example 5.6 (Partial Derivatives Using the Chain Rule)\\nForf(x;y) = (x+ 2y3)2, we obtain the partial derivatives\\n@f(x;y)\\n@x= 2(x+ 2y3)@\\n@x(x+ 2y3) = 2(x+ 2y3); (5.41)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.2 Partial Differentiation and Gradients 147\\n@f(x;y)\\n@y= 2(x+ 2y3)@\\n@y(x+ 2y3) = 12(x+ 2y3)y2: (5.42)\\nwhere we used the chain rule (5.32) to compute the partial derivatives.\\nRemark (Gradient as a Row Vector) .It is not uncommon in the literature\\nto deﬁne the gradient vector as a column vector, following the conven-\\ntion that vectors are generally column vectors. The reason why we deﬁne\\nthe gradient vector as a row vector is twofold: First, we can consistently\\ngeneralize the gradient to vector-valued functions f:Rn!Rm(then\\nthe gradient becomes a matrix). Second, we can immediately apply the\\nmulti-variate chain rule without paying attention to the dimension of the\\ngradient. We will discuss both points in Section 5.3. }\\nExample 5.7 (Gradient)\\nForf(x1;x2) =x2\\n1x2+x1x3\\n22R, the partial derivatives (i.e., the deriva-\\ntives offwith respect to x1andx2) are\\n@f(x1;x2)\\n@x1= 2x1x2+x3\\n2 (5.43)\\n@f(x1;x2)\\n@x2=x2\\n1+ 3x1x2\\n2 (5.44)\\nand the gradient is then\\ndf\\ndx=\\x14@f(x1;x2)\\n@x1@f(x1;x2)\\n@x2\\x15\\n=\\x022x1x2+x3\\n2x2\\n1+ 3x1x2\\n2\\x032R1\\x022:\\n(5.45)\\n5.2.1 Basic Rules of Partial Differentiation\\nProduct rule:\\n(fg)0=f0g+fg0,\\nSum rule:\\n(f+g)0=f0+g0,\\nChain rule:\\n(g(f))0=g0(f)f0In the multivariate case, where x2Rn, the basic differentiation rules that\\nwe know from school (e.g., sum rule, product rule, chain rule; see also\\nSection 5.1.2) still apply. However, when we compute derivatives with re-\\nspect to vectors x2Rnwe need to pay attention: Our gradients now\\ninvolve vectors and matrices, and matrix multiplication is not commuta-\\ntive (Section 2.2.1), i.e., the order matters.\\nHere are the general product rule, sum rule, and chain rule:\\nProduct rule:@\\n@x\\x00f(x)g(x)\\x01=@f\\n@xg(x) +f(x)@g\\n@x(5.46)\\nSum rule:@\\n@x\\x00f(x) +g(x)\\x01=@f\\n@x+@g\\n@x(5.47)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n148 Vector Calculus\\nChain rule:@\\n@x(g\\x0ef)(x) =@\\n@x\\x00g(f(x))\\x01=@g\\n@f@f\\n@x(5.48)\\nLet us have a closer look at the chain rule. The chain rule (5.48) resem- This is only an\\nintuition, but not\\nmathematically\\ncorrect since the\\npartial derivative is\\nnot a fraction.bles to some degree the rules for matrix multiplication where we said that\\nneighboring dimensions have to match for matrix multiplication to be de-\\nﬁned; see Section 2.2.1. If we go from left to right, the chain rule exhibits\\nsimilar properties: @fshows up in the “denominator” of the ﬁrst factor\\nand in the “numerator” of the second factor. If we multiply the factors to-\\ngether, multiplication is deﬁned, i.e., the dimensions of @fmatch, and @f\\n“cancels”, such that @g=@xremains.\\n5.2.2 Chain Rule\\nConsider a function f:R2!Rof two variables x1;x2. Furthermore,\\nx1(t)andx2(t)are themselves functions of t. To compute the gradient of\\nfwith respect to t, we need to apply the chain rule (5.48) for multivariate\\nfunctions as\\ndf\\ndt=h\\n@f\\n@x1@f\\n@x2i\"\\n@x1(t)\\n@t@x2(t)\\n@t#\\n=@f\\n@x1@x1\\n@t+@f\\n@x2@x2\\n@t; (5.49)\\nwhere ddenotes the gradient and @partial derivatives.\\nExample 5.8\\nConsiderf(x1;x2) =x2\\n1+ 2x2, wherex1= sintandx2= cost, then\\ndf\\ndt=@f\\n@x1@x1\\n@t+@f\\n@x2@x2\\n@t(5.50a)\\n= 2 sint@sint\\n@t+ 2@cost\\n@t(5.50b)\\n= 2 sintcost\\x002 sint= 2 sint(cost\\x001) (5.50c)\\nis the corresponding derivative of fwith respect to t.\\nIff(x1;x2)is a function of x1andx2, wherex1(s;t)andx2(s;t)are\\nthemselves functions of two variables sandt, the chain rule yields the\\npartial derivatives\\n@f\\n@s=@f\\n@x1@x1\\n@s+@f\\n@x2@x2\\n@s; (5.51)\\n@f\\n@t=@f\\n@x1@x1\\n@t+@f\\n@x2@x2\\n@t; (5.52)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.3 Gradients of Vector-Valued Functions 149\\nand the gradient is obtained by the matrix multiplication\\ndf\\nd(s;t)=@f\\n@x@x\\n@(s;t)=h@f\\n@x1@f\\n@x2i\\n|{z}\\n=@f\\n@x2\\n64@x1\\n@s@x1\\n@t\\n@x2\\n@s@x2\\n@t3\\n75\\n|{z}\\n=@x\\n@(s;t): (5.53)\\nThis compact way of writing the chain rule as a matrix multiplication only The chain rule can\\nbe written as a\\nmatrix\\nmultiplication.makes sense if the gradient is deﬁned as a row vector. Otherwise, we will\\nneed to start transposing gradients for the matrix dimensions to match.\\nThis may still be straightforward as long as the gradient is a vector or a\\nmatrix; however, when the gradient becomes a tensor (we will discuss this\\nin the following), the transpose is no longer a triviality.\\nRemark (Verifying the Correctness of a Gradient Implementation) .The\\ndeﬁnition of the partial derivatives as the limit of the corresponding dif-\\nference quotient (see (5.39)) can be exploited when numerically checking\\nthe correctness of gradients in computer programs: When we compute Gradient checking\\ngradients and implement them, we can use ﬁnite differences to numer-\\nically test our computation and implementation: We choose the value h\\nto be small (e.g., h= 10\\x004) and compare the ﬁnite-difference approxima-\\ntion from (5.39) with our (analytic) implementation of the gradient. If the\\nerror is small, our gradient implementation is probably correct. “Small”\\ncould mean thatqP\\ni(dhi\\x00dfi)2\\nP\\ni(dhi+dfi)2<10\\x006, wheredhiis the ﬁnite-difference\\napproximation and dfiis the analytic gradient of fwith respect to the ith\\nvariablexi. }\\n5.3 Gradients of Vector-Valued Functions\\nThus far, we discussed partial derivatives and gradients of functions f:\\nRn!Rmapping to the real numbers. In the following, we will generalize\\nthe concept of the gradient to vector-valued functions (vector ﬁelds) f:\\nRn!Rm, wheren>1andm> 1.\\nFor a function f:Rn!Rmand a vectorx= [x1;:::;xn]>2Rn, the\\ncorresponding vector of function values is given as\\nf(x) =2\\n64f1(x)\\n...\\nfm(x)3\\n752Rm: (5.54)\\nWriting the vector-valued function in this way allows us to view a vector-\\nvalued function f:Rn!Rmas a vector of functions [f1;:::;fm]>,\\nfi:Rn!Rthat map onto R. The differentiation rules for every fiare\\nexactly the ones we discussed in Section 5.2.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n150 Vector Calculus\\nTherefore, the partial derivative of a vector-valued function f:Rn!\\nRmwith respect to xi2R,i= 1;:::n , is given as the vector\\n@f\\n@xi=2\\n64@f1\\n@xi...\\n@fm\\n@xi3\\n75=2\\n64limh!0f1(x1;:::;x i\\x001;xi+h;xi+1;:::xn)\\x00f1(x)\\nh...\\nlimh!0fm(x1;:::;x i\\x001;xi+h;xi+1;:::xn)\\x00fm(x)\\nh3\\n752Rm:\\n(5.55)\\nFrom (5.40), we know that the gradient of fwith respect to a vector is\\nthe row vector of the partial derivatives. In (5.55), every partial derivative\\n@f=@xiis itself a column vector. Therefore, we obtain the gradient of f:\\nRn!Rmwith respect to x2Rnby collecting these partial derivatives:\\ndf(x)\\ndx=@f(x)\\n@x1\\x01\\x01\\x01@f(x)\\n@xn\\x14 \\x15\\n(5.56a)\\n=@f1(x)\\n@x1\\x01\\x01\\x01@f1(x)\\n@xn\\n......\\n@fm(x)\\n@x1\\x01\\x01\\x01@fm(x)\\n@xn2\\n666643\\n777752Rm\\x02n:(5.56b)\\nDeﬁnition 5.6 (Jacobian) .The collection of all ﬁrst-order partial deriva-\\ntives of a vector-valued function f:Rn!Rmis called the Jacobian . The Jacobian\\nJacobianJis anm\\x02nmatrix, which we deﬁne and arrange as follows: The gradient of a\\nfunction\\nf:Rn!Rmis a\\nmatrix of size\\nm\\x02n.J=rxf=df(x)\\ndx=\\x14@f(x)\\n@x1\\x01\\x01\\x01@f(x)\\n@xn\\x15\\n(5.57)\\n=2\\n666664@f1(x)\\n@x1\\x01\\x01\\x01@f1(x)\\n@xn......\\n@fm(x)\\n@x1\\x01\\x01\\x01@fm(x)\\n@xn3\\n777775; (5.58)\\nx=2\\n64x1\\n...\\nxn3\\n75; J(i;j) =@fi\\n@xj: (5.59)\\nAs a special case of (5.58), a function f:Rn!R1, which maps a\\nvectorx2Rnonto a scalar (e.g., f(x) =Pn\\ni=1xi), possesses a Jacobian\\nthat is a row vector (matrix of dimension 1\\x02n); see (5.40).\\nRemark. In this book, we use the numerator layout of the derivative, i.e., numerator layout\\nthe derivative df=dxoff2Rmwith respect to x2Rnis anm\\x02\\nnmatrix, where the elements of fdeﬁne the rows and the elements of\\nxdeﬁne the columns of the corresponding Jacobian; see (5.58). There\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.3 Gradients of Vector-Valued Functions 151\\nFigure 5.5 The\\ndeterminant of the\\nJacobian offcan\\nbe used to compute\\nthe magniﬁer\\nbetween the blue\\nand orange area.b1b2 c1 c2f(\\x01)\\nexists also the denominator layout , which is the transpose of the numerator denominator layout\\nlayout. In this book, we will use the numerator layout. }\\nWe will see how the Jacobian is used in the change-of-variable method\\nfor probability distributions in Section 6.7. The amount of scaling due to\\nthe transformation of a variable is provided by the determinant.\\nIn Section 4.1, we saw that the determinant can be used to compute\\nthe area of a parallelogram. If we are given two vectors b1= [1;0]>,\\nb2= [0;1]>as the sides of the unit square (blue; see Figure 5.5), the area\\nof this square is\\n\\x0c\\x0c\\x0c\\x0cdet\\x12\\x141 0\\n0 1\\x15\\x13\\x0c\\x0c\\x0c\\x0c= 1: (5.60)\\nIf we take a parallelogram with the sides c1= [\\x002;1]>,c2= [1;1]>\\n(orange in Figure 5.5), its area is given as the absolute value of the deter-\\nminant (see Section 4.1)\\n\\x0c\\x0c\\x0c\\x0cdet\\x12\\x14\\x002 1\\n1 1\\x15\\x13\\x0c\\x0c\\x0c\\x0c=j\\x003j= 3; (5.61)\\ni.e., the area of this is exactly three times the area of the unit square.\\nWe can ﬁnd this scaling factor by ﬁnding a mapping that transforms the\\nunit square into the other square. In linear algebra terms, we effectively\\nperform a variable transformation from (b1;b2)to(c1;c2). In our case,\\nthe mapping is linear and the absolute value of the determinant of this\\nmapping gives us exactly the scaling factor we are looking for.\\nWe will describe two approaches to identify this mapping. First, we ex-\\nploit that the mapping is linear so that we can use the tools from Chapter 2\\nto identify this mapping. Second, we will ﬁnd the mapping using partial\\nderivatives using the tools we have been discussing in this chapter.\\nApproach 1 To get started with the linear algebra approach, we\\nidentify bothfb1;b2gandfc1;c2gas bases of R2(see Section 2.6.1 for a\\nrecap). What we effectively perform is a change of basis from (b1;b2)to\\n(c1;c2), and we are looking for the transformation matrix that implements\\nthe basis change. Using results from Section 2.7.2, we identify the desired\\nbasis change matrix as\\nJ=\\x14\\x002 1\\n1 1\\x15\\n; (5.62)\\nsuch thatJb1=c1andJb2=c2. The absolute value of the determi-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n152 Vector Calculus\\nnant ofJ, which yields the scaling factor we are looking for, is given as\\njdet(J)j= 3, i.e., the area of the square spanned by (c1;c2)is three times\\ngreater than the area spanned by (b1;b2).\\nApproach 2 The linear algebra approach works for linear trans-\\nformations; for nonlinear transformations (which become relevant in Sec-\\ntion 6.7), we follow a more general approach using partial derivatives.\\nFor this approach, we consider a function f:R2!R2that performs a\\nvariable transformation. In our example, fmaps the coordinate represen-\\ntation of any vector x2R2with respect to (b1;b2)onto the coordinate\\nrepresentation y2R2with respect to (c1;c2). We want to identify the\\nmapping so that we can compute how an area (or volume) changes when\\nit is being transformed by f. For this, we need to ﬁnd out how f(x)\\nchanges if we modify xa bit. This question is exactly answered by the\\nJacobian matrixdf\\ndx2R2\\x022. Since we can write\\ny1=\\x002x1+x2 (5.63)\\ny2=x1+x2 (5.64)\\nwe obtain the functional relationship between xandy, which allows us\\nto get the partial derivatives\\n@y1\\n@x1=\\x002;@y1\\n@x2= 1;@y2\\n@x1= 1;@y2\\n@x2= 1 (5.65)\\nand compose the Jacobian as\\nJ=2\\n64@y1\\n@x1@y1\\n@x2@y2\\n@x1@y2\\n@x23\\n75=\\x14\\x002 1\\n1 1\\x15\\n: (5.66)\\nThe Jacobian represents the coordinate transformation we are looking Geometrically, the\\nJacobian\\ndeterminant gives\\nthe magniﬁcation/\\nscaling factor when\\nwe transform an\\narea or volume.for. It is exact if the coordinate transformation is linear (as in our case),\\nand (5.66) recovers exactly the basis change matrix in (5.62). If the co-\\nordinate transformation is nonlinear, the Jacobian approximates this non-\\nlinear transformation locally with a linear one. The absolute value of the\\nJacobian determinant jdet(J)jis the factor by which areas or volumes are\\nJacobian\\ndeterminantscaled when coordinates are transformed. Our case yields jdet(J)j= 3.\\nThe Jacobian determinant and variable transformations will become\\nrelevant in Section 6.7 when we transform random variables and prob-\\nability distributions. These transformations are extremely relevant in ma- Figure 5.6\\nDimensionality of\\n(partial) derivatives.\\nf(x)\\nx\\n@f\\n@xchine learning in the context of training deep neural networks using the\\nreparametrization trick , also called inﬁnite perturbation analysis .\\nIn this chapter, we encountered derivatives of functions. Figure 5.6 sum-\\nmarizes the dimensions of those derivatives. If f:R!Rthe gradient is\\nsimply a scalar (top-left entry). For f:RD!Rthe gradient is a 1\\x02D\\nrow vector (top-right entry). For f:R!RE, the gradient is an E\\x021\\ncolumn vector, and for f:RD!REthe gradient is an E\\x02Dmatrix.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.3 Gradients of Vector-Valued Functions 153\\nExample 5.9 (Gradient of a Vector-Valued Function)\\nWe are given\\nf(x) =Ax;f(x)2RM;A2RM\\x02N;x2RN:\\nTo compute the gradient df=dxwe ﬁrst determine the dimension of\\ndf=dx: Sincef:RN!RM, it follows that df=dx2RM\\x02N. Second,\\nto compute the gradient we determine the partial derivatives of fwith\\nrespect to every xj:\\nfi(x) =NX\\nj=1Aijxj=)@fi\\n@xj=Aij (5.67)\\nWe collect the partial derivatives in the Jacobian and obtain the gradient\\ndf\\ndx=2\\n64@f1\\n@x1\\x01\\x01\\x01@f1\\n@xN......\\n@fM\\n@x1\\x01\\x01\\x01@fM\\n@xN3\\n75=2\\n64A11\\x01\\x01\\x01A1N\\n......\\nAM1\\x01\\x01\\x01AMN3\\n75=A2RM\\x02N:(5.68)\\nExample 5.10 (Chain Rule)\\nConsider the function h:R!R,h(t) = (f\\x0eg)(t)with\\nf:R2!R (5.69)\\ng:R!R2(5.70)\\nf(x) = exp(x1x2\\n2); (5.71)\\nx=\\x14x1\\nx2\\x15\\n=g(t) =\\x14tcost\\ntsint\\x15\\n(5.72)\\nand compute the gradient of hwith respect to t. Sincef:R2!Rand\\ng:R!R2we note that\\n@f\\n@x2R1\\x022;@g\\n@t2R2\\x021: (5.73)\\nThe desired gradient is computed by applying the chain rule:\\ndh\\ndt=@f\\n@x@x\\n@t=\\x14@f\\n@x1@f\\n@x2\\x152\\n64@x1\\n@t@x2\\n@t3\\n75 (5.74a)\\n=\\x02exp(x1x2\\n2)x2\\n22 exp(x1x2\\n2)x1x2\\x03\\x14cost\\x00tsint\\nsint+tcost\\x15\\n(5.74b)\\n= exp(x1x2\\n2)\\x00x2\\n2(cost\\x00tsint) + 2x1x2(sint+tcost)\\x01;(5.74c)\\nwherex1=tcostandx2=tsint; see (5.72).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n154 Vector Calculus\\nExample 5.11 (Gradient of a Least-Squares Loss in a Linear Model)\\nLet us consider the linear model We will discuss this\\nmodel in much\\nmore detail in\\nChapter 9 in the\\ncontext of linear\\nregression, where\\nwe need derivatives\\nof the least-squares\\nlossLwith respect\\nto the parameters \\x12.y=\\x08\\x12; (5.75)\\nwhere\\x122RDis a parameter vector, \\x082RN\\x02Dare input features and\\ny2RNare the corresponding observations. We deﬁne the functions\\nL(e) :=kek2; (5.76)\\ne(\\x12) :=y\\x00\\x08\\x12: (5.77)\\nWe seek@L\\n@\\x12, and we will use the chain rule for this purpose. Lis called a\\nleast-squares loss function. least-squares loss\\nBefore we start our calculation, we determine the dimensionality of the\\ngradient as\\n@L\\n@\\x122R1\\x02D: (5.78)\\nThe chain rule allows us to compute the gradient as\\n@L\\n@\\x12=@L\\n@e@e\\n@\\x12; (5.79)\\nwhere thedth element is given by dLdtheta =\\nnp.einsum(\\n\\'n,nd\\',\\ndLde,dedtheta)@L\\n@\\x12[1;d] =NX\\nn=1@L\\n@e[n]@e\\n@\\x12[n;d]: (5.80)\\nWe know thatkek2=e>e(see Section 3.2) and determine\\n@L\\n@e= 2e>2R1\\x02N: (5.81)\\nFurthermore, we obtain\\n@e\\n@\\x12=\\x00\\x082RN\\x02D; (5.82)\\nsuch that our desired derivative is\\n@L\\n@\\x12=\\x002e>\\x08(5.77)=\\x002(y>\\x00\\x12>\\x08>)|{z}\\n1\\x02N\\x08|{z}\\nN\\x02D2R1\\x02D: (5.83)\\nRemark. We would have obtained the same result without using the chain\\nrule by immediately looking at the function\\nL2(\\x12) :=ky\\x00\\x08\\x12k2= (y\\x00\\x08\\x12)>(y\\x00\\x08\\x12): (5.84)\\nThis approach is still practical for simple functions like L2but becomes\\nimpractical for deep function compositions. }\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.4 Gradients of Matrices 155\\nFigure 5.7\\nVisualization of\\ngradient\\ncomputation of a\\nmatrix with respect\\nto a vector. We are\\ninterested in\\ncomputing the\\ngradient of\\nA2R4\\x022with\\nrespect to a vector\\nx2R3. We know\\nthat gradient\\ndA\\ndx2R4\\x022\\x023. We\\nfollow two\\nequivalent\\napproaches to arrive\\nthere: (a) collating\\npartial derivatives\\ninto a Jacobian\\ntensor;\\n(b) ﬂattening of the\\nmatrix into a vector,\\ncomputing the\\nJacobian matrix,\\nre-shaping into a\\nJacobian tensor.A2R4\\x022x2R3\\n@A\\n@x12R4\\x022@A\\n@x22R4\\x022@A\\n@x32R4\\x022x1\\nx2\\nx3\\ndA\\ndx2R4\\x022\\x023\\n4\\n23Partial derivatives:\\ncollate\\n(a) Approach 1: We compute the partial derivative\\n@A\\n@x1;@A\\n@x2;@A\\n@x3, each of which is a 4\\x022matrix, and col-\\nlate them in a 4\\x022\\x023tensor.\\nA2R4\\x022x2R3\\nx1\\nx2\\nx3\\ndA\\ndx2R4\\x022\\x023\\nre-shape re-shape gradientA2R4\\x022 ~A2R8d~A\\ndx2R8\\x023\\n(b) Approach 2: We re-shape (ﬂatten) A2R4\\x022into a vec-\\ntor~A2R8. Then, we compute the gradientd~A\\ndx2R8\\x023.\\nWe obtain the gradient tensor by re-shaping this gradient as\\nillustrated above.\\n5.4 Gradients of MatricesWe can think of a\\ntensor as a\\nmultidimensional\\narray.We will encounter situations where we need to take gradients of matrices\\nwith respect to vectors (or other matrices), which results in a multidimen-\\nsional tensor. We can think of this tensor as a multidimensional array that\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n156 Vector Calculus\\ncollects partial derivatives. For example, if we compute the gradient of an\\nm\\x02nmatrixAwith respect to a p\\x02qmatrixB, the resulting Jacobian\\nwould be (m\\x02n)\\x02(p\\x02q), i.e., a four-dimensional tensor J, whose entries\\nare given as Jijkl=@Aij=@Bkl.\\nSince matrices represent linear mappings, we can exploit the fact that\\nthere is a vector-space isomorphism (linear, invertible mapping) between\\nthe space Rm\\x02nofm\\x02nmatrices and the space Rmnofmnvectors.\\nTherefore, we can re-shape our matrices into vectors of lengths mnand\\npq, respectively. The gradient using these mnvectors results in a Jacobian\\nof sizemn\\x02pq. Figure 5.7 visualizes both approaches. In practical ap- Matrices can be\\ntransformed into\\nvectors by stacking\\nthe columns of the\\nmatrix\\n(“ﬂattening”).plications, it is often desirable to re-shape the matrix into a vector and\\ncontinue working with this Jacobian matrix: The chain rule (5.48) boils\\ndown to simple matrix multiplication, whereas in the case of a Jacobian\\ntensor, we will need to pay more attention to what dimensions we need\\nto sum out.\\nExample 5.12 (Gradient of Vectors with Respect to Matrices)\\nLet us consider the following example, where\\nf=Ax;f2RM;A2RM\\x02N;x2RN(5.85)\\nand where we seek the gradient df=dA. Let us start again by determining\\nthe dimension of the gradient as\\ndf\\ndA2RM\\x02(M\\x02N): (5.86)\\nBy deﬁnition, the gradient is the collection of the partial derivatives:\\ndf\\ndA=2\\n64@f1\\n@A...\\n@fM\\n@A3\\n75;@fi\\n@A2R1\\x02(M\\x02N): (5.87)\\nTo compute the partial derivatives, it will be helpful to explicitly write out\\nthe matrix vector multiplication:\\nfi=NX\\nj=1Aijxj; i= 1;:::;M; (5.88)\\nand the partial derivatives are then given as\\n@fi\\n@Aiq=xq: (5.89)\\nThis allows us to compute the partial derivatives of fiwith respect to a\\nrow ofA, which is given as\\n@fi\\n@Ai;:=x>2R1\\x021\\x02N; (5.90)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.4 Gradients of Matrices 157\\n@fi\\n@Ak6=i;:=0>2R1\\x021\\x02N(5.91)\\nwhere we have to pay attention to the correct dimensionality. Since fi\\nmaps onto Rand each row of Ais of size 1\\x02N, we obtain a 1\\x021\\x02N-\\nsized tensor as the partial derivative of fiwith respect to a row of A.\\nWe stack the partial derivatives (5.91) and get the desired gradient\\nin (5.87) via\\n@fi\\n@A=2\\n666666666640>\\n...\\n0>\\nx>\\n0>\\n...\\n0>3\\n777777777752R1\\x02(M\\x02N): (5.92)\\nExample 5.13 (Gradient of Matrices with Respect to Matrices)\\nConsider a matrix R2RM\\x02Nandf:RM\\x02N!RN\\x02Nwith\\nf(R) =R>R=:K2RN\\x02N; (5.93)\\nwhere we seek the gradient dK=dR.\\nTo solve this hard problem, let us ﬁrst write down what we already\\nknow: The gradient has the dimensions\\ndK\\ndR2R(N\\x02N)\\x02(M\\x02N); (5.94)\\nwhich is a tensor. Moreover,\\ndKpq\\ndR2R1\\x02M\\x02N(5.95)\\nforp;q= 1;:::;N , whereKpqis the (p;q)th entry ofK=f(R). De-\\nnoting theith column of Rbyri, every entry of Kis given by the dot\\nproduct of two columns of R, i.e.,\\nKpq=r>\\nprq=MX\\nm=1RmpRmq: (5.96)\\nWhen we now compute the partial derivative@Kpq\\n@Rijwe obtain\\n@Kpq\\n@Rij=MX\\nm=1@\\n@RijRmpRmq=@pqij; (5.97)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n158 Vector Calculus\\n@pqij=8\\n>><\\n>>:Riq ifj=p; p6=q\\nRip ifj=q; p6=q\\n2Riqifj=p; p=q\\n0 otherwise: (5.98)\\nFrom (5.94), we know that the desired gradient has the dimension (N\\x02\\nN)\\x02(M\\x02N), and every single entry of this tensor is given by @pqij\\nin (5.98), where p;q;j = 1;:::;N andi= 1;:::;M .\\n5.5 Useful Identities for Computing Gradients\\nIn the following, we list some useful gradients that are frequently required\\nin a machine learning context (Petersen and Pedersen, 2012). Here, we\\nuse tr (\\x01)as the trace (see Deﬁnition 4.4), det(\\x01)as the determinant (see\\nSection 4.1) and f(X)\\x001as the inverse of f(X), assuming it exists.\\n@\\n@Xf(X)>=\\x12@f(X)\\n@X\\x13>\\n(5.99)\\n@\\n@Xtr(f(X)) = tr\\x12@f(X)\\n@X\\x13\\n(5.100)\\n@\\n@Xdet(f(X)) = det(f(X))tr\\x12\\nf(X)\\x001@f(X)\\n@X\\x13\\n(5.101)\\n@\\n@Xf(X)\\x001=\\x00f(X)\\x001@f(X)\\n@Xf(X)\\x001(5.102)\\n@a>X\\x001b\\n@X=\\x00(X\\x001)>ab>(X\\x001)>(5.103)\\n@x>a\\n@x=a>(5.104)\\n@a>x\\n@x=a>(5.105)\\n@a>Xb\\n@X=ab>(5.106)\\n@x>Bx\\n@x=x>(B+B>) (5.107)\\n@\\n@s(x\\x00As)>W(x\\x00As) =\\x002(x\\x00As)>WA for symmetric W\\n(5.108)\\nRemark. In this book, we only cover traces and transposes of matrices.\\nHowever, we have seen that derivatives can be higher-dimensional ten-\\nsors, in which case the usual trace and transpose are not deﬁned. In these\\ncases, the trace of a D\\x02D\\x02E\\x02Ftensor would be an E\\x02F-dimensional\\nmatrix. This is a special case of a tensor contraction. Similarly, when we\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.6 Backpropagation and Automatic Differentiation 159\\n“transpose” a tensor, we mean swapping the ﬁrst two dimensions. Specif-\\nically, in (5.99) through (5.102), we require tensor-related computations\\nwhen we work with multivariate functions f(\\x01)and compute derivatives\\nwith respect to matrices (and choose not to vectorize them as discussed in\\nSection 5.4). }\\n5.6 Backpropagation and Automatic Differentiation\\nA good discussion\\nabout\\nbackpropagation\\nand the chain rule is\\navailable at a blog\\nby Tim Vieira at\\nhttps://tinyurl.\\ncom/ycfm2yrw .In many machine learning applications, we ﬁnd good model parameters\\nby performing gradient descent (Section 7.1), which relies on the fact\\nthat we can compute the gradient of a learning objective with respect\\nto the parameters of the model. For a given objective function, we can\\nobtain the gradient with respect to the model parameters using calculus\\nand applying the chain rule; see Section 5.2.2. We already had a taste in\\nSection 5.3 when we looked at the gradient of a squared loss with respect\\nto the parameters of a linear regression model.\\nConsider the function\\nf(x) =q\\nx2+ exp(x2) + cos\\x00x2+ exp(x2)\\x01: (5.109)\\nBy application of the chain rule, and noting that differentiation is linear,\\nwe compute the gradient\\ndf\\ndx=2x+ 2xexp(x2)\\n2p\\nx2+ exp(x2)\\x00sin\\x00x2+ exp(x2)\\x01\\x002x+ 2xexp(x2)\\x01\\n= 2x \\n1\\n2p\\nx2+ exp(x2)\\x00sin\\x00x2+ exp(x2)\\x01!\\n\\x001 + exp(x2)\\x01:\\n(5.110)\\nWriting out the gradient in this explicit way is often impractical since it\\noften results in a very lengthy expression for a derivative. In practice,\\nit means that, if we are not careful, the implementation of the gradient\\ncould be signiﬁcantly more expensive than computing the function, which\\nimposes unnecessary overhead. For training deep neural network mod-\\nels, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation\\n1962; Rumelhart et al., 1986) is an efﬁcient way to compute the gradient\\nof an error function with respect to the parameters of the model.\\n5.6.1 Gradients in a Deep Network\\nAn area where the chain rule is used to an extreme is deep learning, where\\nthe function value yis computed as a many-level function composition\\ny= (fK\\x0efK\\x001\\x0e\\x01\\x01\\x01\\x0ef1)(x) =fK(fK\\x001(\\x01\\x01\\x01(f1(x))\\x01\\x01\\x01));(5.111)\\nwherexare the inputs (e.g., images), yare the observations (e.g., class\\nlabels), and every function fi,i= 1;:::;K , possesses its own parameters.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n160 Vector Calculus\\nFigure 5.8 Forward\\npass in a multi-layer\\nneural network to\\ncompute the loss L\\nas a function of the\\ninputsxand the\\nparametersAi;bi.x fK\\nA0;b0 AK\\x001;bK\\x001L fK\\x001\\nAK\\x002;bK\\x002f1\\nA1;b1\\nIn neural networks with multiple layers, we have functions fi(xi\\x001) = We discuss the case,\\nwhere the activation\\nfunctions are\\nidentical in each\\nlayer to unclutter\\nnotation.\\x1b(Ai\\x001xi\\x001+bi\\x001)in theith layer. Here xi\\x001is the output of layer i\\x001\\nand\\x1ban activation function, such as the logistic sigmoid1\\n1+e\\x00x,tanh or a\\nrectiﬁed linear unit (ReLU). In order to train these models, we require the\\ngradient of a loss function Lwith respect to all model parameters Aj;bj\\nforj= 1;:::;K . This also requires us to compute the gradient of Lwith\\nrespect to the inputs of each layer. For example, if we have inputs xand\\nobservationsyand a network structure deﬁned by\\nf0:=x (5.112)\\nfi:=\\x1bi(Ai\\x001fi\\x001+bi\\x001); i= 1;:::;K; (5.113)\\nsee also Figure 5.8 for a visualization, we may be interested in ﬁnding\\nAj;bjforj= 0;:::;K\\x001, such that the squared loss\\nL(\\x12) =ky\\x00fK(\\x12;x)k2(5.114)\\nis minimized, where \\x12=fA0;b0;:::;AK\\x001;bK\\x001g.\\nTo obtain the gradients with respect to the parameter set \\x12, we require\\nthe partial derivatives of Lwith respect to the parameters \\x12j=fAj;bjg\\nof each layer j= 0;:::;K\\x001. The chain rule allows us to determine the\\npartial derivatives as A more in-depth\\ndiscussion about\\ngradients of neural\\nnetworks can be\\nfound in Justin\\nDomke’s lecture\\nnotes\\nhttps://tinyurl.\\ncom/yalcxgtv .@L\\n@\\x12K\\x001=@L\\n@fK@fK\\n@\\x12K\\x001(5.115)\\n@L\\n@\\x12K\\x002=@L\\n@fK@fK\\n@fK\\x001@fK\\x001\\n@\\x12K\\x002(5.116)\\n@L\\n@\\x12K\\x003=@L\\n@fK@fK\\n@fK\\x001@fK\\x001\\n@fK\\x002@fK\\x002\\n@\\x12K\\x003(5.117)\\n@L\\n@\\x12i=@L\\n@fK@fK\\n@fK\\x001\\x01\\x01\\x01@fi+2\\n@fi+1@fi+1\\n@\\x12i(5.118)\\nTheorange terms are partial derivatives of the output of a layer with\\nrespect to its inputs, whereas the blue terms are partial derivatives of\\nthe output of a layer with respect to its parameters. Assuming, we have\\nalready computed the partial derivatives @L=@\\x12i+1, then most of the com-\\nputation can be reused to compute @L=@\\x12i. The additional terms that we\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.6 Backpropagation and Automatic Differentiation 161\\nFigure 5.9\\nBackward pass in a\\nmulti-layer neural\\nnetwork to compute\\nthe gradients of the\\nloss function.x fK\\nA0;b0 AK\\x001;bK\\x001L fK\\x001\\nAK\\x002;bK\\x002f1\\nA1;b1\\nFigure 5.10 Simple\\ngraph illustrating\\nthe ﬂow of data\\nfromxtoyvia\\nsome intermediate\\nvariablesa;b.xaby\\nneed to compute are indicated by the boxes. Figure 5.9 visualizes that the\\ngradients are passed backward through the network.\\n5.6.2 Automatic Differentiation\\nIt turns out that backpropagation is a special case of a general technique\\nin numerical analysis called automatic differentiation . We can think of au- automatic\\ndifferentiation tomatic differentation as a set of techniques to numerically (in contrast to\\nsymbolically) evaluate the exact (up to machine precision) gradient of a\\nfunction by working with intermediate variables and applying the chain\\nrule. Automatic differentiation applies a series of elementary arithmetic Automatic\\ndifferentiation is\\ndifferent from\\nsymbolic\\ndifferentiation and\\nnumerical\\napproximations of\\nthe gradient, e.g., by\\nusing ﬁnite\\ndifferences.operations, e.g., addition and multiplication and elementary functions,\\ne.g.,sin;cos;exp;log. By applying the chain rule to these operations, the\\ngradient of quite complicated functions can be computed automatically.\\nAutomatic differentiation applies to general computer programs and has\\nforward and reverse modes. Baydin et al. (2018) give a great overview of\\nautomatic differentiation in machine learning.\\nFigure 5.10 shows a simple graph representing the data ﬂow from in-\\nputsxto outputsyvia some intermediate variables a;b. If we were to\\ncompute the derivative dy=dx, we would apply the chain rule and obtain\\ndy\\ndx=dy\\ndbdb\\ndada\\ndx: (5.119)\\nIntuitively, the forward and reverse mode differ in the order of multipli- In the general case,\\nwe work with\\nJacobians, which\\ncan be vectors,\\nmatrices, or tensors.cation. Due to the associativity of matrix multiplication, we can choose\\nbetween\\ndy\\ndx=\\x12dy\\ndbdb\\nda\\x13da\\ndx; (5.120)\\ndy\\ndx=dy\\ndb\\x12db\\ndada\\ndx\\x13\\n: (5.121)\\nEquation (5.120) would be the reverse mode because gradients are prop- reverse mode\\nagated backward through the graph, i.e., reverse to the data ﬂow. Equa-\\ntion (5.121) would be the forward mode , where the gradients ﬂow with forward mode\\nthe data from left to right through the graph.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n162 Vector Calculus\\nIn the following, we will focus on reverse mode automatic differentia-\\ntion, which is backpropagation. In the context of neural networks, where\\nthe input dimensionality is often much higher than the dimensionality of\\nthe labels, the reverse mode is computationally signiﬁcantly cheaper than\\nthe forward mode. Let us start with an instructive example.\\nExample 5.14\\nConsider the function\\nf(x) =q\\nx2+ exp(x2) + cos\\x00x2+ exp(x2)\\x01\\n(5.122)\\nfrom (5.109). If we were to implement a function fon a computer, we\\nwould be able to save some computation by using intermediate variables : intermediate\\nvariables\\na=x2; (5.123)\\nb= exp(a); (5.124)\\nc=a+b; (5.125)\\nd=pc; (5.126)\\ne= cos(c); (5.127)\\nf=d+e: (5.128)\\nFigure 5.11\\nComputation graph\\nwith inputsx,\\nfunction values f,\\nand intermediate\\nvariablesa;b;c;d;e .x (\\x01)2aexp(\\x01)b\\n+cp\\x01\\ncos(\\x01)d\\ne+f\\nThis is the same kind of thinking process that occurs when applying\\nthe chain rule. Note that the preceding set of equations requires fewer\\noperations than a direct implementation of the function f(x)as deﬁned\\nin (5.109). The corresponding computation graph in Figure 5.11 shows\\nthe ﬂow of data and computations required to obtain the function value\\nf.\\nThe set of equations that include intermediate variables can be thought\\nof as a computation graph, a representation that is widely used in imple-\\nmentations of neural network software libraries. We can directly compute\\nthe derivatives of the intermediate variables with respect to their corre-\\nsponding inputs by recalling the deﬁnition of the derivative of elementary\\nfunctions. We obtain the following:\\n@a\\n@x= 2x (5.129)\\n@b\\n@a= exp(a) (5.130)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.6 Backpropagation and Automatic Differentiation 163\\n@c\\n@a= 1 =@c\\n@b(5.131)\\n@d\\n@c=1\\n2pc(5.132)\\n@e\\n@c=\\x00sin(c) (5.133)\\n@f\\n@d= 1 =@f\\n@e: (5.134)\\nBy looking at the computation graph in Figure 5.11, we can compute\\n@f=@x by working backward from the output and obtain\\n@f\\n@c=@f\\n@d@d\\n@c+@f\\n@e@e\\n@c(5.135)\\n@f\\n@b=@f\\n@c@c\\n@b(5.136)\\n@f\\n@a=@f\\n@b@b\\n@a+@f\\n@c@c\\n@a(5.137)\\n@f\\n@x=@f\\n@a@a\\n@x: (5.138)\\nNote that we implicitly applied the chain rule to obtain @f=@x . By substi-\\ntuting the results of the derivatives of the elementary functions, we get\\n@f\\n@c= 1\\x011\\n2pc+ 1\\x01(\\x00sin(c)) (5.139)\\n@f\\n@b=@f\\n@c\\x011 (5.140)\\n@f\\n@a=@f\\n@bexp(a) +@f\\n@c\\x011 (5.141)\\n@f\\n@x=@f\\n@a\\x012x: (5.142)\\nBy thinking of each of the derivatives above as a variable, we observe\\nthat the computation required for calculating the derivative is of similar\\ncomplexity as the computation of the function itself. This is quite counter-\\nintuitive since the mathematical expression for the derivative@f\\n@x(5.110)\\nis signiﬁcantly more complicated than the mathematical expression of the\\nfunctionf(x)in (5.109).\\nAutomatic differentiation is a formalization of Example 5.14. Let x1;:::;xd\\nbe the input variables to the function, xd+1;:::;xD\\x001be the intermediate\\nvariables, and xDthe output variable. Then the computation graph can be\\nexpressed as follows:\\nFori=d+ 1;:::;D :xi=gi(xPa(xi)); (5.143)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n164 Vector Calculus\\nwhere thegi(\\x01)are elementary functions and xPa(xi)are the parent nodes\\nof the variable xiin the graph. Given a function deﬁned in this way, we\\ncan use the chain rule to compute the derivative of the function in a step-\\nby-step fashion. Recall that by deﬁnition f=xDand hence\\n@f\\n@xD= 1: (5.144)\\nFor other variables xi, we apply the chain rule\\n@f\\n@xi=X\\nxj:xi2Pa(xj)@f\\n@xj@xj\\n@xi=X\\nxj:xi2Pa(xj)@f\\n@xj@gj\\n@xi; (5.145)\\nwhere Pa(xj)is the set of parent nodes of xjin the computation graph.\\nEquation (5.143) is the forward propagation of a function, whereas (5.145) Auto-differentiation\\nin reverse mode\\nrequires a parse\\ntree.is the backpropagation of the gradient through the computation graph.\\nFor neural network training, we backpropagate the error of the prediction\\nwith respect to the label.\\nThe automatic differentiation approach above works whenever we have\\na function that can be expressed as a computation graph, where the ele-\\nmentary functions are differentiable. In fact, the function may not even be\\na mathematical function but a computer program. However, not all com-\\nputer programs can be automatically differentiated, e.g., if we cannot ﬁnd\\ndifferential elementary functions. Programming structures, such as for\\nloops and ifstatements, require more care as well.\\n5.7 Higher-Order Derivatives\\nSo far, we have discussed gradients, i.e., ﬁrst-order derivatives. Some-\\ntimes, we are interested in derivatives of higher order, e.g., when we want\\nto use Newton’s Method for optimization, which requires second-order\\nderivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed\\nthe Taylor series to approximate functions using polynomials. In the mul-\\ntivariate case, we can do exactly the same. In the following, we will do\\nexactly this. But let us start with some notation.\\nConsider a function f:R2!Rof two variables x;y. We use the\\nfollowing notation for higher-order partial derivatives (and for gradients):\\n@2f\\n@x2is the second partial derivative of fwith respect to x.\\n@nf\\n@xnis thenth partial derivative of fwith respect to x.\\n@2f\\n@y@x=@\\n@y\\x00@f\\n@x\\x01\\nis the partial derivative obtained by ﬁrst partial differ-\\nentiating with respect to xand then with respect to y.\\n@2f\\n@x@yis the partial derivative obtained by ﬁrst partial differentiating by\\nyand thenx.\\nTheHessian is the collection of all second-order partial derivatives. Hessian\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.8 Linearization and Multivariate Taylor Series 165\\nFigure 5.12 Linear\\napproximation of a\\nfunction. The\\noriginal function f\\nis linearized at\\nx0=\\x002using a\\nﬁrst-order Taylor\\nseries expansion.\\n−4−2 0 2 4\\nx−2−101f(x)f(x)\\nf(x0)f(x0) +f/prime(x0)(x−x0)\\nIff(x;y)is a twice (continuously) differentiable function, then\\n@2f\\n@x@y=@2f\\n@y@x; (5.146)\\ni.e., the order of differentiation does not matter, and the corresponding\\nHessian matrix Hessian matrix\\nH=2\\n664@2f\\n@x2@2f\\n@x@y\\n@2f\\n@x@y@2f\\n@y23\\n775(5.147)\\nis symmetric. The Hessian is denoted as r2\\nx;yf(x;y). Generally, for x2Rn\\nandf:Rn!R, the Hessian is an n\\x02nmatrix. The Hessian measures\\nthe curvature of the function locally around (x;y).\\nRemark (Hessian of a Vector Field) .Iff:Rn!Rmis a vector ﬁeld, the\\nHessian is an (m\\x02n\\x02n)-tensor. }\\n5.8 Linearization and Multivariate Taylor Series\\nThe gradientrfof a function fis often used for a locally linear approxi-\\nmation offaroundx0:\\nf(x)\\x19f(x0) + (rxf)(x0)(x\\x00x0): (5.148)\\nHere (rxf)(x0)is the gradient of fwith respect to x, evaluated at x0.\\nFigure 5.12 illustrates the linear approximation of a function fat an input\\nx0. The original function is approximated by a straight line. This approx-\\nimation is locally accurate, but the farther we move away from x0the\\nworse the approximation gets. Equation (5.148) is a special case of a mul-\\ntivariate Taylor series expansion of fatx0, where we consider only the\\nﬁrst two terms. We discuss the more general case in the following, which\\nwill allow for better approximations.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n166 Vector Calculus\\nFigure 5.13\\nVisualizing outer\\nproducts. Outer\\nproducts of vectors\\nincrease the\\ndimensionality of\\nthe array by 1 per\\nterm. (a) The outer\\nproduct of two\\nvectors results in a\\nmatrix; (b) the\\nouter product of\\nthree vectors yields\\na third-order tensor.\\n(a) Given a vector \\x0e2R4, we obtain the outer product \\x0e2:=\\x0e\\n\\x0e=\\x0e\\x0e>2\\nR4\\x024as a matrix.\\n(b) An outer product \\x0e3:=\\x0e\\n\\x0e\\n\\x0e2R4\\x024\\x024results in a third-order tensor (“three-\\ndimensional matrix”), i.e., an array with three indexes.\\nDeﬁnition 5.7 (Multivariate Taylor Series) .We consider a function\\nf:RD!R (5.149)\\nx7!f(x);x2RD; (5.150)\\nthat is smooth at x0. When we deﬁne the difference vector \\x0e:=x\\x00x0,\\nthemultivariate Taylor series offat(x0)is deﬁned as multivariate Taylor\\nseries\\nf(x) =1X\\nk=0Dk\\nxf(x0)\\nk!\\x0ek; (5.151)\\nwhereDk\\nxf(x0)is thek-th (total) derivative of fwith respect to x, eval-\\nuated atx0.\\nDeﬁnition 5.8 (Taylor Polynomial) .TheTaylor polynomial of degreenof Taylor polynomial\\nfatx0contains the ﬁrst n+ 1components of the series in (5.151) and is\\ndeﬁned as\\nTn(x) =nX\\nk=0Dk\\nxf(x0)\\nk!\\x0ek: (5.152)\\nIn (5.151) and (5.152), we used the slightly sloppy notation of \\x0ek,\\nwhich is not deﬁned for vectors x2RD; D > 1;andk > 1. Note that\\nbothDk\\nxfand\\x0ekarek-th order tensors, i.e., k-dimensional arrays. The A vector can be\\nimplemented as a\\none-dimensional\\narray, a matrix as a\\ntwo-dimensional\\narray.kth-order tensor \\x0ek2Rktimesz}|{\\nD\\x02D\\x02:::\\x02Dis obtained as a k-fold outer product,\\ndenoted by\\n, of the vector \\x0e2RD. For example,\\n\\x0e2:=\\x0e\\n\\x0e=\\x0e\\x0e>;\\x0e2[i;j] =\\x0e[i]\\x0e[j] (5.153)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.8 Linearization and Multivariate Taylor Series 167\\n\\x0e3:=\\x0e\\n\\x0e\\n\\x0e;\\x0e3[i;j;k ] =\\x0e[i]\\x0e[j]\\x0e[k]: (5.154)\\nFigure 5.13 visualizes two such outer products. In general, we obtain the\\nterms\\nDk\\nxf(x0)\\x0ek=DX\\ni1=1\\x01\\x01\\x01DX\\nik=1Dk\\nxf(x0)[i1;:::;ik]\\x0e[i1]\\x01\\x01\\x01\\x0e[ik](5.155)\\nin the Taylor series, where Dk\\nxf(x0)\\x0ekcontainsk-th order polynomials.\\nNow that we deﬁned the Taylor series for vector ﬁelds, let us explicitly\\nwrite down the ﬁrst terms Dk\\nxf(x0)\\x0ekof the Taylor series expansion for\\nk= 0;:::; 3and\\x0e:=x\\x00x0:np.einsum(\\n\\'i,i\\',Df1,d)\\nnp.einsum(\\n\\'ij,i,j\\',\\nDf2,d,d)\\nnp.einsum(\\n\\'ijk,i,j,k\\',\\nDf3,d,d,d)k= 0 :D0\\nxf(x0)\\x0e0=f(x0)2R (5.156)\\nk= 1 :D1\\nxf(x0)\\x0e1=rxf(x0)|{z}\\n1\\x02D\\x0e|{z}\\nD\\x021=DX\\ni=1rxf(x0)[i]\\x0e[i]2R(5.157)\\nk= 2 :D2\\nxf(x0)\\x0e2=tr\\x00H(x0)|{z}\\nD\\x02D\\x0e|{z}\\nD\\x021\\x0e>\\n|{z}\\n1\\x02D\\x01=\\x0e>H(x0)\\x0e (5.158)\\n=DX\\ni=1DX\\nj=1H[i;j]\\x0e[i]\\x0e[j]2R (5.159)\\nk= 3 :D3\\nxf(x0)\\x0e3=DX\\ni=1DX\\nj=1DX\\nk=1D3\\nxf(x0)[i;j;k ]\\x0e[i]\\x0e[j]\\x0e[k]2R\\n(5.160)\\nHere,H(x0)is the Hessian of fevaluated atx0.\\nExample 5.15 (Taylor Series Expansion of a Function with Two Vari-\\nables)\\nConsider the function\\nf(x;y) =x2+ 2xy+y3: (5.161)\\nWe want to compute the Taylor series expansion of fat(x0;y0) = (1;2).\\nBefore we start, let us discuss what to expect: The function in (5.161) is\\na polynomial of degree 3. We are looking for a Taylor series expansion,\\nwhich itself is a linear combination of polynomials. Therefore, we do not\\nexpect the Taylor series expansion to contain terms of fourth or higher\\norder to express a third-order polynomial. This means that it should be\\nsufﬁcient to determine the ﬁrst four terms of (5.151) for an exact alterna-\\ntive representation of (5.161).\\nTo determine the Taylor series expansion, we start with the constant\\nterm and the ﬁrst-order derivatives, which are given by\\nf(1;2) = 13 (5.162)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n168 Vector Calculus\\n@f\\n@x= 2x+ 2y=)@f\\n@x(1;2) = 6 (5.163)\\n@f\\n@y= 2x+ 3y2=)@f\\n@y(1;2) = 14: (5.164)\\nTherefore, we obtain\\nD1\\nx;yf(1;2) =rx;yf(1;2) =h\\n@f\\n@x(1;2)@f\\n@y(1;2)i\\n=\\x026 14\\x032R1\\x022\\n(5.165)\\nsuch that\\nD1\\nx;yf(1;2)\\n1!\\x0e=\\x026 14\\x03\\x14x\\x001\\ny\\x002\\x15\\n= 6(x\\x001) + 14(y\\x002):(5.166)\\nNote thatD1\\nx;yf(1;2)\\x0econtains only linear terms, i.e., ﬁrst-order polyno-\\nmials.\\nThe second-order partial derivatives are given by\\n@2f\\n@x2= 2 =)@2f\\n@x2(1;2) = 2 (5.167)\\n@2f\\n@y2= 6y=)@2f\\n@y2(1;2) = 12 (5.168)\\n@2f\\n@y@x= 2 =)@2f\\n@y@x(1;2) = 2 (5.169)\\n@2f\\n@x@y= 2 =)@2f\\n@x@y(1;2) = 2: (5.170)\\nWhen we collect the second-order partial derivatives, we obtain the Hes-\\nsian\\nH=\"@2f\\n@x2@2f\\n@x@y\\n@2f\\n@y@x@2f\\n@y2#\\n=\\x142 2\\n2 6y\\x15\\n; (5.171)\\nsuch that\\nH(1;2) =\\x142 2\\n2 12\\x15\\n2R2\\x022: (5.172)\\nTherefore, the next term of the Taylor-series expansion is given by\\nD2\\nx;yf(1;2)\\n2!\\x0e2=1\\n2\\x0e>H(1;2)\\x0e (5.173a)\\n=1\\n2\\x02x\\x001y\\x002\\x03\\x142 2\\n2 12\\x15\\x14x\\x001\\ny\\x002\\x15\\n(5.173b)\\n= (x\\x001)2+ 2(x\\x001)(y\\x002) + 6(y\\x002)2:(5.173c)\\nHere,D2\\nx;yf(1;2)\\x0e2contains only quadratic terms, i.e., second-order poly-\\nnomials.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n5.8 Linearization and Multivariate Taylor Series 169\\nThe third-order derivatives are obtained as\\nD3\\nx;yf=h\\n@H\\n@x@H\\n@yi\\n2R2\\x022\\x022; (5.174)\\nD3\\nx;yf[:;:;1] =@H\\n@x=\"@3f\\n@x3@3f\\n@x2@y\\n@3f\\n@x@y@x@3f\\n@x@y2#\\n; (5.175)\\nD3\\nx;yf[:;:;2] =@H\\n@y=\"@3f\\n@y@x2@3f\\n@y@x@y\\n@3f\\n@y2@x@3f\\n@y3#\\n: (5.176)\\nSince most second-order partial derivatives in the Hessian in (5.171) are\\nconstant, the only nonzero third-order partial derivative is\\n@3f\\n@y3= 6 =)@3f\\n@y3(1;2) = 6: (5.177)\\nHigher-order derivatives and the mixed derivatives of degree 3 (e.g.,\\n@f3\\n@x2@y) vanish, such that\\nD3\\nx;yf[:;:;1] =\\x140 0\\n0 0\\x15\\n; D3\\nx;yf[:;:;2] =\\x140 0\\n0 6\\x15\\n(5.178)\\nand\\nD3\\nx;yf(1;2)\\n3!\\x0e3= (y\\x002)3; (5.179)\\nwhich collects all cubic terms of the Taylor series. Overall, the (exact)\\nTaylor series expansion of fat(x0;y0) = (1;2)is\\nf(x) =f(1;2) +D1\\nx;yf(1;2)\\x0e+D2\\nx;yf(1;2)\\n2!\\x0e2+D3\\nx;yf(1;2)\\n3!\\x0e3\\n(5.180a)\\n=f(1;2) +@f(1;2)\\n@x(x\\x001) +@f(1;2)\\n@y(y\\x002)\\n+1\\n2!\\x12@2f(1;2)\\n@x2(x\\x001)2+@2f(1;2)\\n@y2(y\\x002)2\\n+ 2@2f(1;2)\\n@x@y(x\\x001)(y\\x002)\\x13\\n+1\\n6@3f(1;2)\\n@y3(y\\x002)3(5.180b)\\n= 13 + 6(x\\x001) + 14(y\\x002)\\n+ (x\\x001)2+ 6(y\\x002)2+ 2(x\\x001)(y\\x002) + (y\\x002)3:(5.180c)\\nIn this case, we obtained an exact Taylor series expansion of the polyno-\\nmial in (5.161), i.e., the polynomial in (5.180c) is identical to the original\\npolynomial in (5.161). In this particular example, this result is not sur-\\nprising since the original function was a third-order polynomial, which\\nwe expressed through a linear combination of constant terms, ﬁrst-order,\\nsecond-order, and third-order polynomials in (5.180c).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n170 Vector Calculus\\n5.9 Further Reading\\nFurther details of matrix differentials, along with a short review of the\\nrequired linear algebra, can be found in Magnus and Neudecker (2007).\\nAutomatic differentiation has had a long history, and we refer to Griewank\\nand Walther (2003), Griewank and Walther (2008), and Elliott (2009)\\nand the references therein.\\nIn machine learning (and other disciplines), we often need to compute\\nexpectations, i.e., we need to solve integrals of the form\\nEx[f(x)] =Z\\nf(x)p(x)dx: (5.181)\\nEven ifp(x)is in a convenient form (e.g., Gaussian), this integral gen-\\nerally cannot be solved analytically. The Taylor series expansion of fis\\none way of ﬁnding an approximate solution: Assuming p(x) =N\\x00\\x16;\\x06\\x01\\nis Gaussian, then the ﬁrst-order Taylor series expansion around \\x16locally\\nlinearizes the nonlinear function f. For linear functions, we can compute\\nthe mean (and the covariance) exactly if p(x)is Gaussian distributed (see\\nSection 6.5). This property is heavily exploited by the extended Kalman extended Kalman\\nﬁlter ﬁlter (Maybeck, 1979) for online state estimation in nonlinear dynami-\\ncal systems (also called “state-space models”). Other deterministic ways\\nto approximate the integral in (5.181) are the unscented transform (Julier unscented transform\\nand Uhlmann, 1997), which does not require any gradients, or the Laplace Laplace\\napproximation approximation (MacKay, 2003; Bishop, 2006; Murphy, 2012), which uses\\na second-order Taylor series expansion (requiring the Hessian) for a local\\nGaussian approximation of p(x)around its mode.\\nExercises\\n5.1 Compute the derivative f0(x)for\\nf(x) = log(x4) sin(x3):\\n5.2 Compute the derivative f0(x)of the logistic sigmoid\\nf(x) =1\\n1 + exp(\\x00x):\\n5.3 Compute the derivative f0(x)of the function\\nf(x) = exp(\\x001\\n2\\x1b2(x\\x00\\x16)2);\\nwhere\\x16; \\x1b2Rare constants.\\n5.4 Compute the Taylor polynomials Tn,n= 0;:::; 5off(x) = sin(x) + cos(x)\\natx0= 0.\\n5.5 Consider the following functions:\\nf1(x) = sin(x1) cos(x2);x2R2\\nf2(x;y) =x>y;x;y2Rn\\nf3(x) =xx>;x2Rn\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 171\\na. What are the dimensions of@fi\\n@x?\\nb. Compute the Jacobians.\\n5.6 Differentiate fwith respect to tandgwith respect to X, where\\nf(t) = sin(log(t>t)); t2RD\\ng(X) =tr(AXB );A2RD\\x02E;X2RE\\x02F;B2RF\\x02D;\\nwhere tr (\\x01)denotes the trace.\\n5.7 Compute the derivatives df=dxof the following functions by using the chain\\nrule. Provide the dimensions of every single partial derivative. Describe your\\nsteps in detail.\\na.\\nf(z) = log(1 + z); z =x>x;x2RD\\nb.\\nf(z) = sin(z);z=Ax+b;A2RE\\x02D;x2RD;b2RE\\nwhere sin(\\x01)is applied to every element of z.\\n5.8 Compute the derivatives df=dxof the following functions. Describe your\\nsteps in detail.\\na. Use the chain rule. Provide the dimensions of every single partial deriva-\\ntive.\\nf(z) = exp(\\x001\\n2z)\\nz=g(y) =y>S\\x001y\\ny=h(x) =x\\x00\\x16\\nwherex;\\x162RD,S2RD\\x02D.\\nb.\\nf(x) =tr(xx>+\\x1b2I);x2RD\\nHere tr (A)is the trace of A, i.e., the sum of the diagonal elements Aii.\\nHint: Explicitly write out the outer product.\\nc. Use the chain rule. Provide the dimensions of every single partial deriva-\\ntive. You do not need to compute the product of the partial derivatives\\nexplicitly.\\nf= tanh(z)2RM\\nz=Ax+b;x2RN;A2RM\\x02N;b2RM:\\nHere, tanh is applied to every component of z.\\n5.9 We deﬁne\\ng(z;\\x17) := logp(x;z)\\x00logq(z;\\x17)\\nz:=t(\\x0f;\\x17)\\nfor differentiable functions p;q;t , andx2RD;z2RE;\\x172RF;\\x0f2RG. By\\nusing the chain rule, compute the gradient\\nd\\nd\\x17g(z;\\x17):\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n6\\nProbability and Distributions\\nProbability, loosely speaking, concerns the study of uncertainty. Probabil-\\nity can be thought of as the fraction of times an event occurs, or as a degree\\nof belief about an event. We then would like to use this probability to mea-\\nsure the chance of something occurring in an experiment. As mentioned\\nin Chapter 1, we often quantify uncertainty in the data, uncertainty in the\\nmachine learning model, and uncertainty in the predictions produced by\\nthe model. Quantifying uncertainty requires the idea of a random variable , random variable\\nwhich is a function that maps outcomes of random experiments to a set of\\nproperties that we are interested in. Associated with the random variable\\nis a function that measures the probability that a particular outcome (or\\nset of outcomes) will occur; this is called the probability distribution . probability\\ndistribution Probability distributions are used as a building block for other con-\\ncepts, such as probabilistic modeling (Section 8.4), graphical models (Sec-\\ntion 8.5), and model selection (Section 8.6). In the next section, we present\\nthe three concepts that deﬁne a probability space (the sample space, the\\nevents, and the probability of an event) and how they are related to a\\nfourth concept called the random variable. The presentation is deliber-\\nately slightly hand wavy since a rigorous presentation may occlude the\\nintuition behind the concepts. An outline of the concepts presented in this\\nchapter are shown in Figure 6.1.\\n6.1 Construction of a Probability Space\\nThe theory of probability aims at deﬁning a mathematical structure to\\ndescribe random outcomes of experiments. For example, when tossing a\\nsingle coin, we cannot determine the outcome, but by doing a large num-\\nber of coin tosses, we can observe a regularity in the average outcome.\\nUsing this mathematical structure of probability, the goal is to perform\\nautomated reasoning, and in this sense, probability generalizes logical\\nreasoning (Jaynes, 2003).\\n6.1.1 Philosophical Issues\\nWhen constructing automated reasoning systems, classical Boolean logic\\ndoes not allow us to express certain forms of plausible reasoning. Consider\\n172\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n6.1 Construction of a Probability Space 173\\nFigure 6.1 A mind\\nmap of the concepts\\nrelated to random\\nvariables and\\nprobability\\ndistributions, as\\ndescribed in this\\nchapter.\\nRandom variable\\n& distributionSum rule Product ruleBayes’ Theorem\\nSummary statisticsMean Variance\\nTransformations\\nIndependence\\nInner productGaussian\\nBernoulli\\nBetaSufﬁcient statistics\\nExponential familyChapter 9\\nRegression\\nChapter 10\\nDimensionality\\nreduction\\nChapter 11\\nDensity estimationProperty\\nSimilarityExample\\nExampleConjugate\\nProperty Finite\\nthe following scenario: We observe that Ais false. We ﬁnd Bbecomes\\nless plausible, although no conclusion can be drawn from classical logic.\\nWe observe that Bis true. It seems Abecomes more plausible. We use\\nthis form of reasoning daily. We are waiting for a friend, and consider\\nthree possibilities: H1, she is on time; H2, she has been delayed by trafﬁc;\\nand H3, she has been abducted by aliens. When we observe our friend\\nis late, we must logically rule out H1. We also tend to consider H2 to be\\nmore likely, though we are not logically required to do so. Finally, we may\\nconsider H3 to be possible, but we continue to consider it quite unlikely.\\nHow do we conclude H2 is the most plausible answer? Seen in this way, “For plausible\\nreasoning it is\\nnecessary to extend\\nthe discrete true and\\nfalse values of truth\\nto continuous\\nplausibilities”\\n(Jaynes, 2003).probability theory can be considered a generalization of Boolean logic. In\\nthe context of machine learning, it is often applied in this way to formalize\\nthe design of automated reasoning systems. Further arguments about how\\nprobability theory is the foundation of reasoning systems can be found\\nin Pearl (1988).\\nThe philosophical basis of probability and how it should be somehow\\nrelated to what we think should be true (in the logical sense) was studied\\nby Cox (Jaynes, 2003). Another way to think about it is that if we are\\nprecise about our common sense we end up constructing probabilities.\\nE. T. Jaynes (1922–1998) identiﬁed three mathematical criteria, which\\nmust apply to all plausibilities:\\n1. The degrees of plausibility are represented by real numbers.\\n2. These numbers must be based on the rules of common sense.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n174 Probability and Distributions\\n3. The resulting reasoning must be consistent, with the three following\\nmeanings of the word “consistent”:\\n(a) Consistency or non-contradiction: When the same result can be\\nreached through different means, the same plausibility value must\\nbe found in all cases.\\n(b) Honesty: All available data must be taken into account.\\n(c) Reproducibility: If our state of knowledge about two problems are\\nthe same, then we must assign the same degree of plausibility to\\nboth of them.\\nThe Cox–Jaynes theorem proves these plausibilities to be sufﬁcient to\\ndeﬁne the universal mathematical rules that apply to plausibility p, up to\\ntransformation by an arbitrary monotonic function. Crucially, these rules\\narethe rules of probability.\\nRemark. In machine learning and statistics, there are two major interpre-\\ntations of probability: the Bayesian and frequentist interpretations (Bishop,\\n2006; Efron and Hastie, 2016). The Bayesian interpretation uses probabil-\\nity to specify the degree of uncertainty that the user has about an event. It\\nis sometimes referred to as “subjective probability” or “degree of belief”.\\nThe frequentist interpretation considers the relative frequencies of events\\nof interest to the total number of events that occurred. The probability of\\nan event is deﬁned as the relative frequency of the event in the limit when\\none has inﬁnite data. }\\nSome machine learning texts on probabilistic models use lazy notation\\nand jargon, which is confusing. This text is no exception. Multiple distinct\\nconcepts are all referred to as “probability distribution”, and the reader\\nhas to often disentangle the meaning from the context. One trick to help\\nmake sense of probability distributions is to check whether we are trying\\nto model something categorical (a discrete random variable) or some-\\nthing continuous (a continuous random variable). The kinds of questions\\nwe tackle in machine learning are closely related to whether we are con-\\nsidering categorical or continuous models.\\n6.1.2 Probability and Random Variables\\nThere are three distinct ideas that are often confused when discussing\\nprobabilities. First is the idea of a probability space, which allows us to\\nquantify the idea of a probability. However, we mostly do not work directly\\nwith this basic probability space. Instead, we work with random variables\\n(the second idea), which transfers the probability to a more convenient\\n(often numerical) space. The third idea is the idea of a distribution or law\\nassociated with a random variable. We will introduce the ﬁrst two ideas\\nin this section and expand on the third idea in Section 6.2.\\nModern probability is based on a set of axioms proposed by Kolmogorov\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.1 Construction of a Probability Space 175\\n(Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con-\\ncepts of sample space, event space, and probability measure. The prob-\\nability space models a real-world process (referred to as an experiment)\\nwith random outcomes.\\nThe sample space \\nThesample space is the set of all possible outcomes of the experiment, sample space\\nusually denoted by \\n. For example, two successive coin tosses have\\na sample space of fhh, tt, ht, thg, where “h” denotes “heads” and “t”\\ndenotes “tails”.\\nThe event spaceA\\nTheevent space is the space of potential results of the experiment. A event space\\nsubsetAof the sample space \\nis in the event space Aif at the end\\nof the experiment we can observe whether a particular outcome !2\\nis inA. The event space Ais obtained by considering the collection of\\nsubsets of \\n, and for discrete probability distributions (Section 6.2.1)\\nAis often the power set of \\n.\\nThe probability P\\nWith each event A2A, we associate a number P(A)that measures the\\nprobability or degree of belief that the event will occur. P(A)is called\\ntheprobability ofA. probability\\nThe probability of a single event must lie in the interval [0;1], and the\\ntotal probability over all outcomes in the sample space \\nmust be 1, i.e.,\\nP(\\n) = 1 . Given a probability space (\\n;A;P), we want to use it to model\\nsome real-world phenomenon. In machine learning, we often avoid explic-\\nitly referring to the probability space, but instead refer to probabilities on\\nquantities of interest, which we denote by T. In this book, we refer to T\\nas the target space and refer to elements of Tas states. We introduce a target space\\nfunctionX: \\n!T that takes an element of \\n(an outcome) and returns\\na particular quantity of interest x, a value inT. This association/mapping\\nfrom \\ntoTis called a random variable . For example, in the case of tossing random variable\\ntwo coins and counting the number of heads, a random variable Xmaps\\nto the three possible outcomes: X(hh) = 2 ,X(ht) = 1 ,X(th) = 1 , and\\nX(tt) = 0 . In this particular case, T=f0;1;2g, and it is the probabilities\\non elements ofTthat we are interested in. For a ﬁnite sample space \\nand The name “random\\nvariable” is a great\\nsource of\\nmisunderstanding\\nas it is neither\\nrandom nor is it a\\nvariable. It is a\\nfunction.ﬁniteT, the function corresponding to a random variable is essentially a\\nlookup table. For any subset S\\x12T , we associate PX(S)2[0;1](the\\nprobability) to a particular event occurring corresponding to the random\\nvariableX. Example 6.1 provides a concrete illustration of the terminol-\\nogy.\\nRemark. The aforementioned sample space \\nunfortunately is referred\\nto by different names in different books. Another common name for \\nis “state space” (Jacod and Protter, 2004), but state space is sometimes\\nreserved for referring to states in a dynamical system (Hasselblatt and\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n176 Probability and Distributions\\nKatok, 2003). Other names sometimes used to describe \\nare: “sample\\ndescription space”, “possibility space,” and “event space”. }\\nExample 6.1\\nWe assume that the reader is already familiar with computing probabilities This toy example is\\nessentially a biased\\ncoin ﬂip example.of intersections and unions of sets of events. A gentler introduction to\\nprobability with many examples can be found in chapter 2 of Walpole\\net al. (2011).\\nConsider a statistical experiment where we model a funfair game con-\\nsisting of drawing two coins from a bag (with replacement). There are\\ncoins from USA (denoted as $) and UK (denoted as £) in the bag, and\\nsince we draw two coins from the bag, there are four outcomes in total.\\nThe state space or sample space \\nof this experiment is then ($, $), ($,\\n£), (£, $), (£, £). Let us assume that the composition of the bag of coins is\\nsuch that a draw returns at random a $ with probability 0:3.\\nThe event we are interested in is the total number of times the repeated\\ndraw returns $. Let us deﬁne a random variable Xthat maps the sample\\nspace \\ntoT, which denotes the number of times we draw $ out of the\\nbag. We can see from the preceding sample space we can get zero $, one $,\\nor two $s, and therefore T=f0;1;2g. The random variable X(a function\\nor lookup table) can be represented as a table like the following:\\nX(($;$)) = 2 (6.1)\\nX(($;$)) = 1 (6.2)\\nX(($;$)) = 1 (6.3)\\nX(($;$)) = 0: (6.4)\\nSince we return the ﬁrst coin we draw before drawing the second, this\\nimplies that the two draws are independent of each other, which we will\\ndiscuss in Section 6.4.5. Note that there are two experimental outcomes,\\nwhich map to the same event, where only one of the draws returns $.\\nTherefore, the probability mass function (Section 6.2.1) of Xis given by\\nP(X= 2) =P(($;$))\\n=P($)\\x01P($)\\n= 0:3\\x010:3 = 0:09 (6.5)\\nP(X= 1) =P(($;$)[($;$))\\n=P(($;$)) +P(($;$))\\n= 0:3\\x01(1\\x000:3) + (1\\x000:3)\\x010:3 = 0:42 (6.6)\\nP(X= 0) =P(($;$))\\n=P($)\\x01P($)\\n= (1\\x000:3)\\x01(1\\x000:3) = 0:49: (6.7)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.1 Construction of a Probability Space 177\\nIn the calculation, we equated two different concepts, the probability\\nof the output of Xand the probability of the samples in \\n. For example,\\nin (6.7) we say P(X= 0) =P(($;$)). Consider the random variable\\nX: \\n!T and a subset S\\x12T (for example, a single element of T,\\nsuch as the outcome that one head is obtained when tossing two coins).\\nLetX\\x001(S)be the pre-image of SbyX, i.e., the set of elements of \\nthat\\nmap toSunderX;f!2\\n :X(!)2Sg. One way to understand the\\ntransformation of probability from events in \\nvia the random variable\\nXis to associate it with the probability of the pre-image of S(Jacod and\\nProtter, 2004). For S\\x12T, we have the notation\\nPX(S) =P(X2S) =P(X\\x001(S)) =P(f!2\\n :X(!)2Sg):(6.8)\\nThe left-hand side of (6.8) is the probability of the set of possible outcomes\\n(e.g., number of $= 1) that we are interested in. Via the random variable\\nX, which maps states to outcomes, we see in the right-hand side of (6.8)\\nthat this is the probability of the set of states (in \\n) that have the property\\n(e.g., $$,$$). We say that a random variable Xis distributed according\\nto a particular probability distribution PX, which deﬁnes the probability\\nmapping between the event and the probability of the outcome of the\\nrandom variable. In other words, the function PXor equivalently P\\x0eX\\x001\\nis the lawordistribution of random variable X. law\\ndistributionRemark. The target space, that is, the range Tof the random variable X,\\nis used to indicate the kind of probability space, i.e., a Trandom variable.\\nWhenTis ﬁnite or countably inﬁnite, this is called a discrete random\\nvariable (Section 6.2.1). For continuous random variables (Section 6.2.2),\\nwe only considerT=RorT=RD. }\\n6.1.3 Statistics\\nProbability theory and statistics are often presented together, but they con-\\ncern different aspects of uncertainty. One way of contrasting them is by the\\nkinds of problems that are considered. Using probability, we can consider\\na model of some process, where the underlying uncertainty is captured\\nby random variables, and we use the rules of probability to derive what\\nhappens. In statistics, we observe that something has happened and try\\nto ﬁgure out the underlying process that explains the observations. In this\\nsense, machine learning is close to statistics in its goals to construct a\\nmodel that adequately represents the process that generated the data. We\\ncan use the rules of probability to obtain a “best-ﬁtting” model for some\\ndata.\\nAnother aspect of machine learning systems is that we are interested\\nin generalization error (see Chapter 8). This means that we are actually\\ninterested in the performance of our system on instances that we will\\nobserve in future, which are not identical to the instances that we have\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n178 Probability and Distributions\\nseen so far. This analysis of future performance relies on probability and\\nstatistics, most of which is beyond what will be presented in this chapter.\\nThe interested reader is encouraged to look at the books by Boucheron\\net al. (2013) and Shalev-Shwartz and Ben-David (2014). We will see more\\nabout statistics in Chapter 8.\\n6.2 Discrete and Continuous Probabilities\\nLet us focus our attention on ways to describe the probability of an event\\nas introduced in Section 6.1. Depending on whether the target space is dis-\\ncrete or continuous, the natural way to refer to distributions is different.\\nWhen the target space Tis discrete, we can specify the probability that a\\nrandom variable Xtakes a particular value x2T, denoted as P(X=x).\\nThe expression P(X=x)for a discrete random variable Xis known as\\ntheprobability mass function . When the target space Tis continuous, e.g., probability mass\\nfunction the real line R, it is more natural to specify the probability that a random\\nvariableXis in an interval, denoted by P(a6X6b)fora<b . By con-\\nvention, we specify the probability that a random variable Xis less than\\na particular value x, denoted by P(X6x). The expression P(X6x)for\\na continuous random variable Xis known as the cumulative distribution cumulative\\ndistribution function function . We will discuss continuous random variables in Section 6.2.2.\\nWe will revisit the nomenclature and contrast discrete and continuous\\nrandom variables in Section 6.2.3.\\nRemark. We will use the phrase univariate distribution to refer to distribu- univariate\\ntions of a single random variable (whose states are denoted by non-bold\\nx). We will refer to distributions of more than one random variable as\\nmultivariate distributions, and will usually consider a vector of random multivariate\\nvariables (whose states are denoted by bold x). }\\n6.2.1 Discrete Probabilities\\nWhen the target space is discrete, we can imagine the probability distri-\\nbution of multiple random variables as ﬁlling out a (multidimensional)\\narray of numbers. Figure 6.2 shows an example. The target space of the\\njoint probability is the Cartesian product of the target spaces of each of\\nthe random variables. We deﬁne the joint probability as the entry of both joint probability\\nvalues jointly\\nP(X=xi;Y=yj) =nij\\nN; (6.9)\\nwherenijis the number of events with state xiandyjandNthe total\\nnumber of events. The joint probability is the probability of the intersec-\\ntion of both events, that is, P(X=xi;Y=yj) =P(X=xi\\\\Y=yj).\\nFigure 6.2 illustrates the probability mass function (pmf) of a discrete prob- probability mass\\nfunction ability distribution. For two random variables XandY, the probability\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.2 Discrete and Continuous Probabilities 179\\nFigure 6.2\\nVisualization of a\\ndiscrete bivariate\\nprobability mass\\nfunction, with\\nrandom variables X\\nandY. This\\ndiagram is adapted\\nfrom Bishop (2006).\\nXx1x2x3x4x5Y\\ny3y2y1\\nnijo\\nrjciz}|{\\nthatX=xandY=yis (lazily) written as p(x;y)and is called the joint\\nprobability. One can think of a probability as a function that takes state\\nxandyand returns a real number, which is the reason we write p(x;y).\\nThemarginal probability thatXtakes the value xirrespective of the value marginal probability\\nof random variable Yis (lazily) written as p(x). We writeX\\x18p(x)to\\ndenote that the random variable Xis distributed according to p(x). If we\\nconsider only the instances where X=x, then the fraction of instances\\n(theconditional probability ) for whichY=yis written (lazily) as p(yjx).conditional\\nprobability\\nExample 6.2\\nConsider two random variables XandY, whereXhas ﬁve possible states\\nandYhas three possible states, as shown in Figure 6.2. We denote by nij\\nthe number of events with state X=xiandY=yj, and denote by\\nNthe total number of events. The value ciis the sum of the individual\\nfrequencies for the ith column, that is, ci=P3\\nj=1nij. Similarly, the value\\nrjis the row sum, that is, rj=P5\\ni=1nij. Using these deﬁnitions, we can\\ncompactly express the distribution of XandY.\\nThe probability distribution of each random variable, the marginal\\nprobability, can be seen as the sum over a row or column\\nP(X=xi) =ci\\nN=P3\\nj=1nij\\nN(6.10)\\nand\\nP(Y=yj) =rj\\nN=P5\\ni=1nij\\nN; (6.11)\\nwhereciandrjare theith column and jth row of the probability table,\\nrespectively. By convention, for discrete random variables with a ﬁnite\\nnumber of events, we assume that probabilties sum up to one, that is,\\n5X\\ni=1P(X=xi) = 1 and3X\\nj=1P(Y=yj) = 1: (6.12)\\nThe conditional probability is the fraction of a row or column in a par-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n180 Probability and Distributions\\nticular cell. For example, the conditional probability of YgivenXis\\nP(Y=yjjX=xi) =nij\\nci; (6.13)\\nand the conditional probability of XgivenYis\\nP(X=xijY=yj) =nij\\nrj: (6.14)\\nIn machine learning, we use discrete probability distributions to model\\ncategorical variables , i.e., variables that take a ﬁnite set of unordered val- categorical variable\\nues. They could be categorical features, such as the degree taken at uni-\\nversity when used for predicting the salary of a person, or categorical la-\\nbels, such as letters of the alphabet when doing handwriting recognition.\\nDiscrete distributions are also often used to construct probabilistic models\\nthat combine a ﬁnite number of continuous distributions (Chapter 11).\\n6.2.2 Continuous Probabilities\\nWe consider real-valued random variables in this section, i.e., we consider\\ntarget spaces that are intervals of the real line R. In this book, we pretend\\nthat we can perform operations on real random variables as if we have dis-\\ncrete probability spaces with ﬁnite states. However, this simpliﬁcation is\\nnot precise for two situations: when we repeat something inﬁnitely often,\\nand when we want to draw a point from an interval. The ﬁrst situation\\narises when we discuss generalization errors in machine learning (Chap-\\nter 8). The second situation arises when we want to discuss continuous\\ndistributions, such as the Gaussian (Section 6.5). For our purposes, the\\nlack of precision allows for a briefer introduction to probability.\\nRemark. In continuous spaces, there are two additional technicalities,\\nwhich are counterintuitive. First, the set of all subsets (used to deﬁne\\nthe event spaceAin Section 6.1) is not well behaved enough. Aneeds\\nto be restricted to behave well under set complements, set intersections,\\nand set unions. Second, the size of a set (which in discrete spaces can be\\nobtained by counting the elements) turns out to be tricky. The size of a\\nset is called its measure . For example, the cardinality of discrete sets, the measure\\nlength of an interval in R, and the volume of a region in Rdare all mea-\\nsures. Sets that behave well under set operations and additionally have\\na topology are called a Borel\\x1b-algebra . Betancourt details a careful con- Borel\\x1b-algebra\\nstruction of probability spaces from set theory without being bogged down\\nin technicalities; see https://tinyurl.com/yb3t6mfd . For a more pre-\\ncise construction, we refer to Billingsley (1995) and Jacod and Protter\\n(2004).\\nIn this book, we consider real-valued random variables with their cor-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.2 Discrete and Continuous Probabilities 181\\nresponding Borel \\x1b-algebra. We consider random variables with values in\\nRDto be a vector of real-valued random variables. }\\nDeﬁnition 6.1 (Probability Density Function) .A functionf:RD!Ris\\ncalled a probability density function (pdf) if probability density\\nfunction\\npdf 1.8x2RD:f(x)>0\\n2. Its integral exists and\\nZ\\nRDf(x)dx= 1: (6.15)\\nFor probability mass functions (pmf) of discrete random variables, the\\nintegral in (6.15) is replaced with a sum (6.12).\\nObserve that the probability density function is any function fthat is\\nnon-negative and integrates to one. We associate a random variable X\\nwith this function fby\\nP(a6X6b) =Zb\\naf(x)dx; (6.16)\\nwherea;b2Randx2Rare outcomes of the continuous random vari-\\nableX. Statesx2RDare deﬁned analogously by considering a vector\\nofx2R. This association (6.16) is called the lawordistribution of the law\\nrandom variable X.P(X=x)is a set of\\nmeasure zero. Remark. In contrast to discrete random variables, the probability of a con-\\ntinuous random variable Xtaking a particular value P(X=x)is zero.\\nThis is like trying to specify an interval in (6.16) where a=b.}\\nDeﬁnition 6.2 (Cumulative Distribution Function) .Acumulative distribu- cumulative\\ndistribution function tion function (cdf) of a multivariate real-valued random variable Xwith\\nstatesx2RDis given by\\nFX(x) =P(X16x1;:::;XD6xD); (6.17)\\nwhereX= [X1;:::;XD]>,x= [x1;:::;xD]>, and the right-hand side\\nrepresents the probability that random variable Xitakes the value smaller\\nthan or equal to xi.\\nThere are cdfs,\\nwhich do not have\\ncorresponding pdfs.The cdf can be expressed also as the integral of the probability density\\nfunctionf(x)so that\\nFX(x) =Zx1\\n\\x001\\x01\\x01\\x01ZxD\\n\\x001f(z1;:::;zD)dz1\\x01\\x01\\x01dzD: (6.18)\\nRemark. We reiterate that there are in fact two distinct concepts when\\ntalking about distributions. First is the idea of a pdf (denoted by f(x)),\\nwhich is a nonnegative function that sums to one. Second is the law of a\\nrandom variable X, that is, the association of a random variable Xwith\\nthe pdff(x). }\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n182 Probability and Distributions\\nFigure 6.3\\nExamples of\\n(a) discrete and\\n(b) continuous\\nuniform\\ndistributions. See\\nExample 6.3 for\\ndetails of the\\ndistributions.\\n−1 0 1 2\\nz0.00.51.01.52.0P(Z=z)\\n(a) Discrete distribution\\n−1 0 1 2\\nx0.00.51.01.52.0p(x) (b) Continuous distribution\\nFor most of this book, we will not use the notation f(x)andFX(x)as\\nwe mostly do not need to distinguish between the pdf and cdf. However,\\nwe will need to be careful about pdfs and cdfs in Section 6.7.\\n6.2.3 Contrasting Discrete and Continuous Distributions\\nRecall from Section 6.1.2 that probabilities are positive and the total prob-\\nability sums up to one. For discrete random variables (see (6.12)), this\\nimplies that the probability of each state must lie in the interval [0;1].\\nHowever, for continuous random variables the normalization (see (6.15))\\ndoes not imply that the value of the density is less than or equal to 1for\\nall values. We illustrate this in Figure 6.3 using the uniform distribution uniform distribution\\nfor both discrete and continuous random variables.\\nExample 6.3\\nWe consider two examples of the uniform distribution, where each state is\\nequally likely to occur. This example illustrates some differences between\\ndiscrete and continuous probability distributions.\\nLetZbe a discrete uniform random variable with three states fz=\\n\\x001:1;z= 0:3;z= 1:5g. The probability mass function can be represented The actual values of\\nthese states are not\\nmeaningful here,\\nand we deliberately\\nchose numbers to\\ndrive home the\\npoint that we do not\\nwant to use (and\\nshould ignore) the\\nordering of the\\nstates.as a table of probability values:\\nz\\nP(Z=z)\\x001:1\\n1\\n30:3\\n1\\n31:5\\n1\\n3\\nAlternatively, we can think of this as a graph (Figure 6.3(a)), where we\\nuse the fact that the states can be located on the x-axis, and the y-axis\\nrepresents the probability of a particular state. The y-axis in Figure 6.3(a)\\nis deliberately extended so that is it the same as in Figure 6.3(b).\\nLetXbe a continuous random variable taking values in the range 0:96\\nX61:6, as represented by Figure 6.3(b). Observe that the height of the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\\nTable 6.1\\nNomenclature for\\nprobability\\ndistributions.Type “Point probability” “Interval probability”\\nDiscrete P(X=x) Not applicable\\nProbability mass function\\nContinuous p(x) P(X6x)\\nProbability density function Cumulative distribution function\\ndensity can be greater than 1. However, it needs to hold that\\nZ1:6\\n0:9p(x)dx= 1: (6.19)\\nRemark. There is an additional subtlety with regards to discrete prob-\\nability distributions. The states z1;:::;zddo not in principle have any\\nstructure, i.e., there is usually no way to compare them, for example\\nz1= red;z2= green;z3= blue . However, in many machine learning\\napplications discrete states take numerical values, e.g., z1=\\x001:1;z2=\\n0:3;z3= 1:5, where we could say z1< z 2< z 3. Discrete states that as-\\nsume numerical values are particularly useful because we often consider\\nexpected values (Section 6.4.1) of random variables. }\\nUnfortunately, machine learning literature uses notation and nomen-\\nclature that hides the distinction between the sample space \\n, the target\\nspaceT, and the random variable X. For a value xof the set of possible\\noutcomes of the random variable X, i.e.,x2T,p(x)denotes the prob- We think of the\\noutcomexas the\\nargument that\\nresults in the\\nprobabilityp(x).ability that random variable Xhas the outcome x. For discrete random\\nvariables, this is written as P(X=x), which is known as the probabil-\\nity mass function. The pmf is often referred to as the “distribution”. For\\ncontinuous variables, p(x)is called the probability density function (often\\nreferred to as a density). To muddy things even further, the cumulative\\ndistribution function P(X6x)is often also referred to as the “distribu-\\ntion”. In this chapter, we will use the notation Xto refer to both univariate\\nand multivariate random variables, and denote the states by xandxre-\\nspectively. We summarize the nomenclature in Table 6.1.\\nRemark. We will be using the expression “probability distribution” not\\nonly for discrete probability mass functions but also for continuous proba-\\nbility density functions, although this is technically incorrect. In line with\\nmost machine learning literature, we also rely on context to distinguish\\nthe different uses of the phrase probability distribution. }\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem\\nWe think of probability theory as an extension to logical reasoning. As we\\ndiscussed in Section 6.1.1, the rules of probability presented here follow\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n184 Probability and Distributions\\nnaturally from fulﬁlling the desiderata (Jaynes, 2003, chapter 2). Prob-\\nabilistic modeling (Section 8.4) provides a principled foundation for de-\\nsigning machine learning methods. Once we have deﬁned probability dis-\\ntributions (Section 6.2) corresponding to the uncertainties of the data and\\nour problem, it turns out that there are only two fundamental rules, the\\nsum rule and the product rule.\\nRecall from (6.9) that p(x;y)is the joint distribution of the two ran-\\ndom variables x;y. The distributions p(x)andp(y)are the correspond-\\ning marginal distributions, and p(yjx)is the conditional distribution of y\\ngivenx. Given the deﬁnitions of the marginal and conditional probability\\nfor discrete and continuous random variables in Section 6.2, we can now\\npresent the two fundamental rules in probability theory. These two rules\\narise\\nnaturally (Jaynes,\\n2003) from the\\nrequirements we\\ndiscussed in\\nSection 6.1.1.The ﬁrst rule, the sum rule , states that\\nsum rulep(x) =8\\n>><\\n>>:X\\ny2Yp(x;y) ifyis discrete\\nZ\\nYp(x;y)dy ifyis continuous; (6.20)\\nwhereYare the states of the target space of random variable Y. This\\nmeans that we sum out (or integrate out) the set of states yof the random\\nvariableY. The sum rule is also known as the marginalization property . marginalization\\nproperty The sum rule relates the joint distribution to a marginal distribution. In\\ngeneral, when the joint distribution contains more than two random vari-\\nables, the sum rule can be applied to any subset of the random variables,\\nresulting in a marginal distribution of potentially more than one random\\nvariable. More concretely, if x= [x1;:::;xD]>, we obtain the marginal\\np(xi) =Z\\np(x1;:::;xD)dxni (6.21)\\nby repeated application of the sum rule where we integrate/sum out all\\nrandom variables except xi, which is indicated by ni, which reads “all\\nexcepti.”\\nRemark. Many of the computational challenges of probabilistic modeling\\nare due to the application of the sum rule. When there are many variables\\nor discrete variables with many states, the sum rule boils down to per-\\nforming a high-dimensional sum or integral. Performing high-dimensional\\nsums or integrals is generally computationally hard, in the sense that there\\nis no known polynomial-time algorithm to calculate them exactly. }\\nThe second rule, known as the product rule , relates the joint distribution product rule\\nto the conditional distribution via\\np(x;y) =p(yjx)p(x): (6.22)\\nThe product rule can be interpreted as the fact that every joint distribu-\\ntion of two random variables can be factorized (written as a product)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem 185\\nof two other distributions. The two factors are the marginal distribu-\\ntion of the ﬁrst random variable p(x), and the conditional distribution\\nof the second random variable given the ﬁrst p(yjx). Since the ordering\\nof random variables is arbitrary in p(x;y), the product rule also implies\\np(x;y) =p(xjy)p(y). To be precise, (6.22) is expressed in terms of the\\nprobability mass functions for discrete random variables. For continuous\\nrandom variables, the product rule is expressed in terms of the probability\\ndensity functions (Section 6.2.3).\\nIn machine learning and Bayesian statistics, we are often interested in\\nmaking inferences of unobserved (latent) random variables given that we\\nhave observed other random variables. Let us assume we have some prior\\nknowledgep(x)about an unobserved random variable xand some rela-\\ntionshipp(yjx)betweenxand a second random variable y, which we\\ncan observe. If we observe y, we can use Bayes’ theorem to draw some\\nconclusions about xgiven the observed values of y.Bayes’ theorem (also Bayes’ theorem\\nBayes’ rule orBayes’ law ) Bayes’ rule\\nBayes’ law\\np(xjy)|{z}\\nposterior=likelihoodz}|{\\np(yjx)priorz}|{\\np(x)\\np(y)|{z}\\nevidence(6.23)\\nis a direct consequence of the product rule in (6.22) since\\np(x;y) =p(xjy)p(y) (6.24)\\nand\\np(x;y) =p(yjx)p(x) (6.25)\\nso that\\np(xjy)p(y) =p(yjx)p(x)()p(xjy) =p(yjx)p(x)\\np(y):(6.26)\\nIn (6.23),p(x)is the prior , which encapsulates our subjective prior prior\\nknowledge of the unobserved (latent) variable xbefore observing any\\ndata. We can choose any prior that makes sense to us, but it is critical to\\nensure that the prior has a nonzero pdf (or pmf) on all plausible x, even\\nif they are very rare.\\nThelikelihoodp(yjx)describes how xandyare related, and in the likelihood\\nThe likelihood is\\nsometimes also\\ncalled the\\n“measurement\\nmodel”.case of discrete probability distributions, it is the probability of the data y\\nif we were to know the latent variable x. Note that the likelihood is not a\\ndistribution in x, but only iny. We callp(yjx)either the “likelihood of\\nx(giveny)” or the “probability of ygivenx” but never the likelihood of\\ny(MacKay, 2003).\\nTheposteriorp(xjy)is the quantity of interest in Bayesian statistics posterior\\nbecause it expresses exactly what we are interested in, i.e., what we know\\naboutxafter having observed y.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n186 Probability and Distributions\\nThe quantity\\np(y) :=Z\\np(yjx)p(x)dx=EX[p(yjx)] (6.27)\\nis the marginal likelihood /evidence . The right-hand side of (6.27) uses the marginal likelihood\\nevidence expectation operator which we deﬁne in Section 6.4.1. By deﬁnition, the\\nmarginal likelihood integrates the numerator of (6.23) with respect to the\\nlatent variable x. Therefore, the marginal likelihood is independent of\\nx, and it ensures that the posterior p(xjy)is normalized. The marginal\\nlikelihood can also be interpreted as the expected likelihood where we\\ntake the expectation with respect to the prior p(x). Beyond normalization\\nof the posterior, the marginal likelihood also plays an important role in\\nBayesian model selection, as we will discuss in Section 8.6. Due to the\\nintegration in (8.44), the evidence is often hard to compute. Bayes’ theorem is\\nalso called the\\n“probabilistic\\ninverse.”Bayes’ theorem (6.23) allows us to invert the relationship between x\\nandygiven by the likelihood. Therefore, Bayes’ theorem is sometimes\\ncalled the probabilistic inverse . We will discuss Bayes’ theorem further inprobabilistic inverse\\nSection 8.4.\\nRemark. In Bayesian statistics, the posterior distribution is the quantity\\nof interest as it encapsulates all available information from the prior and\\nthe data. Instead of carrying the posterior around, it is possible to focus\\non some statistic of the posterior, such as the maximum of the posterior,\\nwhich we will discuss in Section 8.3. However, focusing on some statistic\\nof the posterior leads to loss of information. If we think in a bigger con-\\ntext, then the posterior can be used within a decision-making system, and\\nhaving the full posterior can be extremely useful and lead to decisions that\\nare robust to disturbances. For example, in the context of model-based re-\\ninforcement learning, Deisenroth et al. (2015) show that using the full\\nposterior distribution of plausible transition functions leads to very fast\\n(data/sample efﬁcient) learning, whereas focusing on the maximum of\\nthe posterior leads to consistent failures. Therefore, having the full pos-\\nterior can be very useful for a downstream task. In Chapter 9, we will\\ncontinue this discussion in the context of linear regression. }\\n6.4 Summary Statistics and Independence\\nWe are often interested in summarizing sets of random variables and com-\\nparing pairs of random variables. A statistic of a random variable is a de-\\nterministic function of that random variable. The summary statistics of a\\ndistribution provide one useful view of how a random variable behaves,\\nand as the name suggests, provide numbers that summarize and charac-\\nterize the distribution. We describe the mean and the variance, two well-\\nknown summary statistics. Then we discuss two ways to compare a pair\\nof random variables: ﬁrst, how to say that two random variables are inde-\\npendent; and second, how to compute an inner product between them.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.4 Summary Statistics and Independence 187\\n6.4.1 Means and Covariances\\nMean and (co)variance are often useful to describe properties of probabil-\\nity distributions (expected values and spread). We will see in Section 6.6\\nthat there is a useful family of distributions (called the exponential fam-\\nily), where the statistics of the random variable capture all possible infor-\\nmation.\\nThe concept of the expected value is central to machine learning, and\\nthe foundational concepts of probability itself can be derived from the\\nexpected value (Whittle, 2000).\\nDeﬁnition 6.3 (Expected Value) .Theexpected value of a function g:R! expected value\\nRof a univariate continuous random variable X\\x18p(x)is given by\\nEX[g(x)] =Z\\nXg(x)p(x)dx: (6.28)\\nCorrespondingly, the expected value of a function gof a discrete random\\nvariableX\\x18p(x)is given by\\nEX[g(x)] =X\\nx2Xg(x)p(x); (6.29)\\nwhereXis the set of possible outcomes (the target space) of the random\\nvariableX.\\nIn this section, we consider discrete random variables to have numerical\\noutcomes. This can be seen by observing that the function gtakes real\\nnumbers as inputs. The expected value\\nof a function of a\\nrandom variable is\\nsometimes referred\\nto as the law of the\\nunconscious\\nstatistician (Casella\\nand Berger, 2002,\\nSection 2.2).Remark. We consider multivariate random variables Xas a ﬁnite vector\\nof univariate random variables [X1;:::;XD]>. For multivariate random\\nvariables, we deﬁne the expected value element wise\\nEX[g(x)] =2\\n64EX1[g(x1)]\\n...\\nEXD[g(xD)]3\\n752RD; (6.30)\\nwhere the subscript EXdindicates that we are taking the expected value\\nwith respect to the dth element of the vector x. }\\nDeﬁnition 6.3 deﬁnes the meaning of the notation EXas the operator\\nindicating that we should take the integral with respect to the probabil-\\nity density (for continuous distributions) or the sum over all states (for\\ndiscrete distributions). The deﬁnition of the mean (Deﬁnition 6.4), is a\\nspecial case of the expected value, obtained by choosing gto be the iden-\\ntity function.\\nDeﬁnition 6.4 (Mean) .Themean of a random variable Xwith states mean\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n188 Probability and Distributions\\nx2RDis an average and is deﬁned as\\nEX[x] =2\\n64EX1[x1]\\n...\\nEXD[xD]3\\n752RD; (6.31)\\nwhere\\nEXd[xd] :=8\\n>><\\n>>:Z\\nXxdp(xd)dxd ifXis a continuous random variable\\nX\\nxi2Xxip(xd=xi)ifXis a discrete random variable\\n(6.32)\\nford= 1;:::;D , where the subscript dindicates the corresponding di-\\nmension ofx. The integral and sum are over the states Xof the target\\nspace of the random variable X.\\nIn one dimension, there are two other intuitive notions of “average”,\\nwhich are the median and the mode . The median is the “middle” value if median\\nwe sort the values, i.e., 50% of the values are greater than the median and\\n50% are smaller than the median. This idea can be generalized to contin-\\nuous values by considering the value where the cdf (Deﬁnition 6.2) is 0:5.\\nFor distributions, which are asymmetric or have long tails, the median\\nprovides an estimate of a typical value that is closer to human intuition\\nthan the mean value. Furthermore, the median is more robust to outliers\\nthan the mean. The generalization of the median to higher dimensions is\\nnon-trivial as there is no obvious way to “sort” in more than one dimen-\\nsion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the most mode\\nfrequently occurring value. For a discrete random variable, the mode is\\ndeﬁned as the value of xhaving the highest frequency of occurrence. For\\na continuous random variable, the mode is deﬁned as a peak in the density\\np(x). A particular density p(x)may have more than one mode, and fur-\\nthermore there may be a very large number of modes in high-dimensional\\ndistributions. Therefore, ﬁnding all the modes of a distribution can be\\ncomputationally challenging.\\nExample 6.4\\nConsider the two-dimensional distribution illustrated in Figure 6.4:\\np(x) = 0:4N\\x12\\nx\\x0c\\x0c\\x0c\\x0c\\x1410\\n2\\x15\\n;\\x141 0\\n0 1\\x15\\x13\\n+ 0:6N\\x12\\nx\\x0c\\x0c\\x0c\\x0c\\x140\\n0\\x15\\n;\\x148:4 2:0\\n2:0 1:7\\x15\\x13\\n:\\n(6.33)\\nWe will deﬁne the Gaussian distribution N\\x00\\x16; \\x1b2\\x01\\nin Section 6.5. Also\\nshown is its corresponding marginal distribution in each dimension. Ob-\\nserve that the distribution is bimodal (has two modes), but one of the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.4 Summary Statistics and Independence 189\\nmarginal distributions is unimodal (has one mode). The horizontal bi-\\nmodal univariate distribution illustrates that the mean and median can\\nbe different from each other. While it is tempting to deﬁne the two-\\ndimensional median to be the concatenation of the medians in each di-\\nmension, the fact that we cannot deﬁne an ordering of two-dimensional\\npoints makes it difﬁcult. When we say “cannot deﬁne an ordering”, we\\nmean that there is more than one way to deﬁne the relation <so that\\x143\\n0\\x15\\n<\\x142\\n3\\x15\\n.\\nFigure 6.4\\nIllustration of the\\nmean, mode, and\\nmedian for a\\ntwo-dimensional\\ndataset, as well as\\nits marginal\\ndensities.\\nMean\\nModes\\nMedian\\nRemark. The expected value (Deﬁnition 6.3) is a linear operator. For ex-\\nample, given a real-valued function f(x) =ag(x)+bh(x)wherea;b2R\\nandx2RD, we obtain\\nEX[f(x)] =Z\\nf(x)p(x)dx (6.34a)\\n=Z\\n[ag(x) +bh(x)]p(x)dx (6.34b)\\n=aZ\\ng(x)p(x)dx+bZ\\nh(x)p(x)dx (6.34c)\\n=aEX[g(x)] +bEX[h(x)]: (6.34d)\\n}\\nFor two random variables, we may wish to characterize their correspon-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n190 Probability and Distributions\\ndence to each other. The covariance intuitively represents the notion of\\nhow dependent random variables are to one another.\\nDeﬁnition 6.5 (Covariance (Univariate)) .Thecovariance between two covariance\\nunivariate random variables X;Y2Ris given by the expected product\\nof their deviations from their respective means, i.e.,\\nCovX;Y[x;y] :=EX;Y\\x02(x\\x00EX[x])(y\\x00EY[y])\\x03: (6.35)\\nTerminology: The\\ncovariance of\\nmultivariate random\\nvariables Cov[x;y]\\nis sometimes\\nreferred to as\\ncross-covariance,\\nwith covariance\\nreferring to\\nCov[x;x].Remark. When the random variable associated with the expectation or\\ncovariance is clear by its arguments, the subscript is often suppressed (for\\nexample, EX[x]is often written as E[x]). }\\nBy using the linearity of expectations, the expression in Deﬁnition 6.5\\ncan be rewritten as the expected value of the product minus the product\\nof the expected values, i.e.,\\nCov[x;y] =E[xy]\\x00E[x]E[y]: (6.36)\\nThe covariance of a variable with itself Cov[x;x]is called the variance and variance\\nis denoted by VX[x]. The square root of the variance is called the standard standard deviation\\ndeviation and is often denoted by \\x1b(x). The notion of covariance can be\\ngeneralized to multivariate random variables.\\nDeﬁnition 6.6 (Covariance (Multivariate)) .If we consider two multivari-\\nate random variables XandYwith statesx2RDandy2RErespec-\\ntively, the covariance betweenXandYis deﬁned as covariance\\nCov[x;y] =E[xy>]\\x00E[x]E[y]>= Cov[y;x]>2RD\\x02E: (6.37)\\nDeﬁnition 6.6 can be applied with the same multivariate random vari-\\nable in both arguments, which results in a useful concept that intuitively\\ncaptures the “spread” of a random variable. For a multivariate random\\nvariable, the variance describes the relation between individual dimen-\\nsions of the random variable.\\nDeﬁnition 6.7 (Variance) .The variance of a random variable Xwith variance\\nstatesx2RDand a mean vector \\x162RDis deﬁned as\\nVX[x] = CovX[x;x] (6.38a)\\n=EX[(x\\x00\\x16)(x\\x00\\x16)>] =EX[xx>]\\x00EX[x]EX[x]>(6.38b)\\n=2\\n6664Cov[x1;x1] Cov[x1;x2]::: Cov[x1;xD]\\nCov[x2;x1] Cov[x2;x2]::: Cov[x2;xD]\\n............\\nCov[xD;x1]::: ::: Cov[xD;xD]3\\n7775: (6.38c)\\nTheD\\x02Dmatrix in (6.38c) is called the covariance matrix of the mul- covariance matrix\\ntivariate random variable X. The covariance matrix is symmetric and pos-\\nitive semideﬁnite and tells us something about the spread of the data. On\\nits diagonal, the covariance matrix contains the variances of the marginals marginal\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.4 Summary Statistics and Independence 191\\nFigure 6.5\\nTwo-dimensional\\ndatasets with\\nidentical means and\\nvariances along\\neach axis (colored\\nlines) but with\\ndifferent\\ncovariances.\\n−5 0 5\\nx−20246y\\n(a)xandyare negatively correlated.\\n−5 0 5\\nx−20246y\\n (b)xandyare positively correlated.\\np(xi) =Z\\np(x1;:::;xD)dxni; (6.39)\\nwhere “ni” denotes “all variables but i”. The off-diagonal entries are the\\ncross-covariance terms Cov[xi;xj]fori;j= 1;:::;D; i6=j. cross-covariance\\nRemark. In this book, we generally assume that covariance matrices are\\npositive deﬁnite to enable better intuition. We therefore do not discuss\\ncorner cases that result in positive semideﬁnite (low-rank) covariance ma-\\ntrices. }\\nWhen we want to compare the covariances between different pairs of\\nrandom variables, it turns out that the variance of each random variable\\naffects the value of the covariance. The normalized version of covariance\\nis called the correlation.\\nDeﬁnition 6.8 (Correlation) .Thecorrelation between two random vari- correlation\\nablesX;Y is given by\\ncorr[x;y] =Cov[x;y]p\\nV[x]V[y]2[\\x001;1]: (6.40)\\nThe correlation matrix is the covariance matrix of standardized random\\nvariables,x=\\x1b(x). In other words, each random variable is divided by its\\nstandard deviation (the square root of the variance) in the correlation\\nmatrix.\\nThe covariance (and correlation) indicate how two random variables\\nare related; see Figure 6.5. Positive correlation corr [x;y]means that when\\nxgrows, then yis also expected to grow. Negative correlation means that\\nasxincreases, then ydecreases.\\n6.4.2 Empirical Means and Covariances\\nThe deﬁnitions in Section 6.4.1 are often also called the population mean population mean\\nand covariance and covariance , as it refers to the true statistics for the population. In ma-\\nchine learning, we need to learn from empirical observations of data. Con-\\nsider a random variable X. There are two conceptual steps to go from\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n192 Probability and Distributions\\npopulation statistics to the realization of empirical statistics. First, we use\\nthe fact that we have a ﬁnite dataset (of size N) to construct an empirical\\nstatistic that is a function of a ﬁnite number of identical random variables,\\nX1;:::;XN. Second, we observe the data, that is, we look at the realiza-\\ntionx1;:::;xNof each of the random variables and apply the empirical\\nstatistic.\\nSpeciﬁcally, for the mean (Deﬁnition 6.4), given a particular dataset we\\ncan obtain an estimate of the mean, which is called the empirical mean or empirical mean\\nsample mean . The same holds for the empirical covariance. sample mean\\nDeﬁnition 6.9 (Empirical Mean and Covariance) .Theempirical mean vec- empirical mean\\ntor is the arithmetic average of the observations for each variable, and it\\nis deﬁned as\\n\\x16x:=1\\nNNX\\nn=1xn; (6.41)\\nwherexn2RD.\\nSimilar to the empirical mean, the empirical covariance matrix is aD\\x02D empirical covariance\\nmatrix\\n\\x06:=1\\nNNX\\nn=1(xn\\x00\\x16x)(xn\\x00\\x16x)>: (6.42)\\nThroughout the\\nbook, we use the\\nempirical\\ncovariance, which is\\na biased estimate.\\nThe unbiased\\n(sometimes called\\ncorrected)\\ncovariance has the\\nfactorN\\x001in the\\ndenominator\\ninstead ofN.To compute the statistics for a particular dataset, we would use the\\nrealizations (observations) x1;:::;xNand use (6.41) and (6.42). Em-\\npirical covariance matrices are symmetric, positive semideﬁnite (see Sec-\\ntion 3.2.3).\\n6.4.3 Three Expressions for the Variance\\nWe now focus on a single random variable Xand use the preceding em-\\npirical formulas to derive three possible expressions for the variance. The\\nThe derivations are\\nexercises at the end\\nof this chapter.following derivation is the same for the population variance, except that\\nwe need to take care of integrals. The standard deﬁnition of variance, cor-\\nresponding to the deﬁnition of covariance (Deﬁnition 6.5), is the expec-\\ntation of the squared deviation of a random variable Xfrom its expected\\nvalue\\x16, i.e.,\\nVX[x] :=EX[(x\\x00\\x16)2]: (6.43)\\nThe expectation in (6.43) and the mean \\x16=EX(x)are computed us-\\ning (6.32), depending on whether Xis a discrete or continuous random\\nvariable. The variance as expressed in (6.43) is the mean of a new random\\nvariableZ:= (X\\x00\\x16)2.\\nWhen estimating the variance in (6.43) empirically, we need to resort\\nto a two-pass algorithm: one pass through the data to calculate the mean\\n\\x16using (6.41), and then a second pass using this estimate ^\\x16calculate the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.4 Summary Statistics and Independence 193\\nvariance. It turns out that we can avoid two passes by rearranging the\\nterms. The formula in (6.43) can be converted to the so-called raw-score raw-score formula\\nfor variance formula for variance :\\nVX[x] =EX[x2]\\x00(EX[x])2: (6.44)\\nThe expression in (6.44) can be remembered as “the mean of the square\\nminus the square of the mean”. It can be calculated empirically in one pass\\nthrough data since we can accumulate xi(to calculate the mean) and x2\\ni\\nsimultaneously, where xiis theith observation. Unfortunately, if imple- If the two terms\\nin (6.44) are huge\\nand approximately\\nequal, we may\\nsuffer from an\\nunnecessary loss of\\nnumerical precision\\nin ﬂoating-point\\narithmetic.mented in this way, it can be numerically unstable. The raw-score version\\nof the variance can be useful in machine learning, e.g., when deriving the\\nbias–variance decomposition (Bishop, 2006).\\nA third way to understand the variance is that it is a sum of pairwise dif-\\nferences between all pairs of observations. Consider a sample x1;:::;xN\\nof realizations of random variable X, and we compute the squared differ-\\nence between pairs of xiandxj. By expanding the square, we can show\\nthat the sum of N2pairwise differences is the empirical variance of the\\nobservations:\\n1\\nN2NX\\ni;j=1(xi\\x00xj)2= 22\\n41\\nNNX\\ni=1x2\\ni\\x00 \\n1\\nNNX\\ni=1xi!23\\n5: (6.45)\\nWe see that (6.45) is twice the raw-score expression (6.44). This means\\nthat we can express the sum of pairwise distances (of which there are N2\\nof them) as a sum of deviations from the mean (of which there are N). Ge-\\nometrically, this means that there is an equivalence between the pairwise\\ndistances and the distances from the center of the set of points. From a\\ncomputational perspective, this means that by computing the mean ( N\\nterms in the summation), and then computing the variance (again N\\nterms in the summation), we can obtain an expression (left-hand side\\nof (6.45)) that has N2terms.\\n6.4.4 Sums and Transformations of Random Variables\\nWe may want to model a phenomenon that cannot be well explained by\\ntextbook distributions (we introduce some in Sections 6.5 and 6.6), and\\nhence may perform simple manipulations of random variables (such as\\nadding two random variables).\\nConsider two random variables X;Y with statesx;y2RD. Then:\\nE[x+y] =E[x] +E[y] (6.46)\\nE[x\\x00y] =E[x]\\x00E[y] (6.47)\\nV[x+y] =V[x] +V[y] + Cov[x;y] + Cov[y;x] (6.48)\\nV[x\\x00y] =V[x] +V[y]\\x00Cov[x;y]\\x00Cov[y;x]: (6.49)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n194 Probability and Distributions\\nMean and (co)variance exhibit some useful properties when it comes\\nto afﬁne transformation of random variables. Consider a random variable\\nXwith mean\\x16and covariance matrix \\x06and a (deterministic) afﬁne\\ntransformation y=Ax+bofx. Thenyis itself a random variable\\nwhose mean vector and covariance matrix are given by\\nEY[y] =EX[Ax+b] =AEX[x] +b=A\\x16+b; (6.50)\\nVY[y] =VX[Ax+b] =VX[Ax] =AVX[x]A>=A\\x06A>;(6.51)\\nrespectively. Furthermore, This can be shown\\ndirectly by using the\\ndeﬁnition of the\\nmean and\\ncovariance.Cov[x;y] =E[x(Ax+b)>]\\x00E[x]E[Ax+b]>(6.52a)\\n=E[x]b>+E[xx>]A>\\x00\\x16b>\\x00\\x16\\x16>A>(6.52b)\\n=\\x16b>\\x00\\x16b>+\\x00\\nE[xx>]\\x00\\x16\\x16>\\x01A>(6.52c)\\n(6.38b)=\\x06A>; (6.52d)\\nwhere \\x06=E[xx>]\\x00\\x16\\x16>is the covariance of X.\\n6.4.5 Statistical Independence\\nDeﬁnition 6.10 (Independence) .Two random variables X;Y arestatis- statistical\\nindependence tically independent if and only if\\np(x;y) =p(x)p(y): (6.53)\\nIntuitively, two random variables XandYare independent if the value\\nofy(once known) does not add any additional information about x(and\\nvice versa). If X;Y are (statistically) independent, then\\np(yjx) =p(y)\\np(xjy) =p(x)\\nVX;Y[x+y] =VX[x] +VY[y]\\nCovX;Y[x;y] =0\\nThe last point may not hold in converse, i.e., two random variables can\\nhave covariance zero but are not statistically independent. To understand\\nwhy, recall that covariance measures only linear dependence. Therefore,\\nrandom variables that are nonlinearly dependent could have covariance\\nzero.\\nExample 6.5\\nConsider a random variable Xwith zero mean ( EX[x] = 0 ) and also\\nEX[x3] = 0 . Lety=x2(hence,Yis dependent on X) and consider the\\ncovariance (6.36) between XandY. But this gives\\nCov[x;y] =E[xy]\\x00E[x]E[y] =E[x3] = 0: (6.54)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.4 Summary Statistics and Independence 195\\nIn machine learning, we often consider problems that can be mod-\\neled as independent and identically distributed (i.i.d.) random variables, independent and\\nidentically\\ndistributed\\ni.i.d.X1;:::;XN. For more than two random variables, the word “indepen-\\ndent” (Deﬁnition 6.10) usually refers to mutually independent random\\nvariables, where all subsets are independent (see Pollard (2002, chap-\\nter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically\\ndistributed” means that all the random variables are from the same distri-\\nbution.\\nAnother concept that is important in machine learning is conditional\\nindependence.\\nDeﬁnition 6.11 (Conditional Independence) .Two random variables X\\nandYareconditionally independent givenZif and only if conditionally\\nindependent\\np(x;yjz) =p(xjz)p(yjz) for allz2Z; (6.55)\\nwhereZis the set of states of random variable Z. We writeX? ?YjZto\\ndenote that Xis conditionally independent of YgivenZ.\\nDeﬁnition 6.11 requires that the relation in (6.55) must hold true for\\nevery value of z. The interpretation of (6.55) can be understood as “given\\nknowledge about z, the distribution of xandyfactorizes”. Independence\\ncan be cast as a special case of conditional independence if we write X? ?\\nYj;. By using the product rule of probability (6.22), we can expand the\\nleft-hand side of (6.55) to obtain\\np(x;yjz) =p(xjy;z)p(yjz): (6.56)\\nBy comparing the right-hand side of (6.55) with (6.56), we see that p(yjz)\\nappears in both of them so that\\np(xjy;z) =p(xjz): (6.57)\\nEquation (6.57) provides an alternative deﬁnition of conditional indepen-\\ndence, i.e., X? ?YjZ. This alternative presentation provides the inter-\\npretation “given that we know z, knowledge about ydoes not change our\\nknowledge of x”.\\n6.4.6 Inner Products of Random Variables\\nRecall the deﬁnition of inner products from Section 3.2. We can deﬁne an Inner products\\nbetween\\nmultivariate random\\nvariables can be\\ntreated in a similar\\nfashioninner product between random variables, which we brieﬂy describe in this\\nsection. If we have two uncorrelated random variables X;Y , then\\nV[x+y] =V[x] +V[y]: (6.58)\\nSince variances are measured in squared units, this looks very much like\\nthe Pythagorean theorem for right triangles c2=a2+b2.\\nIn the following, we see whether we can ﬁnd a geometric interpreta-\\ntion of the variance relation of uncorrelated random variables in (6.58).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n196 Probability and Distributions\\nFigure 6.6\\nGeometry of\\nrandom variables. If\\nrandom variables X\\nandYare\\nuncorrelated, they\\nare orthogonal\\nvectors in a\\ncorresponding\\nvector space, and\\nthe Pythagorean\\ntheorem applies.\\np\\nvar[y]\\np\\nvar[x]\\np\\nvar[x+y] =p\\nvar[x] + var[ y]\\na\\nc\\nb\\nRandom variables can be considered vectors in a vector space, and we\\ncan deﬁne inner products to obtain geometric properties of random vari-\\nables (Eaton, 2007). If we deﬁne\\nhX;Yi:= Cov[x;y] (6.59)\\nfor zero mean random variables XandY, we obtain an inner product. We\\nsee that the covariance is symmetric, positive deﬁnite, and linear in either Cov[x;x] = 0()\\nx= 0 argument. The length of a random variable is\\nCov[\\x0bx+z;y] =\\n\\x0bCov[x;y] +\\nCov[z;y]for\\x0b2R.kXk=q\\nCov[x;x] =q\\nV[x] =\\x1b[x]; (6.60)\\ni.e., its standard deviation. The “longer” the random variable, the more\\nuncertain it is; and a random variable with length 0is deterministic.\\nIf we look at the angle \\x12between two random variables X;Y , we get\\ncos\\x12=hX;Yi\\nkXkkYk=Cov[x;y]p\\nV[x]V[y]; (6.61)\\nwhich is the correlation (Deﬁnition 6.8) between the two random vari-\\nables. This means that we can think of correlation as the cosine of the\\nangle between two random variables when we consider them geometri-\\ncally. We know from Deﬁnition 3.7 that X?Y() hX;Yi= 0. In our\\ncase, this means that XandYare orthogonal if and only if Cov[x;y] = 0 ,\\ni.e., they are uncorrelated. Figure 6.6 illustrates this relationship.\\nRemark. While it is tempting to use the Euclidean distance (constructed\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.5 Gaussian Distribution 197\\nFigure 6.7\\nGaussian\\ndistribution of two\\nrandom variables x1\\nandx2.\\nx1−101x2\\n−5.0−2.50.02.55.07.5p(x1,x2)\\n0.000.050.100.150.20\\nfrom the preceding deﬁnition of inner products) to compare probability\\ndistributions, it is unfortunately not the best way to obtain distances be-\\ntween distributions. Recall that the probability mass (or density) is posi-\\ntive and needs to add up to 1. These constraints mean that distributions\\nlive on something called a statistical manifold. The study of this space of\\nprobability distributions is called information geometry. Computing dis-\\ntances between distributions are often done using Kullback-Leibler diver-\\ngence, which is a generalization of distances that account for properties of\\nthe statistical manifold. Just like the Euclidean distance is a special case of\\na metric (Section 3.3), the Kullback-Leibler divergence is a special case of\\ntwo more general classes of divergences called Bregman divergences and\\nf-divergences. The study of divergences is beyond the scope of this book,\\nand we refer for more details to the recent book by Amari (2016), one of\\nthe founders of the ﬁeld of information geometry. }\\n6.5 Gaussian Distribution\\nThe Gaussian distribution is the most well-studied probability distribution\\nfor continuous-valued random variables. It is also referred to as the normal normal distribution\\ndistribution . Its importance originates from the fact that it has many com- The Gaussian\\ndistribution arises\\nnaturally when we\\nconsider sums of\\nindependent and\\nidentically\\ndistributed random\\nvariables. This is\\nknown as the\\ncentral limit\\ntheorem (Grinstead\\nand Snell, 1997).putationally convenient properties, which we will be discussing in the fol-\\nlowing. In particular, we will use it to deﬁne the likelihood and prior for\\nlinear regression (Chapter 9), and consider a mixture of Gaussians for\\ndensity estimation (Chapter 11).\\nThere are many other areas of machine learning that also beneﬁt from\\nusing a Gaussian distribution, for example Gaussian processes, variational\\ninference, and reinforcement learning. It is also widely used in other ap-\\nplication areas such as signal processing (e.g., Kalman ﬁlter), control (e.g.,\\nlinear quadratic regulator), and statistics (e.g., hypothesis testing).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n198 Probability and Distributions\\nFigure 6.8\\nGaussian\\ndistributions\\noverlaid with 100\\nsamples. (a) One-\\ndimensional case;\\n(b) two-dimensional\\ncase.\\n−5.0−2.5 0.0 2.5 5.0 7.5\\nx0.000.050.100.150.20\\np(x)\\nMean\\nSample\\n2σ\\n(a) Univariate (one-dimensional) Gaussian;\\nThe red cross shows the mean and the red\\nline shows the extent of the variance.\\n−1 0 1\\nx1−4−202468x2Mean\\nSample(b) Multivariate (two-dimensional) Gaus-\\nsian, viewed from top. The red cross shows\\nthe mean and the colored lines show the con-\\ntour lines of the density.\\nFor a univariate random variable, the Gaussian distribution has a den-\\nsity that is given by\\np(xj\\x16;\\x1b2) =1p\\n2\\x19\\x1b2exp\\x12\\n\\x00(x\\x00\\x16)2\\n2\\x1b2\\x13\\n: (6.62)\\nThe multivariate Gaussian distribution is fully characterized by a mean multivariate\\nGaussian\\ndistribution\\nmean vectorvector\\x16and a covariance matrix \\x06and deﬁned as\\ncovariance matrixp(xj\\x16;\\x06) = (2\\x19)\\x00D\\n2j\\x06j\\x001\\n2exp\\x00\\x001\\n2(x\\x00\\x16)>\\x06\\x001(x\\x00\\x16)\\x01;(6.63)\\nwherex2RD. We writep(x) =N\\x00xj\\x16;\\x06\\x01\\norX\\x18N\\x00\\x16;\\x06\\x01\\n. Fig- Also known as a\\nmultivariate normal\\ndistribution.ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-\\ntour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian\\nwith corresponding samples. The special case of the Gaussian with zero\\nmean and identity covariance, that is, \\x16=0and\\x06=I, is referred to as\\nthestandard normal distribution . standard normal\\ndistribution Gaussians are widely used in statistical estimation and machine learn-\\ning as they have closed-form expressions for marginal and conditional dis-\\ntributions. In Chapter 9, we use these closed-form expressions extensively\\nfor linear regression. A major advantage of modeling with Gaussian ran-\\ndom variables is that variable transformations (Section 6.7) are often not\\nneeded. Since the Gaussian distribution is fully speciﬁed by its mean and\\ncovariance, we often can obtain the transformed distribution by applying\\nthe transformation to the mean and covariance of the random variable.\\n6.5.1 Marginals and Conditionals of Gaussians are Gaussians\\nIn the following, we present marginalization and conditioning in the gen-\\neral case of multivariate random variables. If this is confusing at ﬁrst read-\\ning, the reader is advised to consider two univariate random variables in-\\nstead. LetXandYbe two multivariate random variables, that may have\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.5 Gaussian Distribution 199\\ndifferent dimensions. To consider the effect of applying the sum rule of\\nprobability and the effect of conditioning, we explicitly write the Gaus-\\nsian distribution in terms of the concatenated states [x>;y>],\\np(x;y) =N\\x12\\x14\\x16x\\n\\x16y\\x15\\n;\\x14\\x06xx\\x06xy\\n\\x06yx\\x06yy\\x15\\x13\\n: (6.64)\\nwhere \\x06xx= Cov[x;x]and\\x06yy= Cov[y;y]are the marginal covari-\\nance matrices of xandy, respectively, and \\x06xy= Cov[x;y]is the cross-\\ncovariance matrix between xandy.\\nThe conditional distribution p(xjy)is also Gaussian (illustrated in Fig-\\nure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006)\\np(xjy) =N\\x00\\x16xjy;\\x06xjy\\x01\\n(6.65)\\n\\x16xjy=\\x16x+\\x06xy\\x06\\x001\\nyy(y\\x00\\x16y) (6.66)\\n\\x06xjy=\\x06xx\\x00\\x06xy\\x06\\x001\\nyy\\x06yx: (6.67)\\nNote that in the computation of the mean in (6.66), the y-value is an\\nobservation and no longer random.\\nRemark. The conditional Gaussian distribution shows up in many places,\\nwhere we are interested in posterior distributions:\\nThe Kalman ﬁlter (Kalman, 1960), one of the most central algorithms\\nfor state estimation in signal processing, does nothing but computing\\nGaussian conditionals of joint distributions (Deisenroth and Ohlsson,\\n2011; S ¨arkk¨a, 2013).\\nGaussian processes (Rasmussen and Williams, 2006), which are a prac-\\ntical implementation of a distribution over functions. In a Gaussian pro-\\ncess, we make assumptions of joint Gaussianity of random variables. By\\n(Gaussian) conditioning on observed data, we can determine a poste-\\nrior distribution over functions.\\nLatent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-\\nphy, 2012), which include probabilistic principal component analysis\\n(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-\\ntail in Section 10.7.\\n}\\nThe marginal distribution p(x)of a joint Gaussian distribution p(x;y)\\n(see (6.64)) is itself Gaussian and computed by applying the sum rule\\n(6.20) and given by\\np(x) =Z\\np(x;y)dy=N\\x00xj\\x16x;\\x06xx\\x01: (6.68)\\nThe corresponding result holds for p(y), which is obtained by marginaliz-\\ning with respect to x. Intuitively, looking at the joint distribution in (6.64),\\nwe ignore (i.e., integrate out) everything we are not interested in. This is\\nillustrated in Figure 6.9(b).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n200 Probability and Distributions\\nExample 6.6\\nFigure 6.9\\n(a) Bivariate\\nGaussian;\\n(b) marginal of a\\njoint Gaussian\\ndistribution is\\nGaussian; (c) the\\nconditional\\ndistribution of a\\nGaussian is also\\nGaussian.\\n−1 0 1\\nx1−4−202468x2\\nx2=−1\\n(a) Bivariate Gaussian.\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx10.00.20.40.6p(x1)\\nMean\\n2σ\\n(b) Marginal distribution.\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx10.00.20.40.60.81.01.2p(x1|x2=−1)\\nMean\\n2σ (c) Conditional distribution.\\nConsider the bivariate Gaussian distribution (illustrated in Figure 6.9):\\np(x1;x2) =N\\x12\\x140\\n2\\x15\\n;\\x140:3\\x001\\n\\x001 5\\x15\\x13\\n: (6.69)\\nWe can compute the parameters of the univariate Gaussian, conditioned\\nonx2=\\x001, by applying (6.66) and (6.67) to obtain the mean and vari-\\nance respectively. Numerically, this is\\n\\x16x1jx2=\\x001= 0 + (\\x001)\\x010:2\\x01(\\x001\\x002) = 0:6 (6.70)\\nand\\n\\x1b2\\nx1jx2=\\x001= 0:3\\x00(\\x001)\\x010:2\\x01(\\x001) = 0:1: (6.71)\\nTherefore, the conditional Gaussian is given by\\np(x1jx2=\\x001) =N\\x000:6;0:1\\x01: (6.72)\\nThe marginal distribution p(x1), in contrast, can be obtained by apply-\\ning (6.68), which is essentially using the mean and variance of the random\\nvariablex1, giving us\\np(x1) =N\\x000;0:3\\x01: (6.73)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.5 Gaussian Distribution 201\\n6.5.2 Product of Gaussian Densities\\nFor linear regression (Chapter 9), we need to compute a Gaussian likeli-\\nhood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3).\\nWe apply Bayes’ Theorem to compute the posterior, which results in a mul-\\ntiplication of the likelihood and the prior, that is, the multiplication of two\\nGaussian densities. The product of two GaussiansN\\x00xja;A\\x01N\\x00xjb;B\\x01\\nThe derivation is an\\nexercise at the end\\nof this chapter.is a Gaussian distribution scaled by a c2R, given bycN\\x00xjc;C\\x01\\nwith\\nC= (A\\x001+B\\x001)\\x001(6.74)\\nc=C(A\\x001a+B\\x001b) (6.75)\\nc= (2\\x19)\\x00D\\n2jA+Bj\\x001\\n2exp\\x00\\x001\\n2(a\\x00b)>(A+B)\\x001(a\\x00b)\\x01:(6.76)\\nThe scaling constant citself can be written in the form of a Gaussian\\ndensity either in aor inbwith an “inﬂated” covariance matrix A+B,\\ni.e.,c=N\\x00ajb;A+B\\x01=N\\x00bja;A+B\\x01\\n.\\nRemark. For notation convenience, we will sometimes use N\\x00xjm;S\\x01\\nto describe the functional form of a Gaussian density even if xis not a\\nrandom variable. We have just done this in the preceding demonstration\\nwhen we wrote\\nc=N\\x00ajb;A+B\\x01=N\\x00bja;A+B\\x01: (6.77)\\nHere, neither anorbare random variables. However, writing cin this way\\nis more compact than (6.76). }\\n6.5.3 Sums and Linear Transformations\\nIfX;Y are independent Gaussian random variables (i.e., the joint distri-\\nbution is given as p(x;y) =p(x)p(y)) withp(x) =N\\x00xj\\x16x;\\x06x\\x01\\nand\\np(y) =N\\x00yj\\x16y;\\x06y\\x01\\n, thenx+yis also Gaussian distributed and given\\nby\\np(x+y) =N\\x00\\x16x+\\x16y;\\x06x+\\x06y\\x01: (6.78)\\nKnowing that p(x+y)is Gaussian, the mean and covariance matrix can\\nbe determined immediately using the results from (6.46) through (6.49).\\nThis property will be important when we consider i.i.d. Gaussian noise\\nacting on random variables, as is the case for linear regression (Chap-\\nter 9).\\nExample 6.7\\nSince expectations are linear operations, we can obtain the weighted sum\\nof independent Gaussian random variables\\np(ax+by) =N\\x00a\\x16x+b\\x16y; a2\\x06x+b2\\x06y\\x01: (6.79)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n202 Probability and Distributions\\nRemark. A case that will be useful in Chapter 11 is the weighted sum of\\nGaussian densities. This is different from the weighted sum of Gaussian\\nrandom variables. }\\nIn Theorem 6.12, the random variable xis from a density that is a\\nmixture of two densities p1(x)andp2(x), weighted by \\x0b. The theorem can\\nbe generalized to the multivariate random variable case, since linearity of\\nexpectations holds also for multivariate random variables. However, the\\nidea of a squared random variable needs to be replaced by xx>.\\nTheorem 6.12. Consider a mixture of two univariate Gaussian densities\\np(x) =\\x0bp1(x) + (1\\x00\\x0b)p2(x); (6.80)\\nwhere the scalar 0<\\x0b< 1is the mixture weight, and p1(x)andp2(x)are\\nunivariate Gaussian densities (Equation (6.62) ) with different parameters,\\ni.e.,(\\x161;\\x1b2\\n1)6= (\\x162;\\x1b2\\n2).\\nThen the mean of the mixture density p(x)is given by the weighted sum\\nof the means of each random variable:\\nE[x] =\\x0b\\x161+ (1\\x00\\x0b)\\x162: (6.81)\\nThe variance of the mixture density p(x)is given by\\nV[x] =\\x02\\x0b\\x1b2\\n1+ (1\\x00\\x0b)\\x1b2\\n2\\x03+\\x10\\x02\\x0b\\x162\\n1+ (1\\x00\\x0b)\\x162\\n2\\x03\\x00[\\x0b\\x161+ (1\\x00\\x0b)\\x162]2\\x11\\n:\\n(6.82)\\nProof The mean of the mixture density p(x)is given by the weighted\\nsum of the means of each random variable. We apply the deﬁnition of the\\nmean (Deﬁnition 6.4), and plug in our mixture (6.80), which yields\\nE[x] =Z1\\n\\x001xp(x)dx (6.83a)\\n=Z1\\n\\x001(\\x0bxp 1(x) + (1\\x00\\x0b)xp2(x)) dx (6.83b)\\n=\\x0bZ1\\n\\x001xp1(x)dx+ (1\\x00\\x0b)Z1\\n\\x001xp2(x)dx (6.83c)\\n=\\x0b\\x161+ (1\\x00\\x0b)\\x162: (6.83d)\\nTo compute the variance, we can use the raw-score version of the vari-\\nance from (6.44), which requires an expression of the expectation of the\\nsquared random variable. Here we use the deﬁnition of an expectation of\\na function (the square) of a random variable (Deﬁnition 6.3),\\nE[x2] =Z1\\n\\x001x2p(x)dx (6.84a)\\n=Z1\\n\\x001\\x00\\x0bx2p1(x) + (1\\x00\\x0b)x2p2(x)\\x01dx (6.84b)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.5 Gaussian Distribution 203\\n=\\x0bZ1\\n\\x001x2p1(x)dx+ (1\\x00\\x0b)Z1\\n\\x001x2p2(x)dx (6.84c)\\n=\\x0b(\\x162\\n1+\\x1b2\\n1) + (1\\x00\\x0b)(\\x162\\n2+\\x1b2\\n2); (6.84d)\\nwhere in the last equality, we again used the raw-score version of the\\nvariance (6.44) giving \\x1b2=E[x2]\\x00\\x162. This is rearranged such that the\\nexpectation of a squared random variable is the sum of the squared mean\\nand the variance.\\nTherefore, the variance is given by subtracting (6.83d) from (6.84d),\\nV[x] =E[x2]\\x00(E[x])2(6.85a)\\n=\\x0b(\\x162\\n1+\\x1b2\\n1) + (1\\x00\\x0b)(\\x162\\n2+\\x1b2\\n2)\\x00(\\x0b\\x161+ (1\\x00\\x0b)\\x162)2(6.85b)\\n=\\x02\\x0b\\x1b2\\n1+ (1\\x00\\x0b)\\x1b2\\n2\\x03\\n+\\x10\\x02\\x0b\\x162\\n1+ (1\\x00\\x0b)\\x162\\n2\\x03\\x00[\\x0b\\x161+ (1\\x00\\x0b)\\x162]2\\x11\\n: (6.85c)\\nRemark. The preceding derivation holds for any density, but since the\\nGaussian is fully determined by the mean and variance, the mixture den-\\nsity can be determined in closed form. }\\nFor a mixture density, the individual components can be considered\\nto be conditional distributions (conditioned on the component identity).\\nEquation (6.85c) is an example of the conditional variance formula, also\\nknown as the law of total variance , which generally states that for two ran- law of total variance\\ndom variables XandYit holds that VX[x] =EY[VX[xjy]]+VY[EX[xjy]],\\ni.e., the (total) variance of Xis the expected conditional variance plus the\\nvariance of a conditional mean.\\nWe consider in Example 6.17 a bivariate standard Gaussian random\\nvariableXand performed a linear transformation Axon it. The outcome\\nis a Gaussian random variable with mean zero and covariance AA>. Ob-\\nserve that adding a constant vector will change the mean of the distribu-\\ntion, without affecting its variance, that is, the random variable x+\\x16is\\nGaussian with mean \\x16and identity covariance. Hence, any linear/afﬁne\\ntransformation of a Gaussian random variable is Gaussian distributed. Any linear/afﬁne\\ntransformation of a\\nGaussian random\\nvariable is also\\nGaussian\\ndistributed.Consider a Gaussian distributed random variable X\\x18N\\x00\\x16;\\x06\\x01\\n. For\\na given matrix Aof appropriate shape, let Ybe a random variable such\\nthaty=Axis a transformed version of x. We can compute the mean of\\nyby exploiting that the expectation is a linear operator (6.50) as follows:\\nE[y] =E[Ax] =AE[x] =A\\x16: (6.86)\\nSimilarly the variance of ycan be found by using (6.51):\\nV[y] =V[Ax] =AV[x]A>=A\\x06A>: (6.87)\\nThis means that the random variable yis distributed according to\\np(y) =N\\x00yjA\\x16;A\\x06A>\\x01: (6.88)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n204 Probability and Distributions\\nLet us now consider the reverse transformation: when we know that a\\nrandom variable has a mean that is a linear transformation of another\\nrandom variable. For a given full rank matrix A2RM\\x02N, whereM>N,\\nlety2RMbe a Gaussian random variable with mean Ax, i.e.,\\np(y) =N\\x00yjAx;\\x06\\x01: (6.89)\\nWhat is the corresponding probability distribution p(x)? IfAis invert-\\nible, then we can write x=A\\x001yand apply the transformation in the\\nprevious paragraph. However, in general Ais not invertible, and we use\\nan approach similar to that of the pseudo-inverse (3.57). That is, we pre-\\nmultiply both sides with A>and then invert A>A, which is symmetric\\nand positive deﬁnite, giving us the relation\\ny=Ax() (A>A)\\x001A>y=x: (6.90)\\nHence,xis a linear transformation of y, and we obtain\\np(x) =N\\x00xj(A>A)\\x001A>y;(A>A)\\x001A>\\x06A(A>A)\\x001\\x01:(6.91)\\n6.5.4 Sampling from Multivariate Gaussian Distributions\\nWe will not explain the subtleties of random sampling on a computer, and\\nthe interested reader is referred to Gentle (2004). In the case of a mul-\\ntivariate Gaussian, this process consists of three stages: ﬁrst, we need a\\nsource of pseudo-random numbers that provide a uniform sample in the\\ninterval [0,1]; second, we use a non-linear transformation such as the\\nBox-M ¨uller transform (Devroye, 1986) to obtain a sample from a univari-\\nate Gaussian; and third, we collate a vector of these samples to obtain a\\nsample from a multivariate standard normal N\\x000;I\\x01\\n.\\nFor a general multivariate Gaussian, that is, where the mean is non\\nzero and the covariance is not the identity matrix, we use the proper-\\nties of linear transformations of a Gaussian random variable. Assume we\\nare interested in generating samples xi;i= 1;:::;n; from a multivariate\\nGaussian distribution with mean \\x16and covariance matrix \\x06. We would To compute the\\nCholesky\\nfactorization of a\\nmatrix, it is required\\nthat the matrix is\\nsymmetric and\\npositive deﬁnite\\n(Section 3.2.3).\\nCovariance matrices\\npossess this\\nproperty.like to construct the sample from a sampler that provides samples from\\nthe multivariate standard normal N\\x000;I\\x01\\n.\\nTo obtain samples from a multivariate normal N\\x00\\x16;\\x06\\x01\\n, we can use\\nthe properties of a linear transformation of a Gaussian random variable:\\nIfx\\x18N\\x000;I\\x01\\n, theny=Ax+\\x16, whereAA>=\\x06is Gaussian dis-\\ntributed with mean \\x16and covariance matrix \\x06. One convenient choice of\\nAis to use the Cholesky decomposition (Section 4.3) of the covariance\\nmatrix \\x06=AA>. The Cholesky decomposition has the beneﬁt that Ais\\ntriangular, leading to efﬁcient computation.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.6 Conjugacy and the Exponential Family 205\\n6.6 Conjugacy and the Exponential Family\\nMany of the probability distributions “with names” that we ﬁnd in statis-\\ntics textbooks were discovered to model particular types of phenomena.\\nFor example, we have seen the Gaussian distribution in Section 6.5. The\\ndistributions are also related to each other in complex ways (Leemis and\\nMcQueston, 2008). For a beginner in the ﬁeld, it can be overwhelming to\\nﬁgure out which distribution to use. In addition, many of these distribu-\\ntions were discovered at a time that statistics and computation were done “Computers” used to\\nbe a job description. by pencil and paper. It is natural to ask what are meaningful concepts\\nin the computing age (Efron and Hastie, 2016). In the previous section,\\nwe saw that many of the operations required for inference can be conve-\\nniently calculated when the distribution is Gaussian. It is worth recalling\\nat this point the desiderata for manipulating probability distributions in\\nthe machine learning context:\\n1. There is some “closure property” when applying the rules of probability,\\ne.g., Bayes’ theorem. By closure, we mean that applying a particular\\noperation returns an object of the same type.\\n2. As we collect more data, we do not need more parameters to describe\\nthe distribution.\\n3. Since we are interested in learning from data, we want parameter es-\\ntimation to behave nicely.\\nIt turns out that the class of distributions called the exponential family exponential family\\nprovides the right balance of generality while retaining favorable compu-\\ntation and inference properties. Before we introduce the exponential fam-\\nily, let us see three more members of “named” probability distributions,\\nthe Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-\\nple 6.10) distributions.\\nExample 6.8\\nThe Bernoulli distribution is a distribution for a single binary random Bernoulli\\ndistribution variableXwith statex2f0;1g. It is governed by a single continuous pa-\\nrameter\\x162[0;1]that represents the probability of X= 1. The Bernoulli\\ndistribution Ber (\\x16)is deﬁned as\\np(xj\\x16) =\\x16x(1\\x00\\x16)1\\x00x; x2f0;1g; (6.92)\\nE[x] =\\x16; (6.93)\\nV[x] =\\x16(1\\x00\\x16); (6.94)\\nwhere E[x]andV[x]are the mean and variance of the binary random\\nvariableX.\\nAn example where the Bernoulli distribution can be used is when we\\nare interested in modeling the probability of “heads” when ﬂipping a coin.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n206 Probability and Distributions\\nFigure 6.10\\nExamples of the\\nBinomial\\ndistribution for\\n\\x162f0:1;0:4;0:75g\\nandN= 15 .\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\\nNumbermof observations x= 1 inN= 15 experiments0.00.10.20.3p(m)µ= 0.1\\nµ= 0.4\\nµ= 0.75\\nRemark. The rewriting above of the Bernoulli distribution, where we use\\nBoolean variables as numerical 0or1and express them in the exponents,\\nis a trick that is often used in machine learning textbooks. Another oc-\\ncurence of this is when expressing the Multinomial distribution. }\\nExample 6.9 (Binomial Distribution)\\nThe Binomial distribution is a generalization of the Bernoulli distribution Binomial\\ndistribution to a distribution over integers (illustrated in Figure 6.10). In particular,\\nthe Binomial can be used to describe the probability of observing moc-\\ncurrences of X= 1 in a set ofNsamples from a Bernoulli distribution\\nwherep(X= 1) =\\x162[0;1]. The Binomial distribution Bin (N;\\x16)is\\ndeﬁned as\\np(mjN;\\x16) = \\nN\\nm!\\n\\x16m(1\\x00\\x16)N\\x00m; (6.95)\\nE[m] =N\\x16; (6.96)\\nV[m] =N\\x16(1\\x00\\x16); (6.97)\\nwhere E[m]andV[m]are the mean and variance of m, respectively.\\nAn example where the Binomial could be used is if we want to describe\\nthe probability of observing m“heads” inNcoin-ﬂip experiments if the\\nprobability for observing head in a single experiment is \\x16.\\nExample 6.10 (Beta Distribution)\\nWe may wish to model a continuous random variable on a ﬁnite interval.\\nThe Beta distribution is a distribution over a continuous random variable Beta distribution\\n\\x162[0;1], which is often used to represent the probability for some binary\\nevent (e.g., the parameter governing the Bernoulli distribution). The Beta\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.6 Conjugacy and the Exponential Family 207\\ndistribution Beta (\\x0b;\\x0c)(illustrated in Figure 6.11) itself is governed by\\ntwo parameters \\x0b>0; \\x0c > 0and is deﬁned as\\np(\\x16j\\x0b;\\x0c) =\\x00(\\x0b+\\x0c)\\n\\x00(\\x0b)\\x00(\\x0c)\\x16\\x0b\\x001(1\\x00\\x16)\\x0c\\x001(6.98)\\nE[\\x16] =\\x0b\\n\\x0b+\\x0c;V[\\x16] =\\x0b\\x0c\\n(\\x0b+\\x0c)2(\\x0b+\\x0c+ 1)(6.99)\\nwhere \\x00(\\x01)is the Gamma function deﬁned as\\n\\x00(t) :=Z1\\n0xt\\x001exp(\\x00x)dx; t> 0: (6.100)\\n\\x00(t+ 1) =t\\x00(t): (6.101)\\nNote that the fraction of Gamma functions in (6.98) normalizes the Beta\\ndistribution.\\nFigure 6.11\\nExamples of the\\nBeta distribution for\\ndifferent values of \\x0b\\nand\\x0c.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nµ0246810p(µ|α,β)α= 0.5 =β\\nα= 1 =β\\nα= 2,β= 0.3\\nα= 4,β= 10\\nα= 5,β= 1\\nIntuitively,\\x0bmoves probability mass toward 1, whereas\\x0cmoves prob-\\nability mass toward 0. There are some special cases (Murphy, 2012):\\nFor\\x0b= 1 =\\x0c, we obtain the uniform distribution U[0;1].\\nFor\\x0b;\\x0c < 1, we get a bimodal distribution with spikes at 0and1.\\nFor\\x0b;\\x0c > 1, the distribution is unimodal.\\nFor\\x0b;\\x0c > 1and\\x0b=\\x0c, the distribution is unimodal, symmetric, and\\ncentered in the interval [0;1], i.e., the mode/mean is at1\\n2.\\nRemark. There is a whole zoo of distributions with names, and they are\\nrelated in different ways to each other (Leemis and McQueston, 2008).\\nIt is worth keeping in mind that each named distribution is created for a\\nparticular reason, but may have other applications. Knowing the reason\\nbehind the creation of a particular distribution often allows insight into\\nhow to best use it. We introduced the preceding three distributions to be\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n208 Probability and Distributions\\nable to illustrate the concepts of conjugacy (Section 6.6.1) and exponen-\\ntial families (Section 6.6.3). }\\n6.6.1 Conjugacy\\nAccording to Bayes’ theorem (6.23), the posterior is proportional to the\\nproduct of the prior and the likelihood. The speciﬁcation of the prior can\\nbe tricky for two reasons: First, the prior should encapsulate our knowl-\\nedge about the problem before we see any data. This is often difﬁcult to\\ndescribe. Second, it is often not possible to compute the posterior distribu-\\ntion analytically. However, there are some priors that are computationally\\nconvenient: conjugate priors . conjugate prior\\nDeﬁnition 6.13 (Conjugate Prior) .A prior is conjugate for the likelihood conjugate\\nfunction if the posterior is of the same form/type as the prior.\\nConjugacy is particularly convenient because we can algebraically cal-\\nculate our posterior distribution by updating the parameters of the prior\\ndistribution.\\nRemark. When considering the geometry of probability distributions, con-\\njugate priors retain the same distance structure as the likelihood (Agarwal\\nand Daum ´e III, 2010). }\\nTo introduce a concrete example of conjugate priors, we describe in Ex-\\nample 6.11 the Binomial distribution (deﬁned on discrete random vari-\\nables) and the Beta distribution (deﬁned on continuous random vari-\\nables).\\nExample 6.11 (Beta-Binomial Conjugacy)\\nConsider a Binomial random variable x\\x18Bin(N;\\x16)where\\np(xjN;\\x16) = \\nN\\nx!\\n\\x16x(1\\x00\\x16)N\\x00x; x = 0;1;:::;N; (6.102)\\nis the probability of ﬁnding xtimes the outcome “heads” in Ncoin ﬂips,\\nwhere\\x16is the probability of a “head”. We place a Beta prior on the pa-\\nrameter\\x16, that is,\\x16\\x18Beta(\\x0b;\\x0c), where\\np(\\x16j\\x0b;\\x0c) =\\x00(\\x0b+\\x0c)\\n\\x00(\\x0b)\\x00(\\x0c)\\x16\\x0b\\x001(1\\x00\\x16)\\x0c\\x001: (6.103)\\nIf we now observe some outcome x=h, that is, we see hheads inNcoin\\nﬂips, we compute the posterior distribution on \\x16as\\np(\\x16jx=h;N;\\x0b;\\x0c )/p(xjN;\\x16)p(\\x16j\\x0b;\\x0c) (6.104a)\\n/\\x16h(1\\x00\\x16)(N\\x00h)\\x16\\x0b\\x001(1\\x00\\x16)\\x0c\\x001(6.104b)\\n=\\x16h+\\x0b\\x001(1\\x00\\x16)(N\\x00h)+\\x0c\\x001(6.104c)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.6 Conjugacy and the Exponential Family 209\\nTable 6.2 Examples\\nof conjugate priors\\nfor common\\nlikelihood functions.Likelihood Conjugate prior Posterior\\nBernoulli Beta Beta\\nBinomial Beta Beta\\nGaussian Gaussian/inverse Gamma Gaussian/inverse Gamma\\nGaussian Gaussian/inverse Wishart Gaussian/inverse Wishart\\nMultinomial Dirichlet Dirichlet\\n/Beta(h+\\x0b;N\\x00h+\\x0c); (6.104d)\\ni.e., the posterior distribution is a Beta distribution as the prior, i.e., the\\nBeta prior is conjugate for the parameter \\x16in the Binomial likelihood\\nfunction.\\nIn the following example, we will derive a result that is similar to the\\nBeta-Binomial conjugacy result. Here we will show that the Beta distribu-\\ntion is a conjugate prior for the Bernoulli distribution.\\nExample 6.12 (Beta-Bernoulli Conjugacy)\\nLetx2f0;1gbe distributed according to the Bernoulli distribution with\\nparameter\\x122[0;1], that is,p(x= 1j\\x12) =\\x12. This can also be expressed\\nasp(xj\\x12) =\\x12x(1\\x00\\x12)1\\x00x. Let\\x12be distributed according to a Beta distri-\\nbution with parameters \\x0b;\\x0c, that is,p(\\x12j\\x0b;\\x0c)/\\x12\\x0b\\x001(1\\x00\\x12)\\x0c\\x001.\\nMultiplying the Beta and the Bernoulli distributions, we get\\np(\\x12jx;\\x0b;\\x0c ) =p(xj\\x12)p(\\x12j\\x0b;\\x0c) (6.105a)\\n/\\x12x(1\\x00\\x12)1\\x00x\\x12\\x0b\\x001(1\\x00\\x12)\\x0c\\x001(6.105b)\\n=\\x12\\x0b+x\\x001(1\\x00\\x12)\\x0c+(1\\x00x)\\x001(6.105c)\\n/p(\\x12j\\x0b+x;\\x0c+ (1\\x00x)): (6.105d)\\nThe last line is the Beta distribution with parameters (\\x0b+x;\\x0c+ (1\\x00x)).\\nTable 6.2 lists examples for conjugate priors for the parameters of some\\nstandard likelihoods used in probabilistic modeling. Distributions such as The Gamma prior is\\nconjugate for the\\nprecision (inverse\\nvariance) in the\\nunivariate Gaussian\\nlikelihood, and the\\nWishart prior is\\nconjugate for the\\nprecision matrix\\n(inverse covariance\\nmatrix) in the\\nmultivariate\\nGaussian likelihood.Multinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found\\nin any statistical text, and are described in Bishop (2006), for example.\\nThe Beta distribution is the conjugate prior for the parameter \\x16in both\\nthe Binomial and the Bernoulli likelihood. For a Gaussian likelihood func-\\ntion, we can place a conjugate Gaussian prior on the mean. The reason\\nwhy the Gaussian likelihood appears twice in the table is that we need\\nto distinguish the univariate from the multivariate case. In the univariate\\n(scalar) case, the inverse Gamma is the conjugate prior for the variance.\\nIn the multivariate case, we use a conjugate inverse Wishart distribution\\nas a prior on the covariance matrix. The Dirichlet distribution is the conju-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n210 Probability and Distributions\\ngate prior for the multinomial likelihood function. For further details, we\\nrefer to Bishop (2006).\\n6.6.2 Sufﬁcient Statistics\\nRecall that a statistic of a random variable is a deterministic function of\\nthat random variable. For example, if x= [x1;:::;xN]>is a vector of\\nunivariate Gaussian random variables, that is, xn\\x18N\\x00\\x16; \\x1b2\\x01\\n, then the\\nsample mean ^\\x16=1\\nN(x1+\\x01\\x01\\x01+xN)is a statistic. Sir Ronald Fisher dis-\\ncovered the notion of sufﬁcient statistics : the idea that there are statistics sufﬁcient statistics\\nthat will contain all available information that can be inferred from data\\ncorresponding to the distribution under consideration. In other words, suf-\\nﬁcient statistics carry all the information needed to make inference about\\nthe population, that is, they are the statistics that are sufﬁcient to repre-\\nsent the distribution.\\nFor a set of distributions parametrized by \\x12, letXbe a random variable\\nwith distribution p(xj\\x120)given an unknown \\x120. A vector\\x1e(x)of statistics\\nis called sufﬁcient statistics for \\x120if they contain all possible informa-\\ntion about\\x120. To be more formal about “contain all possible information”,\\nthis means that the probability of xgiven\\x12can be factored into a part\\nthat does not depend on \\x12, and a part that depends on \\x12only via\\x1e(x).\\nThe Fisher-Neyman factorization theorem formalizes this notion, which\\nwe state in Theorem 6.14 without proof.\\nTheorem 6.14 (Fisher-Neyman) .[Theorem 6.5 in Lehmann and Casella\\n(1998)] Let Xhave probability density function p(xj\\x12). Then the statistics Fisher-Neyman\\ntheorem \\x1e(x)are sufﬁcient for \\x12if and only if p(xj\\x12)can be written in the form\\np(xj\\x12) =h(x)g\\x12(\\x1e(x)); (6.106)\\nwhereh(x)is a distribution independent of \\x12andg\\x12captures all the depen-\\ndence on\\x12via sufﬁcient statistics \\x1e(x).\\nIfp(xj\\x12)does not depend on \\x12, then\\x1e(x)is trivially a sufﬁcient statistic\\nfor any function \\x1e. The more interesting case is that p(xj\\x12)is dependent\\nonly on\\x1e(x)and notxitself. In this case, \\x1e(x)is a sufﬁcient statistic for\\n\\x12.\\nIn machine learning, we consider a ﬁnite number of samples from a\\ndistribution. One could imagine that for simple distributions (such as the\\nBernoulli in Example 6.8) we only need a small number of samples to\\nestimate the parameters of the distributions. We could also consider the\\nopposite problem: If we have a set of data (a sample from an unknown\\ndistribution), which distribution gives the best ﬁt? A natural question to\\nask is, as we observe more data, do we need more parameters \\x12to de-\\nscribe the distribution? It turns out that the answer is yes in general, and\\nthis is studied in non-parametric statistics (Wasserman, 2007). A converse\\nquestion is to consider which class of distributions have ﬁnite-dimensional\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.6 Conjugacy and the Exponential Family 211\\nsufﬁcient statistics, that is the number of parameters needed to describe\\nthem does not increase arbitrarily. The answer is exponential family dis-\\ntributions, described in the following section.\\n6.6.3 Exponential Family\\nThere are three possible levels of abstraction we can have when con-\\nsidering distributions (of discrete or continuous random variables). At\\nlevel one (the most concrete end of the spectrum), we have a particu-\\nlar named distribution with ﬁxed parameters, for example a univariate\\nGaussianN\\x000;1\\x01\\nwith zero mean and unit variance. In machine learning,\\nwe often use the second level of abstraction, that is, we ﬁx the paramet-\\nric form (the univariate Gaussian) and infer the parameters from data. For\\nexample, we assume a univariate Gaussian N\\x00\\x16; \\x1b2\\x01\\nwith unknown mean\\n\\x16and unknown variance \\x1b2, and use a maximum likelihood ﬁt to deter-\\nmine the best parameters (\\x16;\\x1b2). We will see an example of this when\\nconsidering linear regression in Chapter 9. A third level of abstraction is\\nto consider families of distributions, and in this book, we consider the ex-\\nponential family. The univariate Gaussian is an example of a member of\\nthe exponential family. Many of the widely used statistical models, includ-\\ning all the “named” models in Table 6.2, are members of the exponential\\nfamily. They can all be uniﬁed into one concept (Brown, 1986).\\nRemark. A brief historical anecdote: Like many concepts in mathemat-\\nics and science, exponential families were independently discovered at\\nthe same time by different researchers. In the years 1935–1936, Edwin\\nPitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in\\nNew York independently showed that the exponential families are the only\\nfamilies that enjoy ﬁnite-dimensional sufﬁcient statistics under repeated\\nindependent sampling (Lehmann and Casella, 1998). }\\nAnexponential family is a family of probability distributions, parame- exponential family\\nterized by\\x122RD, of the form\\np(xj\\x12) =h(x) exp (h\\x12;\\x1e(x)i\\x00A(\\x12)); (6.107)\\nwhere\\x1e(x)is the vector of sufﬁcient statistics. In general, any inner prod-\\nuct (Section 3.2) can be used in (6.107), and for concreteness we will use\\nthe standard dot product here ( h\\x12;\\x1e(x)i=\\x12>\\x1e(x)). Note that the form\\nof the exponential family is essentially a particular expression of g\\x12(\\x1e(x))\\nin the Fisher-Neyman theorem (Theorem 6.14).\\nThe factorh(x)can be absorbed into the dot product term by adding\\nanother entry ( logh(x)) to the vector of sufﬁcient statistics \\x1e(x), and\\nconstraining the corresponding parameter \\x120= 1. The termA(\\x12)is the\\nnormalization constant that ensures that the distribution sums up or inte-\\ngrates to one and is called the log-partition function . A good intuitive no- log-partition\\nfunction tion of exponential families can be obtained by ignoring these two terms\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n212 Probability and Distributions\\nand considering exponential families as distributions of the form\\np(xj\\x12)/exp\\x00\\x12>\\x1e(x)\\x01: (6.108)\\nFor this form of parametrization, the parameters \\x12are called the natural natural parameters\\nparameters . At ﬁrst glance, it seems that exponential families are a mun-\\ndane transformation by adding the exponential function to the result of a\\ndot product. However, there are many implications that allow for conve-\\nnient modeling and efﬁcient computation based on the fact that we can\\ncapture information about data in \\x1e(x).\\nExample 6.13 (Gaussian as Exponential Family)\\nConsider the univariate Gaussian distribution N\\x00\\x16; \\x1b2\\x01\\n. Let\\x1e(x) =\\x14x\\nx2\\x15\\n.\\nThen by using the deﬁnition of the exponential family,\\np(xj\\x12)/exp(\\x121x+\\x122x2): (6.109)\\nSetting\\n\\x12=\\x14\\x16\\n\\x1b2;\\x001\\n2\\x1b2\\x15>\\n(6.110)\\nand substituting into (6.109), we obtain\\np(xj\\x12)/exp\\x12\\x16x\\n\\x1b2\\x00x2\\n2\\x1b2\\x13\\n/exp\\x12\\n\\x001\\n2\\x1b2(x\\x00\\x16)2\\x13\\n: (6.111)\\nTherefore, the univariate Gaussian distribution is a member of the expo-\\nnential family with sufﬁcient statistic \\x1e(x) =\\x14x\\nx2\\x15\\n, and natural parame-\\nters given by\\x12in (6.110).\\nExample 6.14 (Bernoulli as Exponential Family)\\nRecall the Bernoulli distribution from Example 6.8\\np(xj\\x16) =\\x16x(1\\x00\\x16)1\\x00x; x2f0;1g: (6.112)\\nThis can be written in exponential family form\\np(xj\\x16) = exp\\x02log\\x00\\x16x(1\\x00\\x16)1\\x00x\\x01\\x03\\n(6.113a)\\n= exp [xlog\\x16+ (1\\x00x) log(1\\x00\\x16)] (6.113b)\\n= exp [xlog\\x16\\x00xlog(1\\x00\\x16) + log(1\\x00\\x16)] (6.113c)\\n= exph\\nxlog\\x16\\n1\\x00\\x16+ log(1\\x00\\x16)i\\n: (6.113d)\\nThe last line (6.113d) can be identiﬁed as being in exponential family\\nform (6.107) by observing that\\nh(x) = 1 (6.114)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.6 Conjugacy and the Exponential Family 213\\n\\x12= log\\x16\\n1\\x00\\x16(6.115)\\n\\x1e(x) =x (6.116)\\nA(\\x12) =\\x00log(1\\x00\\x16) = log(1 + exp( \\x12)): (6.117)\\nThe relationship between \\x12and\\x16is invertible so that\\n\\x16=1\\n1 + exp(\\x00\\x12): (6.118)\\nThe relation (6.118) is used to obtain the right equality of (6.117).\\nRemark. The relationship between the original Bernoulli parameter \\x16and\\nthe natural parameter \\x12is known as the sigmoid or logistic function. Ob- sigmoid\\nserve that\\x162(0;1)but\\x122R, and therefore the sigmoid function\\nsqueezes a real value into the range (0;1). This property is useful in ma-\\nchine learning, for example it is used in logistic regression (Bishop, 2006,\\nsection 4.3.2), as well as as a nonlinear activation functions in neural net-\\nworks (Goodfellow et al., 2016, chapter 6). }\\nIt is often not obvious how to ﬁnd the parametric form of the conjugate\\ndistribution of a particular distribution (for example, those in Table 6.2).\\nExponential families provide a convenient way to ﬁnd conjugate pairs of\\ndistributions. Consider the random variable Xis a member of the expo-\\nnential family (6.107):\\np(xj\\x12) =h(x) exp (h\\x12;\\x1e(x)i\\x00A(\\x12)): (6.119)\\nEvery member of the exponential family has a conjugate prior (Brown,\\n1986)\\np(\\x12j\\r) =hc(\\x12) exp\\x12\\x1c\\x14\\r1\\n\\r2\\x15\\n;\\x14\\x12\\n\\x00A(\\x12)\\x15\\x1d\\n\\x00Ac(\\r)\\x13\\n; (6.120)\\nwhere\\r=\\x14\\r1\\n\\r2\\x15\\nhas dimension dim(\\x12) + 1 . The sufﬁcient statistics of\\nthe conjugate prior are\\x14\\x12\\n\\x00A(\\x12)\\x15\\n. By using the knowledge of the general\\nform of conjugate priors for exponential families, we can derive functional\\nforms of conjugate priors corresponding to particular distributions.\\nExample 6.15\\nRecall the exponential family form of the Bernoulli distribution (6.113d)\\np(xj\\x16) = exp\\x14\\nxlog\\x16\\n1\\x00\\x16+ log(1\\x00\\x16)\\x15\\n: (6.121)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n214 Probability and Distributions\\nThe canonical conjugate prior has the form\\np(\\x16j\\x0b;\\x0c) =\\x16\\n1\\x00\\x16exp\\x14\\n\\x0blog\\x16\\n1\\x00\\x16+ (\\x0c+\\x0b) log(1\\x00\\x16)\\x00Ac(\\r)\\x15\\n;\\n(6.122)\\nwhere we deﬁned \\r:= [\\x0b;\\x0c+\\x0b]>andhc(\\x16) :=\\x16=(1\\x00\\x16). Equa-\\ntion (6.122) then simpliﬁes to\\np(\\x16j\\x0b;\\x0c) = exp [(\\x0b\\x001) log\\x16+ (\\x0c\\x001) log(1\\x00\\x16)\\x00Ac(\\x0b;\\x0c)]:\\n(6.123)\\nPutting this in non-exponential family form yields\\np(\\x16j\\x0b;\\x0c)/\\x16\\x0b\\x001(1\\x00\\x16)\\x0c\\x001; (6.124)\\nwhich we identify as the Beta distribution (6.98). In example 6.12, we\\nassumed that the Beta distribution is the conjugate prior of the Bernoulli\\ndistribution and showed that it was indeed the conjugate prior. In this\\nexample, we derived the form of the Beta distribution by looking at the\\ncanonical conjugate prior of the Bernoulli distribution in exponential fam-\\nily form.\\nAs mentioned in the previous section, the main motivation for expo-\\nnential families is that they have ﬁnite-dimensional sufﬁcient statistics.\\nAdditionally, conjugate distributions are easy to write down, and the con-\\njugate distributions also come from an exponential family. From an infer-\\nence perspective, maximum likelihood estimation behaves nicely because\\nempirical estimates of sufﬁcient statistics are optimal estimates of the pop-\\nulation values of sufﬁcient statistics (recall the mean and covariance of a\\nGaussian). From an optimization perspective, the log-likelihood function\\nis concave, allowing for efﬁcient optimization approaches to be applied\\n(Chapter 7).\\n6.7 Change of Variables/Inverse Transform\\nIt may seem that there are very many known distributions, but in reality\\nthe set of distributions for which we have names is quite limited. There-\\nfore, it is often useful to understand how transformed random variables\\nare distributed. For example, assuming that Xis a random variable dis-\\ntributed according to the univariate normal distribution N\\x000;1\\x01\\n, what is\\nthe distribution of X2? Another example, which is quite common in ma-\\nchine learning, is, given that X1andX2are univariate standard normal,\\nwhat is the distribution of1\\n2(X1+X2)?\\nOne option to work out the distribution of1\\n2(X1+X2)is to calculate\\nthe mean and variance of X1andX2and then combine them. As we saw\\nin Section 6.4.4, we can calculate the mean and variance of resulting ran-\\ndom variables when we consider afﬁne transformations of random vari-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.7 Change of Variables/Inverse Transform 215\\nables. However, we may not be able to obtain the functional form of the\\ndistribution under transformations. Furthermore, we may be interested\\nin nonlinear transformations of random variables for which closed-form\\nexpressions are not readily available.\\nRemark (Notation) .In this section, we will be explicit about random vari-\\nables and the values they take. Hence, recall that we use capital letters\\nX;Y to denote random variables and small letters x;yto denote the val-\\nues in the target space Tthat the random variables take. We will explicitly\\nwrite pmfs of discrete random variables XasP(X=x). For continuous\\nrandom variables X(Section 6.2.2), the pdf is written as f(x)and the cdf\\nis written as FX(x). }\\nWe will look at two approaches for obtaining distributions of transfor-\\nmations of random variables: a direct approach using the deﬁnition of a\\ncumulative distribution function and a change-of-variable approach that\\nuses the chain rule of calculus (Section 5.2.2). The change-of-variable ap- Moment generating\\nfunctions can also\\nbe used to study\\ntransformations of\\nrandom\\nvariables (Casella\\nand Berger, 2002,\\nchapter 2).proach is widely used because it provides a “recipe” for attempting to\\ncompute the resulting distribution due to a transformation. We will ex-\\nplain the techniques for univariate random variables, and will only brieﬂy\\nprovide the results for the general case of multivariate random variables.\\nTransformations of discrete random variables can be understood di-\\nrectly. Suppose that there is a discrete random variable Xwith pmfP(X=\\nx)(Section 6.2.1), and an invertible function U(x). Consider the trans-\\nformed random variable Y:=U(X), with pmfP(Y=y). Then\\nP(Y=y) =P(U(X) =y) transformation of interest (6.125a)\\n=P(X=U\\x001(y)) inverse (6.125b)\\nwhere we can observe that x=U\\x001(y). Therefore, for discrete random\\nvariables, transformations directly change the individual events (with the\\nprobabilities appropriately transformed).\\n6.7.1 Distribution Function Technique\\nThe distribution function technique goes back to ﬁrst principles, and uses\\nthe deﬁnition of a cdf FX(x) =P(X6x)and the fact that its differential\\nis the pdff(x)(Wasserman, 2004, chapter 2). For a random variable X\\nand a function U, we ﬁnd the pdf of the random variable Y:=U(X)by\\n1. Finding the cdf:\\nFY(y) =P(Y6y) (6.126)\\n2. Differentiating the cdf FY(y)to get the pdf f(y).\\nf(y) =d\\ndyFY(y): (6.127)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n216 Probability and Distributions\\nWe also need to keep in mind that the domain of the random variable may\\nhave changed due to the transformation by U.\\nExample 6.16\\nLetXbe a continuous random variable with probability density function\\non06x61\\nf(x) = 3x2: (6.128)\\nWe are interested in ﬁnding the pdf of Y=X2.\\nThe function fis an increasing function of x, and therefore the resulting\\nvalue ofylies in the interval [0;1]. We obtain\\nFY(y) =P(Y6y) de\\x0cnition of cdf (6.129a)\\n=P(X26y) transformation of interest (6.129b)\\n=P(X6y1\\n2) inverse (6.129c)\\n=FX(y1\\n2) de\\x0cnition of cdf (6.129d)\\n=Zy1\\n2\\n03t2dt cdf as a de\\x0cnite integral (6.129e)\\n=\\x02t3\\x03t=y1\\n2\\nt=0result of integration (6.129f)\\n=y3\\n2;06y61: (6.129g)\\nTherefore, the cdf of Yis\\nFY(y) =y3\\n2 (6.130)\\nfor06y61. To obtain the pdf, we differentiate the cdf\\nf(y) =d\\ndyFY(y) =3\\n2y1\\n2 (6.131)\\nfor06y61.\\nIn Example 6.16, we considered a strictly monotonically increasing func-\\ntionf(x) = 3x2. This means that we could compute an inverse function. Functions that have\\ninverses are called\\nbijective functions\\n(Section 2.7).In general, we require that the function of interest y=U(x)has an in-\\nversex=U\\x001(y). A useful result can be obtained by considering the cu-\\nmulative distribution function FX(x)of a random variable X, and using\\nit as the transformation U(x). This leads to the following theorem.\\nTheorem 6.15. [Theorem 2.1.10 in Casella and Berger (2002)] Let Xbe a\\ncontinuous random variable with a strictly monotonic cumulative distribu-\\ntion function FX(x). Then the random variable Ydeﬁned as\\nY:=FX(X) (6.132)\\nhas a uniform distribution.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.7 Change of Variables/Inverse Transform 217\\nTheorem 6.15 is known as the probability integral transform , and it is probability integral\\ntransform used to derive algorithms for sampling from distributions by transforming\\nthe result of sampling from a uniform random variable (Bishop, 2006).\\nThe algorithm works by ﬁrst generating a sample from a uniform distribu-\\ntion, then transforming it by the inverse cdf (assuming this is available)\\nto obtain a sample from the desired distribution. The probability integral\\ntransform is also used for hypothesis testing whether a sample comes from\\na particular distribution (Lehmann and Romano, 2005). The idea that the\\noutput of a cdf gives a uniform distribution also forms the basis of copu-\\nlas (Nelsen, 2006).\\n6.7.2 Change of Variables\\nThe distribution function technique in Section 6.7.1 is derived from ﬁrst\\nprinciples, based on the deﬁnitions of cdfs and using properties of in-\\nverses, differentiation, and integration. This argument from ﬁrst principles\\nrelies on two facts:\\n1. We can transform the cdf of Yinto an expression that is a cdf of X.\\n2. We can differentiate the cdf to obtain the pdf.\\nLet us break down the reasoning step by step, with the goal of understand-\\ning the more general change-of-variables approach in Theorem 6.16. Change of variables\\nin probability relies\\non the\\nchange-of-variables\\nmethod in\\ncalculus (Tandra,\\n2014).Remark. The name “change of variables” comes from the idea of chang-\\ning the variable of integration when faced with a difﬁcult integral. For\\nunivariate functions, we use the substitution rule of integration,\\nZ\\nf(g(x))g0(x)dx=Z\\nf(u)du; whereu=g(x): (6.133)\\nThe derivation of this rule is based on the chain rule of calculus (5.32) and\\nby applying twice the fundamental theorem of calculus. The fundamental\\ntheorem of calculus formalizes the fact that integration and differentiation\\nare somehow “inverses” of each other. An intuitive understanding of the\\nrule can be obtained by thinking (loosely) about small changes (differen-\\ntials) to the equation u=g(x), that is by considering \\x01u=g0(x)\\x01xas a\\ndifferential of u=g(x). By substituting u=g(x), the argument inside the\\nintegral on the right-hand side of (6.133) becomes f(g(x)). By pretending\\nthat the term ducan be approximated by du\\x19\\x01u=g0(x)\\x01x, and that\\ndx\\x19\\x01x, we obtain (6.133). }\\nConsider a univariate random variable X, and an invertible function\\nU, which gives us another random variable Y=U(X). We assume that\\nrandom variable Xhas statesx2[a;b]. By the deﬁnition of the cdf, we\\nhave\\nFY(y) =P(Y6y): (6.134)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n218 Probability and Distributions\\nWe are interested in a function Uof the random variable\\nP(Y6y) =P(U(X)6y); (6.135)\\nwhere we assume that the function Uis invertible. An invertible function\\non an interval is either strictly increasing or strictly decreasing. In the case\\nthatUis strictly increasing, then its inverse U\\x001is also strictly increasing.\\nBy applying the inverse U\\x001to the arguments of P(U(X)6y), we obtain\\nP(U(X)6y) =P(U\\x001(U(X))6U\\x001(y)) =P(X6U\\x001(y)):\\n(6.136)\\nThe right-most term in (6.136) is an expression of the cdf of X. Recall the\\ndeﬁnition of the cdf in terms of the pdf\\nP(X6U\\x001(y)) =ZU\\x001(y)\\naf(x)dx: (6.137)\\nNow we have an expression of the cdf of Yin terms ofx:\\nFY(y) =ZU\\x001(y)\\naf(x)dx: (6.138)\\nTo obtain the pdf, we differentiate (6.138) with respect to y:\\nf(y) =d\\ndyFy(y) =d\\ndyZU\\x001(y)\\naf(x)dx: (6.139)\\nNote that the integral on the right-hand side is with respect to x, but we\\nneed an integral with respect to ybecause we are differentiating with\\nrespect toy. In particular, we use (6.133) to get the substitution\\nZ\\nf(U\\x001(y))U\\x0010(y)dy=Z\\nf(x)dxwherex=U\\x001(y):(6.140)\\nUsing (6.140) on the right-hand side of (6.139) gives us\\nf(y) =d\\ndyZU\\x001(y)\\nafx(U\\x001(y))U\\x0010(y)dy: (6.141)\\nWe then recall that differentiation is a linear operator and we use the\\nsubscriptxto remind ourselves that fx(U\\x001(y))is a function of xand not\\ny. Invoking the fundamental theorem of calculus again gives us\\nf(y) =fx(U\\x001(y))\\x01\\x12d\\ndyU\\x001(y)\\x13\\n: (6.142)\\nRecall that we assumed that Uis a strictly increasing function. For decreas-\\ning functions, it turns out that we have a negative sign when we follow\\nthe same derivation. We introduce the absolute value of the differential to\\nhave the same expression for both increasing and decreasing U:\\nf(y) =fx(U\\x001(y))\\x01\\x0c\\x0c\\x0c\\x0cd\\ndyU\\x001(y)\\x0c\\x0c\\x0c\\x0c: (6.143)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.7 Change of Variables/Inverse Transform 219\\nThis is called the change-of-variable technique . The term\\x0c\\x0c\\x0cd\\ndyU\\x001(y)\\x0c\\x0c\\x0cin change-of-variable\\ntechnique(6.143) measures how much a unit volume changes when applying U\\n(see also the deﬁnition of the Jacobian in Section 5.3).\\nRemark. In comparison to the discrete case in (6.125b), we have an addi-\\ntional factor\\x0c\\x0c\\x0cd\\ndyU\\x001(y)\\x0c\\x0c\\x0c. The continuous case requires more care because\\nP(Y=y) = 0 for ally. The probability density function f(y)does not\\nhave a description as a probability of an event involving y.}\\nSo far in this section, we have been studying univariate change of vari-\\nables. The case for multivariate random variables is analogous, but com-\\nplicated by fact that the absolute value cannot be used for multivariate\\nfunctions. Instead, we use the determinant of the Jacobian matrix. Recall\\nfrom (5.58) that the Jacobian is a matrix of partial derivatives, and that\\nthe existence of a nonzero determinant shows that we can invert the Ja-\\ncobian. Recall the discussion in Section 4.1 that the determinant arises\\nbecause our differentials (cubes of volume) are transformed into paral-\\nlelepipeds by the Jacobian. Let us summarize preceding the discussion in\\nthe following theorem, which gives us a recipe for multivariate change of\\nvariables.\\nTheorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f(x)be the value\\nof the probability density of the multivariate continuous random variable X.\\nIf the vector-valued function y=U(x)is differentiable and invertible for\\nall values within the domain of x, then for corresponding values of y, the\\nprobability density of Y=U(X)is given by\\nf(y) =fx(U\\x001(y))\\x01\\x0c\\x0c\\x0c\\x0cdet\\x12@\\n@yU\\x001(y)\\x13\\x0c\\x0c\\x0c\\x0c: (6.144)\\nThe theorem looks intimidating at ﬁrst glance, but the key point is that\\na change of variable of a multivariate random variable follows the pro-\\ncedure of the univariate change of variable. First we need to work out\\nthe inverse transform, and substitute that into the density of x. Then we\\ncalculate the determinant of the Jacobian and multiply the result. The\\nfollowing example illustrates the case of a bivariate random variable.\\nExample 6.17\\nConsider a bivariate random variable Xwith statesx=\\x14x1\\nx2\\x15\\nand proba-\\nbility density function\\nf\\x12\\x14x1\\nx2\\x15\\x13\\n=1\\n2\\x19exp \\n\\x001\\n2\\x14x1\\nx2\\x15>\\x14x1\\nx2\\x15!\\n: (6.145)\\nWe use the change-of-variable technique from Theorem 6.16 to derive the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n220 Probability and Distributions\\neffect of a linear transformation (Section 2.7) of the random variable.\\nConsider a matrix A2R2\\x022deﬁned as\\nA=\\x14a b\\nc d\\x15\\n: (6.146)\\nWe are interested in ﬁnding the probability density function of the trans-\\nformed bivariate random variable Ywith statesy=Ax.\\nRecall that for change of variables we require the inverse transformation\\nofxas a function of y. Since we consider linear transformations, the\\ninverse transformation is given by the matrix inverse (see Section 2.2.2).\\nFor2\\x022matrices, we can explicitly write out the formula, given by\\n\\x14x1\\nx2\\x15\\n=A\\x001\\x14y1\\ny2\\x15\\n=1\\nad\\x00bc\\x14d\\x00b\\n\\x00c a\\x15\\x14y1\\ny2\\x15\\n: (6.147)\\nObserve that ad\\x00bcis the determinant (Section 4.1) of A. The corre-\\nsponding probability density function is given by\\nf(x) =f(A\\x001y) =1\\n2\\x19exp\\x10\\n\\x001\\n2y>A\\x00>A\\x001y\\x11\\n: (6.148)\\nThe partial derivative of a matrix times a vector with respect to the vector\\nis the matrix itself (Section 5.5), and therefore\\n@\\n@yA\\x001y=A\\x001: (6.149)\\nRecall from Section 4.1 that the determinant of the inverse is the inverse\\nof the determinant so that the determinant of the Jacobian matrix is\\ndet\\x12@\\n@yA\\x001y\\x13\\n=1\\nad\\x00bc: (6.150)\\nWe are now able to apply the change-of-variable formula from Theo-\\nrem 6.16 by multiplying (6.148) with (6.150), which yields\\nf(y) =f(x)\\x0c\\x0c\\x0c\\x0cdet\\x12@\\n@yA\\x001y\\x13\\x0c\\x0c\\x0c\\x0c(6.151a)\\n=1\\n2\\x19exp\\x10\\n\\x001\\n2y>A\\x00>A\\x001y\\x11\\njad\\x00bcj\\x001: (6.151b)\\nWhile Example 6.17 is based on a bivariate random variable, which al-\\nlows us to easily compute the matrix inverse, the preceding relation holds\\nfor higher dimensions.\\nRemark. We saw in Section 6.5 that the density f(x)in (6.148) is actually\\nthe standard Gaussian distribution, and the transformed density f(y)is a\\nbivariate Gaussian with covariance \\x06=AA>. }\\nWe will use the ideas in this chapter to describe probabilistic modeling\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n6.8 Further Reading 221\\nin Section 8.4, as well as introduce a graphical language in Section 8.5. We\\nwill see direct machine learning applications of these ideas in Chapters 9\\nand 11.\\n6.8 Further Reading\\nThis chapter is rather terse at times. Grinstead and Snell (1997) and\\nWalpole et al. (2011) provide more relaxed presentations that are suit-\\nable for self-study. Readers interested in more philosophical aspects of\\nprobability should consider Hacking (2001), whereas an approach that\\nis more related to software engineering is presented by Downey (2014).\\nAn overview of exponential families can be found in Barndorff-Nielsen\\n(2014). We will see more about how to use probability distributions to\\nmodel machine learning tasks in Chapter 8. Ironically, the recent surge\\nin interest in neural networks has resulted in a broader appreciation of\\nprobabilistic models. For example, the idea of normalizing ﬂows (Jimenez\\nRezende and Mohamed, 2015) relies on change of variables for transform-\\ning random variables. An overview of methods for variational inference as\\napplied to neural networks is described in chapters 16 to 20 of the book\\nby Goodfellow et al. (2016).\\nWe side stepped a large part of the difﬁculty in continuous random vari-\\nables by avoiding measure theoretic questions (Billingsley, 1995; Pollard,\\n2002), and by assuming without construction that we have real numbers,\\nand ways of deﬁning sets on real numbers as well as their appropriate fre-\\nquency of occurrence. These details do matter, for example, in the speciﬁ-\\ncation of conditional probability p(yjx)for continuous random variables\\nx;y(Proschan and Presnell, 1998). The lazy notation hides the fact that\\nwe want to specify that X=x(which is a set of measure zero). Fur-\\nthermore, we are interested in the probability density function of y. A\\nmore precise notation would have to say Ey[f(y)j\\x1b(x)], where we take\\nthe expectation over yof a test function fconditioned on the \\x1b-algebra of\\nx. A more technical audience interested in the details of probability the-\\nory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter,\\n2004; Grimmett and Welsh, 2014), including some very technical discus-\\nsions (Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel\\nand Doksum, 2006; C ¸inlar, 2011). An alternative way to approach proba-\\nbility is to start with the concept of expectation, and “work backward” to\\nderive the necessary properties of a probability space (Whittle, 2000). As\\nmachine learning allows us to model more intricate distributions on ever\\nmore complex types of data, a developer of probabilistic machine learn-\\ning models would have to understand these more technical aspects. Ma-\\nchine learning texts with a probabilistic modeling focus include the books\\nby MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Bar-\\nber (2012); Murphy (2012).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n222 Probability and Distributions\\nExercises\\n6.1 Consider the following bivariate distribution p(x;y)of two discrete random\\nvariablesXandY.\\nXx1x2x3x4x5Y\\ny3y2y10.01 0.02 0.03 0.1 0.1\\n0.05 0.1 0.05 0.07 0.2\\n0.1 0.05 0.03 0.05 0.04\\nCompute:\\na. The marginal distributions p(x)andp(y).\\nb. The conditional distributions p(xjY=y1)andp(yjX=x3).\\n6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4),\\n0:4N\\x12\\x14\\n10\\n2\\x15\\n;\\x14\\n1 0\\n0 1\\x15\\x13\\n+ 0:6N\\x12\\x14\\n0\\n0\\x15\\n;\\x14\\n8:4 2:0\\n2:0 1:7\\x15\\x13\\n:\\na. Compute the marginal distributions for each dimension.\\nb. Compute the mean, mode and median for each marginal distribution.\\nc. Compute the mean and mode for the two-dimensional distribution.\\n6.3 You have written a computer program that sometimes compiles and some-\\ntimes not (code does not change). You decide to model the apparent stochas-\\nticity (success vs. no success) xof the compiler using a Bernoulli distribution\\nwith parameter \\x16:\\np(xj\\x16) =\\x16x(1\\x00\\x16)1\\x00x; x2f0;1g:\\nChoose a conjugate prior for the Bernoulli likelihood and compute the pos-\\nterior distribution p(\\x16jx1;:::;xN).\\n6.4 There are two bags. The ﬁrst bag contains four mangos and two apples; the\\nsecond bag contains four mangos and four apples.\\nWe also have a biased coin, which shows “heads” with probability 0.6 and\\n“tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at\\nrandom from bag 1; otherwise we pick a fruit at random from bag 2.\\nYour friend ﬂips the coin (you cannot see the result), picks a fruit at random\\nfrom the corresponding bag, and presents you a mango.\\nWhat is the probability that the mango was picked from bag 2?\\nHint: Use Bayes’ theorem.\\n6.5 Consider the time-series model\\nxt+1=Axt+w;w\\x18N\\x00\\n0;Q\\x01\\nyt=Cxt+v;v\\x18N\\x00\\n0;R\\x01\\n;\\nwherew;vare i.i.d. Gaussian noise variables. Further, assume that p(x0) =\\nN\\x00\\n\\x160;\\x060\\x01.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 223\\na. What is the form of p(x0;x1;:::;xT)? Justify your answer (you do not\\nhave to explicitly compute the joint distribution).\\nb. Assume that p(xtjy1;:::;yt) =N\\x00\\n\\x16t;\\x06t\\x01.\\n1. Compute p(xt+1jy1;:::;yt).\\n2. Compute p(xt+1;yt+1jy1;:::;yt).\\n3. At timet+1, we observe the value yt+1=^y. Compute the conditional\\ndistribution p(xt+1jy1;:::;yt+1).\\n6.6 Prove the relationship in (6.44), which relates the standard deﬁnition of the\\nvariance to the raw-score expression for the variance.\\n6.7 Prove the relationship in (6.45), which relates the pairwise difference be-\\ntween examples in a dataset with the raw-score expression for the variance.\\n6.8 Express the Bernoulli distribution in the natural parameter form of the ex-\\nponential family, see (6.107).\\n6.9 Express the Binomial distribution as an exponential family distribution. Also\\nexpress the Beta distribution is an exponential family distribution. Show that\\nthe product of the Beta and the Binomial distribution is also a member of\\nthe exponential family.\\n6.10 Derive the relationship in Section 6.5.2 in two ways:\\na. By completing the square\\nb. By expressing the Gaussian in its exponential family form\\nTheproduct of two Gaussians N\\x00\\nxja;A\\x01\\nN\\x00\\nxjb;B\\x01is an unnormalized\\nGaussian distribution cN\\x00\\nxjc;C\\x01with\\nC= (A\\x001+B\\x001)\\x001\\nc=C(A\\x001a+B\\x001b)\\nc= (2\\x19)\\x00D\\n2jA+Bj\\x001\\n2exp\\x00\\n\\x001\\n2(a\\x00b)>(A+B)\\x001(a\\x00b)\\x01\\n:\\nNote that the normalizing constant citself can be considered a (normalized)\\nGaussian distribution either in aor inbwith an “inﬂated” covariance matrix\\nA+B, i.e.,c=N\\x00\\najb;A+B\\x01\\n=N\\x00\\nbja;A+B\\x01.\\n6.11 Iterated Expectations.\\nConsider two random variables x,ywith joint distribution p(x;y). Show that\\nEX[x] =EY\\x02EX[xjy]\\x03\\n:\\nHere,EX[xjy]denotes the expected value of xunder the conditional distri-\\nbutionp(xjy).\\n6.12 Manipulation of Gaussian Random Variables.\\nConsider a Gaussian random variable x\\x18N\\x00\\nxj\\x16x;\\x06x\\x01, wherex2RD.\\nFurthermore, we have\\ny=Ax+b+w;\\nwherey2RE,A2RE\\x02D,b2RE, andw\\x18N\\x00\\nwj0;Q\\x01is indepen-\\ndent Gaussian noise. “Independent” implies that xandware independent\\nrandom variables and that Qis diagonal.\\na. Write down the likelihood p(yjx).\\nb. The distribution p(y) =R\\np(yjx)p(x)dxis Gaussian. Compute the mean\\n\\x16yand the covariance \\x06y. Derive your result in detail.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n224 Probability and Distributions\\nc. The random variable yis being transformed according to the measure-\\nment mapping\\nz=Cy+v;\\nwherez2RF,C2RF\\x02E, andv\\x18N\\x00\\nvj0;R\\x01is independent Gaus-\\nsian (measurement) noise.\\nWrite down p(zjy).\\nComputep(z), i.e., the mean \\x16zand the covariance \\x06z. Derive your\\nresult in detail.\\nd. Now, a value ^yis measured. Compute the posterior distribution p(xj^y).\\nHint for solution: This posterior is also Gaussian, i.e., we need to de-\\ntermine only its mean and covariance matrix. Start by explicitly com-\\nputing the joint Gaussian p(x;y). This also requires us to compute the\\ncross-covariances Covx;y[x;y]and Covy;x[y;x]. Then apply the rules\\nfor Gaussian conditioning.\\n6.13 Probability Integral Transformation\\nGiven a continuous random variable X, with cdfFX(x), show that the ran-\\ndom variable Y:=FX(X)is uniformly distributed (Theorem 6.15).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7\\nContinuous Optimization\\nSince machine learning algorithms are implemented on a computer, the\\nmathematical formulations are expressed as numerical optimization meth-\\nods. This chapter describes the basic numerical methods for training ma-\\nchine learning models. Training a machine learning model often boils\\ndown to ﬁnding a good set of parameters. The notion of “good” is de-\\ntermined by the objective function or the probabilistic model, which we\\nwill see examples of in the second part of this book. Given an objective\\nfunction, ﬁnding the best value is done using optimization algorithms. Since we consider\\ndata and models in\\nRD, the\\noptimization\\nproblems we face\\narecontinuous\\noptimization\\nproblems, as\\nopposed to\\ncombinatorial\\noptimization\\nproblems for\\ndiscrete variables.This chapter covers two main branches of continuous optimization (Fig-\\nure 7.1): unconstrained and constrained optimization. We will assume in\\nthis chapter that our objective function is differentiable (see Chapter 5),\\nhence we have access to a gradient at each location in the space to help us\\nﬁnd the optimum value. By convention, most objective functions in ma-\\nchine learning are intended to be minimized, that is, the best value is the\\nminimum value. Intuitively ﬁnding the best value is like ﬁnding the val-\\nleys of the objective function, and the gradients point us uphill. The idea is\\nto move downhill (opposite to the gradient) and hope to ﬁnd the deepest\\npoint. For unconstrained optimization, this is the only concept we need,\\nbut there are several design choices, which we discuss in Section 7.1. For\\nconstrained optimization, we need to introduce other concepts to man-\\nage the constraints (Section 7.2). We will also introduce a special class\\nof problems (convex optimization problems in Section 7.3) where we can\\nmake statements about reaching the global optimum.\\nConsider the function in Figure 7.2. The function has a global minimum global minimum\\naroundx=\\x004:5, with a function value of approximately \\x0047. Since\\nthe function is “smooth,” the gradients can be used to help ﬁnd the min-\\nimum by indicating whether we should take a step to the right or left.\\nThis assumes that we are in the correct bowl, as there exists another local local minimum\\nminimum aroundx= 0:7. Recall that we can solve for all the stationary\\npoints of a function by calculating its derivative and setting it to zero. For Stationary points\\nare the real roots of\\nthe derivative, that\\nis, points that have\\nzero gradient.`(x) =x4+ 7x3+ 5x2\\x0017x+ 3; (7.1)\\nwe obtain the corresponding gradient as\\nd`(x)\\ndx= 4x3+ 21x2+ 10x\\x0017: (7.2)\\n225\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n226 Continuous Optimization\\nFigure 7.1 A mind\\nmap of the concepts\\nrelated to\\noptimization, as\\npresented in this\\nchapter. There are\\ntwo main ideas:\\ngradient descent\\nand convex\\noptimization.Continuous\\noptimization\\nUnconstrained\\noptimization\\nConstrained\\noptimizationGradient descentStepsize\\nMomentum\\nStochastic\\ngradient\\ndescent\\nLagrange\\nmultipliers\\nConvex optimization\\n& dualityConvex\\nConvex conjugateLinear\\nprogramming\\nQuadratic\\nprogrammingChapter 10\\nDimension reduc.\\nChapter 11\\nDensity estimation\\nChapter 12\\nClassiﬁcation\\nSince this is a cubic equation, it has in general three solutions when set to\\nzero. In the example, two of them are minimums and one is a maximum\\n(aroundx=\\x001:4). To check whether a stationary point is a minimum\\nor maximum, we need to take the derivative a second time and check\\nwhether the second derivative is positive or negative at the stationary\\npoint. In our case, the second derivative is\\nd2`(x)\\ndx2= 12x2+ 42x+ 10: (7.3)\\nBy substituting our visually estimated values of x=\\x004:5;\\x001:4;0:7;we\\nwill observe that as expected the middle point is a maximum\\x10\\nd2`(x)\\ndx2<0\\x11\\nand the other two stationary points are minimums.\\nNote that we have avoided analytically solving for values of xin the\\nprevious discussion, although for low-order polynomials such as the pre-\\nceding we could do so. In general, we are unable to ﬁnd analytic solu-\\ntions, and hence we need to start at some value, say x0=\\x006, and follow\\nthe negative gradient. The negative gradient indicates that we should go\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.1 Optimization Using Gradient Descent 227\\nFigure 7.2 Example\\nobjective function.\\nNegative gradients\\nare indicated by\\narrows, and the\\nglobal minimum is\\nindicated by the\\ndashed blue line.\\n−6−5−4−3−2−1 0 1 2\\nValue of parameter−60−40−200204060Objectivex4+ 7x3+ 5x2−17x+ 3\\nright, but not how far (this is called the step-size). Furthermore, if we According to the\\nAbel–Rufﬁni\\ntheorem, there is in\\ngeneral no algebraic\\nsolution for\\npolynomials of\\ndegree 5 or more\\n(Abel, 1826).had started at the right side (e.g., x0= 0) the negative gradient would\\nhave led us to the wrong minimum. Figure 7.2 illustrates the fact that for\\nx>\\x001, the negative gradient points toward the minimum on the right of\\nthe ﬁgure, which has a larger objective value.\\nIn Section 7.3, we will learn about a class of functions, called convex\\nfunctions, that do not exhibit this tricky dependency on the starting point\\nof the optimization algorithm. For convex functions, all local minimums\\nare global minimum. It turns out that many machine learning objective For convex functions\\nall local minima are\\nglobal minimum.functions are designed such that they are convex, and we will see an ex-\\nample in Chapter 12.\\nThe discussion in this chapter so far was about a one-dimensional func-\\ntion, where we are able to visualize the ideas of gradients, descent direc-\\ntions, and optimal values. In the rest of this chapter we develop the same\\nideas in high dimensions. Unfortunately, we can only visualize the con-\\ncepts in one dimension, but some concepts do not generalize directly to\\nhigher dimensions, therefore some care needs to be taken when reading.\\n7.1 Optimization Using Gradient Descent\\nWe now consider the problem of solving for the minimum of a real-valued\\nfunction\\nmin\\nxf(x); (7.4)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n228 Continuous Optimization\\nwheref:Rd!Ris an objective function that captures the machine\\nlearning problem at hand. We assume that our function fis differentiable,\\nand we are unable to analytically ﬁnd a solution in closed form.\\nGradient descent is a ﬁrst-order optimization algorithm. To ﬁnd a local\\nminimum of a function using gradient descent, one takes steps propor-\\ntional to the negative of the gradient of the function at the current point.\\nRecall from Section 5.1 that the gradient points in the direction of the We use the\\nconvention of row\\nvectors for\\ngradients.steepest ascent. Another useful intuition is to consider the set of lines\\nwhere the function is at a certain value ( f(x) =cfor some value c2R),\\nwhich are known as the contour lines. The gradient points in a direction\\nthat is orthogonal to the contour lines of the function we wish to optimize.\\nLet us consider multivariate functions. Imagine a surface (described by\\nthe function f(x)) with a ball starting at a particular location x0. When\\nthe ball is released, it will move downhill in the direction of steepest de-\\nscent. Gradient descent exploits the fact that f(x0)decreases fastest if one\\nmoves fromx0in the direction of the negative gradient \\x00((rf)(x0))>of\\nfatx0. We assume in this book that the functions are differentiable, and\\nrefer the reader to more general settings in Section 7.4. Then, if\\nx1=x0\\x00\\r((rf)(x0))>(7.5)\\nfor a small step-size\\r>0, thenf(x1)6f(x0). Note that we use the\\ntranspose for the gradient since otherwise the dimensions will not work\\nout.\\nThis observation allows us to deﬁne a simple gradient descent algo-\\nrithm: If we want to ﬁnd a local optimum f(x\\x03)of a function f:Rn!\\nR;x7!f(x), we start with an initial guess x0of the parameters we wish\\nto optimize and then iterate according to\\nxi+1=xi\\x00\\ri((rf)(xi))>: (7.6)\\nFor suitable step-size \\ri, the sequence f(x0)>f(x1)>:::converges to\\na local minimum.\\nExample 7.1\\nConsider a quadratic function in two dimensions\\nf\\x12\\x14x1\\nx2\\x15\\x13\\n=1\\n2\\x14x1\\nx2\\x15>\\x142 1\\n1 20\\x15\\x14x1\\nx2\\x15\\n\\x00\\x145\\n3\\x15>\\x14x1\\nx2\\x15\\n(7.7)\\nwith gradient\\nrf\\x12\\x14x1\\nx2\\x15\\x13\\n=\\x14x1\\nx2\\x15>\\x142 1\\n1 20\\x15\\n\\x00\\x145\\n3\\x15>\\n: (7.8)\\nStarting at the initial location x0= [\\x003;\\x001]>, we iteratively apply (7.6)\\nto obtain a sequence of estimates that converge to the minimum value\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.1 Optimization Using Gradient Descent 229\\nFigure 7.3 Gradient\\ndescent on a\\ntwo-dimensional\\nquadratic surface\\n(shown as a\\nheatmap). See\\nExample 7.1 for a\\ndescription.\\n\\x004\\x002 0 2 4\\nx1\\x002\\x001012x20.0\\n10.0\\n20.030.0\\n40.040.0\\n50.050.0\\n60.0 70.0\\n80.0\\x00150153045607590\\n(illustrated in Figure 7.3). We can see (both from the ﬁgure and by plug-\\ngingx0into (7.8) with \\r= 0:085) that the negative gradient at x0points\\nnorth and east, leading to x1= [\\x001:98;1:21]>. Repeating that argument\\ngives usx2= [\\x001:32;\\x000:42]>, and so on.\\nRemark. Gradient descent can be relatively slow close to the minimum:\\nIts asymptotic rate of convergence is inferior to many other methods. Us-\\ning the ball rolling down the hill analogy, when the surface is a long, thin\\nvalley, the problem is poorly conditioned (Trefethen and Bau III, 1997).\\nFor poorly conditioned convex problems, gradient descent increasingly\\n“zigzags” as the gradients point nearly orthogonally to the shortest di-\\nrection to a minimum point; see Figure 7.3. }\\n7.1.1 Step-size\\nAs mentioned earlier, choosing a good step-size is important in gradient\\ndescent. If the step-size is too small, gradient descent can be slow. If the The step-size is also\\ncalled the learning\\nrate.step-size is chosen too large, gradient descent can overshoot, fail to con-\\nverge, or even diverge. We will discuss the use of momentum in the next\\nsection. It is a method that smoothes out erratic behavior of gradient up-\\ndates and dampens oscillations.\\nAdaptive gradient methods rescale the step-size at each iteration, de-\\npending on local properties of the function. There are two simple heuris-\\ntics (Toussaint, 2012):\\nWhen the function value increases after a gradient step, the step-size\\nwas too large. Undo the step and decrease the step-size.\\nWhen the function value decreases the step could have been larger. Try\\nto increase the step-size.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n230 Continuous Optimization\\nAlthough the “undo” step seems to be a waste of resources, using this\\nheuristic guarantees monotonic convergence.\\nExample 7.2 (Solving a Linear Equation System)\\nWhen we solve linear equations of the form Ax=b, in practice we solve\\nAx\\x00b=0approximately by ﬁnding x\\x03that minimizes the squared error\\nkAx\\x00bk2= (Ax\\x00b)>(Ax\\x00b) (7.9)\\nif we use the Euclidean norm. The gradient of (7.9) with respect to xis\\nrx= 2(Ax\\x00b)>A: (7.10)\\nWe can use this gradient directly in a gradient descent algorithm. How-\\never, for this particular special case, it turns out that there is an analytic\\nsolution, which can be found by setting the gradient to zero. We will see\\nmore on solving squared error problems in Chapter 9.\\nRemark. When applied to the solution of linear systems of equations Ax=\\nb, gradient descent may converge slowly. The speed of convergence of gra-\\ndient descent is dependent on the condition number \\x14=\\x1b(A)max\\n\\x1b(A)min, which condition number\\nis the ratio of the maximum to the minimum singular value (Section 4.5)\\nofA. The condition number essentially measures the ratio of the most\\ncurved direction versus the least curved direction, which corresponds to\\nour imagery that poorly conditioned problems are long, thin valleys: They\\nare very curved in one direction, but very ﬂat in the other. Instead of di-\\nrectly solving Ax=b, one could instead solve P\\x001(Ax\\x00b) =0, where\\nPis called the preconditioner . The goal is to design P\\x001such thatP\\x001A preconditioner\\nhas a better condition number, but at the same time P\\x001is easy to com-\\npute. For further information on gradient descent, preconditioning, and\\nconvergence we refer to Boyd and Vandenberghe (2004, chapter 9). }\\n7.1.2 Gradient Descent With Momentum\\nAs illustrated in Figure 7.3, the convergence of gradient descent may be\\nvery slow if the curvature of the optimization surface is such that there\\nare regions that are poorly scaled. The curvature is such that the gradient\\ndescent steps hops between the walls of the valley and approaches the\\noptimum in small steps. The proposed tweak to improve convergence is\\nto give gradient descent some memory. Goh (2017) wrote\\nan intuitive blog\\npost on gradient\\ndescent with\\nmomentum.Gradient descent with momentum (Rumelhart et al., 1986) is a method\\nthat introduces an additional term to remember what happened in the\\nprevious iteration. This memory dampens oscillations and smoothes out\\nthe gradient updates. Continuing the ball analogy, the momentum term\\nemulates the phenomenon of a heavy ball that is reluctant to change di-\\nrections. The idea is to have a gradient update with memory to implement\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.1 Optimization Using Gradient Descent 231\\na moving average. The momentum-based method remembers the update\\n\\x01xiat each iteration iand determines the next update as a linear combi-\\nnation of the current and previous gradients\\nxi+1=xi\\x00\\ri((rf)(xi))>+\\x0b\\x01xi (7.11)\\n\\x01xi=xi\\x00xi\\x001=\\x0b\\x01xi\\x001\\x00\\ri\\x001((rf)(xi\\x001))>; (7.12)\\nwhere\\x0b2[0;1]. Sometimes we will only know the gradient approxi-\\nmately. In such cases, the momentum term is useful since it averages out\\ndifferent noisy estimates of the gradient. One particularly useful way to\\nobtain an approximate gradient is by using a stochastic approximation,\\nwhich we discuss next.\\n7.1.3 Stochastic Gradient Descent\\nComputing the gradient can be very time consuming. However, often it is\\npossible to ﬁnd a “cheap” approximation of the gradient. Approximating\\nthe gradient is still useful as long as it points in roughly the same direction\\nas the true gradient. stochastic gradient\\ndescent Stochastic gradient descent (often shortened as SGD) is a stochastic ap-\\nproximation of the gradient descent method for minimizing an objective\\nfunction that is written as a sum of differentiable functions. The word\\nstochastic here refers to the fact that we acknowledge that we do not\\nknow the gradient precisely, but instead only know a noisy approxima-\\ntion to it. By constraining the probability distribution of the approximate\\ngradients, we can still theoretically guarantee that SGD will converge.\\nIn machine learning, given n= 1;:::;N data points, we often consider\\nobjective functions that are the sum of the losses Lnincurred by each\\nexamplen. In mathematical notation, we have the form\\nL(\\x12) =NX\\nn=1Ln(\\x12); (7.13)\\nwhere\\x12is the vector of parameters of interest, i.e., we want to ﬁnd \\x12that\\nminimizesL. An example from regression (Chapter 9) is the negative log-\\nlikelihood, which is expressed as a sum over log-likelihoods of individual\\nexamples so that\\nL(\\x12) =\\x00NX\\nn=1logp(ynjxn;\\x12); (7.14)\\nwherexn2RDare the training inputs, ynare the training targets, and \\x12\\nare the parameters of the regression model.\\nStandard gradient descent, as introduced previously, is a “batch” opti-\\nmization method, i.e., optimization is performed using the full training set\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n232 Continuous Optimization\\nby updating the vector of parameters according to\\n\\x12i+1=\\x12i\\x00\\ri(rL(\\x12i))>=\\x12i\\x00\\riNX\\nn=1(rLn(\\x12i))>(7.15)\\nfor a suitable step-size parameter \\ri. Evaluating the sum gradient may re-\\nquire expensive evaluations of the gradients from all individual functions\\nLn. When the training set is enormous and/or no simple formulas exist,\\nevaluating the sums of gradients becomes very expensive.\\nConsider the termPN\\nn=1(rLn(\\x12i))in (7.15). We can reduce the amount\\nof computation by taking a sum over a smaller set of Ln. In contrast to\\nbatch gradient descent, which uses all Lnforn= 1;:::;N , we randomly\\nchoose a subset of Lnfor mini-batch gradient descent. In the extreme\\ncase, we randomly select only a single Lnto estimate the gradient. The\\nkey insight about why taking a subset of data is sensible is to realize that\\nfor gradient descent to converge, we only require that the gradient is an\\nunbiased estimate of the true gradient. In fact the termPN\\nn=1(rLn(\\x12i))\\nin (7.15) is an empirical estimate of the expected value (Section 6.4.1) of\\nthe gradient. Therefore, any other unbiased empirical estimate of the ex-\\npected value, for example using any subsample of the data, would sufﬁce\\nfor convergence of gradient descent.\\nRemark. When the learning rate decreases at an appropriate rate, and sub-\\nject to relatively mild assumptions, stochastic gradient descent converges\\nalmost surely to local minimum (Bottou, 1998). }\\nWhy should one consider using an approximate gradient? A major rea-\\nson is practical implementation constraints, such as the size of central\\nprocessing unit (CPU)/graphics processing unit (GPU) memory or limits\\non computational time. We can think of the size of the subset used to esti-\\nmate the gradient in the same way that we thought of the size of a sample\\nwhen estimating empirical means (Section 6.4.1). Large mini-batch sizes\\nwill provide accurate estimates of the gradient, reducing the variance in\\nthe parameter update. Furthermore, large mini-batches take advantage of\\nhighly optimized matrix operations in vectorized implementations of the\\ncost and gradient. The reduction in variance leads to more stable conver-\\ngence, but each gradient calculation will be more expensive.\\nIn contrast, small mini-batches are quick to estimate. If we keep the\\nmini-batch size small, the noise in our gradient estimate will allow us to\\nget out of some bad local optima, which we may otherwise get stuck in.\\nIn machine learning, optimization methods are used for training by min-\\nimizing an objective function on the training data, but the overall goal\\nis to improve generalization performance (Chapter 8). Since the goal in\\nmachine learning does not necessarily need a precise estimate of the min-\\nimum of the objective function, approximate gradients using mini-batch\\napproaches have been widely used. Stochastic gradient descent is very\\neffective in large-scale machine learning problems (Bottou et al., 2018),\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.2 Constrained Optimization and Lagrange Multipliers 233\\nFigure 7.4\\nIllustration of\\nconstrained\\noptimization. The\\nunconstrained\\nproblem (indicated\\nby the contour\\nlines) has a\\nminimum on the\\nright side (indicated\\nby the circle). The\\nbox constraints\\n(\\x0016x61and\\n\\x0016y61) require\\nthat the optimal\\nsolution is within\\nthe box, resulting in\\nan optimal value\\nindicated by the\\nstar.\\n−3−2−1 0 1 2 3\\nx1−3−2−10123x2\\nsuch as training deep neural networks on millions of images (Dean et al.,\\n2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih\\net al., 2015), or training of large-scale Gaussian process models (Hensman\\net al., 2013; Gal et al., 2014).\\n7.2 Constrained Optimization and Lagrange Multipliers\\nIn the previous section, we considered the problem of solving for the min-\\nimum of a function\\nmin\\nxf(x); (7.16)\\nwheref:RD!R.\\nIn this section, we have additional constraints. That is, for real-valued\\nfunctionsgi:RD!Rfori= 1;:::;m , we consider the constrained\\noptimization problem (see Figure 7.4 for an illustration)\\nmin\\nxf(x) (7.17)\\nsubject to gi(x)60for alli= 1;:::;m:\\nIt is worth pointing out that the functions fandgicould be non-convex\\nin general, and we will consider the convex case in the next section.\\nOne obvious, but not very practical, way of converting the constrained\\nproblem (7.17) into an unconstrained one is to use an indicator function\\nJ(x) =f(x) +mX\\ni=11(gi(x)); (7.18)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n234 Continuous Optimization\\nwhere 1(z)is an inﬁnite step function\\n1(z) =(\\n0ifz60\\n1otherwise: (7.19)\\nThis gives inﬁnite penalty if the constraint is not satisﬁed, and hence\\nwould provide the same solution. However, this inﬁnite step function is\\nequally difﬁcult to optimize. We can overcome this difﬁculty by introduc-\\ningLagrange multipliers . The idea of Lagrange multipliers is to replace the Lagrange multiplier\\nstep function with a linear function.\\nWe associate to problem (7.17) the Lagrangian by introducing the La- Lagrangian\\ngrange multipliers \\x15i>0corresponding to each inequality constraint re-\\nspectively (Boyd and Vandenberghe, 2004, chapter 4) so that\\nL(x;\\x15) =f(x) +mX\\ni=1\\x15igi(x) (7.20a)\\n=f(x) +\\x15>g(x); (7.20b)\\nwhere in the last line we have concatenated all constraints gi(x)into a\\nvectorg(x), and all the Lagrange multipliers into a vector \\x152Rm.\\nWe now introduce the idea of Lagrangian duality. In general, duality\\nin optimization is the idea of converting an optimization problem in one\\nset of variables x(called the primal variables), into another optimization\\nproblem in a different set of variables \\x15(called the dual variables). We\\nintroduce two different approaches to duality: In this section, we discuss\\nLagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality.\\nDeﬁnition 7.1. The problem in (7.17)\\nmin\\nxf(x) (7.21)\\nsubject to gi(x)60for alli= 1;:::;m\\nis known as the primal problem , corresponding to the primal variables x. primal problem\\nThe associated Lagrangian dual problem is given by Lagrangian dual\\nproblem\\nmax\\n\\x152RmD(\\x15)\\nsubject to\\x15>0;(7.22)\\nwhere\\x15are the dual variables and D(\\x15) = minx2RdL(x;\\x15).\\nRemark. In the discussion of Deﬁnition 7.1, we use two concepts that are\\nalso of independent interest (Boyd and Vandenberghe, 2004).\\nFirst is the minimax inequality , which says that for any function with minimax inequality\\ntwo arguments \\'(x;y), the maximin is less than the minimax, i.e.,\\nmax\\nymin\\nx\\'(x;y)6min\\nxmax\\ny\\'(x;y): (7.23)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.2 Constrained Optimization and Lagrange Multipliers 235\\nThis inequality can be proved by considering the inequality\\nFor allx;y min\\nx\\'(x;y)6max\\ny\\'(x;y): (7.24)\\nNote that taking the maximum over yof the left-hand side of (7.24) main-\\ntains the inequality since the inequality is true for all y. Similarly, we can\\ntake the minimum over xof the right-hand side of (7.24) to obtain (7.23).\\nThe second concept is weak duality , which uses (7.23) to show that weak duality\\nprimal values are always greater than or equal to dual values. This is de-\\nscribed in more detail in (7.27). }\\nRecall that the difference between J(x)in (7.18) and the Lagrangian\\nin (7.20b) is that we have relaxed the indicator function to a linear func-\\ntion. Therefore, when \\x15>0, the Lagrangian L(x;\\x15)is a lower bound of\\nJ(x). Hence, the maximum of L(x;\\x15)with respect to \\x15is\\nJ(x) = max\\n\\x15>0L(x;\\x15): (7.25)\\nRecall that the original problem was minimizing J(x),\\nmin\\nx2Rdmax\\n\\x15>0L(x;\\x15): (7.26)\\nBy the minimax inequality (7.23), it follows that swapping the order of\\nthe minimum and maximum results in a smaller value, i.e.,\\nmin\\nx2Rdmax\\n\\x15>0L(x;\\x15)>max\\n\\x15>0min\\nx2RdL(x;\\x15): (7.27)\\nThis is also known as weak duality . Note that the inner part of the right- weak duality\\nhand side is the dual objective function D(\\x15)and the deﬁnition follows.\\nIn contrast to the original optimization problem, which has constraints,\\nminx2RdL(x;\\x15)is an unconstrained optimization problem for a given\\nvalue of\\x15. If solving minx2RdL(x;\\x15)is easy, then the overall problem is\\neasy to solve. We can see this by observing from (7.20b) that L(x;\\x15)is\\nafﬁne with respect to \\x15. Therefore minx2RdL(x;\\x15)is a pointwise min-\\nimum of afﬁne functions of \\x15, and hence D(\\x15)is concave even though\\nf(\\x01)andgi(\\x01)may be nonconvex. The outer problem, maximization over\\n\\x15, is the maximum of a concave function and can be efﬁciently computed.\\nAssumingf(\\x01)andgi(\\x01)are differentiable, we ﬁnd the Lagrange dual\\nproblem by differentiating the Lagrangian with respect to x, setting the\\ndifferential to zero, and solving for the optimal value. We will discuss two\\nconcrete examples in Sections 7.3.1 and 7.3.2, where f(\\x01)andgi(\\x01)are\\nconvex.\\nRemark (Equality Constraints) .Consider (7.17) with additional equality\\nconstraints\\nmin\\nxf(x)\\nsubject to gi(x)60for alli= 1;:::;m\\nhj(x) = 0 for allj= 1;:::;n:(7.28)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n236 Continuous Optimization\\nWe can model equality constraints by replacing them with two inequality\\nconstraints. That is for each equality constraint hj(x) = 0 we equivalently\\nreplace it by two constraints hj(x)60andhj(x)>0. It turns out that\\nthe resulting Lagrange multipliers are then unconstrained.\\nTherefore, we constrain the Lagrange multipliers corresponding to the\\ninequality constraints in (7.28) to be non-negative, and leave the La-\\ngrange multipliers corresponding to the equality constraints unconstrained.\\n}\\n7.3 Convex Optimization\\nWe focus our attention of a particularly useful class of optimization prob-\\nlems, where we can guarantee global optimality. When f(\\x01)is a convex\\nfunction, and when the constraints involving g(\\x01)andh(\\x01)are convex sets,\\nthis is called a convex optimization problem . In this setting, we have strong convex optimization\\nproblem\\nstrong dualityduality : The optimal solution of the dual problem is the same as the opti-\\nmal solution of the primal problem. The distinction between convex func-\\ntions and convex sets are often not strictly presented in machine learning\\nliterature, but one can often infer the implied meaning from context.\\nDeﬁnition 7.2. A setCis aconvex set if for anyx;y2Cand for any scalar convex set\\n\\x12with 06\\x1261, we have\\n\\x12x+ (1\\x00\\x12)y2C: (7.29)\\nFigure 7.5 Example\\nof a convex set.\\n Convex sets are sets such that a straight line connecting any two ele-\\nments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex\\nand nonconvex sets, respectively.\\nFigure 7.6 Example\\nof a nonconvex set.\\nConvex functions are functions such that a straight line between any\\ntwo points of the function lie above the function. Figure 7.2 shows a non-\\nconvex function, and Figure 7.3 shows a convex function. Another convex\\nfunction is shown in Figure 7.7.\\nDeﬁnition 7.3. Let function f:RD!Rbe a function whose domain is a\\nconvex set. The function fis aconvex function if for allx;yin the domain\\nconvex functionoff, and for any scalar \\x12with 06\\x1261, we have\\nf(\\x12x+ (1\\x00\\x12)y)6\\x12f(x) + (1\\x00\\x12)f(y): (7.30)\\nRemark. Aconcave function is the negative of a convex function. }\\nconcave functionThe constraints involving g(\\x01)andh(\\x01)in (7.28) truncate functions at a\\nscalar value, resulting in sets. Another relation between convex functions\\nand convex sets is to consider the set obtained by “ﬁlling in” a convex\\nfunction. A convex function is a bowl-like object, and we imagine pouring\\nwater into it to ﬁll it up. This resulting ﬁlled-in set, called the epigraph of epigraph\\nthe convex function, is a convex set.\\nIf a function f:Rn!Ris differentiable, we can specify convexity in\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.3 Convex Optimization 237\\nFigure 7.7 Example\\nof a convex\\nfunction.\\n−3−2−1 0 1 2 3\\nx010203040yy= 3x2−5x+ 2\\nterms of its gradient rxf(x)(Section 5.2). A function f(x)is convex if\\nand only if for any two points x;yit holds that\\nf(y)>f(x) +rxf(x)>(y\\x00x): (7.31)\\nIf we further know that a function f(x)is twice differentiable, that is, the\\nHessian (5.147) exists for all values in the domain of x, then the function\\nf(x)is convex if and only if r2\\nxf(x)is positive semideﬁnite (Boyd and\\nVandenberghe, 2004).\\nExample 7.3\\nThe negative entropy f(x) =xlog2xis convex for x>0. A visualization\\nof the function is shown in Figure 7.8, and we can see that the function is\\nconvex. To illustrate the previous deﬁnitions of convexity, let us check the\\ncalculations for two points x= 2andx= 4. Note that to prove convexity\\noff(x)we would need to check for all points x2R.\\nRecall Deﬁnition 7.3. Consider a point midway between the two points\\n(that is\\x12= 0:5); then the left-hand side is f(0:5\\x012 + 0:5\\x014) = 3 log23\\x19\\n4:75. The right-hand side is 0:5(2 log22) + 0:5(4 log24) = 1 + 4 = 5 . And\\ntherefore the deﬁnition is satisﬁed.\\nSincef(x)is differentiable, we can alternatively use (7.31). Calculating\\nthe derivative of f(x), we obtain\\nrx(xlog2x) = 1\\x01log2x+x\\x011\\nxloge2= log2x+1\\nloge2: (7.32)\\nUsing the same two test points x= 2 andx= 4, the left-hand side of\\n(7.31) is given by f(4) = 8 . The right-hand side is\\nf(x) +r>\\nx(y\\x00x) =f(2) +rf(2)\\x01(4\\x002) (7.33a)\\n= 2 + (1 +1\\nloge2)\\x012\\x196:9: (7.33b)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n238 Continuous Optimization\\nFigure 7.8 The\\nnegative entropy\\nfunction (which is\\nconvex) and its\\ntangent atx= 2.\\n0 1 2 3 4 5\\nx0510f(x)xlog2x\\ntangent atx= 2\\nWe can check that a function or set is convex from ﬁrst principles by\\nrecalling the deﬁnitions. In practice, we often rely on operations that pre-\\nserve convexity to check that a particular function or set is convex. Al-\\nthough the details are vastly different, this is again the idea of closure\\nthat we introduced in Chapter 2 for vector spaces.\\nExample 7.4\\nA nonnegative weighted sum of convex functions is convex. Observe that\\niffis a convex function, and \\x0b>0is a nonnegative scalar, then the\\nfunction\\x0bfis convex. We can see this by multiplying \\x0bto both sides of the\\nequation in Deﬁnition 7.3, and recalling that multiplying a nonnegative\\nnumber does not change the inequality.\\nIff1andf2are convex functions, then we have by the deﬁnition\\nf1(\\x12x+ (1\\x00\\x12)y)6\\x12f1(x) + (1\\x00\\x12)f1(y) (7.34)\\nf2(\\x12x+ (1\\x00\\x12)y)6\\x12f2(x) + (1\\x00\\x12)f2(y): (7.35)\\nSumming up both sides gives us\\nf1(\\x12x+ (1\\x00\\x12)y) +f2(\\x12x+ (1\\x00\\x12)y)\\n6\\x12f1(x) + (1\\x00\\x12)f1(y) +\\x12f2(x) + (1\\x00\\x12)f2(y); (7.36)\\nwhere the right-hand side can be rearranged to\\n\\x12(f1(x) +f2(x)) + (1\\x00\\x12)(f1(y) +f2(y)); (7.37)\\ncompleting the proof that the sum of convex functions is convex.\\nCombining the preceding two facts, we see that \\x0bf1(x) +\\x0cf2(x)is\\nconvex for\\x0b;\\x0c>0. This closure property can be extended using a sim-\\nilar argument for nonnegative weighted sums of more than two convex\\nfunctions.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.3 Convex Optimization 239\\nRemark. The inequality in (7.30) is sometimes called Jensen’s inequality .Jensen’s inequality\\nIn fact, a whole class of inequalities for taking nonnegative weighted sums\\nof convex functions are all called Jensen’s inequality. }\\nIn summary, a constrained optimization problem is called a convex opti- convex optimization\\nproblem mization problem if\\nmin\\nxf(x)\\nsubject togi(x)60for alli= 1;:::;m\\nhj(x) = 0 for allj= 1;:::;n;(7.38)\\nwhere all functions f(x)andgi(x)are convex functions, and all hj(x) =\\n0are convex sets. In the following, we will describe two classes of convex\\noptimization problems that are widely used and well understood.\\n7.3.1 Linear Programming\\nConsider the special case when all the preceding functions are linear, i.e.,\\nmin\\nx2Rdc>x (7.39)\\nsubject toAx6b;\\nwhereA2Rm\\x02dandb2Rm. This is known as a linear program . It hasd linear program\\nLinear programs are\\none of the most\\nwidely used\\napproaches in\\nindustry.variables and mlinear constraints. The Lagrangian is given by\\nL(x;\\x15) =c>x+\\x15>(Ax\\x00b); (7.40)\\nwhere\\x152Rmis the vector of non-negative Lagrange multipliers. Rear-\\nranging the terms corresponding to xyields\\nL(x;\\x15) = (c+A>\\x15)>x\\x00\\x15>b: (7.41)\\nTaking the derivative of L(x;\\x15)with respect to xand setting it to zero\\ngives us\\nc+A>\\x15=0: (7.42)\\nTherefore, the dual Lagrangian is D(\\x15) =\\x00\\x15>b. Recall we would like\\nto maximize D(\\x15). In addition to the constraint due to the derivative of\\nL(x;\\x15)being zero, we also have the fact that \\x15>0, resulting in the\\nfollowing dual optimization problem It is convention to\\nminimize the primal\\nand maximize the\\ndual.max\\n\\x152Rm\\x00b>\\x15 (7.43)\\nsubject toc+A>\\x15=0\\n\\x15>0:\\nThis is also a linear program, but with mvariables. We have the choice\\nof solving the primal (7.39) or the dual (7.43) program depending on\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n240 Continuous Optimization\\nwhethermordis larger. Recall that dis the number of variables and mis\\nthe number of constraints in the primal linear program.\\nExample 7.5 (Linear Program)\\nConsider the linear program\\nmin\\nx2R2\\x00\\x145\\n3\\x15>\\x14x1\\nx2\\x15\\nsubject to2\\n666642 2\\n2\\x004\\n\\x002 1\\n0\\x001\\n0 13\\n77775\\x14x1\\nx2\\x15\\n62\\n6666433\\n8\\n5\\n\\x001\\n83\\n77775(7.44)\\nwith two variables. This program is also shown in Figure 7.9. The objective\\nfunction is linear, resulting in linear contour lines. The constraint set in\\nstandard form is translated into the legend. The optimal value must lie in\\nthe shaded (feasible) region, and is indicated by the star.\\nFigure 7.9\\nIllustration of a\\nlinear program. The\\nunconstrained\\nproblem (indicated\\nby the contour\\nlines) has a\\nminimum on the\\nright side. The\\noptimal value given\\nthe constraints are\\nshown by the star.\\n0 2 4 6 8 10 12 14 16\\nx10246810x22x2≤33−2x1\\n4x2≥2x1−8\\nx2≤2x1−5\\nx2≥1\\nx2≤8\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.3 Convex Optimization 241\\n7.3.2 Quadratic Programming\\nConsider the case of a convex quadratic objective function, where the con-\\nstraints are afﬁne, i.e.,\\nmin\\nx2Rd1\\n2x>Qx+c>x (7.45)\\nsubject toAx6b;\\nwhereA2Rm\\x02d,b2Rm, andc2Rd. The square symmetric matrix Q2\\nRd\\x02dis positive deﬁnite, and therefore the objective function is convex.\\nThis is known as a quadratic program . Observe that it has dvariables and\\nmlinear constraints.\\nExample 7.6 (Quadratic Program)\\nConsider the quadratic program\\nmin\\nx2R21\\n2\\x14x1\\nx2\\x15>\\x142 1\\n1 4\\x15\\x14x1\\nx2\\x15\\n+\\x145\\n3\\x15>\\x14x1\\nx2\\x15\\n(7.46)\\nsubject to2\\n6641 0\\n\\x001 0\\n0 1\\n0\\x0013\\n775\\x14x1\\nx2\\x15\\n62\\n6641\\n1\\n1\\n13\\n775(7.47)\\nof two variables. The program is also illustrated in Figure 7.4. The objec-\\ntive function is quadratic with a positive semideﬁnite matrix Q, resulting\\nin elliptical contour lines. The optimal value must lie in the shaded (feasi-\\nble) region, and is indicated by the star.\\nThe Lagrangian is given by\\nL(x;\\x15) =1\\n2x>Qx+c>x+\\x15>(Ax\\x00b) (7.48a)\\n=1\\n2x>Qx+ (c+A>\\x15)>x\\x00\\x15>b; (7.48b)\\nwhere again we have rearranged the terms. Taking the derivative of L(x;\\x15)\\nwith respect to xand setting it to zero gives\\nQx+ (c+A>\\x15) =0: (7.49)\\nAssuming that Qis invertible, we get\\nx=\\x00Q\\x001(c+A>\\x15): (7.50)\\nSubstituting (7.50) into the primal Lagrangian L(x;\\x15), we get the dual\\nLagrangian\\nD(\\x15) =\\x001\\n2(c+A>\\x15)>Q\\x001(c+A>\\x15)\\x00\\x15>b: (7.51)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n242 Continuous Optimization\\nTherefore, the dual optimization problem is given by\\nmax\\n\\x152Rm\\x001\\n2(c+A>\\x15)>Q\\x001(c+A>\\x15)\\x00\\x15>b\\nsubject to\\x15>0:(7.52)\\nWe will see an application of quadratic programming in machine learning\\nin Chapter 12.\\n7.3.3 Legendre–Fenchel Transform and Convex Conjugate\\nLet us revisit the idea of duality from Section 7.2, without considering\\nconstraints. One useful fact about a convex set is that it can be equiva-\\nlently described by its supporting hyperplanes. A hyperplane is called a\\nsupporting hyperplane of a convex set if it intersects the convex set, and supporting\\nhyperplane the convex set is contained on just one side of it. Recall that we can ﬁll up\\na convex function to obtain the epigraph, which is a convex set. Therefore,\\nwe can also describe convex functions in terms of their supporting hyper-\\nplanes. Furthermore, observe that the supporting hyperplane just touches\\nthe convex function, and is in fact the tangent to the function at that point.\\nAnd recall that the tangent of a function f(x)at a given point x0is the\\nevaluation of the gradient of that function at that pointdf(x)\\ndx\\x0c\\x0c\\x0c\\nx=x0. In\\nsummary, because convex sets can be equivalently described by their sup-\\nporting hyperplanes, convex functions can be equivalently described by a\\nfunction of their gradient. The Legendre transform formalizes this concept. Legendre transform\\nPhysics students are\\noften introduced to\\nthe Legendre\\ntransform as\\nrelating the\\nLagrangian and the\\nHamiltonian in\\nclassical mechanics.We begin with the most general deﬁnition, which unfortunately has a\\ncounter-intuitive form, and look at special cases to relate the deﬁnition to\\nthe intuition described in the preceding paragraph. The Legendre-Fenchel\\nLegendre-Fenchel\\ntransformtransform is a transformation (in the sense of a Fourier transform) from\\na convex differentiable function f(x)to a function that depends on the\\ntangentss(x) =rxf(x). It is worth stressing that this is a transformation\\nof the function f(\\x01)and not the variable xor the function evaluated at x.\\nThe Legendre-Fenchel transform is also known as the convex conjugate (for convex conjugate\\nreasons we will see soon) and is closely related to duality (Hiriart-Urruty\\nand Lemar ´echal, 2001, chapter 5).\\nDeﬁnition 7.4. The convex conjugate of a function f:RD!Ris a convex conjugate\\nfunctionf\\x03deﬁned by\\nf\\x03(s) = sup\\nx2RD(hs;xi\\x00f(x)): (7.53)\\nNote that the preceding convex conjugate deﬁnition does not need the\\nfunctionfto be convex nor differentiable. In Deﬁnition 7.4, we have used\\na general inner product (Section 3.2) but in the rest of this section we\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.3 Convex Optimization 243\\nwill consider the standard dot product between ﬁnite-dimensional vectors\\n(hs;xi=s>x) to avoid too many technical details.\\nTo understand Deﬁnition 7.4 in a geometric fashion, consider a nice This derivation is\\neasiest to\\nunderstand by\\ndrawing the\\nreasoning as it\\nprogresses.simple one-dimensional convex and differentiable function, for example\\nf(x) =x2. Note that since we are looking at a one-dimensional problem,\\nhyperplanes reduce to a line. Consider a line y=sx+c. Recall that we are\\nable to describe convex functions by their supporting hyperplanes, so let\\nus try to describe this function f(x)by its supporting lines. Fix the gradi-\\nent of the line s2Rand for each point (x0;f(x0))on the graph of f, ﬁnd\\nthe minimum value of csuch that the line still intersects (x0;f(x0)). Note\\nthat the minimum value of cis the place where a line with slope s“just\\ntouches” the function f(x) =x2. The line passing through (x0;f(x0))\\nwith gradient sis given by\\ny\\x00f(x0) =s(x\\x00x0): (7.54)\\nThey-intercept of this line is \\x00sx0+f(x0). The minimum of cfor which\\ny=sx+cintersects with the graph of fis therefore\\ninf\\nx0\\x00sx0+f(x0): (7.55)\\nThe preceding convex conjugate is by convention deﬁned to be the nega-\\ntive of this. The reasoning in this paragraph did not rely on the fact that\\nwe chose a one-dimensional convex and differentiable function, and holds\\nforf:RD!R, which are nonconvex and non-differentiable.The classical\\nLegendre transform\\nis deﬁned on convex\\ndifferentiable\\nfunctions in RD.Remark. Convex differentiable functions such as the example f(x) =x2is\\na nice special case, where there is no need for the supremum, and there is\\na one-to-one correspondence between a function and its Legendre trans-\\nform. Let us derive this from ﬁrst principles. For a convex differentiable\\nfunction, we know that at x0the tangent touches f(x0)so that\\nf(x0) =sx0+c: (7.56)\\nRecall that we want to describe the convex function f(x)in terms of its\\ngradientrxf(x), and thats=rxf(x0). We rearrange to get an expres-\\nsion for\\x00cto obtain\\n\\x00c=sx0\\x00f(x0): (7.57)\\nNote that\\x00cchanges with x0and therefore with s, which is why we can\\nthink of it as a function of s, which we call\\nf\\x03(s) :=sx0\\x00f(x0): (7.58)\\nComparing (7.58) with Deﬁnition 7.4, we see that (7.58) is a special case\\n(without the supremum). }\\nThe conjugate function has nice properties; for example, for convex\\nfunctions, applying the Legendre transform again gets us back to the orig-\\ninal function. In the same way that the slope of f(x)iss, the slope of f\\x03(s)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n244 Continuous Optimization\\nisx. The following two examples show common uses of convex conjugates\\nin machine learning.\\nExample 7.7 (Convex Conjugates)\\nTo illustrate the application of convex conjugates, consider the quadratic\\nfunction\\nf(y) =\\x15\\n2y>K\\x001y (7.59)\\nbased on a positive deﬁnite matrix K2Rn\\x02n. We denote the primal\\nvariable to be y2Rnand the dual variable to be \\x0b2Rn.\\nApplying Deﬁnition 7.4, we obtain the function\\nf\\x03(\\x0b) = sup\\ny2Rnhy;\\x0bi\\x00\\x15\\n2y>K\\x001y: (7.60)\\nSince the function is differentiable, we can ﬁnd the maximum by taking\\nthe derivative and with respect to ysetting it to zero.\\n@\\x02hy;\\x0bi\\x00\\x15\\n2y>K\\x001y\\x03\\n@y= (\\x0b\\x00\\x15K\\x001y)>(7.61)\\nand hence when the gradient is zero we have y=1\\n\\x15K\\x0b. Substituting\\ninto (7.60) yields\\nf\\x03(\\x0b) =1\\n\\x15\\x0b>K\\x0b\\x00\\x15\\n2\\x121\\n\\x15K\\x0b\\x13>\\nK\\x001\\x121\\n\\x15K\\x0b\\x13\\n=1\\n2\\x15\\x0b>K\\x0b:\\n(7.62)\\nExample 7.8\\nIn machine learning, we often use sums of functions; for example, the ob-\\njective function of the training set includes a sum of the losses for each ex-\\nample in the training set. In the following, we derive the convex conjugate\\nof a sum of losses `(t), where`:R!R. This also illustrates the appli-\\ncation of the convex conjugate to the vector case. Let L(t) =Pn\\ni=1`i(ti).\\nThen,\\nL\\x03(z) = sup\\nt2Rnhz;ti\\x00nX\\ni=1`i(ti) (7.63a)\\n= sup\\nt2RnnX\\ni=1ziti\\x00`i(ti) de\\x0cnition of dot product (7.63b)\\n=nX\\ni=1sup\\nt2Rnziti\\x00`i(ti) (7.63c)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n7.3 Convex Optimization 245\\n=nX\\ni=1`\\x03\\ni(zi): de\\x0cnition of conjugate (7.63d)\\nRecall that in Section 7.2 we derived a dual optimization problem using\\nLagrange multipliers. Furthermore, for convex optimization problems we\\nhave strong duality, that is the solutions of the primal and dual problem\\nmatch. The Legendre-Fenchel transform described here also can be used\\nto derive a dual optimization problem. Furthermore, when the function\\nis convex and differentiable, the supremum is unique. To further investi-\\ngate the relation between these two approaches, let us consider a linear\\nequality constrained convex optimization problem.\\nExample 7.9\\nLetf(y)andg(x)be convex functions, and Aa real matrix of appropriate\\ndimensions such that Ax=y. Then\\nmin\\nxf(Ax) +g(x) = min\\nAx=yf(y) +g(x): (7.64)\\nBy introducing the Lagrange multiplier ufor the constraints Ax=y,\\nmin\\nAx=yf(y) +g(x) = min\\nx;ymax\\nuf(y) +g(x) + (Ax\\x00y)>u (7.65a)\\n= max\\numin\\nx;yf(y) +g(x) + (Ax\\x00y)>u;(7.65b)\\nwhere the last step of swapping max and min is due to the fact that f(y)\\nandg(x)are convex functions. By splitting up the dot product term and\\ncollectingxandy,\\nmax\\numin\\nx;yf(y) +g(x) + (Ax\\x00y)>u (7.66a)\\n= max\\nu\\x14\\nmin\\ny\\x00y>u+f(y)\\x15\\n+h\\nmin\\nx(Ax)>u+g(x)i\\n(7.66b)\\n= max\\nu\\x14\\nmin\\ny\\x00y>u+f(y)\\x15\\n+h\\nmin\\nxx>A>u+g(x)i\\n(7.66c)\\nRecall the convex conjugate (Deﬁnition 7.4) and the fact that dot prod- For general inner\\nproducts,A>is\\nreplaced by the\\nadjointA\\x03.ucts are symmetric,\\nmax\\nu\\x14\\nmin\\ny\\x00y>u+f(y)\\x15\\n+h\\nmin\\nxx>A>u+g(x)i\\n(7.67a)\\n= max\\nu\\x00f\\x03(u)\\x00g\\x03(\\x00A>u): (7.67b)\\nTherefore, we have shown that\\nmin\\nxf(Ax) +g(x) = max\\nu\\x00f\\x03(u)\\x00g\\x03(\\x00A>u): (7.68)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n246 Continuous Optimization\\nThe Legendre-Fenchel conjugate turns out to be quite useful for ma-\\nchine learning problems that can be expressed as convex optimization\\nproblems. In particular, for convex loss functions that apply independently\\nto each example, the conjugate loss is a convenient way to derive a dual\\nproblem.\\n7.4 Further Reading\\nContinuous optimization is an active area of research, and we do not try\\nto provide a comprehensive account of recent advances.\\nFrom a gradient descent perspective, there are two major weaknesses\\nwhich each have their own set of literature. The ﬁrst challenge is the fact\\nthat gradient descent is a ﬁrst-order algorithm, and does not use infor-\\nmation about the curvature of the surface. When there are long valleys,\\nthe gradient points perpendicularly to the direction of interest. The idea\\nof momentum can be generalized to a general class of acceleration meth-\\nods (Nesterov, 2018). Conjugate gradient methods avoid the issues faced\\nby gradient descent by taking previous directions into account (Shewchuk,\\n1994). Second-order methods such as Newton methods use the Hessian to\\nprovide information about the curvature. Many of the choices for choos-\\ning step-sizes and ideas like momentum arise by considering the curvature\\nof the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton\\nmethods such as L-BFGS try to use cheaper computational methods to ap-\\nproximate the Hessian (Nocedal and Wright, 2006). Recently there has\\nbeen interest in other metrics for computing descent directions, result-\\ning in approaches such as mirror descent (Beck and Teboulle, 2003) and\\nnatural gradient (Toussaint, 2012).\\nThe second challenge is to handle non-differentiable functions. Gradi-\\nent methods are not well deﬁned when there are kinks in the function.\\nIn these cases, subgradient methods can be used (Shor, 1985). For fur-\\nther information and algorithms for optimizing non-differentiable func-\\ntions, we refer to the book by Bertsekas (1999). There is a vast amount\\nof literature on different approaches for numerically solving continuous\\noptimization problems, including algorithms for constrained optimization\\nproblems. Good starting points to appreciate this literature are the books\\nby Luenberger (1969) and Bonnans et al. (2006). A recent survey of con-\\ntinuous optimization is provided by Bubeck (2015). Hugo Gonc ¸alves’\\nblog is also a good\\nresource for an\\neasier introduction\\nto Legendre–Fenchel\\ntransforms:\\nhttps://tinyurl.\\ncom/ydaal7hjModern applications of machine learning often mean that the size of\\ndatasets prohibit the use of batch gradient descent, and hence stochastic\\ngradient descent is the current workhorse of large-scale machine learning\\nmethods. Recent surveys of the literature include Hazan (2015) and Bot-\\ntou et al. (2018).\\nFor duality and convex optimization, the book by Boyd and Vanden-\\nberghe (2004) includes lectures and slides online. A more mathematical\\ntreatment is provided by Bertsekas (2009), and recent book by one of\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nExercises 247\\nthe key researchers in the area of optimization is Nesterov (2018). Con-\\nvex optimization is based upon convex analysis, and the reader interested\\nin more foundational results about convex functions is referred to Rock-\\nafellar (1970), Hiriart-Urruty and Lemar ´echal (2001), and Borwein and\\nLewis (2006). Legendre–Fenchel transforms are also covered in the afore-\\nmentioned books on convex analysis, but a more beginner-friendly pre-\\nsentation is available at Zia et al. (2009). The role of Legendre–Fenchel\\ntransforms in the analysis of convex optimization algorithms is surveyed\\nin Polyak (2016).\\nExercises\\n7.1 Consider the univariate function\\nf(x) =x3+ 6x2\\x003x\\x005:\\nFind its stationary points and indicate whether they are maximum, mini-\\nmum, or saddle points.\\n7.2 Consider the update equation for stochastic gradient descent (Equation (7.15)).\\nWrite down the update when we use a mini-batch size of one.\\n7.3 Consider whether the following statements are true or false:\\na. The intersection of any two convex sets is convex.\\nb. The union of any two convex sets is convex.\\nc. The difference of a convex set Afrom another convex set Bis convex.\\n7.4 Consider whether the following statements are true or false:\\na. The sum of any two convex functions is convex.\\nb. The difference of any two convex functions is convex.\\nc. The product of any two convex functions is convex.\\nd. The maximum of any two convex functions is convex.\\n7.5 Express the following optimization problem as a standard linear program in\\nmatrix notation\\nmax\\nx2R2;\\x182Rp>x+\\x18\\nsubject to the constraints that \\x18>0,x060andx163.\\n7.6 Consider the linear program illustrated in Figure 7.9,\\nmin\\nx2R2\\x00\\x14\\n5\\n3\\x15>\\x14\\nx1\\nx2\\x15\\nsubject to2\\n666642 2\\n2\\x004\\n\\x002 1\\n0\\x001\\n0 13\\n77775\\x14\\nx1\\nx2\\x15\\n62\\n6666433\\n8\\n5\\n\\x001\\n83\\n77775\\nDerive the dual linear program using Lagrange duality.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n248 Continuous Optimization\\n7.7 Consider the quadratic program illustrated in Figure 7.4,\\nmin\\nx2R21\\n2\\x14\\nx1\\nx2\\x15>\\x14\\n2 1\\n1 4\\x15\\x14\\nx1\\nx2\\x15\\n+\\x14\\n5\\n3\\x15>\\x14\\nx1\\nx2\\x15\\nsubject to2\\n6641 0\\n\\x001 0\\n0 1\\n0\\x0013\\n775\\x14\\nx1\\nx2\\x15\\n62\\n6641\\n1\\n1\\n13\\n775\\nDerive the dual quadratic program using Lagrange duality.\\n7.8 Consider the following convex optimization problem\\nmin\\nw2RD1\\n2w>w\\nsubject tow>x>1:\\nDerive the Lagrangian dual by introducing the Lagrange multiplier \\x15.\\n7.9 Consider the negative entropy of x2RD,\\nf(x) =DX\\nd=1xdlogxd:\\nDerive the convex conjugate function f\\x03(s), by assuming the standard dot\\nproduct.\\nHint: Take the gradient of an appropriate function and set the gradient to zero.\\n7.10 Consider the function\\nf(x) =1\\n2x>Ax+b>x+c;\\nwhereAis strictly positive deﬁnite, which means that it is invertible. Derive\\nthe convex conjugate of f(x).\\nHint: Take the gradient of an appropriate function and set the gradient to zero.\\n7.11 The hinge loss (which is the loss used by the support vector machine) is\\ngiven by\\nL(\\x0b) = maxf0;1\\x00\\x0bg;\\nIf we are interested in applying gradient methods such as L-BFGS, and do\\nnot want to resort to subgradient methods, we need to smooth the kink in\\nthe hinge loss. Compute the convex conjugate of the hinge loss L\\x03(\\x0c)where\\n\\x0cis the dual variable. Add a `2proximal term, and compute the conjugate\\nof the resulting function\\nL\\x03(\\x0c) +\\r\\n2\\x0c2;\\nwhere\\ris a given hyperparameter.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nPart II\\nCentral Machine Learning Problems\\n249\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n\\n8\\nWhen Models Meet Data\\nIn the ﬁrst part of the book, we introduced the mathematics that form\\nthe foundations of many machine learning methods. The hope is that a\\nreader would be able to learn the rudimentary forms of the language of\\nmathematics from the ﬁrst part, which we will now use to describe and\\ndiscuss machine learning. The second part of the book introduces four\\npillars of machine learning:\\nRegression (Chapter 9)\\nDimensionality reduction (Chapter 10)\\nDensity estimation (Chapter 11)\\nClassiﬁcation (Chapter 12)\\nThe main aim of this part of the book is to illustrate how the mathematical\\nconcepts introduced in the ﬁrst part of the book can be used to design\\nmachine learning algorithms that can be used to solve tasks within the\\nremit of the four pillars. We do not intend to introduce advanced machine\\nlearning concepts, but instead to provide a set of practical methods that\\nallow the reader to apply the knowledge they gained from the ﬁrst part\\nof the book. It also provides a gateway to the wider machine learning\\nliterature for readers already familiar with the mathematics.\\n8.1 Data, Models, and Learning\\nIt is worth at this point, to pause and consider the problem that a ma-\\nchine learning algorithm is designed to solve. As discussed in Chapter 1,\\nthere are three major components of a machine learning system: data,\\nmodels, and learning. The main question of machine learning is “What do\\nwe mean by good models?”. The word model has many subtleties, and we model\\nwill revisit it multiple times in this chapter. It is also not entirely obvious\\nhow to objectively deﬁne the word “good”. One of the guiding principles\\nof machine learning is that good models should perform well on unseen\\ndata. This requires us to deﬁne some performance metrics, such as accu-\\nracy or distance from ground truth, as well as ﬁguring out ways to do well\\nunder these performance metrics. This chapter covers a few necessary bits\\nand pieces of mathematical and statistical language that are commonly\\n251\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n252 When Models Meet Data\\nTable 8.1 Example\\ndata from a\\nﬁctitious human\\nresource database\\nthat is not in a\\nnumerical format.Name Gender Degree Postcode Age Annual salary\\nAditya M MSc W21BG 36 89563\\nBob M PhD EC1A1BA 47 123543\\nChlo´e F BEcon SW1A1BH 26 23989\\nDaisuke M BSc SE207AT 68 138769\\nElisabeth F MBA SE10AA 33 113888\\nused to talk about machine learning models. By doing so, we brieﬂy out-\\nline the current best practices for training a model such that the resulting\\npredictor does well on data that we have not yet seen.\\nAs mentioned in Chapter 1, there are two different senses in which we\\nuse the phrase “machine learning algorithm”: training and prediction. We\\nwill describe these ideas in this chapter, as well as the idea of selecting\\namong different models. We will introduce the framework of empirical\\nrisk minimization in Section 8.2, the principle of maximum likelihood in\\nSection 8.3, and the idea of probabilistic models in Section 8.4. We brieﬂy\\noutline a graphical language for specifying probabilistic models in Sec-\\ntion 8.5 and ﬁnally discuss model selection in Section 8.6. The rest of this\\nsection expands upon the three main components of machine learning:\\ndata, models and learning.\\n8.1.1 Data as Vectors\\nWe assume that our data can be read by a computer, and represented ade-\\nquately in a numerical format. Data is assumed to be tabular (Figure 8.1),\\nwhere we think of each row of the table as representing a particular in-\\nstance or example, and each column to be a particular feature. In recent Data is assumed to\\nbe in a tidy\\nformat (Wickham,\\n2014; Codd, 1990).years, machine learning has been applied to many types of data that do not\\nobviously come in the tabular numerical format, for example genomic se-\\nquences, text and image contents of a webpage, and social media graphs.\\nWe do not discuss the important and challenging aspects of identifying\\ngood features. Many of these aspects depend on domain expertise and re-\\nquire careful engineering, and, in recent years, they have been put under\\nthe umbrella of data science (Stray, 2016; Adhikari and DeNero, 2018).\\nEven when we have data in tabular format, there are still choices to be\\nmade to obtain a numerical representation. For example, in Table 8.1, the\\ngender column (a categorical variable) may be converted into numbers 0\\nrepresenting “Male” and 1representing “Female”. Alternatively, the gen-\\nder could be represented by numbers \\x001;+1, respectively (as shown in\\nTable 8.2). Furthermore, it is often important to use domain knowledge\\nwhen constructing the representation, such as knowing that university\\ndegrees progress from bachelor’s to master’s to PhD or realizing that the\\npostcode provided is not just a string of characters but actually encodes\\nan area in London. In Table 8.2, we converted the data from Table 8.1\\nto a numerical format, and each postcode is represented as two numbers,\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.1 Data, Models, and Learning 253\\nTable 8.2 Example\\ndata from a\\nﬁctitious human\\nresource database\\n(see Table 8.1),\\nconverted to a\\nnumerical format.Gender ID Degree Latitude Longitude Age Annual Salary\\n(in degrees) (in degrees) (in thousands)\\n-1 2 51.5073 0.1290 36 89.563\\n-1 3 51.5074 0.1275 47 123.543\\n+1 1 51.5071 0.1278 26 23.989\\n-1 1 51.5075 0.1281 68 138.769\\n+1 2 51.5074 0.1278 33 113.888\\na latitude and longitude. Even numerical data that could potentially be\\ndirectly read into a machine learning algorithm should be carefully con-\\nsidered for units, scaling, and constraints. Without additional information,\\none should shift and scale all columns of the dataset such that they have\\nan empirical mean of 0and an empirical variance of 1. For the purposes\\nof this book, we assume that a domain expert already converted data ap-\\npropriately, i.e., each input xnis aD-dimensional vector of real numbers,\\nwhich are called features ,attributes , orcovariates . We consider a dataset to feature\\nattribute\\ncovariatebe of the form as illustrated by Table 8.2. Observe that we have dropped\\nthe Name column of Table 8.1 in the new numerical representation. There\\nare two main reasons why this is desirable: (1) we do not expect the iden-\\ntiﬁer (the Name) to be informative for a machine learning task; and (2)\\nwe may wish to anonymize the data to help protect the privacy of the\\nemployees.\\nIn this part of the book, we will use Nto denote the number of exam-\\nples in a dataset and index the examples with lowercase n= 1;:::;N .\\nWe assume that we are given a set of numerical data, represented as an\\narray of vectors (Table 8.2). Each row is a particular individual xn, often\\nreferred to as an example ordata point in machine learning. The subscript example\\ndata point nrefers to the fact that this is the nth example out of a total of Nexam-\\nples in the dataset. Each column represents a particular feature of interest\\nabout the example, and we index the features as d= 1;:::;D . Recall that\\ndata is represented as vectors, which means that each example (each data\\npoint) is aD-dimensional vector. The orientation of the table originates\\nfrom the database community, but for some machine learning algorithms\\n(e.g., in Chapter 10) it is more convenient to represent examples as col-\\numn vectors.\\nLet us consider the problem of predicting annual salary from age, based\\non the data in Table 8.2. This is called a supervised learning problem\\nwhere we have a labelyn(the salary) associated with each example xn label\\n(the age). The label ynhas various other names, including target, re-\\nsponse variable, and annotation. A dataset is written as a set of example-\\nlabel pairsf(x1;y1);:::; (xn;yn);:::; (xN;yN)g. The table of examples\\nfx1;:::;xNgis often concatenated, and written as X2RN\\x02D. Fig-\\nure 8.1 illustrates the dataset consisting of the two rightmost columns\\nof Table 8.2, where x=age andy=salary.\\nWe use the concepts introduced in the ﬁrst part of the book to formalize\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n254 When Models Meet Data\\nFigure 8.1 Toy data\\nfor linear regression.\\nTraining data in\\n(xn;yn)pairs from\\nthe rightmost two\\ncolumns of\\nTable 8.2. We are\\ninterested in the\\nsalary of a person\\naged sixty (x= 60 )\\nillustrated as a\\nvertical dashed red\\nline, which is not\\npart of the training\\ndata.\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\n?\\nthe machine learning problems such as that in the previous paragraph.\\nRepresenting data as vectors xnallows us to use concepts from linear al-\\ngebra (introduced in Chapter 2). In many machine learning algorithms,\\nwe need to additionally be able to compare two vectors. As we will see in\\nChapters 9 and 12, computing the similarity or distance between two ex-\\namples allows us to formalize the intuition that examples with similar fea-\\ntures should have similar labels. The comparison of two vectors requires\\nthat we construct a geometry (explained in Chapter 3) and allows us to\\noptimize the resulting learning problem using techniques from Chapter 7.\\nSince we have vector representations of data, we can manipulate data to\\nﬁnd potentially better representations of it. We will discuss ﬁnding good\\nrepresentations in two ways: ﬁnding lower-dimensional approximations\\nof the original feature vector, and using nonlinear higher-dimensional\\ncombinations of the original feature vector. In Chapter 10, we will see an\\nexample of ﬁnding a low-dimensional approximation of the original data\\nspace by ﬁnding the principal components. Finding principal components\\nis closely related to concepts of eigenvalue and singular value decomposi-\\ntion as introduced in Chapter 4. For the high-dimensional representation,\\nwe will see an explicit feature map \\x1e(\\x01)that allows us to represent in- feature map\\nputsxnusing a higher-dimensional representation \\x1e(xn). The main mo-\\ntivation for higher-dimensional representations is that we can construct\\nnew features as non-linear combinations of the original features, which in\\nturn may make the learning problem easier. We will discuss the feature\\nmap in Section 9.2 and show how this feature map leads to a kernel in kernel\\nSection 12.4. In recent years, deep learning methods (Goodfellow et al.,\\n2016) have shown promise in using the data itself to learn new good fea-\\ntures and have been very successful in areas, such as computer vision,\\nspeech recognition, and natural language processing. We will not cover\\nneural networks in this part of the book, but the reader is referred to\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.1 Data, Models, and Learning 255\\nFigure 8.2 Example\\nfunction (black solid\\ndiagonal line) and\\nits prediction at\\nx= 60 , i.e.,\\nf(60) = 100 .\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nSection 5.6 for the mathematical description of backpropagation, a key\\nconcept for training neural networks.\\n8.1.2 Models as Functions\\nOnce we have data in an appropriate vector representation, we can get to\\nthe business of constructing a predictive function (known as a predictor ). predictor\\nIn Chapter 1, we did not yet have the language to be precise about models.\\nUsing the concepts from the ﬁrst part of the book, we can now introduce\\nwhat “model” means. We present two major approaches in this book: a\\npredictor as a function, and a predictor as a probabilistic model. We de-\\nscribe the former here and the latter in the next subsection.\\nA predictor is a function that, when given a particular input example\\n(in our case, a vector of features), produces an output. For now, consider\\nthe output to be a single number, i.e., a real-valued scalar output. This can\\nbe written as\\nf:RD!R; (8.1)\\nwhere the input vector xisD-dimensional (has Dfeatures), and the func-\\ntionfthen applied to it (written as f(x)) returns a real number. Fig-\\nure 8.2 illustrates a possible function that can be used to compute the\\nvalue of the prediction for input values x.\\nIn this book, we do not consider the general case of all functions, which\\nwould involve the need for functional analysis. Instead, we consider the\\nspecial case of linear functions\\nf(x) =\\x12>x+\\x120 (8.2)\\nfor unknown \\x12and\\x120. This restriction means that the contents of Chap-\\nters 2 and 3 sufﬁce for precisely stating the notion of a predictor for\\nthe non-probabilistic (in contrast to the probabilistic view described next)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n256 When Models Meet Data\\nFigure 8.3 Example\\nfunction (black solid\\ndiagonal line) and\\nits predictive\\nuncertainty at\\nx= 60 (drawn as a\\nGaussian).\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nview of machine learning. Linear functions strike a good balance between\\nthe generality of the problems that can be solved and the amount of back-\\nground mathematics that is needed.\\n8.1.3 Models as Probability Distributions\\nWe often consider data to be noisy observations of some true underlying\\neffect, and hope that by applying machine learning we can identify the\\nsignal from the noise. This requires us to have a language for quantify-\\ning the effect of noise. We often would also like to have predictors that\\nexpress some sort of uncertainty, e.g., to quantify the conﬁdence we have\\nabout the value of the prediction for a particular test data point. As we\\nhave seen in Chapter 6, probability theory provides a language for quan-\\ntifying uncertainty. Figure 8.3 illustrates the predictive uncertainty of the\\nfunction as a Gaussian distribution.\\nInstead of considering a predictor as a single function, we could con-\\nsider predictors to be probabilistic models, i.e., models describing the dis-\\ntribution of possible functions. We limit ourselves in this book to the spe-\\ncial case of distributions with ﬁnite-dimensional parameters, which allows\\nus to describe probabilistic models without needing stochastic processes\\nand random measures. For this special case, we can think about prob-\\nabilistic models as multivariate probability distributions, which already\\nallow for a rich class of models.\\nWe will introduce how to use concepts from probability (Chapter 6) to\\ndeﬁne machine learning models in Section 8.4, and introduce a graphical\\nlanguage for describing probabilistic models in a compact way in Sec-\\ntion 8.5.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.1 Data, Models, and Learning 257\\n8.1.4 Learning is Finding Parameters\\nThe goal of learning is to ﬁnd a model and its corresponding parame-\\nters such that the resulting predictor will perform well on unseen data.\\nThere are conceptually three distinct algorithmic phases when discussing\\nmachine learning algorithms:\\n1. Prediction or inference\\n2. Training or parameter estimation\\n3. Hyperparameter tuning or model selection\\nThe prediction phase is when we use a trained predictor on previously un-\\nseen test data. In other words, the parameters and model choice is already\\nﬁxed and the predictor is applied to new vectors representing new input\\ndata points. As outlined in Chapter 1 and the previous subsection, we will\\nconsider two schools of machine learning in this book, corresponding to\\nwhether the predictor is a function or a probabilistic model. When we\\nhave a probabilistic model (discussed further in Section 8.4) the predic-\\ntion phase is called inference.\\nRemark. Unfortunately, there is no agreed upon naming for the different\\nalgorithmic phases. The word “inference” is sometimes also used to mean\\nparameter estimation of a probabilistic model, and less often may be also\\nused to mean prediction for non-probabilistic models. }\\nThe training or parameter estimation phase is when we adjust our pre-\\ndictive model based on training data. We would like to ﬁnd good predic-\\ntors given training data, and there are two main strategies for doing so:\\nﬁnding the best predictor based on some measure of quality (sometimes\\ncalled ﬁnding a point estimate), or using Bayesian inference. Finding a\\npoint estimate can be applied to both types of predictors, but Bayesian\\ninference requires probabilistic models.\\nFor the non-probabilistic model, we follow the principle of empirical risk empirical risk\\nminimization minimization , which we describe in Section 8.2. Empirical risk minimiza-\\ntion directly provides an optimization problem for ﬁnding good parame-\\nters. With a statistical model, the principle of maximum likelihood is used maximum likelihood\\nto ﬁnd a good set of parameters (Section 8.3). We can additionally model\\nthe uncertainty of parameters using a probabilistic model, which we will\\nlook at in more detail in Section 8.4.\\nWe use numerical methods to ﬁnd good parameters that “ﬁt” the data,\\nand most training methods can be thought of as hill-climbing approaches\\nto ﬁnd the maximum of an objective, for example the maximum of a likeli-\\nhood. To apply hill-climbing approaches we use the gradients described in The convention in\\noptimization is to\\nminimize objectives.\\nHence, there is often\\nan extra minus sign\\nin machine learning\\nobjectives.Chapter 5 and implement numerical optimization approaches from Chap-\\nter 7.\\nAs mentioned in Chapter 1, we are interested in learning a model based\\non data such that it performs well on future data. It is not enough for\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n258 When Models Meet Data\\nthe model to only ﬁt the training data well, the predictor needs to per-\\nform well on unseen data. We simulate the behavior of our predictor on\\nfuture unseen data using cross-validation (Section 8.2.4). As we will see cross-validation\\nin this chapter, to achieve the goal of performing well on unseen data,\\nwe will need to balance between ﬁtting well on training data and ﬁnding\\n“simple” explanations of the phenomenon. This trade-off is achieved us-\\ning regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In\\nphilosophy, this is considered to be neither induction nor deduction, but\\nis called abduction . According to the Stanford Encyclopedia of Philosophy , abduction\\nabduction is the process of inference to the best explanation (Douven,\\n2017). A good movie title is\\n“AI abduction”. We often need to make high-level modeling decisions about the struc-\\nture of the predictor, such as the number of components to use or the\\nclass of probability distributions to consider. The choice of the number of\\ncomponents is an example of a hyperparameter , and this choice can af- hyperparameter\\nfect the performance of the model signiﬁcantly. The problem of choosing\\namong different models is called model selection , which we describe in model selection\\nSection 8.6. For non-probabilistic models, model selection is often done\\nusing nested cross-validation , which is described in Section 8.6.1. We also nested\\ncross-validation use model selection to choose hyperparameters of our model.\\nRemark. The distinction between parameters and hyperparameters is some-\\nwhat arbitrary, and is mostly driven by the distinction between what can\\nbe numerically optimized versus what needs to use search techniques.\\nAnother way to consider the distinction is to consider parameters as the\\nexplicit parameters of a probabilistic model, and to consider hyperparam-\\neters (higher-level parameters) as parameters that control the distribution\\nof these explicit parameters. }\\nIn the following sections, we will look at three ﬂavors of machine learn-\\ning: empirical risk minimization (Section 8.2), the principle of maximum\\nlikelihood (Section 8.3), and probabilistic modeling (Section 8.4).\\n8.2 Empirical Risk Minimization\\nAfter having all the mathematics under our belt, we are now in a posi-\\ntion to introduce what it means to learn. The “learning” part of machine\\nlearning boils down to estimating parameters based on training data.\\nIn this section, we consider the case of a predictor that is a function,\\nand consider the case of probabilistic models in Section 8.3. We describe\\nthe idea of empirical risk minimization, which was originally popularized\\nby the proposal of the support vector machine (described in Chapter 12).\\nHowever, its general principles are widely applicable and allow us to ask\\nthe question of what is learning without explicitly constructing probabilis-\\ntic models. There are four main design choices, which we will cover in\\ndetail in the following subsections:\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.2 Empirical Risk Minimization 259\\nSection 8.2.1 What is the set of functions we allow the predictor to take?\\nSection 8.2.2 How do we measure how well the predictor performs on\\nthe training data?\\nSection 8.2.3 How do we construct predictors from only training data\\nthat performs well on unseen test data?\\nSection 8.2.4 What is the procedure for searching over the space of mod-\\nels?\\n8.2.1 Hypothesis Class of Functions\\nAssume we are given Nexamplesxn2RDand corresponding scalar la-\\nbelsyn2R. We consider the supervised learning setting, where we obtain\\npairs (x1;y1);:::; (xN;yN). Given this data, we would like to estimate a\\npredictorf(\\x01;\\x12) :RD!R, parametrized by \\x12. We hope to be able to ﬁnd\\na good parameter \\x12\\x03such that we ﬁt the data well, that is,\\nf(xn;\\x12\\x03)\\x19ynfor alln= 1;:::;N: (8.3)\\nIn this section, we use the notation ^yn=f(xn;\\x12\\x03)to represent the output\\nof the predictor.\\nRemark. For ease of presentation, we will describe empirical risk mini-\\nmization in terms of supervised learning (where we have labels). This\\nsimpliﬁes the deﬁnition of the hypothesis class and the loss function. It\\nis also common in machine learning to choose a parametrized class of\\nfunctions, for example afﬁne functions. }\\nExample 8.1\\nWe introduce the problem of ordinary least-squares regression to illustrate\\nempirical risk minimization. A more comprehensive account of regression\\nis given in Chapter 9. When the label ynis real-valued, a popular choice\\nof function class for predictors is the set of afﬁne functions. We choose a Afﬁne functions are\\noften referred to as\\nlinear functions in\\nmachine learning.more compact notation for an afﬁne function by concatenating an addi-\\ntional unit feature x(0)= 1toxn, i.e.,xn= [1;x(1)\\nn;x(2)\\nn;:::;x(D)\\nn]>. The\\nparameter vector is correspondingly \\x12= [\\x120;\\x121;\\x122;:::;\\x12D]>, allowing us\\nto write the predictor as a linear function\\nf(xn;\\x12) =\\x12>xn: (8.4)\\nThis linear predictor is equivalent to the afﬁne model\\nf(xn;\\x12) =\\x120+DX\\nd=1\\x12dx(d)\\nn: (8.5)\\nThe predictor takes the vector of features representing a single example\\nxnas input and produces a real-valued output, i.e., f:RD+1!R. The\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n260 When Models Meet Data\\nprevious ﬁgures in this chapter had a straight line as a predictor, which\\nmeans that we have assumed an afﬁne function.\\nInstead of a linear function, we may wish to consider non-linear func-\\ntions as predictors. Recent advances in neural networks allow for efﬁcient\\ncomputation of more complex non-linear function classes.\\nGiven the class of functions, we want to search for a good predictor.\\nWe now move on to the second ingredient of empirical risk minimization:\\nhow to measure how well the predictor ﬁts the training data.\\n8.2.2 Loss Function for Training\\nConsider the label ynfor a particular example; and the corresponding pre-\\ndiction ^ynthat we make based on xn. To deﬁne what it means to ﬁt the\\ndata well, we need to specify a loss function `(yn;^yn)that takes the ground loss function\\ntruth label and the prediction as input and produces a non-negative num-\\nber (referred to as the loss) representing how much error we have made\\non this particular prediction. Our goal for ﬁnding a good parameter vector The expression\\n“error” is often used\\nto mean loss.\\x12\\x03is to minimize the average loss on the set of Ntraining examples.\\nOne assumption that is commonly made in machine learning is that\\nthe set of examples (x1;y1);:::; (xN;yN)isindependent and identically independent and\\nidentically\\ndistributeddistributed . The word independent (Section 6.4.5) means that two data\\npoints (xi;yi)and(xj;yj)do not statistically depend on each other, mean-\\ning that the empirical mean is a good estimate of the population mean\\n(Section 6.4.1). This implies that we can use the empirical mean of the\\nloss on the training data. For a given training setf(x1;y1);:::; (xN;yN)g, training set\\nwe introduce the notation of an example matrix X:= [x1;:::;xN]>2\\nRN\\x02Dand a label vector y:= [y1;:::;yN]>2RN. Using this matrix\\nnotation the average loss is given by\\nRemp(f;X;y) =1\\nNNX\\nn=1`(yn;^yn); (8.6)\\nwhere ^yn=f(xn;\\x12). Equation (8.6) is called the empirical risk and de- empirical risk\\npends on three arguments, the predictor fand the dataX;y. This general\\nstrategy for learning is called empirical risk minimization . empirical risk\\nminimization\\nExample 8.2 (Least-Squares Loss)\\nContinuing the example of least-squares regression, we specify that we\\nmeasure the cost of making an error during training using the squared\\nloss`(yn;^yn) = (yn\\x00^yn)2. We wish to minimize the empirical risk (8.6),\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.2 Empirical Risk Minimization 261\\nwhich is the average of the losses over the data\\nmin\\n\\x122RD1\\nNNX\\nn=1(yn\\x00f(xn;\\x12))2; (8.7)\\nwhere we substituted the predictor ^yn=f(xn;\\x12). By using our choice of\\na linear predictor f(xn;\\x12) =\\x12>xn, we obtain the optimization problem\\nmin\\n\\x122RD1\\nNNX\\nn=1(yn\\x00\\x12>xn)2: (8.8)\\nThis equation can be equivalently expressed in matrix form\\nmin\\n\\x122RD1\\nNky\\x00X\\x12k2: (8.9)\\nThis is known as the least-squares problem . There exists a closed-form an- least-squares\\nproblemalytic solution for this by solving the normal equations, which we will\\ndiscuss in Section 9.2.\\nWe are not interested in a predictor that only performs well on the\\ntraining data. Instead, we seek a predictor that performs well (has low\\nrisk) on unseen test data. More formally, we are interested in ﬁnding a\\npredictorf(with parameters ﬁxed) that minimizes the expected risk expected risk\\nRtrue(f) =Ex;y[`(y;f(x))]; (8.10)\\nwhereyis the label and f(x)is the prediction based on the example x.\\nThe notation Rtrue(f)indicates that this is the true risk if we had access to\\nan inﬁnite amount of data. The expectation is over the (inﬁnite) set of all Another phrase\\ncommonly used for\\nexpected risk is\\n“population risk”.possible data and labels. There are two practical questions that arise from\\nour desire to minimize expected risk, which we address in the following\\ntwo subsections:\\nHow should we change our training procedure to generalize well?\\nHow do we estimate expected risk from (ﬁnite) data?\\nRemark. Many machine learning tasks are speciﬁed with an associated\\nperformance measure, e.g., accuracy of prediction or root mean squared\\nerror. The performance measure could be more complex, be cost sensitive,\\nand capture details about the particular application. In principle, the de-\\nsign of the loss function for empirical risk minimization should correspond\\ndirectly to the performance measure speciﬁed by the machine learning\\ntask. In practice, there is often a mismatch between the design of the loss\\nfunction and the performance measure. This could be due to issues such\\nas ease of implementation or efﬁciency of optimization. }\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n262 When Models Meet Data\\n8.2.3 Regularization to Reduce Overﬁtting\\nThis section describes an addition to empirical risk minimization that al-\\nlows it to generalize well (approximately minimizing expected risk). Re-\\ncall that the aim of training a machine learning predictor is so that we can\\nperform well on unseen data, i.e., the predictor generalizes well. We sim-\\nulate this unseen data by holding out a proportion of the whole dataset.\\nThis hold out set is referred to as the test set . Given a sufﬁciently rich class test set\\nEven knowing only\\nthe performance of\\nthe predictor on the\\ntest set leaks\\ninformation (Blum\\nand Hardt, 2015).of functions for the predictor f, we can essentially memorize the training\\ndata to obtain zero empirical risk. While this is great to minimize the loss\\n(and therefore the risk) on the training data, we would not expect the\\npredictor to generalize well to unseen data. In practice, we have only a\\nﬁnite set of data, and hence we split our data into a training and a test\\nset. The training set is used to ﬁt the model, and the test set (not seen\\nby the machine learning algorithm during training) is used to evaluate\\ngeneralization performance. It is important for the user to not cycle back\\nto a new round of training after having observed the test set. We use the\\nsubscripts trainand testto denote the training and test sets, respectively.\\nWe will revisit this idea of using a ﬁnite dataset to evaluate expected risk\\nin Section 8.2.4.\\nIt turns out that empirical risk minimization can lead to overﬁtting , i.e., overﬁtting\\nthe predictor ﬁts too closely to the training data and does not general-\\nize well to new data (Mitchell, 1997). This general phenomenon of hav-\\ning very small average loss on the training set but large average loss on\\nthe test set tends to occur when we have little data and a complex hy-\\npothesis class. For a particular predictor f(with parameters ﬁxed), the\\nphenomenon of overﬁtting occurs when the risk estimate from the train-\\ning data Remp(f;Xtrain;ytrain)underestimates the expected risk Rtrue(f).\\nSince we estimate the expected risk Rtrue(f)by using the empirical risk\\non the test set Remp(f;Xtest;ytest)if the test risk is much larger than\\nthe training risk, this is an indication of overﬁtting. We revisit the idea of\\noverﬁtting in Section 8.3.3.\\nTherefore, we need to somehow bias the search for the minimizer of\\nempirical risk by introducing a penalty term, which makes it harder for\\nthe optimizer to return an overly ﬂexible predictor. In machine learning,\\nthe penalty term is referred to as regularization . Regularization is a way regularization\\nto compromise between accurate solution of empirical risk minimization\\nand the size or complexity of the solution.\\nExample 8.3 (Regularized Least Squares)\\nRegularization is an approach that discourages complex or extreme solu-\\ntions to an optimization problem. The simplest regularization strategy is\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.2 Empirical Risk Minimization 263\\nto replace the least-squares problem\\nmin\\n\\x121\\nNky\\x00X\\x12k2: (8.11)\\nin the previous example with the “regularized” problem by adding a\\npenalty term involving only \\x12:\\nmin\\n\\x121\\nNky\\x00X\\x12k2+\\x15k\\x12k2: (8.12)\\nThe additional term k\\x12k2is called the regularizer , and the parameter regularizer\\n\\x15is the regularization parameter . The regularization parameter trades regularization\\nparameteroff minimizing the loss on the training set and the magnitude of the pa-\\nrameters\\x12. It often happens that the magnitude of the parameter values\\nbecomes relatively large if we run into overﬁtting (Bishop, 2006).\\nThe regularization term is sometimes called the penalty term , which bi- penalty term\\nases the vector \\x12to be closer to the origin. The idea of regularization also\\nappears in probabilistic models as the prior probability of the parameters.\\nRecall from Section 6.6 that for the posterior distribution to be of the same\\nform as the prior distribution, the prior and the likelihood need to be con-\\njugate. We will revisit this idea in Section 8.3.2. We will see in Chapter 12\\nthat the idea of the regularizer is equivalent to the idea of a large margin.\\n8.2.4 Cross-Validation to Assess the Generalization Performance\\nWe mentioned in the previous section that we measure the generalization\\nerror by estimating it by applying the predictor on test data. This data is\\nalso sometimes referred to as the validation set . The validation set is a sub- validation set\\nset of the available training data that we keep aside. A practical issue with\\nthis approach is that the amount of data is limited, and ideally we would\\nuse as much of the data available to train the model. This would require\\nus to keep our validation set Vsmall, which then would lead to a noisy\\nestimate (with high variance) of the predictive performance. One solu-\\ntion to these contradictory objectives (large training set, large validation\\nset) is to use cross-validation .K-fold cross-validation effectively partitions cross-validation\\nthe data into Kchunks,K\\x001of which form the training set R, and\\nthe last chunk serves as the validation set V(similar to the idea outlined\\npreviously). Cross-validation iterates through (ideally) all combinations\\nof assignments of chunks to RandV; see Figure 8.4. This procedure is\\nrepeated for all Kchoices for the validation set, and the performance of\\nthe model from the Kruns is averaged.\\nWe partition our dataset into two sets D=R[V , such that they do not\\noverlap (R\\\\V =;), whereVis the validation set, and train our model\\nonR. After training, we assess the performance of the predictor fon the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n264 When Models Meet Data\\nFigure 8.4K-fold\\ncross-validation.\\nThe dataset is\\ndivided into K= 5\\nchunks,K\\x001of\\nwhich serve as the\\ntraining set (blue)\\nand one as the\\nvalidation set\\n(orange hatch).Training\\nValidation\\nvalidation setV(e.g., by computing root mean square error (RMSE) of\\nthe trained model on the validation set). More precisely, for each partition\\nkthe training dataR(k)produces a predictor f(k), which is then applied\\nto validation setV(k)to compute the empirical risk R(f(k);V(k)). We cycle\\nthrough all possible partitionings of validation and training sets and com-\\npute the average generalization error of the predictor. Cross-validation\\napproximates the expected generalization error\\nEV[R(f;V)]\\x191\\nKKX\\nk=1R(f(k);V(k)); (8.13)\\nwhereR(f(k);V(k))is the risk (e.g., RMSE) on the validation set V(k)for\\npredictorf(k). The approximation has two sources: ﬁrst, due to the ﬁnite\\ntraining set, which results in not the best possible f(k); and second, due to\\nthe ﬁnite validation set, which results in an inaccurate estimation of the\\nriskR(f(k);V(k)). A potential disadvantage of K-fold cross-validation is\\nthe computational cost of training the model Ktimes, which can be bur-\\ndensome if the training cost is computationally expensive. In practice, it\\nis often not sufﬁcient to look at the direct parameters alone. For example,\\nwe need to explore multiple complexity parameters (e.g., multiple regu-\\nlarization parameters), which may not be direct parameters of the model.\\nEvaluating the quality of the model, depending on these hyperparameters,\\nmay result in a number of training runs that is exponential in the number\\nof model parameters. One can use nested cross-validation (Section 8.6.1)\\nto search for good hyperparameters.\\nHowever, cross-validation is an embarrassingly parallel problem, i.e., lit- embarrassingly\\nparallel tle effort is needed to separate the problem into a number of parallel\\ntasks. Given sufﬁcient computing resources (e.g., cloud computing, server\\nfarms), cross-validation does not require longer than a single performance\\nassessment.\\nIn this section, we saw that empirical risk minimization is based on the\\nfollowing concepts: the hypothesis class of functions, the loss function and\\nregularization. In Section 8.3, we will see the effect of using a probability\\ndistribution to replace the idea of loss functions and regularization.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.3 Parameter Estimation 265\\n8.2.5 Further Reading\\nDue to the fact that the original development of empirical risk minimiza-\\ntion (Vapnik, 1998) was couched in heavily theoretical language, many\\nof the subsequent developments have been theoretical. The area of study\\nis called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statistical learning\\ntheory Hastie et al., 2001; von Luxburg and Sch ¨olkopf, 2011). A recent machine\\nlearning textbook that builds on the theoretical foundations and develops\\nefﬁcient learning algorithms is Shalev-Shwartz and Ben-David (2014).\\nThe concept of regularization has its roots in the solution of ill-posed in-\\nverse problems (Neumaier, 1998). The approach presented here is called\\nTikhonov regularization , and there is a closely related constrained version Tikhonov\\nregularization called Ivanov regularization. Tikhonov regularization has deep relation-\\nships to the bias-variance trade-off and feature selection (B ¨uhlmann and\\nVan De Geer, 2011). An alternative to cross-validation is bootstrap and\\njackknife (Efron and Tibshirani, 1993; Davidson and Hinkley, 1997; Hall,\\n1992).\\nThinking about empirical risk minimization (Section 8.2) as “probabil-\\nity free” is incorrect. There is an underlying unknown probability distri-\\nbutionp(x;y)that governs the data generation. However, the approach\\nof empirical risk minimization is agnostic to that choice of distribution.\\nThis is in contrast to standard statistical approaches that explicitly re-\\nquire the knowledge of p(x;y). Furthermore, since the distribution is a\\njoint distribution on both examples xand labelsy, the labels can be non-\\ndeterministic. In contrast to standard statistics we do not need to specify\\nthe noise distribution for the labels y.\\n8.3 Parameter Estimation\\nIn Section 8.2, we did not explicitly model our problem using probability\\ndistributions. In this section, we will see how to use probability distribu-\\ntions to model our uncertainty due to the observation process and our\\nuncertainty in the parameters of our predictors. In Section 8.3.1, we in-\\ntroduce the likelihood, which is analogous to the concept of loss functions\\n(Section 8.2.2) in empirical risk minimization. The concept of priors (Sec-\\ntion 8.3.2) is analogous to the concept of regularization (Section 8.2.3).\\n8.3.1 Maximum Likelihood Estimation\\nThe idea behind maximum likelihood estimation (MLE) is to deﬁne a func- maximum likelihood\\nestimation tion of the parameters that enables us to ﬁnd a model that ﬁts the data\\nwell. The estimation problem is focused on the likelihood function, or likelihood\\nmore precisely its negative logarithm. For data represented by a random\\nvariablexand for a family of probability densities p(xj\\x12)parametrized\\nby\\x12, the negative log-likelihood is given by negative\\nlog-likelihood\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n266 When Models Meet Data\\nLx(\\x12) =\\x00logp(xj\\x12): (8.14)\\nThe notationLx(\\x12)emphasizes the fact that the parameter \\x12is varying\\nand the dataxis ﬁxed. We very often drop the reference to xwhen writing\\nthe negative log-likelihood, as it is really a function of \\x12, and write it as\\nL(\\x12)when the random variable representing the uncertainty in the data\\nis clear from the context.\\nLet us interpret what the probability density p(xj\\x12)is modeling for a\\nﬁxed value of \\x12. It is a distribution that models the uncertainty of the data\\nfor a given parameter setting. For a given dataset x, the likelihood allows\\nus to express preferences about different settings of the parameters \\x12, and\\nwe can choose the setting that more “likely” has generated the data.\\nIn a complementary view, if we consider the data to be ﬁxed (because\\nit has been observed), and we vary the parameters \\x12, what doesL(\\x12)tell\\nus? It tells us how likely a particular setting of \\x12is for the observations x.\\nBased on this second view, the maximum likelihood estimator gives us the\\nmost likely parameter \\x12for the set of data.\\nWe consider the supervised learning setting, where we obtain pairs\\n(x1;y1);:::; (xN;yN)withxn2RDand labelsyn2R. We are inter-\\nested in constructing a predictor that takes a feature vector xnas input\\nand produces a prediction yn(or something close to it), i.e., given a vec-\\ntorxnwe want the probability distribution of the label yn. In other words,\\nwe specify the conditional probability distribution of the labels given the\\nexamples for the particular parameter setting \\x12.\\nExample 8.4\\nThe ﬁrst example that is often used is to specify that the conditional\\nprobability of the labels given the examples is a Gaussian distribution. In\\nother words, we assume that we can explain our observation uncertainty\\nby independent Gaussian noise (refer to Section 6.5) with zero mean,\\n\"n\\x18N\\x000; \\x1b2\\x01\\n. We further assume that the linear model x>\\nn\\x12is used for\\nprediction. This means we specify a Gaussian likelihood for each example\\nlabel pair (xn;yn),\\np(ynjxn;\\x12) =N\\x00ynjx>\\nn\\x12; \\x1b2\\x01: (8.15)\\nAn illustration of a Gaussian likelihood for a given parameter \\x12is shown\\nin Figure 8.3. We will see in Section 9.2 how to explicitly expand the\\npreceding expression out in terms of the Gaussian distribution.\\nWe assume that the set of examples (x1;y1);:::; (xN;yN)areindependent independent and\\nidentically\\ndistributedand identically distributed (i.i.d.). The word “independent” (Section 6.4.5)\\nimplies that the likelihood involving the whole dataset ( Y=fy1;:::;yNg\\nandX=fx1;:::;xNg) factorizes into a product of the likelihoods of\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.3 Parameter Estimation 267\\neach individual example\\np(YjX;\\x12) =NY\\nn=1p(ynjxn;\\x12); (8.16)\\nwherep(ynjxn;\\x12)is a particular distribution (which was Gaussian in Ex-\\nample 8.4). The expression “identically distributed” means that each term\\nin the product (8.16) is of the same distribution, and all of them share\\nthe same parameters. It is often easier from an optimization viewpoint to\\ncompute functions that can be decomposed into sums of simpler functions.\\nHence, in machine learning we often consider the negative log-likelihood Recall log(ab) =\\nlog(a) + log(b)\\nL(\\x12) =\\x00logp(YjX;\\x12) =\\x00NX\\nn=1logp(ynjxn;\\x12): (8.17)\\nWhile it is temping to interpret the fact that \\x12is on the right of the condi-\\ntioning inp(ynjxn;\\x12)(8.15), and hence should be interpreted as observed\\nand ﬁxed, this interpretation is incorrect. The negative log-likelihood L(\\x12)\\nis a function of \\x12. Therefore, to ﬁnd a good parameter vector \\x12that\\nexplains the data (x1;y1);:::; (xN;yN)well, minimize the negative log-\\nlikelihoodL(\\x12)with respect to \\x12.\\nRemark. The negative sign in (8.17) is a historical artifact that is due\\nto the convention that we want to maximize likelihood, but numerical\\noptimization literature tends to study minimization of functions. }\\nExample 8.5\\nContinuing on our example of Gaussian likelihoods (8.15), the negative\\nlog-likelihood can be rewritten as\\nL(\\x12) =\\x00NX\\nn=1logp(ynjxn;\\x12) =\\x00NX\\nn=1logN\\x00ynjx>\\nn\\x12; \\x1b2\\x01\\n(8.18a)\\n=\\x00NX\\nn=1log1p\\n2\\x19\\x1b2exp\\x12\\n\\x00(yn\\x00x>\\nn\\x12)2\\n2\\x1b2\\x13\\n(8.18b)\\n=\\x00NX\\nn=1log exp\\x12\\n\\x00(yn\\x00x>\\nn\\x12)2\\n2\\x1b2\\x13\\n\\x00NX\\nn=1log1p\\n2\\x19\\x1b2(8.18c)\\n=1\\n2\\x1b2NX\\nn=1(yn\\x00x>\\nn\\x12)2\\x00NX\\nn=1log1p\\n2\\x19\\x1b2: (8.18d)\\nAs\\x1bis given, the second term in (8.18d) is constant, and minimizing L(\\x12)\\ncorresponds to solving the least-squares problem (compare with (8.8))\\nexpressed in the ﬁrst term.\\nIt turns out that for Gaussian likelihoods the resulting optimization\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n268 When Models Meet Data\\nFigure 8.5 For the\\ngiven data, the\\nmaximum likelihood\\nestimate of the\\nparameters results\\nin the black\\ndiagonal line. The\\norange square\\nshows the value of\\nthe maximum\\nlikelihood\\nprediction at\\nx= 60 .\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nFigure 8.6\\nComparing the\\npredictions with the\\nmaximum likelihood\\nestimate and the\\nMAP estimate at\\nx= 60 . The prior\\nbiases the slope to\\nbe less steep and the\\nintercept to be\\ncloser to zero. In\\nthis example, the\\nbias that moves the\\nintercept closer to\\nzero actually\\nincreases the slope.\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nMLE\\nMAP\\nproblem corresponding to maximum likelihood estimation has a closed-\\nform solution. We will see more details on this in Chapter 9. Figure 8.5\\nshows a regression dataset and the function that is induced by the maxi-\\nmum-likelihood parameters. Maximum likelihood estimation may suffer\\nfrom overﬁtting (Section 8.3.3), analogous to unregularized empirical risk\\nminimization (Section 8.2.3). For other likelihood functions, i.e., if we\\nmodel our noise with non-Gaussian distributions, maximum likelihood es-\\ntimation may not have a closed-form analytic solution. In this case, we\\nresort to numerical optimization methods discussed in Chapter 7.\\n8.3.2 Maximum A Posteriori Estimation\\nIf we have prior knowledge about the distribution of the parameters \\x12, we\\ncan multiply an additional term to the likelihood. This additional term is\\na prior probability distribution on parameters p(\\x12). For a given prior, after\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.3 Parameter Estimation 269\\nobserving some data x, how should we update the distribution of \\x12? In\\nother words, how should we represent the fact that we have more speciﬁc\\nknowledge of \\x12after observing data x? Bayes’ theorem, as discussed in\\nSection 6.3, gives us a principled tool to update our probability distribu-\\ntions of random variables. It allows us to compute a posterior distribution posterior\\np(\\x12jx)(the more speciﬁc knowledge) on the parameters \\x12from general\\nprior statements (prior distribution) p(\\x12)and the function p(xj\\x12)that prior\\nlinks the parameters \\x12and the observed data x(called the likelihood ): likelihood\\np(\\x12jx) =p(xj\\x12)p(\\x12)\\np(x): (8.19)\\nRecall that we are interested in ﬁnding the parameter \\x12that maximizes\\nthe posterior. Since the distribution p(x)does not depend on \\x12, we can\\nignore the value of the denominator for the optimization and obtain\\np(\\x12jx)/p(xj\\x12)p(\\x12): (8.20)\\nThe preceding proportion relation hides the density of the data p(x),\\nwhich may be difﬁcult to estimate. Instead of estimating the minimum\\nof the negative log-likelihood, we now estimate the minimum of the neg-\\native log-posterior, which is referred to as maximum a posteriori estima- maximum a\\nposteriori\\nestimationtion(MAP estimation ). An illustration of the effect of adding a zero-mean\\nMAP estimationGaussian prior is shown in Figure 8.6.\\nExample 8.6\\nIn addition to the assumption of Gaussian likelihood in the previous exam-\\nple, we assume that the parameter vector is distributed as a multivariate\\nGaussian with zero mean, i.e., p(\\x12) =N\\x000;\\x06\\x01\\n, where \\x06is the covari-\\nance matrix (Section 6.5). Note that the conjugate prior of a Gaussian\\nis also a Gaussian (Section 6.6.1), and therefore we expect the posterior\\ndistribution to also be a Gaussian. We will see the details of maximum a\\nposteriori estimation in Chapter 9.\\nThe idea of including prior knowledge about where good parameters\\nlie is widespread in machine learning. An alternative view, which we saw\\nin Section 8.2.3, is the idea of regularization, which introduces an addi-\\ntional term that biases the resulting parameters to be close to the origin.\\nMaximum a posteriori estimation can be considered to bridge the non-\\nprobabilistic and probabilistic worlds as it explicitly acknowledges the\\nneed for a prior distribution but it still only produces a point estimate\\nof the parameters.\\nRemark. The maximum likelihood estimate \\x12MLpossesses the following\\nproperties (Lehmann and Casella, 1998; Efron and Hastie, 2016):\\nAsymptotic consistency: The MLE converges to the true value in the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n270 When Models Meet Data\\nFigure 8.7 Model\\nﬁtting. In a\\nparametrized class\\nM\\x12of models, we\\noptimize the model\\nparameters\\x12to\\nminimize the\\ndistance to the true\\n(unknown) model\\nM\\x03.M\\x12\\nM\\x03M\\x12\\x03\\nM\\x120\\nlimit of inﬁnitely many observations, plus a random error that is ap-\\nproximately normal.\\nThe size of the samples necessary to achieve these properties can be\\nquite large.\\nThe error’s variance decays in 1=N, whereNis the number of data\\npoints.\\nEspecially, in the “small” data regime, maximum likelihood estimation\\ncan lead to overﬁtting .\\n}\\nThe principle of maximum likelihood estimation (and maximum a pos-\\nteriori estimation) uses probabilistic modeling to reason about the uncer-\\ntainty in the data and model parameters. However, we have not yet taken\\nprobabilistic modeling to its full extent. In this section, the resulting train-\\ning procedure still produces a point estimate of the predictor, i.e., training\\nreturns one single set of parameter values that represent the best predic-\\ntor. In Section 8.4, we will take the view that the parameter values should\\nalso be treated as random variables, and instead of estimating “best” val-\\nues of that distribution, we will use the full parameter distribution when\\nmaking predictions.\\n8.3.3 Model Fitting\\nConsider the setting where we are given a dataset, and we are interested\\nin ﬁtting a parametrized model to the data. When we talk about “ﬁt-\\nting”, we typically mean optimizing/learning model parameters so that\\nthey minimize some loss function, e.g., the negative log-likelihood. With\\nmaximum likelihood (Section 8.3.1) and maximum a posteriori estima-\\ntion (Section 8.3.2), we already discussed two commonly used algorithms\\nfor model ﬁtting.\\nThe parametrization of the model deﬁnes a model class M\\x12with which\\nwe can operate. For example, in a linear regression setting, we may deﬁne\\nthe relationship between inputs xand (noise-free) observations yto be\\ny=ax+b, where\\x12:=fa;bgare the model parameters. In this case, the\\nmodel parameters \\x12describe the family of afﬁne functions, i.e., straight\\nlines with slope a, which are offset from 0byb. Assume the data comes\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.3 Parameter Estimation 271\\nFigure 8.8 Fitting\\n(by maximum\\nlikelihood) of\\ndifferent model\\nclasses to a\\nregression dataset.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(a) Overﬁtting\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (b) Underﬁtting.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (c) Fitting well.\\nfrom a model M\\x03, which is unknown to us. For a given training dataset,\\nwe optimize\\x12so thatM\\x12is as close as possible to M\\x03, where the “close-\\nness” is deﬁned by the objective function we optimize (e.g., squared loss\\non the training data). Figure 8.7 illustrates a setting where we have a small\\nmodel class (indicated by the circle M\\x12), and the data generation model\\nM\\x03lies outside the set of considered models. We begin our parameter\\nsearch atM\\x120. After the optimization, i.e., when we obtain the best pos-\\nsible parameters \\x12\\x03, we distinguish three different cases: (i) overﬁtting,\\n(ii) underﬁtting, and (iii) ﬁtting well. We will give a high-level intuition\\nof what these three concepts mean.\\nRoughly speaking, overﬁtting refers to the situation where the para- overﬁtting\\nmetrized model class is too rich to model the dataset generated by M\\x03,\\ni.e.,M\\x12could model much more complicated datasets. For instance, if the\\ndataset was generated by a linear function, and we deﬁne M\\x12to be the\\nclass of seventh-order polynomials, we could model not only linear func-\\ntions, but also polynomials of degree two, three, etc. Models that over-\\nﬁt typically have a large number of parameters. An observation we often One way to detect\\noverﬁtting in\\npractice is to\\nobserve that the\\nmodel has low\\ntraining risk but\\nhigh test risk during\\ncross validation\\n(Section 8.2.4).make is that the overly ﬂexible model class M\\x12uses all its modeling power\\nto reduce the training error. If the training data is noisy, it will therefore\\nﬁnd some useful signal in the noise itself. This will cause enormous prob-\\nlems when we predict away from the training data. Figure 8.8(a) gives an\\nexample of overﬁtting in the context of regression where the model pa-\\nrameters are learned by means of maximum likelihood (see Section 8.3.1).\\nWe will discuss overﬁtting in regression more in Section 9.2.2.\\nWhen we run into underﬁtting , we encounter the opposite problem underﬁtting\\nwhere the model class M\\x12is not rich enough. For example, if our dataset\\nwas generated by a sinusoidal function, but \\x12only parametrizes straight\\nlines, the best optimization procedure will not get us close to the true\\nmodel. However, we still optimize the parameters and ﬁnd the best straight\\nline that models the dataset. Figure 8.8(b) shows an example of a model\\nthat underﬁts because it is insufﬁciently ﬂexible. Models that underﬁt typ-\\nically have few parameters.\\nThe third case is when the parametrized model class is about right.\\nThen, our model ﬁts well, i.e., it neither overﬁts nor underﬁts. This means\\nour model class is just rich enough to describe the dataset we are given.\\nFigure 8.8(c) shows a model that ﬁts the given dataset fairly well. Ideally,\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n272 When Models Meet Data\\nthis is the model class we would want to work with since it has good\\ngeneralization properties.\\nIn practice, we often deﬁne very rich model classes M\\x12with many pa-\\nrameters, such as deep neural networks. To mitigate the problem of over-\\nﬁtting, we can use regularization (Section 8.2.3) or priors (Section 8.3.2).\\nWe will discuss how to choose the model class in Section 8.6.\\n8.3.4 Further Reading\\nWhen considering probabilistic models, the principle of maximum likeli-\\nhood estimation generalizes the idea of least-squares regression for linear\\nmodels, which we will discuss in detail in Chapter 9. When restricting\\nthe predictor to have linear form with an additional nonlinear function \\'\\napplied to the output, i.e.,\\np(ynjxn;\\x12) =\\'(\\x12>xn); (8.21)\\nwe can consider other models for other prediction tasks, such as binary\\nclassiﬁcation or modeling count data (McCullagh and Nelder, 1989). An\\nalternative view of this is to consider likelihoods that are from the ex-\\nponential family (Section 6.6). The class of models, which have linear\\ndependence between parameters and data, and have potentially nonlin-\\near transformation \\'(called a link function ), is referred to as generalized link function\\ngeneralized linear\\nmodellinear models (Agresti, 2002, chapter 4).\\nMaximum likelihood estimation has a rich history, and was originally\\nproposed by Sir Ronald Fisher in the 1930s. We will expand upon the idea\\nof a probabilistic model in Section 8.4. One debate among researchers\\nwho use probabilistic models, is the discussion between Bayesian and fre-\\nquentist statistics. As mentioned in Section 6.1.1, it boils down to the\\ndeﬁnition of probability. Recall from Section 6.1 that one can consider\\nprobability to be a generalization (by allowing uncertainty) of logical rea-\\nsoning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-\\nlihood estimation is frequentist in nature, and the interested reader is\\npointed to Efron and Hastie (2016) for a balanced view of both Bayesian\\nand frequentist statistics.\\nThere are some probabilistic models where maximum likelihood esti-\\nmation may not be possible. The reader is referred to more advanced sta-\\ntistical textbooks, e.g., Casella and Berger (2002), for approaches, such as\\nmethod of moments, M-estimation, and estimating equations.\\n8.4 Probabilistic Modeling and Inference\\nIn machine learning, we are frequently concerned with the interpretation\\nand analysis of data, e.g., for prediction of future events and decision\\nmaking. To make this task more tractable, we often build models that\\ndescribe the generative process that generates the observed data. generative process\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.4 Probabilistic Modeling and Inference 273\\nFor example, we can describe the outcome of a coin-ﬂip experiment\\n(“heads” or “tails”) in two steps. First, we deﬁne a parameter \\x16, which\\ndescribes the probability of “heads” as the parameter of a Bernoulli distri-\\nbution (Chapter 6); second, we can sample an outcome x2fhead, tailg\\nfrom the Bernoulli distribution p(xj\\x16) =Ber(\\x16). The parameter \\x16gives\\nrise to a speciﬁc dataset Xand depends on the coin used. Since \\x16is un-\\nknown in advance and can never be observed directly, we need mecha-\\nnisms to learn something about \\x16given observed outcomes of coin-ﬂip\\nexperiments. In the following, we will discuss how probabilistic modeling\\ncan be used for this purpose.\\n8.4.1 Probabilistic ModelsA probabilistic\\nmodel is speciﬁed\\nby the joint\\ndistribution of all\\nrandom variables.Probabilistic models represent the uncertain aspects of an experiment as\\nprobability distributions. The beneﬁt of using probabilistic models is that\\nthey offer a uniﬁed and consistent set of tools from probability theory\\n(Chapter 6) for modeling, inference, prediction, and model selection.\\nIn probabilistic modeling, the joint distribution p(x;\\x12)of the observed\\nvariablesxand the hidden parameters \\x12is of central importance: It en-\\ncapsulates information from the following:\\nThe prior and the likelihood (product rule, Section 6.3).\\nThe marginal likelihood p(x), which will play an important role in\\nmodel selection (Section 8.6), can be computed by taking the joint dis-\\ntribution and integrating out the parameters (sum rule, Section 6.3).\\nThe posterior, which can be obtained by dividing the joint by the marginal\\nlikelihood.\\nOnly the joint distribution has this property. Therefore, a probabilistic\\nmodel is speciﬁed by the joint distribution of all its random variables.\\n8.4.2 Bayesian Inference\\nParameter\\nestimation can be\\nphrased as an\\noptimization\\nproblem.A key task in machine learning is to take a model and the data to uncover\\nthe values of the model’s hidden variables \\x12given the observed variables\\nx. In Section 8.3.1, we already discussed two ways for estimating model\\nparameters\\x12using maximum likelihood or maximum a posteriori esti-\\nmation. In both cases, we obtain a single-best value for \\x12so that the key\\nalgorithmic problem of parameter estimation is solving an optimization\\nproblem. Once these point estimates \\x12\\x03are known, we use them to make\\npredictions. More speciﬁcally, the predictive distribution will be p(xj\\x12\\x03),\\nwhere we use \\x12\\x03in the likelihood function.\\nAs discussed in Section 6.3, focusing solely on some statistic of the pos-\\nterior distribution (such as the parameter \\x12\\x03that maximizes the poste-\\nrior) leads to loss of information, which can be critical in a system that\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n274 When Models Meet Data\\nuses the prediction p(xj\\x12\\x03)to make decisions. These decision-making\\nsystems typically have different objective functions than the likelihood, a Bayesian inference\\nis about learning the\\ndistribution of\\nrandom variables.squared-error loss or a mis-classiﬁcation error. Therefore, having the full\\nposterior distribution around can be extremely useful and leads to more\\nrobust decisions. Bayesian inference is about ﬁnding this posterior distri-Bayesian inference\\nbution (Gelman et al., 2004). For a dataset X, a parameter prior p(\\x12), and\\na likelihood function, the posterior\\np(\\x12jX) =p(Xj\\x12)p(\\x12)\\np(X); p (X) =Z\\np(Xj\\x12)p(\\x12)d\\x12; (8.22)\\nis obtained by applying Bayes’ theorem. The key idea is to exploit Bayes’ Bayesian inference\\ninverts the\\nrelationship\\nbetween parameters\\nand the data.theorem to invert the relationship between the parameters \\x12and the data\\nX(given by the likelihood) to obtain the posterior distribution p(\\x12jX).\\nThe implication of having a posterior distribution on the parameters is\\nthat it can be used to propagate uncertainty from the parameters to the\\ndata. More speciﬁcally, with a distribution p(\\x12)on the parameters our\\npredictions will be\\np(x) =Z\\np(xj\\x12)p(\\x12)d\\x12=E\\x12[p(xj\\x12)]; (8.23)\\nand they no longer depend on the model parameters \\x12, which have been\\nmarginalized/integrated out. Equation (8.23) reveals that the prediction\\nis an average over all plausible parameter values \\x12, where the plausibility\\nis encapsulated by the parameter distribution p(\\x12).\\nHaving discussed parameter estimation in Section 8.3 and Bayesian in-\\nference here, let us compare these two approaches to learning. Parameter\\nestimation via maximum likelihood or MAP estimation yields a consistent\\npoint estimate \\x12\\x03of the parameters, and the key computational problem\\nto be solved is optimization. In contrast, Bayesian inference yields a (pos-\\nterior) distribution, and the key computational problem to be solved is\\nintegration. Predictions with point estimates are straightforward, whereas\\npredictions in the Bayesian framework require solving another integration\\nproblem; see (8.23). However, Bayesian inference gives us a principled\\nway to incorporate prior knowledge, account for side information, and\\nincorporate structural knowledge, all of which is not easily done in the\\ncontext of parameter estimation. Moreover, the propagation of parameter\\nuncertainty to the prediction can be valuable in decision-making systems\\nfor risk assessment and exploration in the context of data-efﬁcient learn-\\ning (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018).\\nWhile Bayesian inference is a mathematically principled framework for\\nlearning about parameters and making predictions, there are some prac-\\ntical challenges that come with it because of the integration problems we\\nneed to solve; see (8.22) and (8.23). More speciﬁcally, if we do not choose\\na conjugate prior on the parameters (Section 6.6.1), the integrals in (8.22)\\nand (8.23) are not analytically tractable, and we cannot compute the pos-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.4 Probabilistic Modeling and Inference 275\\nterior, the predictions, or the marginal likelihood in closed form. In these\\ncases, we need to resort to approximations. Here, we can use stochas-\\ntic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks\\net al., 1996), or deterministic approximations, such as the Laplace ap-\\nproximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational in-\\nference (Jordan et al., 1999; Blei et al., 2017), or expectation propaga-\\ntion (Minka, 2001a).\\nDespite these challenges, Bayesian inference has been successfully ap-\\nplied to a variety of problems, including large-scale topic modeling (Hoff-\\nman et al., 2013), click-through-rate prediction (Graepel et al., 2010),\\ndata-efﬁcient reinforcement learning in control systems (Deisenroth et al.,\\n2015), online ranking systems (Herbrich et al., 2007), and large-scale rec-\\nommender systems. There are generic tools, such as Bayesian optimiza-\\ntion (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that\\nare very useful ingredients for an efﬁcient search of meta parameters of\\nmodels or algorithms.\\nRemark. In the machine learning literature, there can be a somewhat ar-\\nbitrary separation between (random) “variables” and “parameters”. While\\nparameters are estimated (e.g., via maximum likelihood), variables are\\nusually marginalized out. In this book, we are not so strict with this sep-\\naration because, in principle, we can place a prior on any parameter and\\nintegrate it out, which would then turn the parameter into a random vari-\\nable according to the aforementioned separation. }\\n8.4.3 Latent-Variable Models\\nIn practice, it is sometimes useful to have additional latent variables z latent variable\\n(besides the model parameters \\x12) as part of the model (Moustaki et al.,\\n2015). These latent variables are different from the model parameters\\n\\x12as they do not parametrize the model explicitly. Latent variables may\\ndescribe the data-generating process, thereby contributing to the inter-\\npretability of the model. They also often simplify the structure of the\\nmodel and allow us to deﬁne simpler and richer model structures. Sim-\\npliﬁcation of the model structure often goes hand in hand with a smaller\\nnumber of model parameters (Paquet, 2008; Murphy, 2012). Learning in\\nlatent-variable models (at least via maximum likelihood) can be done in a\\nprincipled way using the expectation maximization (EM) algorithm (Demp-\\nster et al., 1977; Bishop, 2006). Examples, where such latent variables\\nare helpful, are principal component analysis for dimensionality reduc-\\ntion (Chapter 10), Gaussian mixture models for density estimation (Chap-\\nter 11), hidden Markov models (Maybeck, 1979) or dynamical systems\\n(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,\\nand meta learning and task generalization (Hausman et al., 2018; Sæ-\\nmundsson et al., 2018). Although the introduction of these latent variables\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n276 When Models Meet Data\\nmay make the model structure and the generative process easier, learning\\nin latent-variable models is generally hard, as we will see in Chapter 11.\\nSince latent-variable models also allow us to deﬁne the process that\\ngenerates data from parameters, let us have a look at this generative pro-\\ncess. Denoting data by x, the model parameters by \\x12and the latent vari-\\nables byz, we obtain the conditional distribution\\np(xjz;\\x12) (8.24)\\nthat allows us to generate data for any model parameters and latent vari-\\nables. Given that zare latent variables, we place a prior p(z)on them.\\nAs the models we discussed previously, models with latent variables\\ncan be used for parameter learning and inference within the frameworks\\nwe discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by\\nmeans of maximum likelihood estimation or Bayesian inference), we fol-\\nlow a two-step procedure. First, we compute the likelihood p(xj\\x12)of the\\nmodel, which does not depend on the latent variables. Second, we use this\\nlikelihood for parameter estimation or Bayesian inference, where we use\\nexactly the same expressions as in Sections 8.3 and 8.4.2, respectively.\\nSince the likelihood function p(xj\\x12)is the predictive distribution of the\\ndata given the model parameters, we need to marginalize out the latent\\nvariables so that\\np(xj\\x12) =Z\\np(xjz;\\x12)p(z)dz; (8.25)\\nwherep(xjz;\\x12)is given in (8.24) and p(z)is the prior on the latent\\nvariables. Note that the likelihood must not depend on the latent variables The likelihood is a\\nfunction of the data\\nand the model\\nparameters, but is\\nindependent of the\\nlatent variables.z, but it is only a function of the data xand the model parameters \\x12.\\nThe likelihood in (8.25) directly allows for parameter estimation via\\nmaximum likelihood. MAP estimation is also straightforward with an ad-\\nditional prior on the model parameters \\x12as discussed in Section 8.3.2.\\nMoreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)\\nin a latent-variable model works in the usual way: We place a prior p(\\x12)\\non the model parameters and use Bayes’ theorem to obtain a posterior\\ndistribution\\np(\\x12jX) =p(Xj\\x12)p(\\x12)\\np(X)(8.26)\\nover the model parameters given a dataset X. The posterior in (8.26) can\\nbe used for predictions within a Bayesian inference framework; see (8.23).\\nOne challenge we have in this latent-variable model is that the like-\\nlihoodp(Xj\\x12)requires the marginalization of the latent variables ac-\\ncording to (8.25). Except when we choose a conjugate prior p(z)for\\np(xjz;\\x12), the marginalization in (8.25) is not analytically tractable, and\\nwe need to resort to approximations (Bishop, 2006; Paquet, 2008; Mur-\\nphy, 2012; Moustaki et al., 2015).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.4 Probabilistic Modeling and Inference 277\\nSimilar to the parameter posterior (8.26) we can compute a posterior\\non the latent variables according to\\np(zjX) =p(Xjz)p(z)\\np(X); p (Xjz) =Z\\np(Xjz;\\x12)p(\\x12)d\\x12;(8.27)\\nwherep(z)is the prior on the latent variables and p(Xjz)requires us to\\nintegrate out the model parameters \\x12.\\nGiven the difﬁculty of solving integrals analytically, it is clear that mar-\\nginalizing out both the latent variables and the model parameters at the\\nsame time is not possible in general (Bishop, 2006; Murphy, 2012). A\\nquantity that is easier to compute is the posterior distribution on the latent\\nvariables, but conditioned on the model parameters, i.e.,\\np(zjX;\\x12) =p(Xjz;\\x12)p(z)\\np(Xj\\x12); (8.28)\\nwherep(z)is the prior on the latent variables and p(Xjz;\\x12)is given\\nin (8.24).\\nIn Chapters 10 and 11, we derive the likelihood functions for PCA and\\nGaussian mixture models, respectively. Moreover, we compute the poste-\\nrior distributions (8.28) on the latent variables for both PCA and Gaussian\\nmixture models.\\nRemark. In the following chapters, we may not be drawing such a clear\\ndistinction between latent variables zand uncertain model parameters \\x12\\nand call the model parameters “latent” or “hidden” as well because they\\nare unobserved. In Chapters 10 and 11, where we use the latent variables\\nz, we will pay attention to the difference as we will have two different\\ntypes of hidden variables: model parameters \\x12and latent variables z.}\\nWe can exploit the fact that all the elements of a probabilistic model are\\nrandom variables to deﬁne a uniﬁed language for representing them. In\\nSection 8.5, we will see a concise graphical language for representing the\\nstructure of probabilistic models. We will use this graphical language to\\ndescribe the probabilistic models in the subsequent chapters.\\n8.4.4 Further Reading\\nProbabilistic models in machine learning (Bishop, 2006; Barber, 2012;\\nMurphy, 2012) provide a way for users to capture uncertainty about data\\nand predictive models in a principled fashion. Ghahramani (2015) presents\\na short review of probabilistic models in machine learning. Given a proba-\\nbilistic model, we may be lucky enough to be able to compute parameters\\nof interest analytically. However, in general, analytic solutions are rare,\\nand computational methods such as sampling (Gilks et al., 1996; Brooks\\net al., 2011) and variational inference (Jordan et al., 1999; Blei et al.,\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n278 When Models Meet Data\\n2017) are used. Moustaki et al. (2015) and Paquet (2008) provide a good\\noverview of Bayesian inference in latent-variable models.\\nIn recent years, several programming languages have been proposed\\nthat aim to treat the variables deﬁned in software as random variables\\ncorresponding to probability distributions. The objective is to be able to\\nwrite complex functions of probability distributions, while under the hood\\nthe compiler automatically takes care of the rules of Bayesian inference.\\nThis rapidly changing ﬁeld is called probabilistic programming . probabilistic\\nprogramming\\n8.5 Directed Graphical Models\\nIn this section, we introduce a graphical language for specifying a prob-\\nabilistic model, called the directed graphical model . It provides a compact directed graphical\\nmodel and succinct way to specify probabilistic models, and allows the reader to\\nvisually parse dependencies between random variables. A graphical model\\nvisually captures the way in which the joint distribution over all random\\nvariables can be decomposed into a product of factors depending only on\\na subset of these variables. In Section 8.4, we identiﬁed the joint distri-\\nbution of a probabilistic model as the key quantity of interest because it\\ncomprises information about the prior, the likelihood, and the posterior.\\nHowever, the joint distribution by itself can be quite complicated, and Directed graphical\\nmodels are also\\nknown as Bayesian\\nnetworks.it does not tell us anything about structural properties of the probabilis-\\ntic model. For example, the joint distribution p(a;b;c )does not tell us\\nanything about independence relations. This is the point where graphical\\nmodels come into play. This section relies on the concepts of independence\\nand conditional independence, as described in Section 6.4.5.\\nIn agraphical model , nodes are random variables. In Figure 8.9(a), the graphical model\\nnodes represent the random variables a;b;c . Edges represent probabilistic\\nrelations between variables, e.g., conditional probabilities.\\nRemark. Not every distribution can be represented in a particular choice of\\ngraphical model. A discussion of this can be found in Bishop (2006). }\\nProbabilistic graphical models have some convenient properties:\\nThey are a simple way to visualize the structure of a probabilistic model.\\nThey can be used to design or motivate new kinds of statistical models.\\nInspection of the graph alone gives us insight into properties, e.g., con-\\nditional independence.\\nComplex computations for inference and learning in statistical models\\ncan be expressed in terms of graphical manipulations.\\n8.5.1 Graph Semantics\\nDirected graphical models /Bayesian networks are a method for representing directed graphical\\nmodel/Bayesian\\nnetworkconditional dependencies in a probabilistic model. They provide a visual\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.5 Directed Graphical Models 279\\ndescription of the conditional probabilities, hence, providing a simple lan-\\nguage for describing complex interdependence. The modular description With additional\\nassumptions, the\\narrows can be used\\nto indicate causal\\nrelationships (Pearl,\\n2009).also entails computational simpliﬁcation. Directed links (arrows) between\\ntwo nodes (random variables) indicate conditional probabilities. For ex-\\nample, the arrow between aandbin Figure 8.9(a) gives the conditional\\nprobabilityp(bja)ofbgivena.\\nFigure 8.9\\nExamples of\\ndirected graphical\\nmodels.a b\\nc\\n(a) Fully connected.x1 x2\\nx3 x4x5\\n(b) Not fully connected.\\nDirected graphical models can be derived from joint distributions if we\\nknow something about their factorization.\\nExample 8.7\\nConsider the joint distribution\\np(a;b;c ) =p(cja;b)p(bja)p(a) (8.29)\\nof three random variables a;b;c . The factorization of the joint distribution\\nin (8.29) tells us something about the relationship between the random\\nvariables:\\ncdepends directly on aandb.\\nbdepends directly on a.\\nadepends neither on bnor onc.\\nFor the factorization in (8.29), we obtain the directed graphical model in\\nFigure 8.9(a).\\nIn general, we can construct the corresponding directed graphical model\\nfrom a factorized joint distribution as follows:\\n1. Create a node for all random variables.\\n2. For each conditional distribution, we add a directed link (arrow) to\\nthe graph from the nodes corresponding to the variables on which the\\ndistribution is conditioned.\\nThe graph layout\\ndepends on the\\nfactorization of the\\njoint distribution.The graph layout depends on the choice of factorization of the joint dis-\\ntribution.\\nWe discussed how to get from a known factorization of the joint dis-\\ntribution to the corresponding directed graphical model. Now, we will do\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n280 When Models Meet Data\\nexactly the opposite and describe how to extract the joint distribution of\\na set of random variables from a given graphical model.\\nExample 8.8\\nLooking at the graphical model in Figure 8.9(b), we exploit two proper-\\nties:\\nThe joint distribution p(x1;:::;x 5)we seek is the product of a set of\\nconditionals, one for each node in the graph. In this particular example,\\nwe will need ﬁve conditionals.\\nEach conditional depends only on the parents of the corresponding\\nnode in the graph. For example, x4will be conditioned on x2.\\nThese two properties yield the desired factorization of the joint distribu-\\ntion\\np(x1;x2;x3;x4;x5) =p(x1)p(x5)p(x2jx5)p(x3jx1;x2)p(x4jx2):(8.30)\\nIn general, the joint distribution p(x) =p(x1;:::;xK)is given as\\np(x) =KY\\nk=1p(xkjPak); (8.31)\\nwhere Pa kmeans “the parent nodes of xk”. Parent nodes of xkare nodes\\nthat have arrows pointing to xk.\\nWe conclude this subsection with a concrete example of the coin-ﬂip\\nexperiment. Consider a Bernoulli experiment (Example 6.8) where the\\nprobability that the outcome xof this experiment is “heads” is\\np(xj\\x16) =Ber(\\x16): (8.32)\\nWe now repeat this experiment Ntimes and observe outcomes x1;:::;xN\\nso that we obtain the joint distribution\\np(x1;:::;xNj\\x16) =NY\\nn=1p(xnj\\x16): (8.33)\\nThe expression on the right-hand side is a product of Bernoulli distribu-\\ntions on each individual outcome because the experiments are indepen-\\ndent. Recall from Section 6.4.5 that statistical independence means that\\nthe distribution factorizes. To write the graphical model down for this set-\\nting, we make the distinction between unobserved/latent variables and\\nobserved variables. Graphically, observed variables are denoted by shaded\\nnodes so that we obtain the graphical model in Figure 8.10(a). We see\\nthat the single parameter \\x16is the same for all xn,n= 1;:::;N as the\\noutcomesxnare identically distributed. A more compact, but equivalent,\\ngraphical model for this setting is given in Figure 8.10(b), where we use\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.5 Directed Graphical Models 281\\nFigure 8.10\\nGraphical models\\nfor a repeated\\nBernoulli\\nexperiment.\\x16\\nx1 xN\\n(a) Version with xnexplicit.\\x16\\nxn\\nn= 1;:::;N\\n(b) Version with\\nplate notation.\\x16\\nxn\\x0c \\x0b\\nn= 1;:::;N\\n(c) Hyperparameters \\x0b\\nand\\x0con the latent \\x16.\\ntheplate notation. The plate (box) repeats everything inside (in this case, plate\\nthe observations xn)Ntimes. Therefore, both graphical models are equiv-\\nalent, but the plate notation is more compact. Graphical models immedi-\\nately allow us to place a hyperprior on \\x16. Ahyperprior is a second layer hyperprior\\nof prior distributions on the parameters of the ﬁrst layer of priors. Fig-\\nure 8.10(c) places a Beta (\\x0b;\\x0c)prior on the latent variable \\x16. If we treat\\n\\x0band\\x0cas deterministic parameters, i.e., not random variables, we omit\\nthe circle around it.\\n8.5.2 Conditional Independence and d-Separation\\nDirected graphical models allow us to ﬁnd conditional independence (Sec-\\ntion 6.4.5) relationship properties of the joint distribution only by looking\\nat the graph. A concept called d-separation (Pearl, 1988) is key to this. d-separation\\nConsider a general directed graph in which A;B;Care arbitrary nonin-\\ntersecting sets of nodes (whose union may be smaller than the complete\\nset of nodes in the graph). We wish to ascertain whether a particular con-\\nditional independence statement, “ Ais conditionally independent of B\\ngivenC”, denoted by\\nA? ?BjC; (8.34)\\nis implied by a given directed acyclic graph. To do so, we consider all\\npossible trails (paths that ignore the direction of the arrows) from any\\nnode inAto any nodes inB. Any such path is said to be blocked if it\\nincludes any node such that either of the following are true:\\nThe arrows on the path meet either head to tail or tail to tail at the\\nnode, and the node is in the set C.\\nThe arrows meet head to head at the node, and neither the node nor\\nany of its descendants is in the set C.\\nIf all paths are blocked, then Ais said to be d-separated fromBbyC,\\nand the joint distribution over all of the variables in the graph will satisfy\\nA? ?BjC .\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n282 When Models Meet Data\\nFigure 8.12 Three\\ntypes of graphical\\nmodels: (a) Directed\\ngraphical models\\n(Bayesian\\nnetworks);\\n(b) Undirected\\ngraphical models\\n(Markov random\\nﬁelds); (c) Factor\\ngraphs.a b\\nc\\n(a) Directed graphical modela b\\nc\\n(b) Undirected graphical\\nmodela b\\nc\\n(c) Factor graph\\nExample 8.9 (Conditional Independence)\\nFigure 8.11\\nD-separation\\nexample.abc\\nd\\ne\\nConsider the graphical model in Figure 8.11. Visual inspection gives us\\nb? ?dja;c (8.35)\\na? ?cjb (8.36)\\nb6? ?djc (8.37)\\na6? ?cjb;e (8.38)\\nDirected graphical models allow a compact representation of proba-\\nbilistic models, and we will see examples of directed graphical models in\\nChapters 9, 10, and 11. The representation, along with the concept of con-\\nditional independence, allows us to factorize the respective probabilistic\\nmodels into expressions that are easier to optimize.\\nThe graphical representation of the probabilistic model allows us to\\nvisually see the impact of design choices we have made on the structure\\nof the model. We often need to make high-level assumptions about the\\nstructure of the model. These modeling assumptions (hyperparameters)\\naffect the prediction performance, but cannot be selected directly using\\nthe approaches we have seen so far. We will discuss different ways to\\nchoose the structure in Section 8.6.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.6 Model Selection 283\\n8.5.3 Further Reading\\nAn introduction to probabilistic graphical models can be found in Bishop\\n(2006, chapter 8), and an extensive description of the different applica-\\ntions and corresponding algorithmic implications can be found in the book\\nby Koller and Friedman (2009). There are three main types of probabilistic\\ngraphical models:\\ndirected graphical\\nmodel Directed graphical models (Bayesian networks ); see Figure 8.12(a)\\nBayesian network\\nundirected graphical\\nmodelUndirected graphical models (Markov random ﬁelds ); see Figure 8.12(b)\\nMarkov random\\nﬁeld\\nfactor graphFactor graphs ; see Figure 8.12(c)\\nGraphical models allow for graph-based algorithms for inference and\\nlearning, e.g., via local message passing. Applications range from rank-\\ning in online games (Herbrich et al., 2007) and computer vision (e.g.,\\nimage segmentation, semantic labeling, image denoising, image restora-\\ntion (Kittler and F ¨oglein, 1984; Sucar and Gillies, 1994; Shotton et al.,\\n2006; Szeliski et al., 2008)) to coding theory (McEliece et al., 1998), solv-\\ning linear equation systems (Shental et al., 2008), and iterative Bayesian\\nstate estimation in signal processing (Bickson et al., 2007; Deisenroth and\\nMohamed, 2012).\\nOne topic that is particularly important in real applications that we do\\nnot discuss in this book is the idea of structured prediction (Bakir et al.,\\n2007; Nowozin et al., 2014), which allows machine learning models to\\ntackle predictions that are structured, for example sequences, trees, and\\ngraphs. The popularity of neural network models has allowed more ﬂex-\\nible probabilistic models to be used, resulting in many useful applica-\\ntions of structured models (Goodfellow et al., 2016, chapter 16). In recent\\nyears, there has been a renewed interest in graphical models due to their\\napplications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;\\nPeters et al., 2017; Rosenbaum, 2017).\\n8.6 Model Selection\\nIn machine learning, we often need to make high-level modeling decisions\\nthat critically inﬂuence the performance of the model. The choices we\\nmake (e.g., the functional form of the likelihood) inﬂuence the number\\nand type of free parameters in the model and thereby also the ﬂexibility\\nand expressivity of the model. More complex models are more ﬂexible in A polynomial\\ny=a0+a1x+a2x2\\ncan also describe\\nlinear functions by\\nsettinga2= 0, i.e.,\\nit is strictly more\\nexpressive than a\\nﬁrst-order\\npolynomial.the sense that they can be used to describe more datasets. For instance, a\\npolynomial of degree 1(a liney=a0+a1x) can only be used to describe\\nlinear relations between inputs xand observations y. A polynomial of\\ndegree 2can additionally describe quadratic relationships between inputs\\nand observations.\\nOne would now think that very ﬂexible models are generally preferable\\nto simple models because they are more expressive. A general problem\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n284 When Models Meet Data\\nFigure 8.13 Nested\\ncross-validation. We\\nperform two levels\\nofK-fold\\ncross-validation.All labeled data\\nAll training data Test data\\nTo train model Validation\\nis that at training time we can only use the training set to evaluate the\\nperformance of the model and learn its parameters. However, the per-\\nformance on the training set is not really what we are interested in. In\\nSection 8.3, we have seen that maximum likelihood estimation can lead\\nto overﬁtting, especially when the training dataset is small. Ideally, our\\nmodel (also) works well on the test set (which is not available at training\\ntime). Therefore, we need some mechanisms for assessing how a model\\ngeneralizes to unseen test data. Model selection is concerned with exactly\\nthis problem.\\n8.6.1 Nested Cross-Validation\\nWe have already seen an approach (cross-validation in Section 8.2.4) that\\ncan be used for model selection. Recall that cross-validation provides an\\nestimate of the generalization error by repeatedly splitting the dataset into\\ntraining and validation sets. We can apply this idea one more time, i.e.,\\nfor each split, we can perform another round of cross-validation. This is\\nsometimes referred to as nested cross-validation ; see Figure 8.13. The inner nested\\ncross-validation level is used to estimate the performance of a particular choice of model\\nor hyperparameter on a internal validation set. The outer level is used to\\nestimate generalization performance for the best choice of model chosen\\nby the inner loop. We can test different model and hyperparameter choices\\nin the inner loop. To distinguish the two levels, the set used to estimate\\nthe generalization performance is often called the test set and the set used test set\\nfor choosing the best model is called the validation set . The inner loop validation set\\nestimates the expected value of the generalization error for a given model\\n(8.39), by approximating it using the empirical error on the validation set,\\ni.e., The standard error\\nis deﬁned as\\x1bp\\nK,\\nwhereKis the\\nnumber of\\nexperiments and \\x1b\\nis the standard\\ndeviation of the risk\\nof each experiment.EV[R(VjM)]\\x191\\nKKX\\nk=1R(V(k)jM); (8.39)\\nwhere R(VjM)is the empirical risk (e.g., root mean square error) on the\\nvalidation setVfor modelM. We repeat this procedure for all models and\\nchoose the model that performs best. Note that cross-validation not only\\ngives us the expected generalization error, but we can also obtain high-\\norder statistics, e.g., the standard error, an estimate of how uncertain the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.6 Model Selection 285\\nFigure 8.14\\nBayesian inference\\nembodies Occam’s\\nrazor. The\\nhorizontal axis\\ndescribes the space\\nof all possible\\ndatasetsD. The\\nevidence (vertical\\naxis) evaluates how\\nwell a model\\npredicts available\\ndata. Since\\np(DjMi)needs to\\nintegrate to 1, we\\nshould choose the\\nmodel with the\\ngreatest evidence.\\nAdapted\\nfrom MacKay\\n(2003).Evidence\\nD\\nCp(DjM1)\\np(DjM2)\\nmean estimate is. Once the model is chosen, we can evaluate the ﬁnal\\nperformance on the test set.\\n8.6.2 Bayesian Model Selection\\nThere are many approaches to model selection, some of which are covered\\nin this section. Generally, they all attempt to trade off model complexity\\nand data ﬁt. We assume that simpler models are less prone to overﬁtting\\nthan complex models, and hence the objective of model selection is to ﬁnd\\nthe simplest model that explains the data reasonably well. This concept is\\nalso known as Occam’s razor . Occam’s razor\\nRemark. If we treat model selection as a hypothesis testing problem, we\\nare looking for the simplest hypothesis that is consistent with the data (Mur-\\nphy, 2012). }\\nOne may consider placing a prior on models that favors simpler models.\\nHowever, it is not necessary to do this: An “automatic Occam’s Razor” is\\nquantitatively embodied in the application of Bayesian probability (Smith\\nand Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-\\nure 8.14, adapted from MacKay (2003), gives us the basic intuition why\\ncomplex and very expressive models may turn out to be a less probable\\nchoice for modeling a given dataset D. Let us think of the horizontal axis These predictions\\nare quantiﬁed by a\\nnormalized\\nprobability\\ndistribution onD,\\ni.e., it needs to\\nintegrate/sum to 1.representing the space of all possible datasets D. If we are interested in\\nthe posterior probability p(MijD)of modelMigiven the dataD, we can\\nemploy Bayes’ theorem. Assuming a uniform prior p(M)over all mod-\\nels, Bayes’ theorem rewards models in proportion to how much they pre-\\ndicted the data that occurred. This prediction of the data given model\\nMi,p(DjMi), is called the evidence forMi. A simple model M1can only evidence\\npredict a small number of datasets, which is shown by p(DjM1); a more\\npowerful model M2that has, e.g., more free parameters than M1, is able\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n286 When Models Meet Data\\nto predict a greater variety of datasets. This means, however, that M2\\ndoes not predict the datasets in region Cas well asM1. Suppose that\\nequal prior probabilities have been assigned to the two models. Then, if\\nthe dataset falls into region C, the less powerful model M1is the more\\nprobable model.\\nEarlier in this chapter, we argued that models need to be able to explain\\nthe data, i.e., there should be a way to generate data from a given model.\\nFurthermore, if the model has been appropriately learned from the data,\\nthen we expect that the generated data should be similar to the empirical\\ndata. For this, it is helpful to phrase model selection as a hierarchical\\ninference problem, which allows us to compute the posterior distribution\\nover models.\\nLet us consider a ﬁnite number of models M=fM1;:::;MKg, where\\neach model Mkpossesses parameters \\x12k. InBayesian model selection , we Bayesian model\\nselection place a prior p(M)on the set of models. The corresponding generative\\ngenerative processprocess that allows us to generate data from this model is\\nFigure 8.15\\nIllustration of the\\nhierarchical\\ngenerative process\\nin Bayesian model\\nselection. We place\\na priorp(M)on the\\nset of models. For\\neach model, there is\\na distribution\\np(\\x12jM)on the\\ncorresponding\\nmodel parameters,\\nwhich is used to\\ngenerate the dataD.\\nM\\n\\x12\\nDMk\\x18p(M) (8.40)\\n\\x12k\\x18p(\\x12jMk) (8.41)\\nD\\x18p(Dj\\x12k) (8.42)\\nand illustrated in Figure 8.15. Given a training set D, we apply Bayes’\\ntheorem and compute the posterior distribution over models as\\np(MkjD)/p(Mk)p(DjMk): (8.43)\\nNote that this posterior no longer depends on the model parameters \\x12k\\nbecause they have been integrated out in the Bayesian setting since\\np(DjMk) =Z\\np(Dj\\x12k)p(\\x12kjMk)d\\x12k; (8.44)\\nwherep(\\x12kjMk)is the prior distribution of the model parameters \\x12kof\\nmodelMk. The term (8.44) is referred to as the model evidence ormarginal\\nmodel evidence\\nmarginal likelihoodlikelihood . From the posterior in (8.43), we determine the MAP estimate\\nM\\x03= arg max\\nMkp(MkjD): (8.45)\\nWith a uniform prior p(Mk) =1\\nK, which gives every model equal (prior)\\nprobability, determining the MAP estimate over models amounts to pick-\\ning the model that maximizes the model evidence (8.44).\\nRemark (Likelihood and Marginal Likelihood) .There are some important\\ndifferences between a likelihood and a marginal likelihood (evidence):\\nWhile the likelihood is prone to overﬁtting, the marginal likelihood is typ-\\nically not as the model parameters have been marginalized out (i.e., we\\nno longer have to ﬁt the parameters). Furthermore, the marginal likeli-\\nhood automatically embodies a trade-off between model complexity and\\ndata ﬁt (Occam’s razor). }\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n8.6 Model Selection 287\\n8.6.3 Bayes Factors for Model Comparison\\nConsider the problem of comparing two probabilistic models M1;M2,\\ngiven a datasetD. If we compute the posteriors p(M1jD)andp(M2jD),\\nwe can compute the ratio of the posteriors\\np(M1jD)\\np(M2jD)|{z}\\nposterior odds=p(DjM1)p(M1)\\np(D)\\np(DjM2)p(M2)\\np(D)=p(M1)\\np(M2)|{z}\\nprior oddsp(DjM1)\\np(DjM2)|{z}\\nBayes factor: (8.46)\\nThe ratio of the posteriors is also called the posterior odds . The ﬁrst frac- posterior odds\\ntion on the right-hand side of (8.46), the prior odds , measures how much prior odds\\nour prior (initial) beliefs favor M1overM2. The ratio of the marginal like-\\nlihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor\\nand measures how well the data Dis predicted by M1compared to M2.\\nRemark. TheJeffreys-Lindley paradox states that the “Bayes factor always Jeffreys-Lindley\\nparadox favors the simpler model since the probability of the data under a complex\\nmodel with a diffuse prior will be very small” (Murphy, 2012). Here, a\\ndiffuse prior refers to a prior that does not favor speciﬁc models, i.e.,\\nmany models are a priori plausible under this prior. }\\nIf we choose a uniform prior over models, the prior odds term in (8.46)\\nis1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes\\nfactor)\\np(DjM1)\\np(DjM2): (8.47)\\nIf the Bayes factor is greater than 1, we choose model M1, otherwise\\nmodelM2. In a similar way to frequentist statistics, there are guidelines\\non the size of the ratio that one should consider before ”signiﬁcance” of\\nthe result (Jeffreys, 1961).\\nRemark (Computing the Marginal Likelihood) .The marginal likelihood\\nplays an important role in model selection: We need to compute Bayes\\nfactors (8.46) and posterior distributions over models (8.43).\\nUnfortunately, computing the marginal likelihood requires us to solve\\nan integral (8.44). This integration is generally analytically intractable,\\nand we will have to resort to approximation techniques, e.g., numerical\\nintegration (Stoer and Burlirsch, 2002), stochastic approximations using\\nMonte Carlo (Murphy, 2012), or Bayesian Monte Carlo techniques (O’Hagan,\\n1991; Rasmussen and Ghahramani, 2003).\\nHowever, there are special cases in which we can solve it. In Section 6.6.1,\\nwe discussed conjugate models. If we choose a conjugate parameter prior\\np(\\x12), we can compute the marginal likelihood in closed form. In Chap-\\nter 9, we will do exactly this in the context of linear regression. }\\nWe have seen a brief introduction to the basic concepts of machine\\nlearning in this chapter. For the rest of this part of the book we will see\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n288 When Models Meet Data\\nhow the three different ﬂavors of learning in Sections 8.2, 8.3, and 8.4 are\\napplied to the four pillars of machine learning (regression, dimensionality\\nreduction, density estimation, and classiﬁcation).\\n8.6.4 Further Reading\\nWe mentioned at the start of the section that there are high-level modeling\\nchoices that inﬂuence the performance of the model. Examples include the\\nfollowing:\\nThe degree of a polynomial in a regression setting\\nThe number of components in a mixture model\\nThe network architecture of a (deep) neural network\\nThe type of kernel in a support vector machine\\nThe dimensionality of the latent space in PCA\\nThe learning rate (schedule) in an optimization algorithm\\nIn parametric\\nmodels, the number\\nof parameters is\\noften related to the\\ncomplexity of the\\nmodel class.Rasmussen and Ghahramani (2001) showed that the automatic Occam’s\\nrazor does not necessarily penalize the number of parameters in a model,\\nbut it is active in terms of the complexity of functions. They also showed\\nthat the automatic Occam’s razor also holds for Bayesian nonparametric\\nmodels with many parameters, e.g., Gaussian processes.\\nIf we focus on the maximum likelihood estimate, there exist a number of\\nheuristics for model selection that discourage overﬁtting. They are called\\ninformation criteria, and we choose the model with the largest value. The\\nAkaike information criterion (AIC) (Akaike, 1974) Akaike information\\ncriterion\\nlogp(xj\\x12)\\x00M (8.48)\\ncorrects for the bias of the maximum likelihood estimator by addition of\\na penalty term to compensate for the overﬁtting of more complex models\\nwith lots of parameters. Here, Mis the number of model parameters. The\\nAIC estimates the relative information lost by a given model.\\nTheBayesian information criterion (BIC) (Schwarz, 1978) Bayesian\\ninformation\\ncriterion logp(x) = logZ\\np(xj\\x12)p(\\x12)d\\x12\\x19logp(xj\\x12)\\x001\\n2MlogN (8.49)\\ncan be used for exponential family distributions. Here, Nis the number\\nof data points and Mis the number of parameters. BIC penalizes model\\ncomplexity more heavily than AIC.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9\\nLinear Regression\\nIn the following, we will apply the mathematical concepts from Chap-\\nters 2, 5, 6, and 7 to solve linear regression (curve ﬁtting) problems. In\\nregression , we aim to ﬁnd a function fthat maps inputs x2RDto corre- regression\\nsponding function values f(x)2R. We assume we are given a set of train-\\ning inputsxnand corresponding noisy observations yn=f(xn)+\\x0f, where\\n\\x0fis an i.i.d. random variable that describes measurement/observation\\nnoise and potentially unmodeled processes (which we will not consider\\nfurther in this chapter). Throughout this chapter, we assume zero-mean\\nGaussian noise. Our task is to ﬁnd a function that not only models the\\ntraining data, but generalizes well to predicting function values at input\\nlocations that are not part of the training data (see Chapter 8). An il-\\nlustration of such a regression problem is given in Figure 9.1. A typical\\nregression setting is given in Figure 9.1(a): For some input values xn, we\\nobserve (noisy) function values yn=f(xn) +\\x0f. The task is to infer the\\nfunctionfthat generated the data and generalizes well to function values\\nat new input locations. A possible solution is given in Figure 9.1(b), where\\nwe also show three distributions centered at the function values f(x)that\\nrepresent the noise in the data.\\nRegression is a fundamental problem in machine learning, and regres-\\nsion problems appear in a diverse range of research areas and applica-\\nFigure 9.1\\n(a) Dataset;\\n(b) possible solution\\nto the regression\\nproblem.\\n−4−2 0 2 4\\nx−0.4−0.20.00.20.4y\\n(a) Regression problem: observed noisy func-\\ntion values from which we wish to infer the\\nunderlying function that generated the data.\\n−4−2 0 2 4\\nx−0.4−0.20.00.20.4y\\n(b) Regression solution: possible function\\nthat could have generated the data (blue)\\nwith indication of the measurement noise of\\nthe function value at the corresponding in-\\nputs (orange distributions).\\n289\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n290 Linear Regression\\ntions, including time-series analysis (e.g., system identiﬁcation), control\\nand robotics (e.g., reinforcement learning, forward/inverse model learn-\\ning), optimization (e.g., line searches, global optimization), and deep-\\nlearning applications (e.g., computer games, speech-to-text translation,\\nimage recognition, automatic video annotation). Regression is also a key\\ningredient of classiﬁcation algorithms. Finding a regression function re-\\nquires solving a variety of problems, including the following:\\nChoice of the model (type) and the parametrization of the regres-\\nsion function. Given a dataset, what function classes (e.g., polynomi- Normally, the type\\nof noise could also\\nbe a “model choice”,\\nbut we ﬁx the noise\\nto be Gaussian in\\nthis chapter.als) are good candidates for modeling the data, and what particular\\nparametrization (e.g., degree of the polynomial) should we choose?\\nModel selection, as discussed in Section 8.6, allows us to compare var-\\nious models to ﬁnd the simplest model that explains the training data\\nreasonably well.\\nFinding good parameters. Having chosen a model of the regression\\nfunction, how do we ﬁnd good model parameters? Here, we will need to\\nlook at different loss/objective functions (they determine what a “good”\\nﬁt is) and optimization algorithms that allow us to minimize this loss.\\nOverﬁtting and model selection. Overﬁtting is a problem when the\\nregression function ﬁts the training data “too well” but does not gen-\\neralize to unseen test data. Overﬁtting typically occurs if the underly-\\ning model (or its parametrization) is overly ﬂexible and expressive; see\\nSection 8.6. We will look at the underlying reasons and discuss ways to\\nmitigate the effect of overﬁtting in the context of linear regression.\\nRelationship between loss functions and parameter priors. Loss func-\\ntions (optimization objectives) are often motivated and induced by prob-\\nabilistic models. We will look at the connection between loss functions\\nand the underlying prior assumptions that induce these losses.\\nUncertainty modeling. In any practical setting, we have access to only\\na ﬁnite, potentially large, amount of (training) data for selecting the\\nmodel class and the corresponding parameters. Given that this ﬁnite\\namount of training data does not cover all possible scenarios, we may\\nwant to describe the remaining parameter uncertainty to obtain a mea-\\nsure of conﬁdence of the model’s prediction at test time; the smaller the\\ntraining set, the more important uncertainty modeling. Consistent mod-\\neling of uncertainty equips model predictions with conﬁdence bounds.\\nIn the following, we will be using the mathematical tools from Chap-\\nters 3, 5, 6 and 7 to solve linear regression problems. We will discuss\\nmaximum likelihood and maximum a posteriori (MAP) estimation to ﬁnd\\noptimal model parameters. Using these parameter estimates, we will have\\na brief look at generalization errors and overﬁtting. Toward the end of\\nthis chapter, we will discuss Bayesian linear regression, which allows us to\\nreason about model parameters at a higher level, thereby removing some\\nof the problems encountered in maximum likelihood and MAP estimation.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.1 Problem Formulation 291\\n9.1 Problem Formulation\\nBecause of the presence of observation noise, we will adopt a probabilis-\\ntic approach and explicitly model the noise using a likelihood function.\\nMore speciﬁcally, throughout this chapter, we consider a regression prob-\\nlem with the likelihood function\\np(yjx) =N\\x00yjf(x); \\x1b2\\x01: (9.1)\\nHere,x2RDare inputs and y2Rare noisy function values (targets).\\nWith (9.1), the functional relationship between xandyis given as\\ny=f(x) +\\x0f; (9.2)\\nwhere\\x0f\\x18N\\x000; \\x1b2\\x01\\nis independent, identically distributed (i.i.d.) Gaus-\\nsian measurement noise with mean 0and variance \\x1b2. Our objective is\\nto ﬁnd a function that is close (similar) to the unknown function fthat\\ngenerated the data and that generalizes well.\\nIn this chapter, we focus on parametric models, i.e., we choose a para-\\nmetrized function and ﬁnd parameters \\x12that “work well” for modeling the\\ndata. For the time being, we assume that the noise variance \\x1b2is known\\nand focus on learning the model parameters \\x12. In linear regression, we\\nconsider the special case that the parameters \\x12appear linearly in our\\nmodel. An example of linear regression is given by\\np(yjx;\\x12) =N\\x00yjx>\\x12; \\x1b2\\x01\\n(9.3)\\n()y=x>\\x12+\\x0f; \\x0f\\x18N\\x000; \\x1b2\\x01; (9.4)\\nwhere\\x122RDare the parameters we seek. The class of functions de-\\nscribed by (9.4) are straight lines that pass through the origin. In (9.4),\\nwe chose a parametrization f(x) =x>\\x12. A Dirac delta (delta\\nfunction) is zero\\neverywhere except\\nat a single point,\\nand its integral is 1.\\nIt can be considered\\na Gaussian in the\\nlimit of\\x1b2!0.Thelikelihood in (9.3) is the probability density function of yevalu-\\nlikelihoodated atx>\\x12. Note that the only source of uncertainty originates from the\\nobservation noise (as xand\\x12are assumed known in (9.3)). Without ob-\\nservation noise, the relationship between xandywould be deterministic\\nand (9.3) would be a Dirac delta.\\nExample 9.1\\nForx;\\x122Rthe linear regression model in (9.4) describes straight lines\\n(linear functions), and the parameter \\x12is the slope of the line. Fig-\\nure 9.2(a) shows some example functions for different values of \\x12.\\nLinear regression\\nrefers to models that\\nare linear in the\\nparameters.The linear regression model in (9.3)–(9.4) is not only linear in the pa-\\nrameters, but also linear in the inputs x. Figure 9.2(a) shows examples\\nof such functions. We will see later that y=\\x1e>(x)\\x12for nonlinear trans-\\nformations\\x1eis also a linear regression model because “linear regression”\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n292 Linear Regression\\nFigure 9.2 Linear\\nregression example.\\n(a) Example\\nfunctions that fall\\ninto this category;\\n(b) training set;\\n(c) maximum\\nlikelihood estimate.\\n−10 0 10\\nx−20020y (a) Example functions (straight\\nlines) that can be described us-\\ning the linear model in (9.4).\\n−10−5 0 5 10\\nx−10010y\\n(b) Training set.\\n−10−5 0 5 10\\nx−10010y\\n (c) Maximum likelihood esti-\\nmate.\\nrefers to models that are “linear in the parameters”, i.e., models that de-\\nscribe a function by a linear combination of input features. Here, a “fea-\\nture” is a representation \\x1e(x)of the inputsx.\\nIn the following, we will discuss in more detail how to ﬁnd good pa-\\nrameters\\x12and how to evaluate whether a parameter set “works well”.\\nFor the time being, we assume that the noise variance \\x1b2is known.\\n9.2 Parameter Estimation\\nConsider the linear regression setting (9.4) and assume we are given a\\ntraining setD:=f(x1;y1);:::; (xN;yN)gconsisting of Ninputsxn2 training set\\nRDand corresponding observations/targets yn2R,n= 1;:::;N . The Figure 9.3\\nProbabilistic\\ngraphical model for\\nlinear regression.\\nObserved random\\nvariables are\\nshaded,\\ndeterministic/\\nknown values are\\nwithout circles.\\n\\x12\\nyn\\x1b\\nxn\\nn= 1;:::;Ncorresponding graphical model is given in Figure 9.3. Note that yiandyj\\nare conditionally independent given their respective inputs xi;xjso that\\nthe likelihood factorizes according to\\np(YjX;\\x12) =p(y1;:::;yNjx1;:::;xN;\\x12) (9.5a)\\n=NY\\nn=1p(ynjxn;\\x12) =NY\\nn=1N\\x00ynjx>\\nn\\x12; \\x1b2\\x01; (9.5b)\\nwhere we deﬁned X:=fx1;:::;xNgandY:=fy1;:::;yNgas the sets\\nof training inputs and corresponding targets, respectively. The likelihood\\nand the factors p(ynjxn;\\x12)are Gaussian due to the noise distribution;\\nsee (9.3).\\nIn the following, we will discuss how to ﬁnd optimal parameters \\x12\\x032\\nRDfor the linear regression model (9.4). Once the parameters \\x12\\x03are\\nfound, we can predict function values by using this parameter estimate\\nin (9.4) so that at an arbitrary test input x\\x03the distribution of the corre-\\nsponding target y\\x03is\\np(y\\x03jx\\x03;\\x12\\x03) =N\\x00y\\x03jx>\\n\\x03\\x12\\x03; \\x1b2\\x01: (9.6)\\nIn the following, we will have a look at parameter estimation by maxi-\\nmizing the likelihood, a topic that we already covered to some degree in\\nSection 8.3.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.2 Parameter Estimation 293\\n9.2.1 Maximum Likelihood Estimation\\nA widely used approach to ﬁnding the desired parameters \\x12MLismaximum maximum likelihood\\nestimation likelihood estimation , where we ﬁnd parameters \\x12MLthat maximize the\\nlikelihood (9.5b). Intuitively, maximizing the likelihood means maximiz- Maximizing the\\nlikelihood means\\nmaximizing the\\npredictive\\ndistribution of the\\n(training) data\\ngiven the\\nparameters.ing the predictive distribution of the training data given the model param-\\neters. We obtain the maximum likelihood parameters as\\n\\x12ML= arg max\\n\\x12p(YjX;\\x12): (9.7)\\nThe likelihood is not\\na probability\\ndistribution in the\\nparameters.Remark. The likelihood p(yjx;\\x12)is not a probability distribution in \\x12: It\\nis simply a function of the parameters \\x12but does not integrate to 1(i.e.,\\nit is unnormalized), and may not even be integrable with respect to \\x12.\\nHowever, the likelihood in (9.7) is a normalized probability distribution\\niny. }\\nTo ﬁnd the desired parameters \\x12MLthat maximize the likelihood, we\\ntypically perform gradient ascent (or gradient descent on the negative\\nlikelihood). In the case of linear regression we consider here, however, Since the logarithm\\nis a (strictly)\\nmonotonically\\nincreasing function,\\nthe optimum of a\\nfunctionfis\\nidentical to the\\noptimum of logf.a closed-form solution exists, which makes iterative gradient descent un-\\nnecessary. In practice, instead of maximizing the likelihood directly, we\\napply the log-transformation to the likelihood function and minimize the\\nnegative log-likelihood.\\nRemark (Log-Transformation) .Since the likelihood (9.5b) is a product of\\nNGaussian distributions, the log-transformation is useful since (a) it does\\nnot suffer from numerical underﬂow, and (b) the differentiation rules will\\nturn out simpler. More speciﬁcally, numerical underﬂow will be a prob-\\nlem when we multiply Nprobabilities, where Nis the number of data\\npoints, since we cannot represent very small numbers, such as 10\\x00256.\\nFurthermore, the log-transform will turn the product into a sum of log-\\nprobabilities such that the corresponding gradient is a sum of individual\\ngradients, instead of a repeated application of the product rule (5.46) to\\ncompute the gradient of a product of Nterms. }\\nTo ﬁnd the optimal parameters \\x12MLof our linear regression problem,\\nwe minimize the negative log-likelihood\\n\\x00logp(YjX;\\x12) =\\x00logNY\\nn=1p(ynjxn;\\x12) =\\x00NX\\nn=1logp(ynjxn;\\x12);(9.8)\\nwhere we exploited that the likelihood (9.5b) factorizes over the number\\nof data points due to our independence assumption on the training set.\\nIn the linear regression model (9.4), the likelihood is Gaussian (due to\\nthe Gaussian additive noise term), such that we arrive at\\nlogp(ynjxn;\\x12) =\\x001\\n2\\x1b2(yn\\x00x>\\nn\\x12)2+const; (9.9)\\nwhere the constant includes all terms independent of \\x12. Using (9.9) in the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n294 Linear Regression\\nnegative log-likelihood (9.8), we obtain (ignoring the constant terms)\\nL(\\x12) :=1\\n2\\x1b2NX\\nn=1(yn\\x00x>\\nn\\x12)2(9.10a)\\n=1\\n2\\x1b2(y\\x00X\\x12)>(y\\x00X\\x12) =1\\n2\\x1b2ky\\x00X\\x12k2; (9.10b)\\nwhere we deﬁne the design matrix X:= [x1;:::;xN]>2RN\\x02Das the The negative\\nlog-likelihood\\nfunction is also\\ncalled error function .\\ndesign matrixcollection of training inputs and y:= [y1;:::;yN]>2RNas a vector that\\ncollects all training targets. Note that the nth row in the design matrix X\\ncorresponds to the training input xn. In (9.10b), we used the fact that the\\nThe squared error is\\noften used as a\\nmeasure of distance.sum of squared errors between the observations ynand the corresponding\\nmodel prediction x>\\nn\\x12equals the squared distance between yandX\\x12.\\nRecall from\\nSection 3.1 that\\nkxk2=x>xif we\\nchoose the dot\\nproduct as the inner\\nproduct.With (9.10b), we have now a concrete form of the negative log-likelihood\\nfunction we need to optimize. We immediately see that (9.10b) is quadratic\\nin\\x12. This means that we can ﬁnd a unique global solution \\x12MLfor mini-\\nmizing the negative log-likelihood L. We can ﬁnd the global optimum by\\ncomputing the gradient of L, setting it to 0and solving for \\x12.\\nUsing the results from Chapter 5, we compute the gradient of Lwith\\nrespect to the parameters as\\ndL\\nd\\x12=d\\nd\\x12\\x121\\n2\\x1b2(y\\x00X\\x12)>(y\\x00X\\x12)\\x13\\n(9.11a)\\n=1\\n2\\x1b2d\\nd\\x12\\x10\\ny>y\\x002y>X\\x12+\\x12>X>X\\x12\\x11\\n(9.11b)\\n=1\\n\\x1b2(\\x00y>X+\\x12>X>X)2R1\\x02D: (9.11c)\\nThe maximum likelihood estimator \\x12MLsolvesdL\\nd\\x12=0>(necessary opti-\\nmality condition) and we obtain Ignoring the\\npossibility of\\nduplicate data\\npoints, rk(X) =D\\nifN>D, i.e., we\\ndo not have more\\nparameters than\\ndata points.dL\\nd\\x12=0>(9.11c)()\\x12>\\nMLX>X=y>X (9.12a)\\n()\\x12>\\nML=y>X(X>X)\\x001(9.12b)\\n()\\x12ML= (X>X)\\x001X>y: (9.12c)\\nWe could right-multiply the ﬁrst equation by (X>X)\\x001becauseX>Xis\\npositive deﬁnite if rk(X) =D, where rk(X)denotes the rank of X.\\nRemark. Setting the gradient to 0>is a necessary and sufﬁcient condition,\\nand we obtain a global minimum since the Hessian r2\\n\\x12L(\\x12) =X>X2\\nRD\\x02Dis positive deﬁnite. }\\nRemark. The maximum likelihood solution in (9.12c) requires us to solve\\na system of linear equations of the form A\\x12=bwithA= (X>X)and\\nb=X>y. }\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.2 Parameter Estimation 295\\nExample 9.2 (Fitting Lines)\\nLet us have a look at Figure 9.2, where we aim to ﬁt a straight line f(x) =\\n\\x12x, where\\x12is an unknown slope, to a dataset using maximum likelihood\\nestimation. Examples of functions in this model class (straight lines) are\\nshown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we ﬁnd\\nthe maximum likelihood estimate of the slope parameter \\x12using (9.12c)\\nand obtain the maximum likelihood linear function in Figure 9.2(c).\\nMaximum Likelihood Estimation with Features\\nSo far, we considered the linear regression setting described in (9.4),\\nwhich allowed us to ﬁt straight lines to data using maximum likelihood\\nestimation. However, straight lines are not sufﬁciently expressive when it Linear regression\\nrefers to “linear-in-\\nthe-parameters”\\nregression models,\\nbut the inputs can\\nundergo any\\nnonlinear\\ntransformation.comes to ﬁtting more interesting data. Fortunately, linear regression offers\\nus a way to ﬁt nonlinear functions within the linear regression framework:\\nSince “linear regression” only refers to “linear in the parameters”, we can\\nperform an arbitrary nonlinear transformation \\x1e(x)of the inputs xand\\nthen linearly combine the components of this transformation. The corre-\\nsponding linear regression model is\\np(yjx;\\x12) =N\\x00yj\\x1e>(x)\\x12; \\x1b2\\x01\\n()y=\\x1e>(x)\\x12+\\x0f=K\\x001X\\nk=0\\x12k\\x1ek(x) +\\x0f;(9.13)\\nwhere\\x1e:RD!RKis a (nonlinear) transformation of the inputs xand\\n\\x1ek:RD!Ris thekth component of the feature vector \\x1e. Note that the feature vector\\nmodel parameters \\x12still appear only linearly.\\nExample 9.3 (Polynomial Regression)\\nWe are concerned with a regression problem y=\\x1e>(x)\\x12+\\x0f, wherex2R\\nand\\x122RK. A transformation that is often used in this context is\\n\\x1e(x) =2\\n6664\\x1e0(x)\\n\\x1e1(x)\\n...\\n\\x1eK\\x001(x)3\\n7775=2\\n666666641\\nx\\nx2\\nx3\\n...\\nxK\\x0013\\n777777752RK: (9.14)\\nThis means that we “lift” the original one-dimensional input space into\\naK-dimensional feature space consisting of all monomials xkfork=\\n0;:::;K\\x001. With these features, we can model polynomials of degree\\n6K\\x001within the framework of linear regression: A polynomial of degree\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n296 Linear Regression\\nK\\x001is\\nf(x) =K\\x001X\\nk=0\\x12kxk=\\x1e>(x)\\x12; (9.15)\\nwhere\\x1eis deﬁned in (9.14) and \\x12= [\\x120;:::;\\x12K\\x001]>2RKcontains the\\n(linear) parameters \\x12k.\\nLet us now have a look at maximum likelihood estimation of the param-\\neters\\x12in the linear regression model (9.13). We consider training inputs\\nxn2RDand targets yn2R,n= 1;:::;N , and deﬁne the feature matrix feature matrix\\n(design matrix ) as design matrix\\n\\x08:=2\\n64\\x1e>(x1)\\n...\\n\\x1e>(xN)3\\n75=2\\n6664\\x1e0(x1)\\x01\\x01\\x01\\x1eK\\x001(x1)\\n\\x1e0(x2)\\x01\\x01\\x01\\x1eK\\x001(x2)\\n......\\n\\x1e0(xN)\\x01\\x01\\x01\\x1eK\\x001(xN)3\\n77752RN\\x02K;(9.16)\\nwhere \\x08ij=\\x1ej(xi)and\\x1ej:RD!R.\\nExample 9.4 (Feature Matrix for Second-order Polynomials)\\nFor a second-order polynomial and Ntraining points xn2R;n=\\n1;:::;N , the feature matrix is\\n\\x08=2\\n66641x1x2\\n1\\n1x2x2\\n2.........\\n1xNx2\\nN3\\n7775: (9.17)\\nWith the feature matrix \\x08deﬁned in (9.16), the negative log-likelihood\\nfor the linear regression model (9.13) can be written as\\n\\x00logp(YjX;\\x12) =1\\n2\\x1b2(y\\x00\\x08\\x12)>(y\\x00\\x08\\x12) +const: (9.18)\\nComparing (9.18) with the negative log-likelihood in (9.10b) for the “fea-\\nture-free” model, we immediately see we just need to replace Xwith\\x08.\\nSince bothXand\\x08are independent of the parameters \\x12that we wish to\\noptimize, we arrive immediately at the maximum likelihood estimate maximum likelihood\\nestimate\\n\\x12ML= (\\x08>\\x08)\\x001\\x08>y (9.19)\\nfor the linear regression problem with nonlinear features deﬁned in (9.13).\\nRemark. When we were working without features, we required X>Xto\\nbe invertible, which is the case when rk(X) =D, i.e., the columns of X\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.2 Parameter Estimation 297\\nare linearly independent. In (9.19), we therefore require \\x08>\\x082RK\\x02K\\nto be invertible. This is the case if and only if rk(\\x08) =K.}\\nExample 9.5 (Maximum Likelihood Polynomial Fit)\\nFigure 9.4\\nPolynomial\\nregression:\\n(a) dataset\\nconsisting of\\n(xn;yn)pairs,\\nn= 1;:::; 10;\\n(b) maximum\\nlikelihood\\npolynomial of\\ndegree 4.\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Regression dataset.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (b) Polynomial of degree 4 determined by max-\\nimum likelihood estimation.\\nConsider the dataset in Figure 9.4(a). The dataset consists of N= 10\\npairs (xn;yn), wherexn\\x18U[\\x005;5]andyn=\\x00sin(xn=5) + cos(xn) +\\x0f,\\nwhere\\x0f\\x18N\\x000;0:22\\x01\\n.\\nWe ﬁt a polynomial of degree 4using maximum likelihood estimation,\\ni.e., parameters \\x12MLare given in (9.19). The maximum likelihood estimate\\nyields function values \\x1e>(x\\x03)\\x12MLat any test location x\\x03. The result is\\nshown in Figure 9.4(b).\\nEstimating the Noise Variance\\nThus far, we assumed that the noise variance \\x1b2is known. However, we\\ncan also use the principle of maximum likelihood estimation to obtain the\\nmaximum likelihood estimator \\x1b2\\nMLfor the noise variance. To do this, we\\nfollow the standard procedure: We write down the log-likelihood, com-\\npute its derivative with respect to \\x1b2>0, set it to 0, and solve. The\\nlog-likelihood is given by\\nlogp(YjX;\\x12;\\x1b2) =NX\\nn=1logN\\x00ynj\\x1e>(xn)\\x12; \\x1b2\\x01\\n(9.20a)\\n=NX\\nn=1\\x12\\n\\x001\\n2log(2\\x19)\\x001\\n2log\\x1b2\\x001\\n2\\x1b2(yn\\x00\\x1e>(xn)\\x12)2\\x13\\n(9.20b)\\n=\\x00N\\n2log\\x1b2\\x001\\n2\\x1b2NX\\nn=1(yn\\x00\\x1e>(xn)\\x12)2\\n|{z}\\n=:s+const: (9.20c)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n298 Linear Regression\\nThe partial derivative of the log-likelihood with respect to \\x1b2is then\\n@logp(YjX;\\x12;\\x1b2)\\n@\\x1b2=\\x00N\\n2\\x1b2+1\\n2\\x1b4s= 0 (9.21a)\\n()N\\n2\\x1b2=s\\n2\\x1b4(9.21b)\\nso that we identify\\n\\x1b2\\nML=s\\nN=1\\nNNX\\nn=1(yn\\x00\\x1e>(xn)\\x12)2: (9.22)\\nTherefore, the maximum likelihood estimate of the noise variance is the\\nempirical mean of the squared distances between the noise-free function\\nvalues\\x1e>(xn)\\x12and the corresponding noisy observations ynat input lo-\\ncationsxn.\\n9.2.2 Overﬁtting in Linear Regression\\nWe just discussed how to use maximum likelihood estimation to ﬁt lin-\\near models (e.g., polynomials) to data. We can evaluate the quality of\\nthe model by computing the error/loss incurred. One way of doing this\\nis to compute the negative log-likelihood (9.10b), which we minimized\\nto determine the maximum likelihood estimator. Alternatively, given that\\nthe noise parameter \\x1b2is not a free model parameter, we can ignore the\\nscaling by 1=\\x1b2, so that we end up with a squared-error-loss function\\nky\\x00\\x08\\x12k2. Instead of using this squared loss, we often use the root mean root mean square\\nerror square error (RMSE )\\nRMSEr\\n1\\nNky\\x00\\x08\\x12k2=vuut1\\nNNX\\nn=1(yn\\x00\\x1e>(xn)\\x12)2; (9.23)\\nwhich (a) allows us to compare errors of datasets with different sizes\\nand (b) has the same scale and the same units as the observed func- The RMSE is\\nnormalized. tion values yn. For example, if we ﬁt a model that maps post-codes ( x\\nis given in latitude, longitude) to house prices ( y-values are EUR) then\\nthe RMSE is also measured in EUR, whereas the squared error is given\\nin EUR2. If we choose to include the factor \\x1b2from the original negative The negative\\nlog-likelihood is\\nunitless.log-likelihood (9.10b), then we end up with a unitless objective, i.e., in\\nthe preceding example, our objective would no longer be in EUR or EUR2.\\nFor model selection (see Section 8.6), we can use the RMSE (or the\\nnegative log-likelihood) to determine the best degree of the polynomial by\\nﬁnding the polynomial degree Mthat minimizes the objective. Given that\\nthe polynomial degree is a natural number, we can perform a brute-force\\nsearch and enumerate all (reasonable) values of M. For a training set of\\nsizeNit is sufﬁcient to test 06M6N\\x001. ForM <N , the maximum\\nlikelihood estimator is unique. For M>N, we have more parameters\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.2 Parameter Estimation 299\\nFigure 9.5\\nMaximum\\nlikelihood ﬁts for\\ndifferent polynomial\\ndegreesM.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(a)M= 0\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (b)M= 1\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (c)M= 3\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(d)M= 4\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (e)M= 6\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (f)M= 9\\nthan data points, and would need to solve an underdetermined system of\\nlinear equations ( \\x08>\\x08in (9.19) would also no longer be invertible) so\\nthat there are inﬁnitely many possible maximum likelihood estimators.\\nFigure 9.5 shows a number of polynomial ﬁts determined by maximum\\nlikelihood for the dataset from Figure 9.4(a) with N= 10 observations.\\nWe notice that polynomials of low degree (e.g., constants ( M= 0) or\\nlinear (M= 1)) ﬁt the data poorly and, hence, are poor representations\\nof the true underlying function. For degrees M= 3;:::; 6, the ﬁts look\\nplausible and smoothly interpolate the data. When we go to higher-degree The case of\\nM=N\\x001is\\nextreme in the sense\\nthat otherwise the\\nnull space of the\\ncorresponding\\nsystem of linear\\nequations would be\\nnon-trivial, and we\\nwould have\\ninﬁnitely many\\noptimal solutions to\\nthe linear regression\\nproblem.polynomials, we notice that they ﬁt the data better and better. In the ex-\\ntreme case of M=N\\x001 = 9 , the function will pass through every single\\ndata point. However, these high-degree polynomials oscillate wildly and\\nare a poor representation of the underlying function that generated the\\ndata, such that we suffer from overﬁtting .\\noverﬁtting\\nNote that the noise\\nvariance\\x1b2>0.Remember that the goal is to achieve good generalization by making\\naccurate predictions for new (unseen) data. We obtain some quantita-\\ntive insight into the dependence of the generalization performance on the\\npolynomial of degree Mby considering a separate test set comprising 200\\ndata points generated using exactly the same procedure used to generate\\nthe training set. As test inputs, we chose a linear grid of 200points in the\\ninterval of [\\x005;5]. For each choice of M, we evaluate the RMSE (9.23) for\\nboth the training data and the test data.\\nLooking now at the test error, which is a qualitive measure of the gen-\\neralization properties of the corresponding polynomial, we notice that ini-\\ntially the test error decreases; see Figure 9.6 (orange). For fourth-order\\npolynomials, the test error is relatively low and stays relatively constant up\\nto degree 5. However, from degree 6onward the test error increases signif-\\nicantly, and high-order polynomials have very bad generalization proper-\\nties. In this particular example, this also is evident from the corresponding\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n300 Linear Regression\\nFigure 9.6 Training\\nand test error.\\n0 2 4 6 8 10\\nDegree of polynomial0246810RMSETraining error\\nTest error\\nmaximum likelihood ﬁts in Figure 9.5. Note that the training error (blue training error\\ncurve in Figure 9.6) never increases when the degree of the polynomial in-\\ncreases. In our example, the best generalization (the point of the smallest\\ntest error ) is obtained for a polynomial of degree M= 4. test error\\n9.2.3 Maximum A Posteriori Estimation\\nWe just saw that maximum likelihood estimation is prone to overﬁtting.\\nWe often observe that the magnitude of the parameter values becomes\\nrelatively large if we run into overﬁtting (Bishop, 2006).\\nTo mitigate the effect of huge parameter values, we can place a prior\\ndistribution p(\\x12)on the parameters. The prior distribution explicitly en-\\ncodes what parameter values are plausible (before having seen any data).\\nFor example, a Gaussian prior p(\\x12) =N\\x000;1\\x01\\non a single parameter\\n\\x12encodes that parameter values are expected lie in the interval [\\x002;2]\\n(two standard deviations around the mean value). Once a dataset X;Y\\nis available, instead of maximizing the likelihood we seek parameters that\\nmaximize the posterior distribution p(\\x12jX;Y). This procedure is called\\nmaximum a posteriori (MAP) estimation. maximum a\\nposteriori\\nMAPThe posterior over the parameters \\x12, given the training data X;Y, is\\nobtained by applying Bayes’ theorem (Section 6.3) as\\np(\\x12jX;Y) =p(YjX;\\x12)p(\\x12)\\np(YjX ): (9.24)\\nSince the posterior explicitly depends on the parameter prior p(\\x12), the\\nprior will have an effect on the parameter vector we ﬁnd as the maximizer\\nof the posterior. We will see this more explicitly in the following. The\\nparameter vector \\x12MAPthat maximizes the posterior (9.24) is the MAP\\nestimate.\\nTo ﬁnd the MAP estimate, we follow steps that are similar in ﬂavor\\nto maximum likelihood estimation. We start with the log-transform and\\ncompute the log-posterior as\\nlogp(\\x12jX;Y) = logp(YjX;\\x12) + logp(\\x12) +const; (9.25)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.2 Parameter Estimation 301\\nwhere the constant comprises the terms that are independent of \\x12. We see\\nthat the log-posterior in (9.25) is the sum of the log-likelihood p(YjX;\\x12)\\nand the log-prior logp(\\x12)so that the MAP estimate will be a “compromise”\\nbetween the prior (our suggestion for plausible parameter values before\\nobserving data) and the data-dependent likelihood.\\nTo ﬁnd the MAP estimate \\x12MAP, we minimize the negative log-posterior\\ndistribution with respect to \\x12, i.e., we solve\\n\\x12MAP2arg min\\n\\x12f\\x00logp(YjX;\\x12)\\x00logp(\\x12)g: (9.26)\\nThe gradient of the negative log-posterior with respect to \\x12is\\n\\x00d logp(\\x12jX;Y)\\nd\\x12=\\x00d logp(YjX;\\x12)\\nd\\x12\\x00d logp(\\x12)\\nd\\x12; (9.27)\\nwhere we identify the ﬁrst term on the right-hand side as the gradient of\\nthe negative log-likelihood from (9.11c).\\nWith a (conjugate) Gaussian prior p(\\x12) =N\\x000; b2I\\x01\\non the parameters\\n\\x12, the negative log-posterior for the linear regression setting (9.13), we\\nobtain the negative log posterior\\n\\x00logp(\\x12jX;Y) =1\\n2\\x1b2(y\\x00\\x08\\x12)>(y\\x00\\x08\\x12) +1\\n2b2\\x12>\\x12+const:(9.28)\\nHere, the ﬁrst term corresponds to the contribution from the log-likelihood,\\nand the second term originates from the log-prior. The gradient of the log-\\nposterior with respect to the parameters \\x12is then\\n\\x00d logp(\\x12jX;Y)\\nd\\x12=1\\n\\x1b2(\\x12>\\x08>\\x08\\x00y>\\x08) +1\\nb2\\x12>: (9.29)\\nWe will ﬁnd the MAP estimate \\x12MAPby setting this gradient to 0>and\\nsolving for\\x12MAP. We obtain\\n1\\n\\x1b2(\\x12>\\x08>\\x08\\x00y>\\x08) +1\\nb2\\x12>=0>(9.30a)\\n()\\x12>\\x121\\n\\x1b2\\x08>\\x08+1\\nb2I\\x13\\n\\x001\\n\\x1b2y>\\x08=0>(9.30b)\\n()\\x12>\\x12\\n\\x08>\\x08+\\x1b2\\nb2I\\x13\\n=y>\\x08 (9.30c)\\n()\\x12>=y>\\x08\\x12\\n\\x08>\\x08+\\x1b2\\nb2I\\x13\\x001\\n(9.30d)\\nso that the MAP estimate is (by transposing both sides of the last equality) \\x08>\\x08is symmetric,\\npositive semi\\ndeﬁnite. The\\nadditional term\\nin (9.31) is strictly\\npositive deﬁnite so\\nthat the inverse\\nexists.\\x12MAP=\\x12\\n\\x08>\\x08+\\x1b2\\nb2I\\x13\\x001\\n\\x08>y: (9.31)\\nComparing the MAP estimate in (9.31) with the maximum likelihood es-\\ntimate in (9.19), we see that the only difference between both solutions\\nis the additional term\\x1b2\\nb2Iin the inverse matrix. This term ensures that\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n302 Linear Regression\\n\\x08>\\x08+\\x1b2\\nb2Iis symmetric and strictly positive deﬁnite (i.e., its inverse\\nexists and the MAP estimate is the unique solution of a system of linear\\nequations). Moreover, it reﬂects the impact of the regularizer.\\nExample 9.6 (MAP Estimation for Polynomial Regression)\\nIn the polynomial regression example from Section 9.2.1, we place a Gaus-\\nsian priorp(\\x12) =N\\x000;I\\x01\\non the parameters \\x12and determine the MAP\\nestimates according to (9.31). In Figure 9.7, we show both the maximum\\nlikelihood and the MAP estimates for polynomials of degree 6(left) and\\ndegree 8(right). The prior (regularizer) does not play a signiﬁcant role\\nfor the low-degree polynomial, but keeps the function relatively smooth\\nfor higher-degree polynomials. Although the MAP estimate can push the\\nboundaries of overﬁtting, it is not a general solution to this problem, so\\nwe need a more principled approach to tackle overﬁtting.\\nFigure 9.7\\nPolynomial\\nregression:\\nmaximum likelihood\\nand MAP estimates.\\n(a) Polynomials of\\ndegree 6;\\n(b) polynomials of\\ndegree 8.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\n(a) Polynomials of degree 6.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP (b) Polynomials of degree 8.\\n9.2.4 MAP Estimation as Regularization\\nInstead of placing a prior distribution on the parameters \\x12, it is also pos-\\nsible to mitigate the effect of overﬁtting by penalizing the amplitude of\\nthe parameter by means of regularization . Inregularized least squares , we regularization\\nregularized least\\nsquaresconsider the loss function\\nky\\x00\\x08\\x12k2+\\x15k\\x12k2\\n2; (9.32)\\nwhich we minimize with respect to \\x12(see Section 8.2.3). Here, the ﬁrst\\nterm is a data-ﬁt term (also called misﬁt term ), which is proportional to data-ﬁt term\\nmisﬁt term the negative log-likelihood; see (9.10b). The second term is called the\\nregularizer , and the regularization parameter \\x15>0controls the “strict- regularizer\\nregularization\\nparameterness” of the regularization.\\nRemark. Instead of the Euclidean norm k\\x01k2, we can choose any p-norm\\nk\\x01kpin (9.32). In practice, smaller values for plead to sparser solutions.\\nHere, “sparse” means that many parameter values \\x12d= 0, which is also\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.3 Bayesian Linear Regression 303\\nuseful for variable selection. For p= 1, the regularizer is called LASSO LASSO\\n(least absolute shrinkage and selection operator) and was proposed by Tib-\\nshirani (1996). }\\nThe regularizer \\x15k\\x12k2\\n2in (9.32) can be interpreted as a negative log-\\nGaussian prior, which we use in MAP estimation; see (9.26). More specif-\\nically, with a Gaussian prior p(\\x12) =N\\x000; b2I\\x01\\n, we obtain the negative\\nlog-Gaussian prior\\n\\x00logp(\\x12) =1\\n2b2k\\x12k2\\n2+const (9.33)\\nso that for\\x15=1\\n2b2the regularization term and the negative log-Gaussian\\nprior are identical.\\nGiven that the regularized least-squares loss function in (9.32) consists\\nof terms that are closely related to the negative log-likelihood plus a neg-\\native log-prior, it is not surprising that, when we minimize this loss, we\\nobtain a solution that closely resembles the MAP estimate in (9.31). More\\nspeciﬁcally, minimizing the regularized least-squares loss function yields\\n\\x12RLS= (\\x08>\\x08+\\x15I)\\x001\\x08>y; (9.34)\\nwhich is identical to the MAP estimate in (9.31) for \\x15=\\x1b2\\nb2, where\\x1b2is\\nthe noise variance and b2the variance of the (isotropic) Gaussian prior\\np(\\x12) =N\\x000; b2I\\x01\\n. A point estimate is a\\nsingle speciﬁc\\nparameter value,\\nunlike a distribution\\nover plausible\\nparameter settings.So far, we have covered parameter estimation using maximum likeli-\\nhood and MAP estimation where we found point estimates \\x12\\x03that op-\\ntimize an objective function (likelihood or posterior). We saw that both\\nmaximum likelihood and MAP estimation can lead to overﬁtting. In the\\nnext section, we will discuss Bayesian linear regression, where we use\\nBayesian inference (Section 8.4) to ﬁnd a posterior distribution over the\\nunknown parameters, which we subsequently use to make predictions.\\nMore speciﬁcally, for predictions we will average over all plausible sets of\\nparameters instead of focusing on a point estimate.\\n9.3 Bayesian Linear Regression\\nPreviously, we looked at linear regression models where we estimated the\\nmodel parameters \\x12, e.g., by means of maximum likelihood or MAP esti-\\nmation. We discovered that MLE can lead to severe overﬁtting, in particu-\\nlar, in the small-data regime. MAP addresses this issue by placing a prior\\non the parameters that plays the role of a regularizer. Bayesian linear\\nregression Bayesian linear regression pushes the idea of the parameter prior a step\\nfurther and does not even attempt to compute a point estimate of the\\nparameters, but instead the full posterior distribution over the parameters\\nis taken into account when making predictions. This means we do not ﬁt\\nany parameters, but we compute a mean over all plausible parameters\\nsettings (according to the posterior).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n304 Linear Regression\\n9.3.1 Model\\nIn Bayesian linear regression, we consider the model\\nprior p(\\x12) =N\\x00m0;S0\\x01;\\nlikelihood p(yjx;\\x12) =N\\x00yj\\x1e>(x)\\x12; \\x1b2\\x01;(9.35)\\nwhere we now explicitly place a Gaussian prior p(\\x12) =N\\x00m0;S0\\x01\\non\\x12, Figure 9.8\\nGraphical model for\\nBayesian linear\\nregression.\\n\\x12\\ny\\x1b\\nxm0S0which turns the parameter vector into a random variable. This allows us\\nto write down the corresponding graphical model in Figure 9.8, where we\\nmade the parameters of the Gaussian prior on \\x12explicit. The full proba-\\nbilistic model, i.e., the joint distribution of observed and unobserved ran-\\ndom variables, yand\\x12, respectively, is\\np(y;\\x12jx) =p(yjx;\\x12)p(\\x12): (9.36)\\n9.3.2 Prior Predictions\\nIn practice, we are usually not so much interested in the parameter values\\n\\x12themselves. Instead, our focus often lies in the predictions we make\\nwith those parameter values. In a Bayesian setting, we take the parameter\\ndistribution and average over all plausible parameter settings when we\\nmake predictions. More speciﬁcally, to make predictions at an input x\\x03,\\nwe integrate out \\x12and obtain\\np(y\\x03jx\\x03) =Z\\np(y\\x03jx\\x03;\\x12)p(\\x12)d\\x12=E\\x12[p(y\\x03jx\\x03;\\x12)]; (9.37)\\nwhich we can interpret as the average prediction of y\\x03jx\\x03;\\x12for all plau-\\nsible parameters \\x12according to the prior distribution p(\\x12). Note that pre-\\ndictions using the prior distribution only require us to specify the input\\nx\\x03, but no training data.\\nIn our model (9.35), we chose a conjugate (Gaussian) prior on \\x12so\\nthat the predictive distribution is Gaussian as well (and can be computed\\nin closed form): With the prior distribution p(\\x12) =N\\x00m0;S0\\x01\\n, we obtain\\nthe predictive distribution as\\np(y\\x03jx\\x03) =N\\x00\\x1e>(x\\x03)m0;\\x1e>(x\\x03)S0\\x1e(x\\x03) +\\x1b2\\x01; (9.38)\\nwhere we exploited that (i) the prediction is Gaussian due to conjugacy\\n(see Section 6.6) and the marginalization property of Gaussians (see Sec-\\ntion 6.5), (ii) the Gaussian noise is independent so that\\nV[y\\x03] =V\\x12[\\x1e>(x\\x03)\\x12] +V\\x0f[\\x0f]; (9.39)\\nand (iii)y\\x03is a linear transformation of \\x12so that we can apply the rules\\nfor computing the mean and covariance of the prediction analytically by\\nusing (6.50) and (6.51), respectively. In (9.38), the term \\x1e>(x\\x03)S0\\x1e(x\\x03)\\nin the predictive variance explicitly accounts for the uncertainty associated\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.3 Bayesian Linear Regression 305\\nwith the parameters \\x12, whereas\\x1b2is the uncertainty contribution due to\\nthe measurement noise.\\nIf we are interested in predicting noise-free function values f(x\\x03) =\\n\\x1e>(x\\x03)\\x12instead of the noise-corrupted targets y\\x03we obtain\\np(f(x\\x03)) =N\\x00\\x1e>(x\\x03)m0;\\x1e>(x\\x03)S0\\x1e(x\\x03)\\x01; (9.40)\\nwhich only differs from (9.38) in the omission of the noise variance \\x1b2in\\nthe predictive variance.\\nRemark (Distribution over Functions) .Since we can represent the distri- The parameter\\ndistribution p(\\x12)\\ninduces a\\ndistribution over\\nfunctions.butionp(\\x12)using a set of samples \\x12iand every sample \\x12igives rise to a\\nfunctionfi(\\x01) =\\x12>\\ni\\x1e(\\x01), it follows that the parameter distribution p(\\x12)\\ninduces a distribution p(f(\\x01))over functions. Here we use the notation (\\x01)\\nto explicitly denote a functional relationship. }\\nExample 9.7 (Prior over Functions)\\nFigure 9.9 Prior\\nover functions.\\n(a) Distribution over\\nfunctions\\nrepresented by the\\nmean function\\n(black line) and the\\nmarginal\\nuncertainties\\n(shaded),\\nrepresenting the\\n67% and 95%\\nconﬁdence bounds,\\nrespectively;\\n(b) samples from\\nthe prior over\\nfunctions, which are\\ninduced by the\\nsamples from the\\nparameter prior.\\n−4−2 0 2 4\\nx−4−2024y(a) Prior distribution over functions.\\n−4−2 0 2 4\\nx−4−2024y (b) Samples from the prior distribution over\\nfunctions.\\nLet us consider a Bayesian linear regression problem with polynomials\\nof degree 5. We choose a parameter prior p(\\x12) =N\\x000;1\\n4I\\x01\\n. Figure 9.9\\nvisualizes the induced prior distribution over functions (shaded area: dark\\ngray: 67% conﬁdence bound; light gray: 95% conﬁdence bound) induced\\nby this parameter prior, including some function samples from this prior.\\nA function sample is obtained by ﬁrst sampling a parameter vector\\n\\x12i\\x18p(\\x12)and then computing fi(\\x01) =\\x12>\\ni\\x1e(\\x01). We used 200input lo-\\ncationsx\\x032[\\x005;5]to which we apply the feature function \\x1e(\\x01). The\\nuncertainty (represented by the shaded area) in Figure 9.9 is solely due to\\nthe parameter uncertainty because we considered the noise-free predictive\\ndistribution (9.40).\\nSo far, we looked at computing predictions using the parameter prior\\np(\\x12). However, when we have a parameter posterior (given some train-\\ning dataX;Y), the same principles for prediction and inference hold\\nas in (9.37) – we just need to replace the prior p(\\x12)with the posterior\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n306 Linear Regression\\np(\\x12jX;Y). In the following, we will derive the posterior distribution in\\ndetail before using it to make predictions.\\n9.3.3 Posterior Distribution\\nGiven a training set of inputs xn2RDand corresponding observations\\nyn2R,n= 1;:::;N , we compute the posterior over the parameters\\nusing Bayes’ theorem as\\np(\\x12jX;Y) =p(YjX;\\x12)p(\\x12)\\np(YjX ); (9.41)\\nwhereXis the set of training inputs and Ythe collection of correspond-\\ning training targets. Furthermore, p(YjX;\\x12)is the likelihood, p(\\x12)the\\nparameter prior, and\\np(YjX ) =Z\\np(YjX;\\x12)p(\\x12)d\\x12=E\\x12[p(YjX;\\x12)] (9.42)\\nthemarginal likelihood /evidence , which is independent of the parameters marginal likelihood\\nevidence \\x12and ensures that the posterior is normalized, i.e., it integrates to 1. We\\nThe marginal\\nlikelihood is the\\nexpected likelihood\\nunder the parameter\\nprior.can think of the marginal likelihood as the likelihood averaged over all\\npossible parameter settings (with respect to the prior distribution p(\\x12)).\\nTheorem 9.1 (Parameter Posterior) .In our model (9.35) , the parameter\\nposterior (9.41) can be computed in closed form as\\np(\\x12jX;Y) =N\\x00\\x12jmN;SN\\x01; (9.43a)\\nSN= (S\\x001\\n0+\\x1b\\x002\\x08>\\x08)\\x001; (9.43b)\\nmN=SN(S\\x001\\n0m0+\\x1b\\x002\\x08>y); (9.43c)\\nwhere the subscript Nindicates the size of the training set.\\nProof Bayes’ theorem tells us that the posterior p(\\x12jX;Y)is propor-\\ntional to the product of the likelihood p(YjX;\\x12)and the prior p(\\x12):\\nPosterior p(\\x12jX;Y) =p(YjX;\\x12)p(\\x12)\\np(YjX )(9.44a)\\nLikelihood p(YjX;\\x12) =N\\x00yj\\x08\\x12; \\x1b2I\\x01\\n(9.44b)\\nPrior p(\\x12) =N\\x00\\x12jm0;S0\\x01: (9.44c)\\nInstead of looking at the product of the prior and the likelihood, we\\ncan transform the problem into log-space and solve for the mean and\\ncovariance of the posterior by completing the squares.\\nThe sum of the log-prior and the log-likelihood is\\nlogN\\x00yj\\x08\\x12; \\x1b2I\\x01+ logN\\x00\\x12jm0;S0\\x01\\n(9.45a)\\n=\\x001\\n2\\x00\\x1b\\x002(y\\x00\\x08\\x12)>(y\\x00\\x08\\x12) + (\\x12\\x00m0)>S\\x001\\n0(\\x12\\x00m0)\\x01+const\\n(9.45b)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.3 Bayesian Linear Regression 307\\nwhere the constant contains terms independent of \\x12. We will ignore the\\nconstant in the following. We now factorize (9.45b), which yields\\n\\x001\\n2\\x00\\x1b\\x002y>y\\x002\\x1b\\x002y>\\x08\\x12+\\x12>\\x1b\\x002\\x08>\\x08\\x12+\\x12>S\\x001\\n0\\x12\\n\\x002m>\\n0S\\x001\\n0\\x12+m>\\n0S\\x001\\n0m0\\x01(9.46a)\\n=\\x001\\n2\\x00\\x12>(\\x1b\\x002\\x08>\\x08+S\\x001\\n0)\\x12\\x002(\\x1b\\x002\\x08>y+S\\x001\\n0m0)>\\x12\\x01+const;\\n(9.46b)\\nwhere the constant contains the black terms in (9.46a), which are inde-\\npendent of\\x12. The orange terms are terms that are linear in \\x12, and the\\nblue terms are the ones that are quadratic in \\x12. Inspecting (9.46b), we\\nﬁnd that this equation is quadratic in \\x12. The fact that the unnormalized\\nlog-posterior distribution is a (negative) quadratic form implies that the\\nposterior is Gaussian, i.e.,\\np(\\x12jX;Y) = exp(log p(\\x12jX;Y))/exp(logp(YjX;\\x12) + logp(\\x12))\\n(9.47a)\\n/exp\\x10\\n\\x001\\n2\\x00\\x12>(\\x1b\\x002\\x08>\\x08+S\\x001\\n0)\\x12\\x002(\\x1b\\x002\\x08>y+S\\x001\\n0m0)>\\x12\\x01\\x11\\n;\\n(9.47b)\\nwhere we used (9.46b) in the last expression.\\nThe remaining task is it to bring this (unnormalized) Gaussian into the\\nform that is proportional to N\\x00\\x12jmN;SN\\x01\\n, i.e., we need to identify the\\nmeanmNand the covariance matrix SN. To do this, we use the concept\\nofcompleting the squares . The desired log-posterior is completing the\\nsquares\\nlogN\\x00\\x12jmN;SN\\x01=\\x001\\n2(\\x12\\x00mN)>S\\x001\\nN(\\x12\\x00mN) +const (9.48a)\\n=\\x001\\n2\\x00\\x12>S\\x001\\nN\\x12\\x002m>\\nNS\\x001\\nN\\x12+m>\\nNS\\x001\\nNmN\\x01: (9.48b)\\nHere, we factorized the quadratic form (\\x12\\x00mN)>S\\x001\\nN(\\x12\\x00mN)into a Sincep(\\x12jX;Y) =\\nN\\x00\\nmN;SN\\x01\\n, it\\nholds that\\n\\x12MAP=mN.term that is quadratic in \\x12alone (blue), a term that is linear in \\x12(orange),\\nand a constant term (black). This allows us now to ﬁnd SNandmNby\\nmatching the colored expressions in (9.46b) and (9.48b), which yields\\nS\\x001\\nN=\\x08>\\x1b\\x002I\\x08+S\\x001\\n0 (9.49a)\\n()SN= (\\x1b\\x002\\x08>\\x08+S\\x001\\n0)\\x001(9.49b)\\nand\\nm>\\nNS\\x001\\nN= (\\x1b\\x002\\x08>y+S\\x001\\n0m0)>(9.50a)\\n()mN=SN(\\x1b\\x002\\x08>y+S\\x001\\n0m0): (9.50b)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n308 Linear Regression\\nRemark (General Approach to Completing the Squares) .If we are given\\nan equation\\nx>Ax\\x002a>x+const 1; (9.51)\\nwhereAis symmetric and positive deﬁnite, which we wish to bring into\\nthe form\\n(x\\x00\\x16)>\\x06(x\\x00\\x16) +const 2; (9.52)\\nwe can do this by setting\\n\\x06:=A; (9.53)\\n\\x16:=\\x06\\x001a (9.54)\\nand const 2=const 1\\x00\\x16>\\x06\\x16. }\\nWe can see that the terms inside the exponential in (9.47b) are of the\\nform (9.51) with\\nA:=\\x1b\\x002\\x08>\\x08+S\\x001\\n0; (9.55)\\na:=\\x1b\\x002\\x08>y+S\\x001\\n0m0: (9.56)\\nSinceA;acan be difﬁcult to identify in equations like (9.46a), it is of-\\nten helpful to bring these equations into the form (9.51) that decouples\\nquadratic term, linear terms, and constants, which simpliﬁes ﬁnding the\\ndesired solution.\\n9.3.4 Posterior Predictions\\nIn (9.37), we computed the predictive distribution of y\\x03at a test input\\nx\\x03using the parameter prior p(\\x12). In principle, predicting with the pa-\\nrameter posterior p(\\x12jX;Y)is not fundamentally different given that\\nin our conjugate model the prior and posterior are both Gaussian (with\\ndifferent parameters). Therefore, by following the same reasoning as in\\nSection 9.3.2, we obtain the (posterior) predictive distribution\\np(y\\x03jX;Y;x\\x03) =Z\\np(y\\x03jx\\x03;\\x12)p(\\x12jX;Y)d\\x12 (9.57a)\\n=Z\\nN\\x00y\\x03j\\x1e>(x\\x03)\\x12; \\x1b2\\x01N\\x00\\x12jmN;SN\\x01d\\x12(9.57b)\\n=N\\x00y\\x03j\\x1e>(x\\x03)mN;\\x1e>(x\\x03)SN\\x1e(x\\x03) +\\x1b2\\x01:(9.57c)\\nThe term\\x1e>(x\\x03)SN\\x1e(x\\x03)reﬂects the posterior uncertainty associated E[y\\x03jX;Y;x\\x03] =\\n\\x1e>(x\\x03)mN=\\n\\x1e>(x\\x03)\\x12MAP.with the parameters \\x12. Note thatSNdepends on the training inputs\\nthrough \\x08; see (9.43b). The predictive mean \\x1e>(x\\x03)mNcoincides with\\nthe predictions made with the MAP estimate \\x12MAP.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.3 Bayesian Linear Regression 309\\nRemark (Marginal Likelihood and Posterior Predictive Distribution) .By\\nreplacing the integral in (9.57a), the predictive distribution can be equiv-\\nalently written as the expectation E\\x12jX;Y[p(y\\x03jx\\x03;\\x12)], where the expec-\\ntation is taken with respect to the parameter posterior p(\\x12jX;Y).\\nWriting the posterior predictive distribution in this way highlights a\\nclose resemblance to the marginal likelihood (9.42). The key difference\\nbetween the marginal likelihood and the posterior predictive distribution\\nare (i) the marginal likelihood can be thought of predicting the training\\ntargetsyand not the test targets y\\x03, and (ii) the marginal likelihood av-\\nerages with respect to the parameter prior and not the parameter poste-\\nrior. }\\nRemark (Mean and Variance of Noise-Free Function Values) .In many\\ncases, we are not interested in the predictive distribution p(y\\x03jX;Y;x\\x03)\\nof a (noisy) observation y\\x03. Instead, we would like to obtain the distribu-\\ntion of the (noise-free) function values f(x\\x03) =\\x1e>(x\\x03)\\x12. We determine\\nthe corresponding moments by exploiting the properties of means and\\nvariances, which yields\\nE[f(x\\x03)jX;Y] =E\\x12[\\x1e>(x\\x03)\\x12jX;Y] =\\x1e>(x\\x03)E\\x12[\\x12jX;Y]\\n=\\x1e>(x\\x03)mN=m>\\nN\\x1e(x\\x03);(9.58)\\nV\\x12[f(x\\x03)jX;Y] =V\\x12[\\x1e>(x\\x03)\\x12jX;Y]\\n=\\x1e>(x\\x03)V\\x12[\\x12jX;Y]\\x1e(x\\x03)\\n=\\x1e>(x\\x03)SN\\x1e(x\\x03):(9.59)\\nWe see that the predictive mean is the same as the predictive mean for\\nnoisy observations as the noise has mean 0, and the predictive variance\\nonly differs by \\x1b2, which is the variance of the measurement noise: When\\nwe predict noisy function values, we need to include \\x1b2as a source of\\nuncertainty, but this term is not needed for noise-free predictions. Here,\\nthe only remaining uncertainty stems from the parameter posterior. }Integrating out\\nparameters induces\\na distribution over\\nfunctions.Remark (Distribution over Functions) .The fact that we integrate out the\\nparameters\\x12induces a distribution over functions: If we sample \\x12i\\x18\\np(\\x12jX;Y)from the parameter posterior, we obtain a single function re-\\nalization\\x12>\\ni\\x1e(\\x01). The mean function , i.e., the set of all expected function mean function\\nvalues E\\x12[f(\\x01)j\\x12;X;Y], of this distribution over functions is m>\\nN\\x1e(\\x01).\\nThe (marginal) variance, i.e., the variance of the function f(\\x01), is given by\\n\\x1e>(\\x01)SN\\x1e(\\x01). }\\nExample 9.8 (Posterior over Functions)\\nLet us revisit the Bayesian linear regression problem with polynomials\\nof degree 5. We choose a parameter prior p(\\x12) =N\\x000;1\\n4I\\x01\\n. Figure 9.9\\nvisualizes the prior over functions induced by the parameter prior and\\nsample functions from this prior.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n310 Linear Regression\\nFigure 9.10 shows the posterior over functions that we obtain via\\nBayesian linear regression. The training dataset is shown in panel (a);\\npanel (b) shows the posterior distribution over functions, including the\\nfunctions we would obtain via maximum likelihood and MAP estimation.\\nThe function we obtain using the MAP estimate also corresponds to the\\nposterior mean function in the Bayesian linear regression setting. Panel (c)\\nshows some plausible realizations (samples) of functions under that pos-\\nterior over functions.\\nFigure 9.10\\nBayesian linear\\nregression and\\nposterior over\\nfunctions.\\n(a) training data;\\n(b) posterior\\ndistribution over\\nfunctions;\\n(c) Samples from\\nthe posterior over\\nfunctions.\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Training data.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR (b) Posterior over functions rep-\\nresented by the marginal uncer-\\ntainties (shaded) showing the\\n67% and 95% predictive con-\\nﬁdence bounds, the maximum\\nlikelihood estimate (MLE) and\\nthe MAP estimate (MAP), the\\nlatter of which is identical to\\nthe posterior mean function.\\n−4−2 0 2 4\\nx−4−2024y\\n(c) Samples from the posterior\\nover functions, which are in-\\nduced by the samples from the\\nparameter posterior.\\nFigure 9.11 shows some posterior distributions over functions induced\\nby the parameter posterior. For different polynomial degrees M, the left\\npanels show the maximum likelihood function \\x12>\\nML\\x1e(\\x01), the MAP func-\\ntion\\x12>\\nMAP\\x1e(\\x01)(which is identical to the posterior mean function), and the\\n67% and 95% predictive conﬁdence bounds obtained by Bayesian linear\\nregression, represented by the shaded areas.\\nThe right panels show samples from the posterior over functions: Here,\\nwe sampled parameters \\x12ifrom the parameter posterior and computed\\nthe function\\x1e>(x\\x03)\\x12i, which is a single realization of a function under\\nthe posterior distribution over functions. For low-order polynomials, the\\nparameter posterior does not allow the parameters to vary much: The\\nsampled functions are nearly identical. When we make the model more\\nﬂexible by adding more parameters (i.e., we end up with a higher-order\\npolynomial), these parameters are not sufﬁciently constrained by the pos-\\nterior, and the sampled functions can be easily visually separated. We also\\nsee in the corresponding panels on the left how the uncertainty increases,\\nespecially at the boundaries.\\nAlthough for a seventh-order polynomial the MAP estimate yields a rea-\\nsonable ﬁt, the Bayesian linear regression model additionally tells us that\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.3 Bayesian Linear Regression 311\\nFigure 9.11\\nBayesian linear\\nregression. Left\\npanels: Shaded\\nareas indicate the\\n67% (dark gray)\\nand 95% (light\\ngray) predictive\\nconﬁdence bounds.\\nThe mean of the\\nBayesian linear\\nregression model\\ncoincides with the\\nMAP estimate. The\\npredictive\\nuncertainty is the\\nsum of the noise\\nterm and the\\nposterior parameter\\nuncertainty, which\\ndepends on the\\nlocation of the test\\ninput. Right panels:\\nsampled functions\\nfrom the posterior\\ndistribution.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Posterior distribution for polynomials of degree M= 3(left) and samples from the pos-\\nterior over functions (right).\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4−2 0 2 4\\nx−4−2024y\\n(b) Posterior distribution for polynomials of degree M= 5 (left) and samples from the\\nposterior over functions (right).\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4−2 0 2 4\\nx−4−2024y\\n(c) Posterior distribution for polynomials of degree M= 7(left) and samples from the pos-\\nterior over functions (right).\\nthe posterior uncertainty is huge. This information can be critical when\\nwe use these predictions in a decision-making system, where bad deci-\\nsions can have signiﬁcant consequences (e.g., in reinforcement learning\\nor robotics).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n312 Linear Regression\\n9.3.5 Computing the Marginal Likelihood\\nIn Section 8.6.2, we highlighted the importance of the marginal likelihood\\nfor Bayesian model selection. In the following, we compute the marginal\\nlikelihood for Bayesian linear regression with a conjugate Gaussian prior\\non the parameters, i.e., exactly the setting we have been discussing in this\\nchapter.\\nJust to recap, we consider the following generative process:\\n\\x12\\x18N\\x00m0;S0\\x01\\n(9.60a)\\nynjxn;\\x12\\x18N\\x00x>\\nn\\x12; \\x1b2\\x01; (9.60b)\\nn= 1;:::;N . The marginal likelihood is given by The marginal\\nlikelihood can be\\ninterpreted as the\\nexpected likelihood\\nunder the prior, i.e.,\\nE\\x12[p(YjX;\\x12)].p(YjX ) =Z\\np(YjX;\\x12)p(\\x12)d\\x12 (9.61a)\\n=Z\\nN\\x00yjX\\x12; \\x1b2I\\x01N\\x00\\x12jm0;S0\\x01d\\x12; (9.61b)\\nwhere we integrate out the model parameters \\x12. We compute the marginal\\nlikelihood in two steps: First, we show that the marginal likelihood is\\nGaussian (as a distribution in y); second, we compute the mean and co-\\nvariance of this Gaussian.\\n1. The marginal likelihood is Gaussian: From Section 6.5.2, we know that\\n(i) the product of two Gaussian random variables is an (unnormalized)\\nGaussian distribution, and (ii) a linear transformation of a Gaussian\\nrandom variable is Gaussian distributed. In (9.61b), we require a linear\\ntransformation to bring N\\x00yjX\\x12; \\x1b2I\\x01\\ninto the formN\\x00\\x12j\\x16;\\x06\\x01\\nfor\\nsome\\x16;\\x06. Once this is done, the integral can be solved in closed form.\\nThe result is the normalizing constant of the product of the two Gaus-\\nsians. The normalizing constant itself has Gaussian shape; see (6.76).\\n2. Mean and covariance. We compute the mean and covariance matrix\\nof the marginal likelihood by exploiting the standard results for means\\nand covariances of afﬁne transformations of random variables; see Sec-\\ntion 6.4.4. The mean of the marginal likelihood is computed as\\nE[YjX ] =E\\x12;\\x0f[X\\x12+\\x0f] =XE\\x12[\\x12] =Xm 0: (9.62)\\nNote that\\x0f\\x18N\\x000; \\x1b2I\\x01\\nis a vector of i.i.d. random variables. The\\ncovariance matrix is given as\\nCov[YjX] = Cov\\x12;\\x0f[X\\x12+\\x0f] = Cov\\x12[X\\x12] +\\x1b2I (9.63a)\\n=XCov\\x12[\\x12]X>+\\x1b2I=XS 0X>+\\x1b2I:(9.63b)\\nHence, the marginal likelihood is\\np(YjX ) = (2\\x19)\\x00N\\n2det(XS 0X>+\\x1b2I)\\x001\\n2 (9.64a)\\n\\x01exp\\x00\\x001\\n2(y\\x00Xm 0)>(XS 0X>+\\x1b2I)\\x001(y\\x00Xm 0)\\x01\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.4 Maximum Likelihood as Orthogonal Projection 313\\nFigure 9.12\\nGeometric\\ninterpretation of\\nleast squares.\\n(a) Dataset;\\n(b) maximum\\nlikelihood solution\\ninterpreted as a\\nprojection.\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Regression dataset consisting of noisy ob-\\nservationsyn(blue) of function values f(xn)\\nat input locations xn.\\n−4−2 0 2 4\\nx−4−2024y\\nProjection\\nObservations\\nMaximum likelihood estimate(b) The orange dots are the projections of\\nthe noisy observations (blue dots) onto the\\nline\\x12MLx. The maximum likelihood solution to\\na linear regression problem ﬁnds a subspace\\n(line) onto which the overall projection er-\\nror (orange lines) of the observations is mini-\\nmized.\\n=N\\x00yjXm 0;XS 0X>+\\x1b2I\\x01: (9.64b)\\nGiven the close connection with the posterior predictive distribution (see\\nRemark on Marginal Likelihood and Posterior Predictive Distribution ear-\\nlier in this section), the functional form of the marginal likelihood should\\nnot be too surprising.\\n9.4 Maximum Likelihood as Orthogonal Projection\\nHaving crunched through much algebra to derive maximum likelihood\\nand MAP estimates, we will now provide a geometric interpretation of\\nmaximum likelihood estimation. Let us consider a simple linear regression\\nsetting\\ny=x\\x12+\\x0f; \\x0f\\x18N\\x000; \\x1b2\\x01; (9.65)\\nin which we consider linear functions f:R!Rthat go through the\\norigin (we omit features here for clarity). The parameter \\x12determines the\\nslope of the line. Figure 9.12(a) shows a one-dimensional dataset.\\nWith a training data set f(x1;y1);:::; (xN;yN)gwe recall the results\\nfrom Section 9.2.1 and obtain the maximum likelihood estimator for the\\nslope parameter as\\n\\x12ML= (X>X)\\x001X>y=X>y\\nX>X2R; (9.66)\\nwhereX= [x1;:::;xN]>2RN,y= [y1;:::;yN]>2RN.\\nThis means for the training inputs Xwe obtain the optimal (maximum\\nlikelihood) reconstruction of the training targets as\\nX\\x12ML=XX>y\\nX>X=XX>\\nX>Xy; (9.67)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n314 Linear Regression\\ni.e., we obtain the approximation with the minimum least-squares error\\nbetweenyandX\\x12.\\nAs we are looking for a solution of y=X\\x12, we can think of linear\\nregression as a problem for solving systems of linear equations. There- Linear regression\\ncan be thought of as\\na method for solving\\nsystems of linear\\nequations.fore, we can relate to concepts from linear algebra and analytic geometry\\nthat we discussed in Chapters 2 and 3. In particular, looking carefully\\nat (9.67) we see that the maximum likelihood estimator \\x12MLin our ex-\\nample from (9.65) effectively does an orthogonal projection of yonto\\nthe one-dimensional subspace spanned by X. Recalling the results on or- Maximum\\nlikelihood linear\\nregression performs\\nan orthogonal\\nprojection.thogonal projections from Section 3.8, we identifyXX>\\nX>Xas the projection\\nmatrix,\\x12MLas the coordinates of the projection onto the one-dimensional\\nsubspace of RNspanned byXandX\\x12MLas the orthogonal projection of\\nyonto this subspace.\\nTherefore, the maximum likelihood solution provides also a geometri-\\ncally optimal solution by ﬁnding the vectors in the subspace spanned by\\nXthat are “closest” to the corresponding observations y, where “clos-\\nest” means the smallest (squared) distance of the function values ynto\\nxn\\x12. This is achieved by orthogonal projections. Figure 9.12(b) shows the\\nprojection of the noisy observations onto the subspace that minimizes the\\nsquared distance between the original dataset and its projection (note that\\nthex-coordinate is ﬁxed), which corresponds to the maximum likelihood\\nsolution.\\nIn the general linear regression case where\\ny=\\x1e>(x)\\x12+\\x0f; \\x0f\\x18N\\x000; \\x1b2\\x01\\n(9.68)\\nwith vector-valued features \\x1e(x)2RK, we again can interpret the maxi-\\nmum likelihood result\\ny\\x19\\x08\\x12ML; (9.69)\\n\\x12ML= (\\x08>\\x08)\\x001\\x08>y (9.70)\\nas a projection onto a K-dimensional subspace of RN, which is spanned\\nby the columns of the feature matrix \\x08; see Section 3.8.2.\\nIf the feature functions \\x1ekthat we use to construct the feature ma-\\ntrix\\x08are orthonormal (see Section 3.7), we obtain a special case where\\nthe columns of \\x08form an orthonormal basis (see Section 3.5), such that\\n\\x08>\\x08=I. This will then lead to the projection\\n\\x08(\\x08>\\x08)\\x001\\x08>y=\\x08\\x08>y= KX\\nk=1\\x1ek\\x1e>\\nk!\\ny (9.71)\\nso that the maximum likelihood projection is simply the sum of projections\\nofyonto the individual basis vectors \\x1ek, i.e., the columns of \\x08. Further-\\nmore, the coupling between different features has disappeared due to the\\northogonality of the basis. Many popular basis functions in signal process-\\ning, such as wavelets and Fourier bases, are orthogonal basis functions.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n9.5 Further Reading 315\\nWhen the basis is not orthogonal, one can convert a set of linearly inde-\\npendent basis functions to an orthogonal basis by using the Gram-Schmidt\\nprocess; see Section 3.8.3 and (Strang, 2003).\\n9.5 Further Reading\\nIn this chapter, we discussed linear regression for Gaussian likelihoods\\nand conjugate Gaussian priors on the parameters of the model. This al-\\nlowed for closed-form Bayesian inference. However, in some applications\\nwe may want to choose a different likelihood function. For example, in\\na binary classiﬁcation setting, we observe only two possible (categorical) classiﬁcation\\noutcomes, and a Gaussian likelihood is inappropriate in this setting. In-\\nstead, we can choose a Bernoulli likelihood that will return a probability of\\nthe predicted label to be 1(or0). We refer to the books by Barber (2012),\\nBishop (2006), and Murphy (2012) for an in-depth introduction to classiﬁ-\\ncation problems. A different example where non-Gaussian likelihoods are\\nimportant is count data. Counts are non-negative integers, and in this case\\na Binomial or Poisson likelihood would be a better choice than a Gaussian.\\nAll these examples fall into the category of generalized linear models , a ﬂex- generalized linear\\nmodel ible generalization of linear regression that allows for response variables\\nthat have error distributions other than a Gaussian distribution. The GLM Generalized linear\\nmodels are the\\nbuilding blocks of\\ndeep neural\\nnetworks.generalizes linear regression by allowing the linear model to be related\\nto the observed values via a smooth and invertible function \\x1b(\\x01)that may\\nbe nonlinear so that y=\\x1b(f(x)), wheref(x) =\\x12>\\x1e(x)is the linear\\nregression model from (9.13). We can therefore think of a generalized\\nlinear model in terms of function composition y=\\x1b\\x0ef, wherefis a\\nlinear regression model and \\x1bthe activation function. Note that although\\nwe are talking about “generalized linear models”, the outputs yare no\\nlonger linear in the parameters \\x12. Inlogistic regression , we choose the logistic regression\\nlogistic sigmoid \\x1b(f) =1\\n1+exp(\\x00f)2[0;1], which can be interpreted as the logistic sigmoid\\nprobability of observing y= 1of a Bernoulli random variable y2f0;1g.\\nThe function \\x1b(\\x01)is called transfer function oractivation function , and its transfer function\\nactivation function inverse is called the canonical link function . From this perspective, it is\\ncanonical link\\nfunction\\nFor ordinary linear\\nregression the\\nactivation function\\nwould simply be the\\nidentity.also clear that generalized linear models are the building blocks of (deep)\\nfeedforward neural networks: If we consider a generalized linear model\\ny=\\x1b(Ax+b), whereAis a weight matrix and ba bias vector, we iden-\\ntify this generalized linear model as a single-layer neural network with\\nactivation function \\x1b(\\x01). We can now recursively compose these functions\\nvia\\nA great post on the\\nrelation between\\nGLMs and deep\\nnetworks is\\navailable at\\nhttps://tinyurl.\\ncom/glm-dnn .xk+1=fk(xk)\\nfk(xk) =\\x1bk(Akxk+bk)(9.72)\\nfork= 0;:::;K\\x001, wherex0are the input features and xK=yare\\nthe observed outputs, such that fK\\x001\\x0e\\x01\\x01\\x01\\x0ef0is aK-layer deep neural\\nnetwork. Therefore, the building blocks of this deep neural network are\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n316 Linear Regression\\nthe generalized linear models deﬁned in (9.72). Neural networks (Bishop,\\n1995; Goodfellow et al., 2016) are signiﬁcantly more expressive and ﬂexi-\\nble than linear regression models. However, maximum likelihood parame-\\nter estimation is a non-convex optimization problem, and marginalization\\nof the parameters in a fully Bayesian setting is analytically intractable.\\nWe brieﬂy hinted at the fact that a distribution over parameters in-\\nduces a distribution over regression functions. Gaussian processes (Ras- Gaussian process\\nmussen and Williams, 2006) are regression models where the concept of\\na distribution over function is central. Instead of placing a distribution\\nover parameters, a Gaussian process places a distribution directly on the\\nspace of functions without the “detour” via the parameters. To do so, the\\nGaussian process exploits the kernel trick (Sch¨olkopf and Smola, 2002), kernel trick\\nwhich allows us to compute inner products between two function values\\nf(xi);f(xj)only by looking at the corresponding input xi;xj. A Gaus-\\nsian process is closely related to both Bayesian linear regression and sup-\\nport vector regression but can also be interpreted as a Bayesian neural\\nnetwork with a single hidden layer where the number of units tends to\\ninﬁnity (Neal, 1996; Williams, 1997). Excellent introductions to Gaussian\\nprocesses can be found in MacKay (1998) and Rasmussen and Williams\\n(2006).\\nWe focused on Gaussian parameter priors in the discussions in this chap-\\nter, because they allow for closed-form inference in linear regression mod-\\nels. However, even in a regression setting with Gaussian likelihoods, we\\nmay choose a non-Gaussian prior. Consider a setting, where the inputs are\\nx2RDand our training set is small and of size N\\x1cD. This means that\\nthe regression problem is underdetermined. In this case, we can choose\\na parameter prior that enforces sparsity, i.e., a prior that tries to set as\\nmany parameters to 0as possible ( variable selection ). This prior provides variable selection\\na stronger regularizer than the Gaussian prior, which often leads to an in-\\ncreased prediction accuracy and interpretability of the model. The Laplace\\nprior is one example that is frequently used for this purpose. A linear re-\\ngression model with the Laplace prior on the parameters is equivalent to\\nlinear regression with L1 regularization ( LASSO ) (Tibshirani, 1996). The LASSO\\nLaplace distribution is sharply peaked at zero (its ﬁrst derivative is discon-\\ntinuous) and it concentrates its probability mass closer to zero than the\\nGaussian distribution, which encourages parameters to be 0. Therefore,\\nthe nonzero parameters are relevant for the regression problem, which is\\nthe reason why we also speak of “variable selection”.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10\\nDimensionality Reduction with Principal\\nComponent Analysis\\nWorking directly with high-dimensional data, such as images, comes with A640\\x02480pixel\\ncolor image is a data\\npoint in a\\nmillion-dimensional\\nspace, where every\\npixel responds to\\nthree dimensions,\\none for each color\\nchannel (red, green,\\nblue).some difﬁculties: It is hard to analyze, interpretation is difﬁcult, visualiza-\\ntion is nearly impossible, and (from a practical point of view) storage of\\nthe data vectors can be expensive. However, high-dimensional data often\\nhas properties that we can exploit. For example, high-dimensional data is\\noften overcomplete, i.e., many dimensions are redundant and can be ex-\\nplained by a combination of other dimensions. Furthermore, dimensions\\nin high-dimensional data are often correlated so that the data possesses an\\nintrinsic lower-dimensional structure. Dimensionality reduction exploits\\nstructure and correlation and allows us to work with a more compact rep-\\nresentation of the data, ideally without losing information. We can think\\nof dimensionality reduction as a compression technique, similar to jpeg or\\nmp3, which are compression algorithms for images and music.\\nIn this chapter, we will discuss principal component analysis (PCA), an principal component\\nanalysis\\nPCAalgorithm for linear dimensionality reduction . PCA, proposed by Pearson\\ndimensionality\\nreduction(1901) and Hotelling (1933), has been around for more than 100years\\nand is still one of the most commonly used techniques for data compres-\\nsion and data visualization. It is also used for the identiﬁcation of simple\\npatterns, latent factors, and structures of high-dimensional data. In the\\nFigure 10.1\\nIllustration:\\ndimensionality\\nreduction. (a) The\\noriginal dataset\\ndoes not vary much\\nalong thex2\\ndirection. (b) The\\ndata from (a) can be\\nrepresented using\\nthex1-coordinate\\nalone with nearly no\\nloss.\\n−5.0−2.5 0.0 2.5 5.0\\nx1−4−2024x2\\n(a) Dataset with x1andx2coordinates.\\n−5.0−2.5 0.0 2.5 5.0\\nx1−4−2024x2\\n (b) Compressed dataset where only the x1coor-\\ndinate is relevant.\\n317\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n318 Dimensionality Reduction with Principal Component Analysis\\nsignal processing community, PCA is also known as the Karhunen-Lo `eve Karhunen-Lo `eve\\ntransform transform . In this chapter, we derive PCA from ﬁrst principles, drawing on\\nour understanding of basis and basis change (Sections 2.6.1 and 2.7.2),\\nprojections (Section 3.8), eigenvalues (Section 4.2), Gaussian distribu-\\ntions (Section 6.5), and constrained optimization (Section 7.2).\\nDimensionality reduction generally exploits a property of high-dimen-\\nsional data (e.g., images) that it often lies on a low-dimensional subspace.\\nFigure 10.1 gives an illustrative example in two dimensions. Although\\nthe data in Figure 10.1(a) does not quite lie on a line, the data does not\\nvary much in the x2-direction, so that we can express it as if it were on\\na line – with nearly no loss; see Figure 10.1(b). To describe the data in\\nFigure 10.1(b), only the x1-coordinate is required, and the data lies in a\\none-dimensional subspace of R2.\\n10.1 Problem Setting\\nIn PCA, we are interested in ﬁnding projections ~xnof data points xnthat\\nare as similar to the original data points as possible, but which have a sig-\\nniﬁcantly lower intrinsic dimensionality. Figure 10.1 gives an illustration\\nof what this could look like.\\nMore concretely, we consider an i.i.d. dataset X=fx1;:::;xNg,xn2\\nRD, with mean 0that possesses the data covariance matrix (6.42) data covariance\\nmatrix\\nS=1\\nNNX\\nn=1xnx>\\nn: (10.1)\\nFurthermore, we assume there exists a low-dimensional compressed rep-\\nresentation (code)\\nzn=B>xn2RM(10.2)\\nofxn, where we deﬁne the projection matrix\\nB:= [b1;:::;bM]2RD\\x02M: (10.3)\\nWe assume that the columns of Bare orthonormal (Deﬁnition 3.7) so that\\nb>\\nibj= 0if and only if i6=jandb>\\nibi= 1. We seek an M-dimensional The columns\\nb1;:::;bMofB\\nform a basis of the\\nM-dimensional\\nsubspace in which\\nthe projected data\\n~x=BB>x2RD\\nlive.subspaceU\\x12RD,dim(U) =M <D onto which we project the data. We\\ndenote the projected data by ~xn2U, and their coordinates (with respect\\nto the basis vectors b1;:::;bMofU) byzn. Our aim is to ﬁnd projections\\n~xn2RD(or equivalently the codes znand the basis vectors b1;:::;bM)\\nso that they are as similar to the original data xnand minimize the loss\\ndue to compression.\\nExample 10.1 (Coordinate Representation/Code)\\nConsider R2with the canonical basis e1= [1;0]>,e2= [0;1]>. From\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.1 Problem Setting 319\\nFigure 10.2\\nGraphical\\nillustration of PCA.\\nIn PCA, we ﬁnd a\\ncompressed version\\nzof original data x.\\nThe compressed\\ndata can be\\nreconstructed into\\n~x, which lives in the\\noriginal data space,\\nbut has an intrinsic\\nlower-dimensional\\nrepresentation than\\nx.x ~xzOriginal\\nCompressedReconstructed\\nRDRD\\nRM\\nChapter 2, we know that x2R2can be represented as a linear combina-\\ntion of these basis vectors, e.g.,\\n\\x145\\n3\\x15\\n= 5e1+ 3e2: (10.4)\\nHowever, when we consider vectors of the form\\n~x=\\x140\\nz\\x15\\n2R2; z2R; (10.5)\\nthey can always be written as 0e1+ze2. To represent these vectors it is\\nsufﬁcient to remember/store the coordinate/code zof~xwith respect to\\nthee2vector. The dimension of a\\nvector space\\ncorresponds to the\\nnumber of its basis\\nvectors (see\\nSection 2.6.1).More precisely, the set of ~xvectors (with the standard vector addition\\nand scalar multiplication) forms a vector subspace U(see Section 2.4)\\nwith dim(U) = 1 becauseU= span[e2].\\nIn Section 10.2, we will ﬁnd low-dimensional representations that re-\\ntain as much information as possible and minimize the compression loss.\\nAn alternative derivation of PCA is given in Section 10.3, where we will\\nbe looking at minimizing the squared reconstruction error kxn\\x00~xnk2be-\\ntween the original data xnand its projection ~xn.\\nFigure 10.2 illustrates the setting we consider in PCA, where zrepre-\\nsents the lower-dimensional representation of the compressed data ~xand\\nplays the role of a bottleneck, which controls how much information can\\nﬂow between xand~x. In PCA, we consider a linear relationship between\\nthe original data xand its low-dimensional code zso thatz=B>xand\\n~x=Bzfor a suitable matrix B. Based on the motivation of thinking\\nof PCA as a data compression technique, we can interpret the arrows in\\nFigure 10.2 as a pair of operations representing encoders and decoders.\\nThe linear mapping represented by Bcan be thought of as a decoder,\\nwhich maps the low-dimensional code z2RMback into the original data\\nspaceRD. Similarly,B>can be thought of an encoder, which encodes the\\noriginal dataxas a low-dimensional (compressed) code z.\\nThroughout this chapter, we will use the MNIST digits dataset as a re-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n320 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.3\\nExamples of\\nhandwritten digits\\nfrom the MNIST\\ndataset. http:\\n//yann.lecun.\\ncom/exdb/mnist/ .\\noccurring example, which contains 60;000examples of handwritten digits\\n0through 9. Each digit is a grayscale image of size 28\\x0228, i.e., it contains\\n784pixels so that we can interpret every image in this dataset as a vector\\nx2R784. Examples of these digits are shown in Figure 10.3.\\n10.2 Maximum Variance Perspective\\nFigure 10.1 gave an example of how a two-dimensional dataset can be\\nrepresented using a single coordinate. In Figure 10.1(b), we chose to ig-\\nnore thex2-coordinate of the data because it did not add too much in-\\nformation so that the compressed data is similar to the original data in\\nFigure 10.1(a). We could have chosen to ignore the x1-coordinate, but\\nthen the compressed data had been very dissimilar from the original data,\\nand much information in the data would have been lost.\\nIf we interpret information content in the data as how “space ﬁlling”\\nthe dataset is, then we can describe the information contained in the data\\nby looking at the spread of the data. From Section 6.4.1, we know that the\\nvariance is an indicator of the spread of the data, and we can derive PCA as\\na dimensionality reduction algorithm that maximizes the variance in the\\nlow-dimensional representation of the data to retain as much information\\nas possible. Figure 10.4 illustrates this.\\nConsidering the setting discussed in Section 10.1, our aim is to ﬁnd\\na matrixB(see (10.3)) that retains as much information as possible\\nwhen compressing data by projecting it onto the subspace spanned by\\nthe columnsb1;:::;bMofB. Retaining most information after data com-\\npression is equivalent to capturing the largest amount of variance in the\\nlow-dimensional code (Hotelling, 1933).\\nRemark. (Centered Data) For the data covariance matrix in (10.1), we\\nassumed centered data. We can make this assumption without loss of gen-\\nerality: Let us assume that \\x16is the mean of the data. Using the properties\\nof the variance, which we discussed in Section 6.4.4, we obtain\\nVz[z] =Vx[B>(x\\x00\\x16)] =Vx[B>x\\x00B>\\x16] =Vx[B>x];(10.6)\\ni.e., the variance of the low-dimensional code does not depend on the\\nmean of the data. Therefore, we assume without loss of generality that the\\ndata has mean 0for the remainder of this section. With this assumption\\nthe mean of the low-dimensional code is also 0sinceEz[z] =Ex[B>x] =\\nB>Ex[x] =0. }\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.2 Maximum Variance Perspective 321\\nFigure 10.4 PCA\\nﬁnds a\\nlower-dimensional\\nsubspace (line) that\\nmaintains as much\\nvariance (spread of\\nthe data) as possible\\nwhen the data\\n(blue) is projected\\nonto this subspace\\n(orange).\\n10.2.1 Direction with Maximal Variance\\nWe maximize the variance of the low-dimensional code using a sequential\\napproach. We start by seeking a single vector b12RDthat maximizes the The vectorb1will\\nbe the ﬁrst column\\nof the matrixBand\\ntherefore the ﬁrst of\\nMorthonormal\\nbasis vectors that\\nspan the\\nlower-dimensional\\nsubspace.variance of the projected data, i.e., we aim to maximize the variance of\\nthe ﬁrst coordinate z1ofz2RMso that\\nV1:=V[z1] =1\\nNNX\\nn=1z2\\n1n (10.7)\\nis maximized, where we exploited the i.i.d. assumption of the data and\\ndeﬁnedz1nas the ﬁrst coordinate of the low-dimensional representation\\nzn2RMofxn2RD. Note that ﬁrst component of znis given by\\nz1n=b>\\n1xn; (10.8)\\ni.e., it is the coordinate of the orthogonal projection of xnonto the one-\\ndimensional subspace spanned by b1(Section 3.8). We substitute (10.8)\\ninto (10.7), which yields\\nV1=1\\nNNX\\nn=1(b>\\n1xn)2=1\\nNNX\\nn=1b>\\n1xnx>\\nnb1 (10.9a)\\n=b>\\n1 \\n1\\nNNX\\nn=1xnx>\\nn!\\nb1=b>\\n1Sb1; (10.9b)\\nwhereSis the data covariance matrix deﬁned in (10.1). In (10.9a), we\\nhave used the fact that the dot product of two vectors is symmetric with\\nrespect to its arguments, that is, b>\\n1xn=x>\\nnb1.\\nNotice that arbitrarily increasing the magnitude of the vector b1in-\\ncreasesV1, that is, a vector b1that is two times longer can result in V1\\nthat is potentially four times larger. Therefore, we restrict all solutions to kb1k2= 1\\n() kb1k= 1. kb1k2= 1, which results in a constrained optimization problem in which\\nwe seek the direction along which the data varies most.\\nWith the restriction of the solution space to unit vectors the vector b1\\nthat points in the direction of maximum variance can be found by the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n322 Dimensionality Reduction with Principal Component Analysis\\nconstrained optimization problem\\nmax\\nb1b>\\n1Sb1\\nsubject tokb1k2= 1:(10.10)\\nFollowing Section 7.2, we obtain the Lagrangian\\nL(b1;\\x15) =b>\\n1Sb1+\\x151(1\\x00b>\\n1b1) (10.11)\\nto solve this constrained optimization problem. The partial derivatives of\\nLwith respect to b1and\\x151are\\n@L\\n@b1= 2b>\\n1S\\x002\\x151b>\\n1;@L\\n@\\x151= 1\\x00b>\\n1b1; (10.12)\\nrespectively. Setting these partial derivatives to 0gives us the relations\\nSb1=\\x151b1; (10.13)\\nb>\\n1b1= 1: (10.14)\\nBy comparing this with the deﬁnition of an eigenvalue decomposition\\n(Section 4.4), we see that b1is an eigenvector of the data covariance\\nmatrixS, and the Lagrange multiplier \\x151plays the role of the correspond-\\ning eigenvalue. This eigenvector property (10.13) allows us to rewrite our The quantityp\\x151is\\nalso called the\\nloading of the unit\\nvectorb1and\\nrepresents the\\nstandard deviation\\nof the data\\naccounted for by the\\nprincipal subspace\\nspan[b1].variance objective (10.10) as\\nV1=b>\\n1Sb1=\\x151b>\\n1b1=\\x151; (10.15)\\ni.e., the variance of the data projected onto a one-dimensional subspace\\nequals the eigenvalue that is associated with the basis vector b1that spans\\nthis subspace. Therefore, to maximize the variance of the low-dimensional\\ncode, we choose the basis vector associated with the largest eigenvalue\\nof the data covariance matrix. This eigenvector is called the ﬁrst principalprincipal component\\ncomponent . We can determine the effect/contribution of the principal com-\\nponentb1in the original data space by mapping the coordinate z1nback\\ninto data space, which gives us the projected data point\\n~xn=b1z1n=b1b>\\n1xn2RD(10.16)\\nin the original data space.\\nRemark. Although ~xnis aD-dimensional vector, it only requires a single\\ncoordinatez1nto represent it with respect to the basis vector b12RD.}\\n10.2.2M-dimensional Subspace with Maximal Variance\\nAssume we have found the ﬁrst m\\x001principal components as the m\\x001\\neigenvectors of Sthat are associated with the largest m\\x001eigenvalues.\\nSinceSis symmetric, the spectral theorem (Theorem 4.15) states that we\\ncan use these eigenvectors to construct an orthonormal eigenbasis of an\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.2 Maximum Variance Perspective 323\\n(m\\x001)-dimensional subspace of RD. Generally, the mth principal com-\\nponent can be found by subtracting the effect of the ﬁrst m\\x001principal\\ncomponentsb1;:::;bm\\x001from the data, thereby trying to ﬁnd principal\\ncomponents that compress the remaining information. We then arrive at\\nthe new data matrix\\n^X:=X\\x00m\\x001X\\ni=1bib>\\niX=X\\x00Bm\\x001X; (10.17)\\nwhereX= [x1;:::;xN]2RD\\x02Ncontains the data points as column The matrix ^X:=\\n[^x1;:::; ^xN]2\\nRD\\x02Nin (10.17)\\ncontains the\\ninformation in the\\ndata that has not yet\\nbeen compressed.vectors andBm\\x001:=Pm\\x001\\ni=1bib>\\niis a projection matrix that projects onto\\nthe subspace spanned by b1;:::;bm\\x001.\\nRemark (Notation) .Throughout this chapter, we do not follow the con-\\nvention of collecting data x1;:::;xNas the rows of the data matrix, but\\nwe deﬁne them to be the columns of X. This means that our data ma-\\ntrixXis aD\\x02Nmatrix instead of the conventional N\\x02Dmatrix. The\\nreason for our choice is that the algebra operations work out smoothly\\nwithout the need to either transpose the matrix or to redeﬁne vectors as\\nrow vectors that are left-multiplied onto matrices. }\\nTo ﬁnd themth principal component, we maximize the variance\\nVm=V[zm] =1\\nNNX\\nn=1z2\\nmn=1\\nNNX\\nn=1(b>\\nm^xn)2=b>\\nm^Sbm; (10.18)\\nsubject tokbmk2= 1, where we followed the same steps as in (10.9b)\\nand deﬁned ^Sas the data covariance matrix of the transformed dataset\\n^X:=f^x1;:::; ^xNg. As previously, when we looked at the ﬁrst principal\\ncomponent alone, we solve a constrained optimization problem and dis-\\ncover that the optimal solution bmis the eigenvector of ^Sthat is associated\\nwith the largest eigenvalue of ^S.\\nIt turns out that bmis also an eigenvector of S. More generally, the sets\\nof eigenvectors of Sand^Sare identical. Since both Sand^Sare sym-\\nmetric, we can ﬁnd an ONB of eigenvectors (spectral theorem 4.15), i.e.,\\nthere existDdistinct eigenvectors for both Sand^S. Next, we show that\\nevery eigenvector of Sis an eigenvector of ^S. Assume we have already\\nfound eigenvectors b1;:::;bm\\x001of^S. Consider an eigenvector biofS,\\ni.e.,Sbi=\\x15ibi. In general,\\n^Sbi=1\\nN^X^X>bi=1\\nN(X\\x00Bm\\x001X)(X\\x00Bm\\x001X)>bi(10.19a)\\n= (S\\x00SBm\\x001\\x00Bm\\x001S+Bm\\x001SBm\\x001)bi: (10.19b)\\nWe distinguish between two cases. If i>m, i.e.,biis an eigenvector\\nthat is not among the ﬁrst m\\x001principal components, then biis orthogo-\\nnal to the ﬁrst m\\x001principal components and Bm\\x001bi=0. Ifi<m , i.e.,\\nbiis among the ﬁrst m\\x001principal components, then biis a basis vector\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n324 Dimensionality Reduction with Principal Component Analysis\\nof the principal subspace onto which Bm\\x001projects. Since b1;:::;bm\\x001\\nare an ONB of this principal subspace, we obtain Bm\\x001bi=bi. The two\\ncases can be summarized as follows:\\nBm\\x001bi=biifi<m;Bm\\x001bi=0ifi>m: (10.20)\\nIn the case i>m, by using (10.20) in (10.19b), we obtain ^Sbi= (S\\x00\\nBm\\x001S)bi=Sbi=\\x15ibi;i.e.,biis also an eigenvector of ^Swith eigen-\\nvalue\\x15i. Speciﬁcally,\\n^Sbm=Sbm=\\x15mbm: (10.21)\\nEquation (10.21) reveals that bmis not only an eigenvector of Sbut also\\nof^S. Speciﬁcally, \\x15mis the largest eigenvalue of ^Sand\\x15mis themth\\nlargest eigenvalue of S, and both have the associated eigenvector bm.\\nIn the casei<m , by using (10.20) in (10.19b), we obtain\\n^Sbi= (S\\x00SBm\\x001\\x00Bm\\x001S+Bm\\x001SBm\\x001)bi=0= 0bi(10.22)\\nThis means that b1;:::;bm\\x001are also eigenvectors of ^S, but they are as-\\nsociated with eigenvalue 0so thatb1;:::;bm\\x001span the null space of ^S.\\nOverall, every eigenvector of Sis also an eigenvector of ^S. However,\\nif the eigenvectors of Sare part of the (m\\x001)dimensional principal\\nsubspace, then the associated eigenvalue of ^Sis0. This derivation\\nshows that there is\\nan intimate\\nconnection between\\ntheM-dimensional\\nsubspace with\\nmaximal variance\\nand the eigenvalue\\ndecomposition. We\\nwill revisit this\\nconnection in\\nSection 10.4.With the relation (10.21) and b>\\nmbm= 1, the variance of the data pro-\\njected onto the mth principal component is\\nVm=b>\\nmSbm(10.21)=\\x15mb>\\nmbm=\\x15m: (10.23)\\nThis means that the variance of the data, when projected onto an M-\\ndimensional subspace, equals the sum of the eigenvalues that are associ-\\nated with the corresponding eigenvectors of the data covariance matrix.\\nExample 10.2 (Eigenvalues of MNIST “8”)\\nFigure 10.5\\nProperties of the\\ntraining data of\\nMNIST “8”. (a)\\nEigenvalues sorted\\nin descending order;\\n(b) Variance\\ncaptured by the\\nprincipal\\ncomponents\\nassociated with the\\nlargest eigenvalues.\\n0 50 100 150 200\\nIndex01020304050Eigenvalue\\n(a) Eigenvalues (sorted in descending order) of\\nthe data covariance matrix of all digits “8” in\\nthe MNIST training set.\\n0 50 100 150 200\\nNumber of principal components100200300400500Captured variance(b) Variance captured by the principal compo-\\nnents.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.3 Projection Perspective 325\\nFigure 10.6\\nIllustration of the\\nprojection\\napproach: Find a\\nsubspace (line) that\\nminimizes the\\nlength of the\\ndifference vector\\nbetween projected\\n(orange) and\\noriginal (blue) data.\\nTaking all digits “8” in the MNIST training data, we compute the eigen-\\nvalues of the data covariance matrix. Figure 10.5(a) shows the 200largest\\neigenvalues of the data covariance matrix. We see that only a few of\\nthem have a value that differs signiﬁcantly from 0. Therefore, most of\\nthe variance, when projecting data onto the subspace spanned by the cor-\\nresponding eigenvectors, is captured by only a few principal components,\\nas shown in Figure 10.5(b).\\nOverall, to ﬁnd an M-dimensional subspace of RDthat retains as much\\ninformation as possible, PCA tells us to choose the columns of the matrix\\nBin (10.3) as the Meigenvectors of the data covariance matrix Sthat\\nare associated with the Mlargest eigenvalues. The maximum amount of\\nvariance PCA can capture with the ﬁrst Mprincipal components is\\nVM=MX\\nm=1\\x15m; (10.24)\\nwhere the\\x15mare theMlargest eigenvalues of the data covariance matrix\\nS. Consequently, the variance lost by data compression via PCA is\\nJM:=DX\\nj=M+1\\x15j=VD\\x00VM: (10.25)\\nInstead of these absolute quantities, we can deﬁne the relative variance\\ncaptured asVM\\nVD, and the relative variance lost by compression as 1\\x00VM\\nVD.\\n10.3 Projection Perspective\\nIn the following, we will derive PCA as an algorithm that directly mini-\\nmizes the average reconstruction error. This perspective allows us to in-\\nterpret PCA as implementing an optimal linear auto-encoder. We will draw\\nheavily from Chapters 2 and 3.\\nIn the previous section, we derived PCA by maximizing the variance\\nin the projected space to retain as much information as possible. In the\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n326 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.7\\nSimpliﬁed\\nprojection setting.\\n(a) A vectorx2R2\\n(red cross) shall be\\nprojected onto a\\none-dimensional\\nsubspaceU\\x12R2\\nspanned byb. (b)\\nshows the difference\\nvectors between x\\nand some\\ncandidates ~x.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx1−0.50.00.51.01.52.02.5x2\\nbU\\n(a) Setting.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx1−0.50.00.51.01.52.02.5x2\\nbU (b) Differences x\\x00~xifor50different ~xiare\\nshown by the red lines.\\nfollowing, we will look at the difference vectors between the original data\\nxnand their reconstruction ~xnand minimize this distance so that xnand\\n~xnare as close as possible. Figure 10.6 illustrates this setting.\\n10.3.1 Setting and Objective\\nAssume an (ordered) orthonormal basis (ONB) B= (b1;:::;bD)ofRD,\\ni.e.,b>\\nibj= 1if and only if i=jand0otherwise.\\nFrom Section 2.5 we know that for a basis (b1;:::;bD)ofRDanyx2\\nRDcan be written as a linear combination of the basis vectors of RD, i.e.,\\nVectors ~x2Ucould\\nbe vectors on a\\nplane in R3. The\\ndimensionality of\\nthe plane is 2, but\\nthe vectors still have\\nthree coordinates\\nwith respect to the\\nstandard basis of\\nR3.x=DX\\nd=1\\x10dbd=MX\\nm=1\\x10mbm+DX\\nj=M+1\\x10jbj (10.26)\\nfor suitable coordinates \\x10d2R.\\nWe are interested in ﬁnding vectors ~x2RD, which live in lower-\\ndimensional subspace U\\x12RD,dim(U) =M, so that\\n~x=MX\\nm=1zmbm2U\\x12RD(10.27)\\nis as similar to xas possible. Note that at this point we need to assume\\nthat the coordinates zmof~xand\\x10mofxare not identical.\\nIn the following, we use exactly this kind of representation of ~xto ﬁnd\\noptimal coordinates zand basis vectors b1;:::;bMsuch that ~xis as sim-\\nilar to the original data point xas possible, i.e., we aim to minimize the\\n(Euclidean) distance kx\\x00~xk. Figure 10.7 illustrates this setting.\\nWithout loss of generality, we assume that the dataset X=fx1;:::;xNg,\\nxn2RD, is centered at 0, i.e.,E[X] =0. Without the zero-mean assump-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.3 Projection Perspective 327\\ntion, we would arrive at exactly the same solution, but the notation would\\nbe substantially more cluttered.\\nWe are interested in ﬁnding the best linear projection of Xonto a lower-\\ndimensional subspace UofRDwith dim(U) =Mand orthonormal basis\\nvectorsb1;:::;bM. We will call this subspace Utheprincipal subspace .principal subspace\\nThe projections of the data points are denoted by\\n~xn:=MX\\nm=1zmnbm=Bzn2RD; (10.28)\\nwherezn:= [z1n;:::;zMn]>2RMis the coordinate vector of ~xnwith\\nrespect to the basis (b1;:::;bM). More speciﬁcally, we are interested in\\nhaving the ~xnas similar toxnas possible.\\nThe similarity measure we use in the following is the squared distance\\n(Euclidean norm) kx\\x00~xk2betweenxand~x. We therefore deﬁne our ob-\\njective as minimizing the average squared Euclidean distance ( reconstruction reconstruction error\\nerror ) (Pearson, 1901)\\nJM:=1\\nNNX\\nn=1kxn\\x00~xnk2; (10.29)\\nwhere we make it explicit that the dimension of the subspace onto which\\nwe project the data is M. In order to ﬁnd this optimal linear projection,\\nwe need to ﬁnd the orthonormal basis of the principal subspace and the\\ncoordinateszn2RMof the projections with respect to this basis.\\nTo ﬁnd the coordinates znand the ONB of the principal subspace, we\\nfollow a two-step approach. First, we optimize the coordinates znfor a\\ngiven ONB (b1;:::;bM); second, we ﬁnd the optimal ONB.\\n10.3.2 Finding Optimal Coordinates\\nLet us start by ﬁnding the optimal coordinates z1n;:::;zMnof the projec-\\ntions ~xnforn= 1;:::;N . Consider Figure 10.7(b), where the principal\\nsubspace is spanned by a single vector b. Geometrically speaking, ﬁnding\\nthe optimal coordinates zcorresponds to ﬁnding the representation of the\\nlinear projection ~xwith respect to bthat minimizes the distance between\\n~x\\x00x. From Figure 10.7(b), it is clear that this will be the orthogonal\\nprojection, and in the following we will show exactly this.\\nWe assume an ONB (b1;:::;bM)ofU\\x12RD. To ﬁnd the optimal co-\\nordinateszmwith respect to this basis, we require the partial derivatives\\n@JM\\n@zin=@JM\\n@~xn@~xn\\n@zin; (10.30a)\\n@JM\\n@~xn=\\x002\\nN(xn\\x00~xn)>2R1\\x02D; (10.30b)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n328 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.8\\nOptimal projection\\nof a vectorx2R2\\nonto a\\none-dimensional\\nsubspace\\n(continuation from\\nFigure 10.7).\\n(a) Distances\\nkx\\x00~xkfor some\\n~x2U.\\n(b) Orthogonal\\nprojection and\\noptimal coordinates.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx11.251.501.752.002.252.502.753.003.25/bardblx−˜x/bardbl\\n(a) Distanceskx\\x00~xkfor some ~x=z1b2\\nU= span[b]; see panel (b) for the setting.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx1−0.50.00.51.01.52.02.5x2\\nbU\\n˜x(b) The vector ~xthat minimizes the distance\\nin panel (a) is its orthogonal projection onto\\nU. The coordinate of the projection ~xwith\\nrespect to the basis vector bthat spansU\\nis the factor we need to scale bin order to\\n“reach” ~x.\\n@~xn\\n@zin(10.28)=@\\n@zin MX\\nm=1zmnbm!\\n=bi (10.30c)\\nfori= 1;:::;M , such that we obtain\\n@JM\\n@zin(10.30b)\\n(10.30c)=\\x002\\nN(xn\\x00~xn)>bi(10.28)=\\x002\\nN \\nxn\\x00MX\\nm=1zmnbm!>\\nbi\\n(10.31a)\\nONB=\\x002\\nN(x>\\nnbi\\x00zinb>\\nibi) =\\x002\\nN(x>\\nnbi\\x00zin): (10.31b)\\nsinceb>\\nibi= 1. Setting this partial derivative to 0yields immediately the The coordinates of\\nthe optimal\\nprojection ofxn\\nwith respect to the\\nbasis vectors\\nb1;:::;bMare the\\ncoordinates of the\\northogonal\\nprojection ofxn\\nonto the principal\\nsubspace.optimal coordinates\\nzin=x>\\nnbi=b>\\nixn (10.32)\\nfori= 1;:::;M andn= 1;:::;N . This means that the optimal co-\\nordinateszinof the projection ~xnare the coordinates of the orthogonal\\nprojection (see Section 3.8) of the original data point xnonto the one-\\ndimensional subspace that is spanned by bi. Consequently:\\nThe optimal linear projection ~xnofxnis an orthogonal projection.\\nThe coordinates of ~xnwith respect to the basis (b1;:::;bM)are the\\ncoordinates of the orthogonal projection of xnonto the principal sub-\\nspace.\\nAn orthogonal projection is the best linear mapping given the objec-\\ntive (10.29).\\nThe coordinates \\x10mofxin (10.26) and the coordinates zmof~xin (10.27)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.3 Projection Perspective 329\\nmust be identical for m= 1;:::;M sinceU?= span[bM+1;:::;bD]is\\nthe orthogonal complement (see Section 3.6) of U= span[b1;:::;bM].\\nRemark (Orthogonal Projections with Orthonormal Basis Vectors) .Let us\\nbrieﬂy recap orthogonal projections from Section 3.8. If (b1;:::;bD)is an\\northonormal basis of RDthen b>\\njxis the\\ncoordinate of the\\northogonal\\nprojection ofxonto\\nthe subspace\\nspanned bybj.~x=bj(b>\\njbj)\\x001b>\\njx=bjb>\\njx2RD(10.33)\\nis the orthogonal projection of xonto the subspace spanned by the jth ba-\\nsis vector, and zj=b>\\njxis the coordinate of this projection with respect to\\nthe basis vector bjthat spans that subspace since zjbj=~x. Figure 10.8(b)\\nillustrates this setting.\\nMore generally, if we aim to project onto an M-dimensional subspace\\nofRD, we obtain the orthogonal projection of xonto theM-dimensional\\nsubspace with orthonormal basis vectors b1;:::;bMas\\n~x=B(B>B|{z}\\n=I)\\x001B>x=BB>x; (10.34)\\nwhere we deﬁned B:= [b1;:::;bM]2RD\\x02M. The coordinates of this\\nprojection with respect to the ordered basis (b1;:::;bM)arez:=B>x\\nas discussed in Section 3.8.\\nWe can think of the coordinates as a representation of the projected\\nvector in a new coordinate system deﬁned by (b1;:::;bM). Note that al-\\nthough ~x2RD, we only need Mcoordinates z1;:::;zMto represent\\nthis vector; the other D\\x00Mcoordinates with respect to the basis vectors\\n(bM+1;:::;bD)are always 0. }\\nSo far we have shown that for a given ONB we can ﬁnd the optimal\\ncoordinates of ~xby an orthogonal projection onto the principal subspace.\\nIn the following, we will determine what the best basis is.\\n10.3.3 Finding the Basis of the Principal Subspace\\nTo determine the basis vectors b1;:::;bMof the principal subspace, we\\nrephrase the loss function (10.29) using the results we have so far. This\\nwill make it easier to ﬁnd the basis vectors. To reformulate the loss func-\\ntion, we exploit our results from before and obtain\\n~xn=MX\\nm=1zmnbm(10.32)=MX\\nm=1(x>\\nnbm)bm: (10.35)\\nWe now exploit the symmetry of the dot product, which yields\\n~xn= MX\\nm=1bmb>\\nm!\\nxn: (10.36)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n330 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.9\\nOrthogonal\\nprojection and\\ndisplacement\\nvectors. When\\nprojecting data\\npointsxn(blue)\\nonto subspace U1,\\nwe obtain ~xn\\n(orange). The\\ndisplacement vector\\n~xn\\x00xnlies\\ncompletely in the\\northogonal\\ncomplement U2of\\nU1.\\n−5 0 5\\nx1−6−4−20246x2\\nUU⊥\\nSince we can generally write the original data point xnas a linear combi-\\nnation of all basis vectors, it holds that\\nxn=DX\\nd=1zdnbd(10.32)=DX\\nd=1(x>\\nnbd)bd= DX\\nd=1bdb>\\nd!\\nxn (10.37a)\\n= MX\\nm=1bmb>\\nm!\\nxn+ DX\\nj=M+1bjb>\\nj!\\nxn; (10.37b)\\nwhere we split the sum with Dterms into a sum over Mand a sum\\noverD\\x00Mterms. With this result, we ﬁnd that the displacement vector\\nxn\\x00~xn, i.e., the difference vector between the original data point and its\\nprojection, is\\nxn\\x00~xn= DX\\nj=M+1bjb>\\nj!\\nxn (10.38a)\\n=DX\\nj=M+1(x>\\nnbj)bj: (10.38b)\\nThis means the difference is exactly the projection of the data point onto\\nthe orthogonal complement of the principal subspace: We identify the ma-\\ntrixPD\\nj=M+1bjb>\\njin (10.38a) as the projection matrix that performs this\\nprojection. Hence the displacement vector xn\\x00~xnlies in the subspace\\nthat is orthogonal to the principal subspace as illustrated in Figure 10.9.\\nRemark (Low-Rank Approximation) .In (10.38a), we saw that the projec-\\ntion matrix, which projects xonto ~x, is given by\\nMX\\nm=1bmb>\\nm=BB>: (10.39)\\nBy construction as a sum of rank-one matrices bmb>\\nmwe see thatBB>is\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.3 Projection Perspective 331\\nsymmetric and has rank M. Therefore, the average squared reconstruction\\nerror can also be written as\\n1\\nNNX\\nn=1kxn\\x00~xnk2=1\\nNNX\\nn=1\\r\\r\\rxn\\x00BB>xn\\r\\r\\r2\\n(10.40a)\\n=1\\nNNX\\nn=1\\r\\r\\r(I\\x00BB>)xn\\r\\r\\r2\\n: (10.40b)\\nFinding orthonormal basis vectors b1;:::;bM, which minimize the differ- PCA ﬁnds the best\\nrank-M\\napproximation of\\nthe identity matrix.ence between the original data xnand their projections ~xn, is equivalent\\nto ﬁnding the best rank- Mapproximation BB>of the identity matrix I\\n(see Section 4.6). }\\nNow we have all the tools to reformulate the loss function (10.29).\\nJM=1\\nNNX\\nn=1kxn\\x00~xnk2(10.38b)=1\\nNNX\\nn=1\\r\\r\\r\\r\\rDX\\nj=M+1(b>\\njxn)bj\\r\\r\\r\\r\\r2\\n:(10.41)\\nWe now explicitly compute the squared norm and exploit the fact that the\\nbjform an ONB, which yields\\nJM=1\\nNNX\\nn=1DX\\nj=M+1(b>\\njxn)2=1\\nNNX\\nn=1DX\\nj=M+1b>\\njxnb>\\njxn (10.42a)\\n=1\\nNNX\\nn=1DX\\nj=M+1b>\\njxnx>\\nnbj; (10.42b)\\nwhere we exploited the symmetry of the dot product in the last step to\\nwriteb>\\njxn=x>\\nnbj. We now swap the sums and obtain\\nJM=DX\\nj=M+1b>\\nj \\n1\\nNNX\\nn=1xnx>\\nn!\\n|{z}\\n=:Sbj=DX\\nj=M+1b>\\njSbj (10.43a)\\n=DX\\nj=M+1tr(b>\\njSbj) =DX\\nj=M+1tr(Sbjb>\\nj) =tr\\x10\\x10DX\\nj=M+1bjb>\\nj\\x11\\n|{z}\\nprojection matrixS\\x11\\n;\\n(10.43b)\\nwhere we exploited the property that the trace operator tr (\\x01)(see (4.18))\\nis linear and invariant to cyclic permutations of its arguments. Since we\\nassumed that our dataset is centered, i.e., E[X] =0, we identifySas the\\ndata covariance matrix. Since the projection matrix in (10.43b) is con-\\nstructed as a sum of rank-one matrices bjb>\\njit itself is of rank D\\x00M.\\nEquation (10.43a) implies that we can formulate the average squared\\nreconstruction error equivalently as the covariance matrix of the data,\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n332 Dimensionality Reduction with Principal Component Analysis\\nprojected onto the orthogonal complement of the principal subspace. Min-\\nimizing the average squared reconstruction error is therefore equivalent to Minimizing the\\naverage squared\\nreconstruction error\\nis equivalent to\\nminimizing the\\nprojection of the\\ndata covariance\\nmatrix onto the\\northogonal\\ncomplement of the\\nprincipal subspace.minimizing the variance of the data when projected onto the subspace we\\nignore, i.e., the orthogonal complement of the principal subspace. Equiva-\\nlently, we maximize the variance of the projection that we retain in the\\nprincipal subspace, which links the projection loss immediately to the\\nmaximum-variance formulation of PCA discussed in Section 10.2. But this\\nthen also means that we will obtain the same solution that we obtained\\nMinimizing the\\naverage squared\\nreconstruction error\\nis equivalent to\\nmaximizing the\\nvariance of the\\nprojected data.for the maximum-variance perspective. Therefore, we omit a derivation\\nthat is identical to the one presented in Section 10.2 and summarize the\\nresults from earlier in the light of the projection perspective.\\nThe average squared reconstruction error, when projecting onto the M-\\ndimensional principal subspace, is\\nJM=DX\\nj=M+1\\x15j; (10.44)\\nwhere\\x15jare the eigenvalues of the data covariance matrix. Therefore,\\nto minimize (10.44) we need to select the smallest D\\x00Meigenvalues,\\nwhich then implies that their corresponding eigenvectors are the basis of\\nthe orthogonal complement of the principal subspace. Consequently, this\\nmeans that the basis of the principal subspace comprises the eigenvectors\\nb1;:::;bMthat are associated with the largest Meigenvalues of the data\\ncovariance matrix.\\nExample 10.3 (MNIST Digits Embedding)\\nFigure 10.10\\nEmbedding of\\nMNIST digits 0\\n(blue) and 1\\n(orange) in a\\ntwo-dimensional\\nprincipal subspace\\nusing PCA. Four\\nembeddings of the\\ndigits “0” and “1” in\\nthe principal\\nsubspace are\\nhighlighted in red\\nwith their\\ncorresponding\\noriginal digit.\\nFigure 10.10 visualizes the training data of the MMIST digits “0” and “1”\\nembedded in the vector subspace spanned by the ﬁrst two principal com-\\nponents. We observe a relatively clear separation between “0”s (blue dots)\\nand “1”s (orange dots), and we see the variation within each individual\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.4 Eigenvector Computation and Low-Rank Approximations 333\\ncluster. Four embeddings of the digits “0” and “1” in the principal subspace\\nare highlighted in red with their corresponding original digit. The ﬁgure\\nreveals that the variation within the set of “0” is signiﬁcantly greater than\\nthe variation within the set of “1”.\\n10.4 Eigenvector Computation and Low-Rank Approximations\\nIn the previous sections, we obtained the basis of the principal subspace\\nas the eigenvectors that are associated with the largest eigenvalues of the\\ndata covariance matrix\\nS=1\\nNNX\\nn=1xnx>\\nn=1\\nNXX>; (10.45)\\nX= [x1;:::;xN]2RD\\x02N: (10.46)\\nNote thatXis aD\\x02Nmatrix, i.e., it is the transpose of the “typical”\\ndata matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and\\nthe corresponding eigenvectors) of S, we can follow two approaches: Use\\neigendecomposition\\nor SVD to compute\\neigenvectors.We perform an eigendecomposition (see Section 4.2) and compute the\\neigenvalues and eigenvectors of Sdirectly.\\nWe use a singular value decomposition (see Section 4.5). Since Sis\\nsymmetric and factorizes into XX>(ignoring the factor1\\nN), the eigen-\\nvalues ofSare the squared singular values of X.\\nMore speciﬁcally, the SVD of Xis given by\\nX|{z}\\nD\\x02N=U|{z}\\nD\\x02D\\x06|{z}\\nD\\x02NV>\\n|{z}\\nN\\x02N; (10.47)\\nwhereU2RD\\x02DandV>2RN\\x02Nare orthogonal matrices and \\x062\\nRD\\x02Nis a matrix whose only nonzero entries are the singular values \\x1bii>\\n0. It then follows that\\nS=1\\nNXX>=1\\nNU\\x06V>V|{z}\\n=IN\\x06>U>=1\\nNU\\x06\\x06>U>: (10.48)\\nWith the results from Section 4.5, we get that the columns of Uare the The columns of U\\nare the eigenvectors\\nofS.eigenvectors of XX>(and therefore S). Furthermore, the eigenvalues\\n\\x15dofSare related to the singular values of Xvia\\n\\x15d=\\x1b2\\nd\\nN: (10.49)\\nThis relationship between the eigenvalues of Sand the singular values\\nofXprovides the connection between the maximum variance view (Sec-\\ntion 10.2) and the singular value decomposition.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n334 Dimensionality Reduction with Principal Component Analysis\\n10.4.1 PCA Using Low-Rank Matrix Approximations\\nTo maximize the variance of the projected data (or minimize the average\\nsquared reconstruction error), PCA chooses the columns of Uin (10.48)\\nto be the eigenvectors that are associated with the Mlargest eigenvalues\\nof the data covariance matrix Sso that we identify Uas the projection ma-\\ntrixBin (10.3), which projects the original data onto a lower-dimensional\\nsubspace of dimension M. The Eckart-Young theorem (Theorem 4.25 in Eckart-Young\\ntheorem Section 4.6) offers a direct way to estimate the low-dimensional represen-\\ntation. Consider the best rank- Mapproximation\\n~XM:= argminrk(A)6MkX\\x00Ak22RD\\x02N(10.50)\\nofX, wherek\\x01k2is the spectral norm deﬁned in (4.93). The Eckart-Young\\ntheorem states that ~XMis given by truncating the SVD at the top- M\\nsingular value. In other words, we obtain\\n~XM=UM|{z}\\nD\\x02M\\x06M|{z}\\nM\\x02MV>\\nM|{z}\\nM\\x02N2RD\\x02N(10.51)\\nwith orthogonal matrices UM:= [u1;:::;uM]2RD\\x02MandVM:=\\n[v1;:::;vM]2RN\\x02Mand a diagonal matrix \\x06M2RM\\x02Mwhose diago-\\nnal entries are the Mlargest singular values of X.\\n10.4.2 Practical Aspects\\nFinding eigenvalues and eigenvectors is also important in other funda-\\nmental machine learning methods that require matrix decompositions. In\\ntheory, as we discussed in Section 4.2, we can solve for the eigenvalues as\\nroots of the characteristic polynomial. However, for matrices larger than\\n4\\x024this is not possible because we would need to ﬁnd the roots of a poly-\\nnomial of degree 5or higher. However, the Abel-Rufﬁni theorem (Rufﬁni, Abel-Rufﬁni\\ntheorem 1799; Abel, 1826) states that there exists no algebraic solution to this\\nproblem for polynomials of degree 5or more. Therefore, in practice, wenp.linalg.eigh\\nor\\nnp.linalg.svdsolve for eigenvalues or singular values using iterative methods, which are\\nimplemented in all modern packages for linear algebra.\\nIn many applications (such as PCA presented in this chapter), we only\\nrequire a few eigenvectors. It would be wasteful to compute the full de-\\ncomposition, and then discard all eigenvectors with eigenvalues that are\\nbeyond the ﬁrst few. It turns out that if we are interested in only the ﬁrst\\nfew eigenvectors (with the largest eigenvalues), then iterative processes,\\nwhich directly optimize these eigenvectors, are computationally more efﬁ-\\ncient than a full eigendecomposition (or SVD). In the extreme case of only\\nneeding the ﬁrst eigenvector, a simple method called the power iteration power iteration\\nis very efﬁcient. Power iteration chooses a random vector x0that is not in\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.5 PCA in High Dimensions 335\\nthe null space of Sand follows the iteration\\nxk+1=Sxk\\nkSxkk; k = 0;1;::: : (10.52)\\nThis means the vector xkis multiplied by Sin every iteration and then IfSis invertible, it\\nis sufﬁcient to\\nensure thatx06=0.normalized, i.e., we always have kxkk= 1. This sequence of vectors con-\\nverges to the eigenvector associated with the largest eigenvalue of S. The\\noriginal Google PageRank algorithm (Page et al., 1999) uses such an al-\\ngorithm for ranking web pages based on their hyperlinks.\\n10.5 PCA in High Dimensions\\nIn order to do PCA, we need to compute the data covariance matrix. In D\\ndimensions, the data covariance matrix is a D\\x02Dmatrix. Computing the\\neigenvalues and eigenvectors of this matrix is computationally expensive\\nas it scales cubically in D. Therefore, PCA, as we discussed earlier, will be\\ninfeasible in very high dimensions. For example, if our xnare images with\\n10;000pixels (e.g., 100\\x02100pixel images), we would need to compute\\nthe eigendecomposition of a 10;000\\x0210;000covariance matrix. In the\\nfollowing, we provide a solution to this problem for the case that we have\\nsubstantially fewer data points than dimensions, i.e., N\\x1cD.\\nAssume we have a centered dataset x1;:::;xN,xn2RD. Then the\\ndata covariance matrix is given as\\nS=1\\nNXX>2RD\\x02D; (10.53)\\nwhereX= [x1;:::;xN]is aD\\x02Nmatrix whose columns are the data\\npoints.\\nWe now assume that N\\x1cD, i.e., the number of data points is smaller\\nthan the dimensionality of the data. If there are no duplicate data points,\\nthe rank of the covariance matrix SisN, so it hasD\\x00N+1many eigen-\\nvalues that are 0. Intuitively, this means that there are some redundancies.\\nIn the following, we will exploit this and turn the D\\x02Dcovariance matrix\\ninto anN\\x02Ncovariance matrix whose eigenvalues are all positive.\\nIn PCA, we ended up with the eigenvector equation\\nSbm=\\x15mbm; m = 1;:::;M; (10.54)\\nwherebmis a basis vector of the principal subspace. Let us rewrite this\\nequation a bit: With Sdeﬁned in (10.53), we obtain\\nSbm=1\\nNXX>bm=\\x15mbm: (10.55)\\nWe now multiply X>2RN\\x02Dfrom the left-hand side, which yields\\n1\\nNX>X|{z}\\nN\\x02NX>bm|{z}\\n=:cm=\\x15mX>bm()1\\nNX>Xcm=\\x15mcm;(10.56)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n336 Dimensionality Reduction with Principal Component Analysis\\nand we get a new eigenvector/eigenvalue equation: \\x15mremains eigen-\\nvalue, which conﬁrms our results from Section 4.5.3 that the nonzero\\neigenvalues of XX>equal the nonzero eigenvalues of X>X. We obtain\\nthe eigenvector of the matrix1\\nNX>X2RN\\x02Nassociated with \\x15mas\\ncm:=X>bm. Assuming we have no duplicate data points, this matrix\\nhas rankNand is invertible. This also implies that1\\nNX>Xhas the same\\n(nonzero) eigenvalues as the data covariance matrix S. But this is now an\\nN\\x02Nmatrix, so that we can compute the eigenvalues and eigenvectors\\nmuch more efﬁciently than for the original D\\x02Ddata covariance matrix.\\nNow that we have the eigenvectors of1\\nNX>X, we are going to re-\\ncover the original eigenvectors, which we still need for PCA. Currently,\\nwe know the eigenvectors of1\\nNX>X. If we left-multiply our eigenvalue/\\neigenvector equation with X, we get\\n1\\nNXX>\\n|{z}\\nSXcm=\\x15mXcm (10.57)\\nand we recover the data covariance matrix again. This now also means\\nthat we recover Xcmas an eigenvector of S.\\nRemark. If we want to apply the PCA algorithm that we discussed in Sec-\\ntion 10.6, we need to normalize the eigenvectors XcmofSso that they\\nhave norm 1. }\\n10.6 Key Steps of PCA in Practice\\nIn the following, we will go through the individual steps of PCA using a\\nrunning example, which is summarized in Figure 10.11. We are given a\\ntwo-dimensional dataset (Figure 10.11(a)), and we want to use PCA to\\nproject it onto a one-dimensional subspace.\\n1.Mean subtraction We start by centering the data by computing the\\nmean\\x16of the dataset and subtracting it from every single data point.\\nThis ensures that the dataset has mean 0(Figure 10.11(b)). Mean sub-\\ntraction is not strictly necessary but reduces the risk of numerical prob-\\nlems.\\n2.Standardization Divide the data points by the standard deviation \\x1bd\\nof the dataset for every dimension d= 1;:::;D . Now the data is unit\\nfree, and it has variance 1along each axis, which is indicated by the\\ntwo arrows in Figure 10.11(c). This step completes the standardization standardization\\nof the data.\\n3.Eigendecomposition of the covariance matrix Compute the data\\ncovariance matrix and its eigenvalues and corresponding eigenvectors.\\nSince the covariance matrix is symmetric, the spectral theorem (The-\\norem 4.15) states that we can ﬁnd an ONB of eigenvectors. In Fig-\\nure 10.11(d), the eigenvectors are scaled by the magnitude of the cor-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.6 Key Steps of PCA in Practice 337\\nFigure 10.11 Steps\\nof PCA. (a) Original\\ndataset;\\n(b) centering;\\n(c) divide by\\nstandard deviation;\\n(d) eigendecomposi-\\ntion; (e) projection;\\n(f) mapping back to\\noriginal data space.\\n0 5\\nx1−2.50.02.55.0x2\\n(a) Original dataset.\\n0 5\\nx1−2.50.02.55.0x2\\n (b) Step 1: Centering by sub-\\ntracting the mean from each\\ndata point.\\n0 5\\nx1−2.50.02.55.0x2\\n(c) Step 2: Dividing by the\\nstandard deviation to make\\nthe data unit free. Data has\\nvariance 1along each axis.\\n0 5\\nx1−2.50.02.55.0x2\\n(d) Step 3: Compute eigenval-\\nues and eigenvectors (arrows)\\nof the data covariance matrix\\n(ellipse).\\n0 5\\nx1−2.50.02.55.0x2\\n(e) Step 4: Project data onto\\nthe principal subspace.\\n0 5\\nx1−2.50.02.55.0x2\\n(f) Undo the standardization\\nand move projected data back\\ninto the original data space\\nfrom (a).\\nresponding eigenvalue. The longer vector spans the principal subspace,\\nwhich we denote by U. The data covariance matrix is represented by\\nthe ellipse.\\n4.Projection We can project any data point x\\x032RDonto the principal\\nsubspace: To get this right, we need to standardize x\\x03using the mean\\n\\x16dand standard deviation \\x1bdof the training data in the dth dimension,\\nrespectively, so that\\nx(d)\\n\\x03 x(d)\\n\\x03\\x00\\x16d\\n\\x1bd; d = 1;:::;D; (10.58)\\nwherex(d)\\n\\x03is thedth component of x\\x03. We obtain the projection as\\n~x\\x03=BB>x\\x03 (10.59)\\nwith coordinates\\nz\\x03=B>x\\x03 (10.60)\\nwith respect to the basis of the principal subspace. Here, Bis the ma-\\ntrix that contains the eigenvectors that are associated with the largest\\neigenvalues of the data covariance matrix as columns. PCA returns the\\ncoordinates (10.60), not the projections x\\x03.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n338 Dimensionality Reduction with Principal Component Analysis\\nHaving standardized our dataset, (10.59) only yields the projections in\\nthe context of the standardized dataset. To obtain our projection in the\\noriginal data space (i.e., before standardization), we need to undo the\\nstandardization (10.58) and multiply by the standard deviation before\\nadding the mean so that we obtain\\n~x(d)\\n\\x03 ~x(d)\\n\\x03\\x1bd+\\x16d; d = 1;:::;D: (10.61)\\nFigure 10.11(f) illustrates the projection in the original data space.\\nExample 10.4 (MNIST Digits: Reconstruction)\\nIn the following, we will apply PCA to the MNIST digits dataset, which\\ncontains 60;000examples of handwritten digits 0through 9. Each digit is\\nan image of size 28\\x0228, i.e., it contains 784pixels so that we can interpret\\nevery image in this dataset as a vector x2R784. Examples of these digits\\nare shown in Figure 10.3.\\nFigure 10.12 Effect\\nof increasing the\\nnumber of principal\\ncomponents on\\nreconstruction.\\nOriginal\\nPCs: 1\\nPCs: 10\\nPCs: 100\\nPCs: 500\\nFor illustration purposes, we apply PCA to a subset of the MNIST digits,\\nand we focus on the digit “8”. We used 5,389 training images of the digit\\n“8” and determined the principal subspace as detailed in this chapter. We\\nthen used the learned projection matrix to reconstruct a set of test im-\\nages, which is illustrated in Figure 10.12. The ﬁrst row of Figure 10.12\\nshows a set of four original digits from the test set. The following rows\\nshow reconstructions of exactly these digits when using a principal sub-\\nspace of dimensions 1,10,100, and 500, respectively. We see that even\\nwith a single-dimensional principal subspace we get a halfway decent re-\\nconstruction of the original digits, which, however, is blurry and generic.\\nWith an increasing number of principal components (PCs), the reconstruc-\\ntions become sharper and more details are accounted for. With 500prin-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.7 Latent Variable Perspective 339\\ncipal components, we effectively obtain a near-perfect reconstruction. If\\nwe were to choose 784PCs, we would recover the exact digit without any\\ncompression loss.\\nFigure 10.13 shows the average squared reconstruction error, which is\\n1\\nNNX\\nn=1kxn\\x00~xnk2=DX\\ni=M+1\\x15i; (10.62)\\nas a function of the number Mof principal components. We can see that\\nthe importance of the principal components drops off rapidly, and only\\nmarginal gains can be achieved by adding more PCs. This matches exactly\\nour observation in Figure 10.5, where we discovered that the most of the\\nvariance of the projected data is captured by only a few principal compo-\\nnents. With about 550PCs, we can essentially fully reconstruct the training\\ndata that contains the digit “8” (some pixels around the boundaries show\\nno variation across the dataset as they are always black).\\nFigure 10.13\\nAverage squared\\nreconstruction error\\nas a function of the\\nnumber of principal\\ncomponents. The\\naverage squared\\nreconstruction error\\nis the sum of the\\neigenvalues in the\\northogonal\\ncomplement of the\\nprincipal subspace.\\n0 200 400 600 800\\nNumber of PCs0100200300400500Average squared reconstruction error\\n10.7 Latent Variable Perspective\\nIn the previous sections, we derived PCA without any notion of a prob-\\nabilistic model using the maximum-variance and the projection perspec-\\ntives. On the one hand, this approach may be appealing as it allows us to\\nsidestep all the mathematical difﬁculties that come with probability the-\\nory, but on the other hand, a probabilistic model would offer us more ﬂex-\\nibility and useful insights. More speciﬁcally, a probabilistic model would\\nCome with a likelihood function, and we can explicitly deal with noisy\\nobservations (which we did not even discuss earlier)\\nAllow us to do Bayesian model comparison via the marginal likelihood\\nas discussed in Section 8.6\\nView PCA as a generative model, which allows us to simulate new data\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n340 Dimensionality Reduction with Principal Component Analysis\\nAllow us to make straightforward connections to related algorithms\\nDeal with data dimensions that are missing at random by applying\\nBayes’ theorem\\nGive us a notion of the novelty of a new data point\\nGive us a principled way to extend the model, e.g., to a mixture of PCA\\nmodels\\nHave the PCA we derived in earlier sections as a special case\\nAllow for a fully Bayesian treatment by marginalizing out the model\\nparameters\\nBy introducing a continuous-valued latent variable z2RMit is possible\\nto phrase PCA as a probabilistic latent-variable model. Tipping and Bishop\\n(1999) proposed this latent-variable model as probabilistic PCA (PPCA ). probabilistic PCA\\nPPCA PPCA addresses most of the aforementioned issues, and the PCA solution\\nthat we obtained by maximizing the variance in the projected space or\\nby minimizing the reconstruction error is obtained as the special case of\\nmaximum likelihood estimation in a noise-free setting.\\n10.7.1 Generative Process and Probabilistic Model\\nIn PPCA, we explicitly write down the probabilistic model for linear di-\\nmensionality reduction. For this we assume a continuous latent variable\\nz2RMwith a standard-normal prior p(z) =N\\x000;I\\x01\\nand a linear rela-\\ntionship between the latent variables and the observed xdata where\\nx=Bz+\\x16+\\x0f2RD; (10.63)\\nwhere\\x0f\\x18 N\\x000; \\x1b2I\\x01\\nis Gaussian observation noise and B2RD\\x02M\\nand\\x162RDdescribe the linear/afﬁne mapping from latent to observed\\nvariables. Therefore, PPCA links latent and observed variables via\\np(xjz;B;\\x16;\\x1b2) =N\\x00xjBz+\\x16; \\x1b2I\\x01: (10.64)\\nOverall, PPCA induces the following generative process:\\nzn\\x18N\\x00zj0;I\\x01\\n(10.65)\\nxnjzn\\x18N\\x00xjBzn+\\x16; \\x1b2I\\x01\\n(10.66)\\nTo generate a data point that is typical given the model parameters, we\\nfollow an ancestral sampling scheme: We ﬁrst sample a latent variable zn ancestral sampling\\nfromp(z). Then we use znin (10.64) to sample a data point conditioned\\non the sampled zn, i.e.,xn\\x18p(xjzn;B;\\x16;\\x1b2).\\nThis generative process allows us to write down the probabilistic model\\n(i.e., the joint distribution of all random variables; see Section 8.4) as\\np(x;zjB;\\x16;\\x1b2) =p(xjz;B;\\x16;\\x1b2)p(z); (10.67)\\nwhich immediately gives rise to the graphical model in Figure 10.14 using\\nthe results from Section 8.5.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.7 Latent Variable Perspective 341\\nFigure 10.14\\nGraphical model for\\nprobabilistic PCA.\\nThe observations xn\\nexplicitly depend on\\ncorresponding\\nlatent variables\\nzn\\x18N\\x00\\n0;I\\x01\\n. The\\nmodel parameters\\nB;\\x16and the\\nlikelihood\\nparameter\\x1bare\\nshared across the\\ndataset.xnBzn\\n\\x1b\\x16\\nn= 1;:::;N\\nRemark. Note the direction of the arrow that connects the latent variables\\nzand the observed data x: The arrow points from ztox, which means\\nthat the PPCA model assumes a lower-dimensional latent cause zfor high-\\ndimensional observations x. In the end, we are obviously interested in\\nﬁnding something out about zgiven some observations. To get there we\\nwill apply Bayesian inference to “invert” the arrow implicitly and go from\\nobservations to latent variables. }\\nExample 10.5 (Generating New Data Using Latent Variables)\\nFigure 10.15\\nGenerating new\\nMNIST digits. The\\nlatent variables z\\ncan be used to\\ngenerate new data\\n~x=Bz. The closer\\nwe stay to the\\ntraining data, the\\nmore realistic the\\ngenerated data.\\nFigure 10.15 shows the latent coordinates of the MNIST digits “8” found\\nby PCA when using a two-dimensional principal subspace (blue dots). We\\ncan query any vector z\\x03in this latent space and generate an image ~x\\x03=\\nBz\\x03that resembles the digit “8”. We show eight of such generated images\\nwith their corresponding latent space representation. Depending on where\\nwe query the latent space, the generated images look different (shape,\\nrotation, size, etc.). If we query away from the training data, we see more\\nand more artifacts, e.g., the top-left and top-right digits. Note that the\\nintrinsic dimensionality of these generated images is only two.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n342 Dimensionality Reduction with Principal Component Analysis\\n10.7.2 Likelihood and Joint DistributionThe likelihood does\\nnot depend on the\\nlatent variables z.Using the results from Chapter 6, we obtain the likelihood of this proba-\\nbilistic model by integrating out the latent variable z(see Section 8.4.3)\\nso that\\np(xjB;\\x16;\\x1b2) =Z\\np(xjz;B;\\x16;\\x1b2)p(z)dz (10.68a)\\n=Z\\nN\\x00xjBz+\\x16; \\x1b2I\\x01N\\x00zj0;I\\x01dz:(10.68b)\\nFrom Section 6.5, we know that the solution to this integral is a Gaussian\\ndistribution with mean\\nEx[x] =Ez[Bz+\\x16] +E\\x0f[\\x0f] =\\x16 (10.69)\\nand with covariance matrix\\nV[x] =Vz[Bz+\\x16] +V\\x0f[\\x0f] =Vz[Bz] +\\x1b2I (10.70a)\\n=BVz[z]B>+\\x1b2I=BB>+\\x1b2I: (10.70b)\\nThe likelihood in (10.68b) can be used for maximum likelihood or MAP\\nestimation of the model parameters.\\nRemark. We cannot use the conditional distribution in (10.64) for maxi-\\nmum likelihood estimation as it still depends on the latent variables. The\\nlikelihood function we require for maximum likelihood (or MAP) estima-\\ntion should only be a function of the data xand the model parameters,\\nbut must not depend on the latent variables. }\\nFrom Section 6.5, we know that a Gaussian random variable zand\\na linear/afﬁne transformation x=Bzof it are jointly Gaussian dis-\\ntributed. We already know the marginals p(z) =N\\x00zj0;I\\x01\\nandp(x) =\\nN\\x00xj\\x16;BB>+\\x1b2I\\x01\\n. The missing cross-covariance is given as\\nCov[x;z] = Covz[Bz+\\x16] =BCovz[z;z] =B: (10.71)\\nTherefore, the probabilistic model of PPCA, i.e., the joint distribution of\\nlatent and observed random variables is explicitly given by\\np(x;zjB;\\x16;\\x1b2) =N\\x12\\x14x\\nz\\x15\\x0c\\x0c\\x0c\\x0c\\x14\\x16\\n0\\x15\\n;\\x14BB>+\\x1b2I B\\nB>I\\x15\\x13\\n;(10.72)\\nwith a mean vector of length D+Mand a covariance matrix of size\\n(D+M)\\x02(D+M).\\n10.7.3 Posterior Distribution\\nThe joint Gaussian distribution p(x;zjB;\\x16;\\x1b2)in (10.72) allows us to\\ndetermine the posterior distribution p(zjx)immediately by applying the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.8 Further Reading 343\\nrules of Gaussian conditioning from Section 6.5.1. The posterior distribu-\\ntion of the latent variable given an observation xis then\\np(zjx) =N\\x00zjm;C\\x01; (10.73)\\nm=B>(BB>+\\x1b2I)\\x001(x\\x00\\x16); (10.74)\\nC=I\\x00B>(BB>+\\x1b2I)\\x001B: (10.75)\\nNote that the posterior covariance does not depend on the observed data\\nx. For a new observation x\\x03in data space, we use (10.73) to determine\\nthe posterior distribution of the corresponding latent variable z\\x03. The co-\\nvariance matrix Callows us to assess how conﬁdent the embedding is. A\\ncovariance matrix Cwith a small determinant (which measures volumes)\\ntells us that the latent embedding z\\x03is fairly certain. If we obtain a pos-\\nterior distribution p(z\\x03jx\\x03)with much variance, we may be faced with\\nan outlier. However, we can explore this posterior distribution to under-\\nstand what other data points xare plausible under this posterior. To do\\nthis, we exploit the generative process underlying PPCA, which allows us\\nto explore the posterior distribution on the latent variables by generating\\nnew data that is plausible under this posterior:\\n1. Sample a latent variable z\\x03\\x18p(zjx\\x03)from the posterior distribution\\nover the latent variables (10.73).\\n2. Sample a reconstructed vector ~x\\x03\\x18p(xjz\\x03;B;\\x16;\\x1b2) from (10.64).\\nIf we repeat this process many times, we can explore the posterior dis-\\ntribution (10.73) on the latent variables z\\x03and its implications on the\\nobserved data. The sampling process effectively hypothesizes data, which\\nis plausible under the posterior distribution.\\n10.8 Further Reading\\nWe derived PCA from two perspectives: (a) maximizing the variance in the\\nprojected space; (b) minimizing the average reconstruction error. How-\\never, PCA can also be interpreted from different perspectives. Let us recap\\nwhat we have done: We took high-dimensional data x2RDand used\\na matrixB>to ﬁnd a lower-dimensional representation z2RM. The\\ncolumns ofBare the eigenvectors of the data covariance matrix Sthat are\\nassociated with the largest eigenvalues. Once we have a low-dimensional\\nrepresentation z, we can get a high-dimensional version of it (in the orig-\\ninal data space) as x\\x19~x=Bz=BB>x2RD, whereBB>is a\\nprojection matrix.\\nWe can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder\\nure 10.16. An auto-encoder encodes the data xn2RDto acodezn2RMcode\\nand decodes it to a ~xnsimilar toxn. The mapping from the data to the\\ncode is called the encoder , and the mapping from the code back to the orig- encoder\\ninal data space is called the decoder . If we consider linear mappings where decoder\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n344 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.16 PCA\\ncan be viewed as a\\nlinear auto-encoder.\\nIt encodes the\\nhigh-dimensional\\ndataxinto a\\nlower-dimensional\\nrepresentation\\n(code)z2RMand\\ndecodeszusing a\\ndecoder. The\\ndecoded vector ~xis\\nthe orthogonal\\nprojection of the\\noriginal dataxonto\\ntheM-dimensional\\nprincipal subspace.B>\\nx ~xzB\\nEncoder DecoderOriginal\\nCodeRDRD\\nRM\\nthe code is given by zn=B>xn2RMand we are interested in minimiz-\\ning the average squared error between the data xnand its reconstruction\\n~xn=Bzn,n= 1;:::;N , we obtain\\n1\\nNNX\\nn=1kxn\\x00~xnk2=1\\nNNX\\nn=1\\r\\r\\rxn\\x00BB>xn\\r\\r\\r2\\n: (10.76)\\nThis means we end up with the same objective function as in (10.29) that\\nwe discussed in Section 10.3 so that we obtain the PCA solution when we\\nminimize the squared auto-encoding loss. If we replace the linear map-\\nping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.\\nA prominent example of this is a deep auto-encoder where the linear func-\\ntions are replaced with deep neural networks. In this context, the encoder\\nis also known as a recognition network orinference network , whereas the recognition network\\ninference network decoder is also called a generator .\\ngeneratorAnother interpretation of PCA is related to information theory. We can\\nthink of the code as a smaller or compressed version of the original data\\npoint. When we reconstruct our original data using the code, we do not\\nget the exact data point back, but a slightly distorted or noisy version\\nof it. This means that our compression is “lossy”. Intuitively, we want The code is a\\ncompressed version\\nof the original data.to maximize the correlation between the original data and the lower-\\ndimensional code. More formally, this is related to the mutual information.\\nWe would then get the same solution to PCA we discussed in Section 10.3\\nby maximizing the mutual information, a core concept in information the-\\nory (MacKay, 2003).\\nIn our discussion on PPCA, we assumed that the parameters of the\\nmodel, i.e.,B;\\x16, and the likelihood parameter \\x1b2, are known. Tipping\\nand Bishop (1999) describe how to derive maximum likelihood estimates\\nfor these parameters in the PPCA setting (note that we use a different\\nnotation in this chapter). The maximum likelihood parameters, when pro-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.8 Further Reading 345\\njectingD-dimensional data onto an M-dimensional subspace, are\\n\\x16ML=1\\nNNX\\nn=1xn; (10.77)\\nBML=T(\\x03\\x00\\x1b2I)1\\n2R; (10.78)\\n\\x1b2\\nML=1\\nD\\x00MDX\\nj=M+1\\x15j; (10.79)\\nwhereT2RD\\x02McontainsMeigenvectors of the data covariance matrix, The matrix \\x03\\x00\\x1b2I\\nin (10.78) is\\nguaranteed to be\\npositive semideﬁnite\\nas the smallest\\neigenvalue of the\\ndata covariance\\nmatrix is bounded\\nfrom below by the\\nnoise variance \\x1b2.\\x03= diag(\\x151;:::;\\x15M)2RM\\x02Mis a diagonal matrix with the eigenvalues\\nassociated with the principal axes on its diagonal, and R2RM\\x02Mis\\nan arbitrary orthogonal matrix. The maximum likelihood solution BMLis\\nunique up to an arbitrary orthogonal transformation, e.g., we can right-\\nmultiplyBMLwith any rotation matrix Rso that (10.78) essentially is a\\nsingular value decomposition (see Section 4.5). An outline of the proof is\\ngiven by Tipping and Bishop (1999).\\nThe maximum likelihood estimate for \\x16given in (10.77) is the sample\\nmean of the data. The maximum likelihood estimator for the observation\\nnoise variance \\x1b2given in (10.79) is the average variance in the orthog-\\nonal complement of the principal subspace, i.e., the average leftover vari-\\nance that we cannot capture with the ﬁrst Mprincipal components is\\ntreated as observation noise.\\nIn the noise-free limit where \\x1b!0, PPCA and PCA provide identical\\nsolutions: Since the data covariance matrix Sis symmetric, it can be di-\\nagonalized (see Section 4.4), i.e., there exists a matrix Tof eigenvectors\\nofSso that\\nS=T\\x03T\\x001: (10.80)\\nIn the PPCA model, the data covariance matrix is the covariance matrix of\\nthe Gaussian likelihood p(xjB;\\x16;\\x1b2), which isBB>+\\x1b2I, see (10.70b).\\nFor\\x1b!0, we obtainBB>so that this data covariance must equal the\\nPCA data covariance (and its factorization given in (10.80)) so that\\nCov[X] =T\\x03T\\x001=BB>()B=T\\x031\\n2R; (10.81)\\ni.e., we obtain the maximum likelihood estimate in (10.78) for \\x1b= 0.\\nFrom (10.78) and (10.80), it becomes clear that (P)PCA performs a de-\\ncomposition of the data covariance matrix.\\nIn a streaming setting, where data arrives sequentially, it is recom-\\nmended to use the iterative expectation maximization (EM) algorithm for\\nmaximum likelihood estimation (Roweis, 1998).\\nTo determine the dimensionality of the latent variables (the length of\\nthe code, the dimensionality of the lower-dimensional subspace onto which\\nwe project the data), Gavish and Donoho (2014) suggest the heuristic\\nthat, if we can estimate the noise variance \\x1b2of the data, we should\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n346 Dimensionality Reduction with Principal Component Analysis\\ndiscard all singular values smaller than4\\x1bp\\nDp\\n3. Alternatively, we can use\\n(nested) cross-validation (Section 8.6.1) or Bayesian model selection cri-\\nteria (discussed in Section 8.6.2) to determine a good estimate of the\\nintrinsic dimensionality of the data (Minka, 2001b).\\nSimilar to our discussion on linear regression in Chapter 9, we can place\\na prior distribution on the parameters of the model and integrate them\\nout. By doing so, we (a) avoid point estimates of the parameters and the\\nissues that come with these point estimates (see Section 8.6) and (b) al-\\nlow for an automatic selection of the appropriate dimensionality Mof the\\nlatent space. In this Bayesian PCA , which was proposed by Bishop (1999), Bayesian PCA\\na priorp(\\x16;B;\\x1b2)is placed on the model parameters. The generative\\nprocess allows us to integrate the model parameters out instead of condi-\\ntioning on them, which addresses overﬁtting issues. Since this integration\\nis analytically intractable, Bishop (1999) proposes to use approximate in-\\nference methods, such as MCMC or variational inference. We refer to the\\nwork by Gilks et al. (1996) and Blei et al. (2017) for more details on these\\napproximate inference techniques.\\nIn PPCA, we considered the linear model p(xnjzn) =N\\x00xnjBzn+\\n\\x16; \\x1b2I\\x01\\nwith priorp(zn) =N\\x000;I\\x01\\n, where all observation dimensions\\nare affected by the same amount of noise. If we allow each observation\\ndimensiondto have a different variance \\x1b2\\nd, we obtain factor analysis factor analysis\\n(FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA\\ngives the likelihood some more ﬂexibility than PPCA, but still forces the\\ndata to be explained by the model parameters B;\\x16.However, FA no An overly ﬂexible\\nlikelihood would be\\nable to explain more\\nthan just the noise.longer allows for a closed-form maximum likelihood solution so that we\\nneed to use an iterative scheme, such as the expectation maximization\\nalgorithm, to estimate the model parameters. While in PPCA all station-\\nary points are global optima, this no longer holds for FA. Compared to\\nPPCA, FA does not change if we scale the data, but it does return different\\nsolutions if we rotate the data.\\nAn algorithm that is also closely related to PCA is independent com- independent\\ncomponent analysis ponent analysis (ICA(Hyvarinen et al., 2001)). Starting again with the\\nICAlatent-variable perspective p(xnjzn) =N\\x00xnjBzn+\\x16; \\x1b2I\\x01\\nwe now\\nchange the prior on znto non-Gaussian distributions. ICA can be used\\nforblind-source separation . Imagine you are in a busy train station with blind-source\\nseparation many people talking. Your ears play the role of microphones, and they\\nlinearly mix different speech signals in the train station. The goal of blind-\\nsource separation is to identify the constituent parts of the mixed signals.\\nAs discussed previously in the context of maximum likelihood estimation\\nfor PPCA, the original PCA solution is invariant to any rotation. Therefore,\\nPCA can identify the best lower-dimensional subspace in which the sig-\\nnals live, but not the signals themselves (Murphy, 2012). ICA addresses\\nthis issue by modifying the prior distribution p(z)on the latent sources\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n10.8 Further Reading 347\\nto require non-Gaussian priors p(z). We refer to the books by Hyvarinen\\net al. (2001) and Murphy (2012) for more details on ICA.\\nPCA, factor analysis, and ICA are three examples for dimensionality re-\\nduction with linear models. Cunningham and Ghahramani (2015) provide\\na broader survey of linear dimensionality reduction.\\nThe (P)PCA model we discussed here allows for several important ex-\\ntensions. In Section 10.5, we explained how to do PCA when the in-\\nput dimensionality Dis signiﬁcantly greater than the number Nof data\\npoints. By exploiting the insight that PCA can be performed by computing\\n(many) inner products, this idea can be pushed to the extreme by consid-\\nering inﬁnite-dimensional features. The kernel trick is the basis of kernel kernel trick\\nkernel PCA PCAand allows us to implicitly compute inner products between inﬁnite-\\ndimensional features (Sch ¨olkopf et al., 1998; Sch ¨olkopf and Smola, 2002).\\nThere are nonlinear dimensionality reduction techniques that are de-\\nrived from PCA (Burges (2010) provides a good overview). The auto-\\nencoder perspective of PCA that we discussed previously in this section\\ncan be used to render PCA as a special case of a deep auto-encoder . In the deep auto-encoder\\ndeep auto-encoder, both the encoder and the decoder are represented by\\nmultilayer feedforward neural networks, which themselves are nonlinear\\nmappings. If we set the activation functions in these neural networks to be\\nthe identity, the model becomes equivalent to PCA. A different approach to\\nnonlinear dimensionality reduction is the Gaussian process latent-variable Gaussian process\\nlatent-variable\\nmodelmodel (GP-LVM ) proposed by Lawrence (2005). The GP-LVM starts off with\\nGP-LVMthe latent-variable perspective that we used to derive PPCA and replaces\\nthe linear relationship between the latent variables zand the observations\\nxwith a Gaussian process (GP). Instead of estimating the parameters of\\nthe mapping (as we do in PPCA), the GP-LVM marginalizes out the model\\nparameters and makes point estimates of the latent variables z. Similar\\nto Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence Bayesian GP-LVM\\n(2010) maintains a distribution on the latent variables zand uses approx-\\nimate inference to integrate them out as well.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n11\\nDensity Estimation with Gaussian Mixture\\nModels\\nIn earlier chapters, we covered already two fundamental problems in\\nmachine learning: regression (Chapter 9) and dimensionality reduction\\n(Chapter 10). In this chapter, we will have a look at a third pillar of ma-\\nchine learning: density estimation. On our journey, we introduce impor-\\ntant concepts, such as the expectation maximization (EM) algorithm and\\na latent variable perspective of density estimation with mixture models.\\nWhen we apply machine learning to data we often aim to represent\\ndata in some way. A straightforward way is to take the data points them-\\nselves as the representation of the data; see Figure 11.1 for an example.\\nHowever, this approach may be unhelpful if the dataset is huge or if we\\nare interested in representing characteristics of the data. In density esti-\\nmation, we represent the data compactly using a density from a paramet-\\nric family, e.g., a Gaussian or Beta distribution. For example, we may be\\nlooking for the mean and variance of a dataset in order to represent the\\ndata compactly using a Gaussian distribution. The mean and variance can\\nbe found using tools we discussed in Section 8.3: maximum likelihood or\\nmaximum a posteriori estimation. We can then use the mean and variance\\nof this Gaussian to represent the distribution underlying the data, i.e., we\\nthink of the dataset to be a typical realization from this distribution if we\\nwere to sample from it.\\nFigure 11.1\\nTwo-dimensional\\ndataset that cannot\\nbe meaningfully\\nrepresented by a\\nGaussian.\\n−5 0 5\\nx1−4−2024x2\\n348\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n11.1 Gaussian Mixture Model 349\\nIn practice, the Gaussian (or similarly all other distributions we encoun-\\ntered so far) have limited modeling capabilities. For example, a Gaussian\\napproximation of the density that generated the data in Figure 11.1 would\\nbe a poor approximation. In the following, we will look at a more ex-\\npressive family of distributions, which we can use for density estimation:\\nmixture models . mixture model\\nMixture models can be used to describe a distribution p(x)by a convex\\ncombination of Ksimple (base) distributions\\np(x) =KX\\nk=1\\x19kpk(x) (11.1)\\n06\\x19k61;KX\\nk=1\\x19k= 1; (11.2)\\nwhere the components pkare members of a family of basic distributions,\\ne.g., Gaussians, Bernoullis, or Gammas, and the \\x19karemixture weights .mixture weight\\nMixture models are more expressive than the corresponding base distri-\\nbutions because they allow for multimodal data representations, i.e., they\\ncan describe datasets with multiple “clusters”, such as the example in Fig-\\nure 11.1.\\nWe will focus on Gaussian mixture models (GMMs), where the basic\\ndistributions are Gaussians. For a given dataset, we aim to maximize the\\nlikelihood of the model parameters to train the GMM. For this purpose,\\nwe will use results from Chapter 5, Chapter 6, and Section 7.2. However,\\nunlike other applications we discussed earlier (linear regression or PCA),\\nwe will not ﬁnd a closed-form maximum likelihood solution. Instead, we\\nwill arrive at a set of dependent simultaneous equations, which we can\\nonly solve iteratively.\\n11.1 Gaussian Mixture Model\\nAGaussian mixture model is a density model where we combine a ﬁnite Gaussian mixture\\nmodel number ofKGaussian distributions N\\x00xj\\x16k;\\x06k\\x01\\nso that\\np(xj\\x12) =KX\\nk=1\\x19kN\\x00xj\\x16k;\\x06k\\x01\\n(11.3)\\n06\\x19k61;KX\\nk=1\\x19k= 1; (11.4)\\nwhere we deﬁned \\x12:=f\\x16k;\\x06k;\\x19k:k= 1;:::;Kgas the collection of\\nall parameters of the model. This convex combination of Gaussian distri-\\nbution gives us signiﬁcantly more ﬂexibility for modeling complex densi-\\nties than a simple Gaussian distribution (which we recover from (11.3) for\\nK= 1). An illustration is given in Figure 11.2, displaying the weighted\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n350 Density Estimation with Gaussian Mixture Models\\nFigure 11.2\\nGaussian mixture\\nmodel. The\\nGaussian mixture\\ndistribution (black)\\nis composed of a\\nconvex combination\\nof Gaussian\\ndistributions and is\\nmore expressive\\nthan any individual\\ncomponent. Dashed\\nlines represent the\\nweighted Gaussian\\ncomponents.\\n\\x004\\x002 0 2 4 6 8\\nx0:000:050:100:150:200:250:30p(x)Component 1\\nComponent 2\\nComponent 3\\nGMM density\\ncomponents and the mixture density, which is given as\\np(xj\\x12) = 0:5N\\x00xj\\x002;1\\n2\\x01+ 0:2N\\x00xj1;2\\x01+ 0:3N\\x00xj4;1\\x01:(11.5)\\n11.2 Parameter Learning via Maximum Likelihood\\nAssume we are given a dataset X=fx1;:::;xNg, wherexn; n =\\n1;:::;N , are drawn i.i.d. from an unknown distribution p(x). Our ob-\\njective is to ﬁnd a good approximation/representation of this unknown\\ndistribution p(x)by means of a GMM with Kmixture components. The\\nparameters of the GMM are the Kmeans\\x16k, the covariances \\x06k, and\\nmixture weights \\x19k. We summarize all these free parameters in \\x12:=\\nf\\x19k;\\x16k;\\x06k:k= 1;:::;Kg.\\nExample 11.1 (Initial Setting)\\nFigure 11.3 Initial\\nsetting: GMM\\n(black) with\\nmixture three\\nmixture components\\n(dashed) and seven\\ndata points (discs).\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density\\nThroughout this chapter, we will have a simple running example that\\nhelps us illustrate and visualize important concepts.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.2 Parameter Learning via Maximum Likelihood 351\\nWe consider a one-dimensional dataset X=f\\x003;\\x002:5;\\x001;0;2;4;5g\\nconsisting of seven data points and wish to ﬁnd a GMM with K= 3\\ncomponents that models the density of the data. We initialize the mixture\\ncomponents as\\np1(x) =N\\x00xj\\x004;1\\x01\\n(11.6)\\np2(x) =N\\x00xj0;0:2\\x01\\n(11.7)\\np3(x) =N\\x00xj8;3\\x01\\n(11.8)\\nand assign them equal weights \\x191=\\x192=\\x193=1\\n3. The corresponding\\nmodel (and the data points) are shown in Figure 11.3.\\nIn the following, we detail how to obtain a maximum likelihood esti-\\nmate\\x12MLof the model parameters \\x12. We start by writing down the like-\\nlihood, i.e., the predictive distribution of the training data given the pa-\\nrameters. We exploit our i.i.d. assumption, which leads to the factorized\\nlikelihood\\np(Xj\\x12) =NY\\nn=1p(xnj\\x12); p(xnj\\x12) =KX\\nk=1\\x19kN\\x00xnj\\x16k;\\x06k\\x01;(11.9)\\nwhere every individual likelihood term p(xnj\\x12)is a Gaussian mixture\\ndensity. Then we obtain the log-likelihood as\\nlogp(Xj\\x12) =NX\\nn=1logp(xnj\\x12) =NX\\nn=1logKX\\nk=1\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\n|{z }\\n=:L:(11.10)\\nWe aim to ﬁnd parameters \\x12\\x03\\nMLthat maximize the log-likelihood Ldeﬁned\\nin (11.10). Our “normal” procedure would be to compute the gradient\\ndL=d\\x12of the log-likelihood with respect to the model parameters \\x12, set\\nit to0, and solve for \\x12. However, unlike our previous examples for max-\\nimum likelihood estimation (e.g., when we discussed linear regression in\\nSection 9.2), we cannot obtain a closed-form solution. However, we can\\nexploit an iterative scheme to ﬁnd good model parameters \\x12ML, which will\\nturn out to be the EM algorithm for GMMs. The key idea is to update one\\nmodel parameter at a time while keeping the others ﬁxed.\\nRemark. If we were to consider a single Gaussian as the desired density,\\nthe sum over kin (11.10) vanishes, and the logcan be applied directly to\\nthe Gaussian component, such that we get\\nlogN\\x00xj\\x16;\\x06\\x01=\\x00D\\n2log(2\\x19)\\x001\\n2log det( \\x06)\\x001\\n2(x\\x00\\x16)>\\x06\\x001(x\\x00\\x16):\\n(11.11)\\nThis simple form allows us to ﬁnd closed-form maximum likelihood esti-\\nmates of\\x16and\\x06, as discussed in Chapter 8. In (11.10), we cannot move\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n352 Density Estimation with Gaussian Mixture Models\\ntheloginto the sum over kso that we cannot obtain a simple closed-form\\nmaximum likelihood solution. }\\nAny local optimum of a function exhibits the property that its gradi-\\nent with respect to the parameters must vanish (necessary condition); see\\nChapter 7. In our case, we obtain the following necessary conditions when\\nwe optimize the log-likelihood in (11.10) with respect to the GMM param-\\neters\\x16k;\\x06k;\\x19k:\\n@L\\n@\\x16k=0>()NX\\nn=1@logp(xnj\\x12)\\n@\\x16k=0>; (11.12)\\n@L\\n@\\x06k=0()NX\\nn=1@logp(xnj\\x12)\\n@\\x06k=0; (11.13)\\n@L\\n@\\x19k= 0()NX\\nn=1@logp(xnj\\x12)\\n@\\x19k= 0: (11.14)\\nFor all three necessary conditions, by applying the chain rule (see Sec-\\ntion 5.2.2), we require partial derivatives of the form\\n@logp(xnj\\x12)\\n@\\x12=1\\np(xnj\\x12)@p(xnj\\x12)\\n@\\x12; (11.15)\\nwhere\\x12=f\\x16k;\\x06k;\\x19k;k= 1;:::;Kgare the model parameters and\\n1\\np(xnj\\x12)=1\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01: (11.16)\\nIn the following, we will compute the partial derivatives (11.12) through\\n(11.14). But before we do this, we introduce a quantity that will play a\\ncentral role in the remainder of this chapter: responsibilities.\\n11.2.1 Responsibilities\\nWe deﬁne the quantity\\nrnk:=\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01 (11.17)\\nas the responsibility of thekth mixture component for the nth data point. responsibility\\nThe responsibility rnkof thekth mixture component for data point xnis\\nproportional to the likelihood\\np(xnj\\x19k;\\x16k;\\x06k) =\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\n(11.18)\\nof the mixture component given the data point. Therefore, mixture com- rnfollows a\\nBoltzmann/Gibbs\\ndistribution.ponents have a high responsibility for a data point when the data point\\ncould be a plausible sample from that mixture component. Note that\\nrn:= [rn1;:::;rnK]>2RKis a (normalized) probability vector, i.e.,\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.2 Parameter Learning via Maximum Likelihood 353\\nP\\nkrnk= 1 withrnk>0. This probability vector distributes probabil-\\nity mass among the Kmixture components, and we can think of rnas a\\n“soft assignment” of xnto theKmixture components. Therefore, the re- The responsibility\\nrnkis the\\nprobability that the\\nkth mixture\\ncomponent\\ngenerated the nth\\ndata point.sponsibility rnkfrom (11.17) represents the probability that xnhas been\\ngenerated by the kth mixture component.\\nExample 11.2 (Responsibilities)\\nFor our example from Figure 11.3, we compute the responsibilities rnk\\n2\\n6666666641:0 0:0 0:0\\n1:0 0:0 0:0\\n0:057 0:943 0:0\\n0:001 0:999 0:0\\n0:0 0:066 0:934\\n0:0 0:0 1:0\\n0:0 0:0 1:03\\n7777777752RN\\x02K: (11.19)\\nHere thenth row tells us the responsibilities of all mixture components\\nforxn. The sum of all Kresponsibilities for a data point (sum of every\\nrow) is 1. Thekth column gives us an overview of the responsibility of\\nthekth mixture component. We can see that the third mixture component\\n(third column) is not responsible for any of the ﬁrst four data points, but\\ntakes much responsibility of the remaining data points. The sum of all\\nentries of a column gives us the values Nk, i.e., the total responsibility of\\nthekth mixture component. In our example, we get N1= 2:058; N 2=\\n2:008; N3= 2:934.\\nIn the following, we determine the updates of the model parameters\\n\\x16k;\\x06k;\\x19kfor given responsibilities. We will see that the update equa-\\ntions all depend on the responsibilities, which makes a closed-form solu-\\ntion to the maximum likelihood estimation problem impossible. However,\\nfor given responsibilities we will be updating one model parameter at a\\ntime, while keeping the others ﬁxed. After this, we will recompute the\\nresponsibilities. Iterating these two steps will eventually converge to a lo-\\ncal optimum and is a speciﬁc instantiation of the EM algorithm. We will\\ndiscuss this in some more detail in Section 11.3.\\n11.2.2 Updating the Means\\nTheorem 11.1 (Update of the GMM Means) .The update of the mean pa-\\nrameters\\x16k,k= 1;:::;K , of the GMM is given by\\n\\x16new\\nk=PN\\nn=1rnkxnPN\\nn=1rnk; (11.20)\\nwhere the responsibilities rnkare deﬁned in (11.17) .\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n354 Density Estimation with Gaussian Mixture Models\\nRemark. The update of the means \\x16kof the individual mixture compo-\\nnents in (11.20) depends on all means, covariance matrices \\x06k, and mix-\\nture weights \\x19kviarnkgiven in (11.17). Therefore, we cannot obtain a\\nclosed-form solution for all \\x16kat once. }\\nProof From (11.15), we see that the gradient of the log-likelihood with\\nrespect to the mean parameters \\x16k,k= 1;:::;K , requires us to compute\\nthe partial derivative\\n@p(xnj\\x12)\\n@\\x16k=KX\\nj=1\\x19j@N\\x00xnj\\x16j;\\x06j\\x01\\n@\\x16k=\\x19k@N\\x00xnj\\x16k;\\x06k\\x01\\n@\\x16k(11.21a)\\n=\\x19k(xn\\x00\\x16k)>\\x06\\x001\\nkN\\x00xnj\\x16k;\\x06k\\x01; (11.21b)\\nwhere we exploited that only the kth mixture component depends on \\x16k.\\nWe use our result from (11.21b) in (11.15) and put everything together\\nso that the desired partial derivative of Lwith respect to \\x16kis given as\\n@L\\n@\\x16k=NX\\nn=1@logp(xnj\\x12)\\n@\\x16k=NX\\nn=11\\np(xnj\\x12)@p(xnj\\x12)\\n@\\x16k(11.22a)\\n=NX\\nn=1(xn\\x00\\x16k)>\\x06\\x001\\nk\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01\\n|{z}\\n=rnk(11.22b)\\n=NX\\nn=1rnk(xn\\x00\\x16k)>\\x06\\x001\\nk: (11.22c)\\nHere we used the identity from (11.16) and the result of the partial deriva-\\ntive in (11.21b) to get to (11.22b). The values rnkare the responsibilities\\nwe deﬁned in (11.17).\\nWe now solve (11.22c) for \\x16new\\nkso that@L(\\x16new\\nk)\\n@\\x16k=0>and obtain\\nNX\\nn=1rnkxn=NX\\nn=1rnk\\x16new\\nk()\\x16new\\nk=PN\\nn=1rnkxn\\nPN\\nn=1rnk=1\\nNkNX\\nn=1rnkxn;\\n(11.23)\\nwhere we deﬁned\\nNk:=NX\\nn=1rnk (11.24)\\nas the total responsibility of the kth mixture component for the entire\\ndataset. This concludes the proof of Theorem 11.1.\\nIntuitively, (11.20) can be interpreted as an importance-weighted Monte\\nCarlo estimate of the mean, where the importance weights of data point\\nxnare the responsibilities rnkof thekth cluster for xn,k= 1;:::;K .\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.2 Parameter Learning via Maximum Likelihood 355\\nTherefore, the mean \\x16kis pulled toward a data point xnwith strength Figure 11.4 Update\\nof the mean\\nparameter of\\nmixture component\\nin a GMM. The\\nmean\\x16is being\\npulled toward\\nindividual data\\npoints with the\\nweights given by the\\ncorresponding\\nresponsibilities.\\nr1r2\\nr3x1x2x3\\n\\x16given byrnk. The means are pulled stronger toward data points for which\\nthe corresponding mixture component has a high responsibility, i.e., a high\\nlikelihood. Figure 11.4 illustrates this. We can also interpret the mean up-\\ndate in (11.20) as the expected value of all data points under the distri-\\nbution given by\\nrk:= [r1k;:::;rNk]>=Nk; (11.25)\\nwhich is a normalized probability vector, i.e.,\\n\\x16k Erk[X]: (11.26)\\nExample 11.3 (Mean Updates)\\nFigure 11.5 Effect\\nof updating the\\nmean values in a\\nGMM. (a) GMM\\nbefore updating the\\nmean values;\\n(b) GMM after\\nupdating the mean\\nvalues\\x16kwhile\\nretaining the\\nvariances and\\nmixture weights.\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density (a) GMM density and individual components\\nprior to updating the mean values.\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(b) GMM density and individual components\\nafter updating the mean values.\\nIn our example from Figure 11.3, the mean values are updated as fol-\\nlows:\\n\\x161:\\x004!\\x002:7 (11.27)\\n\\x162: 0!\\x000:4 (11.28)\\n\\x163: 8!3:7 (11.29)\\nHere we see that the means of the ﬁrst and third mixture component\\nmove toward the regime of the data, whereas the mean of the second\\ncomponent does not change so dramatically. Figure 11.5 illustrates this\\nchange, where Figure 11.5(a) shows the GMM density prior to updating\\nthe means and Figure 11.5(b) shows the GMM density after updating the\\nmean values \\x16k.\\nThe update of the mean parameters in (11.20) look fairly straight-\\nforward. However, note that the responsibilities rnkare a function of\\n\\x19j;\\x16j;\\x06jfor allj= 1;:::;K , such that the updates in (11.20) depend\\non all parameters of the GMM, and a closed-form solution, which we ob-\\ntained for linear regression in Section 9.2 or PCA in Chapter 10, cannot\\nbe obtained.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n356 Density Estimation with Gaussian Mixture Models\\n11.2.3 Updating the Covariances\\nTheorem 11.2 (Updates of the GMM Covariances) .The update of the co-\\nvariance parameters \\x06k,k= 1;:::;K of the GMM is given by\\n\\x06new\\nk=1\\nNkNX\\nn=1rnk(xn\\x00\\x16k)(xn\\x00\\x16k)>; (11.30)\\nwherernkandNkare deﬁned in (11.17) and(11.24) , respectively.\\nProof To prove Theorem 11.2, our approach is to compute the partial\\nderivatives of the log-likelihood Lwith respect to the covariances \\x06k, set\\nthem to 0, and solve for \\x06k. We start with our general approach\\n@L\\n@\\x06k=NX\\nn=1@logp(xnj\\x12)\\n@\\x06k=NX\\nn=11\\np(xnj\\x12)@p(xnj\\x12)\\n@\\x06k: (11.31)\\nWe already know 1=p(xnj\\x12)from (11.16). To obtain the remaining par-\\ntial derivative @p(xnj\\x12)=@\\x06k, we write down the deﬁnition of the Gaus-\\nsian distribution p(xnj\\x12)(see (11.9)) and drop all terms but the kth. We\\nthen obtain\\n@p(xnj\\x12)\\n@\\x06k(11.32a)\\n=@\\n@\\x06k\\x12\\n\\x19k(2\\x19)\\x00D\\n2det(\\x06k)\\x001\\n2exp\\x00\\x001\\n2(xn\\x00\\x16k)>\\x06\\x001\\nk(xn\\x00\\x16k)\\x01\\x13\\n(11.32b)\\n=\\x19k(2\\x19)\\x00D\\n2\\x14@\\n@\\x06kdet(\\x06k)\\x001\\n2exp\\x00\\x001\\n2(xn\\x00\\x16k)>\\x06\\x001\\nk(xn\\x00\\x16k)\\x01\\n+ det( \\x06k)\\x001\\n2@\\n@\\x06kexp\\x00\\x001\\n2(xn\\x00\\x16k)>\\x06\\x001\\nk(xn\\x00\\x16k)\\x01\\x15\\n:(11.32c)\\nWe now use the identities\\n@\\n@\\x06kdet(\\x06k)\\x001\\n2(5.101)=\\x001\\n2det(\\x06k)\\x001\\n2\\x06\\x001\\nk; (11.33)\\n@\\n@\\x06k(xn\\x00\\x16k)>\\x06\\x001\\nk(xn\\x00\\x16k)(5.103)=\\x00\\x06\\x001\\nk(xn\\x00\\x16k)(xn\\x00\\x16k)>\\x06\\x001\\nk\\n(11.34)\\nand obtain (after some rearranging) the desired partial derivative required\\nin (11.31) as\\n@p(xnj\\x12)\\n@\\x06k=\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\n\\x01\\x02\\x001\\n2(\\x06\\x001\\nk\\x00\\x06\\x001\\nk(xn\\x00\\x16k)(xn\\x00\\x16k)>\\x06\\x001\\nk)\\x03:(11.35)\\nPutting everything together, the partial derivative of the log-likelihood\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.2 Parameter Learning via Maximum Likelihood 357\\nwith respect to \\x06kis given by\\n@L\\n@\\x06k=NX\\nn=1@logp(xnj\\x12)\\n@\\x06k=NX\\nn=11\\np(xnj\\x12)@p(xnj\\x12)\\n@\\x06k(11.36a)\\n=NX\\nn=1\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01\\n|{z}\\n=rnk\\n\\x01\\x02\\x001\\n2(\\x06\\x001\\nk\\x00\\x06\\x001\\nk(xn\\x00\\x16k)(xn\\x00\\x16k)>\\x06\\x001\\nk)\\x03\\n(11.36b)\\n=\\x001\\n2NX\\nn=1rnk(\\x06\\x001\\nk\\x00\\x06\\x001\\nk(xn\\x00\\x16k)(xn\\x00\\x16k)>\\x06\\x001\\nk) (11.36c)\\n=\\x001\\n2\\x06\\x001\\nkNX\\nn=1rnk\\n|{z}\\n=Nk+1\\n2\\x06\\x001\\nk NX\\nn=1rnk(xn\\x00\\x16k)(xn\\x00\\x16k)>!\\n\\x06\\x001\\nk:\\n(11.36d)\\nWe see that the responsibilities rnkalso appear in this partial derivative.\\nSetting this partial derivative to 0, we obtain the necessary optimality\\ncondition\\nNk\\x06\\x001\\nk=\\x06\\x001\\nk NX\\nn=1rnk(xn\\x00\\x16k)(xn\\x00\\x16k)>!\\n\\x06\\x001\\nk (11.37a)\\n()NkI= NX\\nn=1rnk(xn\\x00\\x16k)(xn\\x00\\x16k)>!\\n\\x06\\x001\\nk: (11.37b)\\nBy solving for \\x06k, we obtain\\n\\x06new\\nk=1\\nNkNX\\nn=1rnk(xn\\x00\\x16k)(xn\\x00\\x16k)>; (11.38)\\nwhererkis the probability vector deﬁned in (11.25). This gives us a sim-\\nple update rule for \\x06kfork= 1;:::;K and proves Theorem 11.2.\\nSimilar to the update of \\x16kin (11.20), we can interpret the update of\\nthe covariance in (11.30) as an importance-weighted expected value of\\nthe square of the centered data ~Xk:=fx1\\x00\\x16k;:::;xN\\x00\\x16kg.\\nExample 11.4 (Variance Updates)\\nIn our example from Figure 11.3, the variances are updated as follows:\\n\\x1b2\\n1: 1!0:14 (11.39)\\n\\x1b2\\n2: 0:2!0:44 (11.40)\\n\\x1b2\\n3: 3!1:53 (11.41)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n358 Density Estimation with Gaussian Mixture Models\\nHere we see that the variances of the ﬁrst and third component shrink\\nsigniﬁcantly, whereas the variance of the second component increases\\nslightly.\\nFigure 11.6 illustrates this setting. Figure 11.6(a) is identical (but\\nzoomed in) to Figure 11.5(b) and shows the GMM density and its indi-\\nvidual components prior to updating the variances. Figure 11.6(b) shows\\nthe GMM density after updating the variances.\\nFigure 11.6 Effect\\nof updating the\\nvariances in a GMM.\\n(a) GMM before\\nupdating the\\nvariances; (b) GMM\\nafter updating the\\nvariances while\\nretaining the means\\nand mixture\\nweights.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density\\n(a) GMM density and individual components\\nprior to updating the variances.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.300.35p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(b) GMM density and individual components\\nafter updating the variances.\\nSimilar to the update of the mean parameters, we can interpret (11.30)\\nas a Monte Carlo estimate of the weighted covariance of data points xn\\nassociated with the kth mixture component, where the weights are the\\nresponsibilities rnk. As with the updates of the mean parameters, this up-\\ndate depends on all \\x19j;\\x16j;\\x06j; j= 1;:::;K , through the responsibilities\\nrnk, which prohibits a closed-form solution.\\n11.2.4 Updating the Mixture Weights\\nTheorem 11.3 (Update of the GMM Mixture Weights) .The mixture weights\\nof the GMM are updated as\\n\\x19new\\nk=Nk\\nN; k = 1;:::;K; (11.42)\\nwhereNis the number of data points and Nkis deﬁned in (11.24) .\\nProof To ﬁnd the partial derivative of the log-likelihood with respect\\nto the weight parameters \\x19k,k= 1;:::;K , we account for the con-\\nstraintP\\nk\\x19k= 1 by using Lagrange multipliers (see Section 7.2). The\\nLagrangian is\\nL=L+\\x15 KX\\nk=1\\x19k\\x001!\\n(11.43a)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.2 Parameter Learning via Maximum Likelihood 359\\n=NX\\nn=1logKX\\nk=1\\x19kN\\x00xnj\\x16k;\\x06k\\x01+\\x15 KX\\nk=1\\x19k\\x001!\\n; (11.43b)\\nwhereLis the log-likelihood from (11.10) and the second term encodes\\nfor the equality constraint that all the mixture weights need to sum up to\\n1. We obtain the partial derivative with respect to \\x19kas\\n@L\\n@\\x19k=NX\\nn=1N\\x00xnj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01+\\x15 (11.44a)\\n=1\\n\\x19kNX\\nn=1\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01\\n|{z }\\n=Nk+\\x15=Nk\\n\\x19k+\\x15; (11.44b)\\nand the partial derivative with respect to the Lagrange multiplier \\x15as\\n@L\\n@\\x15=KX\\nk=1\\x19k\\x001: (11.45)\\nSetting both partial derivatives to 0(necessary condition for optimum)\\nyields the system of equations\\n\\x19k=\\x00Nk\\n\\x15; (11.46)\\n1 =KX\\nk=1\\x19k: (11.47)\\nUsing (11.46) in (11.47) and solving for \\x19k, we obtain\\nKX\\nk=1\\x19k= 1() \\x00KX\\nk=1Nk\\n\\x15= 1() \\x00N\\n\\x15= 1()\\x15=\\x00N:\\n(11.48)\\nThis allows us to substitute \\x00Nfor\\x15in (11.46) to obtain\\n\\x19new\\nk=Nk\\nN; (11.49)\\nwhich gives us the update for the weight parameters \\x19kand proves Theo-\\nrem 11.3.\\nWe can identify the mixture weight in (11.42) as the ratio of the to-\\ntal responsibility of the kth cluster and the number of data points. Since\\nN=P\\nkNk, the number of data points can also be interpreted as the\\ntotal responsibility of all mixture components together, such that \\x19kis the\\nrelative importance of the kth mixture component for the dataset.\\nRemark. SinceNk=PN\\ni=1rnk, the update equation (11.42) for the mix-\\nture weights \\x19kalso depends on all \\x19j;\\x16j;\\x06j;j= 1;:::;K via the re-\\nsponsibilities rnk. }\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n360 Density Estimation with Gaussian Mixture Models\\nExample 11.5 (Weight Parameter Updates)\\nFigure 11.7 Effect\\nof updating the\\nmixture weights in a\\nGMM. (a) GMM\\nbefore updating the\\nmixture weights;\\n(b) GMM after\\nupdating the\\nmixture weights\\nwhile retaining the\\nmeans and\\nvariances. Note the\\ndifferent scales of\\nthe vertical axes.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.300.35p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(a) GMM density and individual components\\nprior to updating the mixture weights.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(b) GMM density and individual components\\nafter updating the mixture weights.\\nIn our running example from Figure 11.3, the mixture weights are up-\\ndated as follows:\\n\\x191:1\\n3!0:29 (11.50)\\n\\x192:1\\n3!0:29 (11.51)\\n\\x193:1\\n3!0:42 (11.52)\\nHere we see that the third component gets more weight/importance,\\nwhile the other components become slightly less important. Figure 11.7\\nillustrates the effect of updating the mixture weights. Figure 11.7(a) is\\nidentical to Figure 11.6(b) and shows the GMM density and its individual\\ncomponents prior to updating the mixture weights. Figure 11.7(b) shows\\nthe GMM density after updating the mixture weights.\\nOverall, having updated the means, the variances, and the weights\\nonce, we obtain the GMM shown in Figure 11.7(b). Compared with the\\ninitialization shown in Figure 11.3, we can see that the parameter updates\\ncaused the GMM density to shift some of its mass toward the data points.\\nAfter updating the means, variances, and weights once, the GMM ﬁt\\nin Figure 11.7(b) is already remarkably better than its initialization from\\nFigure 11.3. This is also evidenced by the log-likelihood values, which in-\\ncreased from 28:3(initialization) to 14:4after one complete update cycle.\\n11.3 EM Algorithm\\nUnfortunately, the updates in (11.20), (11.30), and (11.42) do not consti-\\ntute a closed-form solution for the updates of the parameters \\x16k;\\x06k;\\x19k\\nof the mixture model because the responsibilities rnkdepend on those pa-\\nrameters in a complex way. However, the results suggest a simple iterative\\nscheme for ﬁnding a solution to the parameters estimation problem via\\nmaximum likelihood. The expectation maximization algorithm ( EM algo- EM algorithm\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.3 EM Algorithm 361\\nrithm ) was proposed by Dempster et al. (1977) and is a general iterative\\nscheme for learning parameters (maximum likelihood or MAP) in mixture\\nmodels and, more generally, latent-variable models.\\nIn our example of the Gaussian mixture model, we choose initial values\\nfor\\x16k;\\x06k;\\x19kand alternate until convergence between\\nE-step: Evaluate the responsibilities rnk(posterior probability of data\\npointnbelonging to mixture component k).\\nM-step: Use the updated responsibilities to reestimate the parameters\\n\\x16k;\\x06k;\\x19k.\\nEvery step in the EM algorithm increases the log-likelihood function (Neal\\nand Hinton, 1999). For convergence, we can check the log-likelihood or\\nthe parameters directly. A concrete instantiation of the EM algorithm for\\nestimating the parameters of a GMM is as follows:\\n1. Initialize\\x16k;\\x06k;\\x19k.\\n2.E-step: Evaluate responsibilities rnkfor every data point xnusing cur-\\nrent parameters \\x19k;\\x16k;\\x06k:\\nrnk=\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\nP\\nj\\x19jN\\x00xnj\\x16j;\\x06j\\x01: (11.53)\\n3.M-step: Reestimate parameters \\x19k;\\x16k;\\x06kusing the current responsi-\\nbilitiesrnk(from E-step): Having updated the\\nmeans\\x16k\\nin (11.54), they are\\nsubsequently used\\nin (11.55) to update\\nthe corresponding\\ncovariances.\\x16k=1\\nNkNX\\nn=1rnkxn; (11.54)\\n\\x06k=1\\nNkNX\\nn=1rnk(xn\\x00\\x16k)(xn\\x00\\x16k)>; (11.55)\\n\\x19k=Nk\\nN: (11.56)\\nExample 11.6 (GMM Fit)\\nFigure 11.8 EM\\nalgorithm applied to\\nthe GMM from\\nFigure 11.2. (a)\\nFinal GMM ﬁt;\\n(b) negative\\nlog-likelihood as a\\nfunction of the EM\\niteration.\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density\\n(a) Final GMM ﬁt. After ﬁve iterations, the EM\\nalgorithm converges and returns this GMM.\\n0 1 2 3 4 5\\nIteration1416182022242628Negative log-likelihood\\n(b) Negative log-likelihood as a function of the\\nEM iterations.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n362 Density Estimation with Gaussian Mixture Models\\nFigure 11.9\\nIllustration of the\\nEM algorithm for\\nﬁtting a Gaussian\\nmixture model with\\nthree components to\\na two-dimensional\\ndataset. (a) Dataset;\\n(b) negative\\nlog-likelihood\\n(lower is better) as\\na function of the EM\\niterations. The red\\ndots indicate the\\niterations for which\\nthe mixture\\ncomponents of the\\ncorresponding GMM\\nﬁts are shown in (c)\\nthrough (f). The\\nyellow discs indicate\\nthe means of the\\nGaussian mixture\\ncomponents.\\nFigure 11.10(a)\\nshows the ﬁnal\\nGMM ﬁt.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(a) Dataset.\\n0 20 40 60\\nEM iteration104\\n4×1036×103Negative log-likelihood (b) Negative log-likelihood.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(c) EM initialization.\\n−10−5 0 5 10\\nx1−10−50510x2\\n (d) EM after one iteration.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(e) EM after 10iterations.\\n−10−5 0 5 10\\nx1−10−50510x2\\n (f) EM after 62iterations.\\nWhen we run EM on our example from Figure 11.3, we obtain the ﬁnal\\nresult shown in Figure 11.8(a) after ﬁve iterations, and Figure 11.8(b)\\nshows how the negative log-likelihood evolves as a function of the EM\\niterations. The ﬁnal GMM is given as\\np(x) = 0:29N\\x00xj\\x002:75;0:06\\x01+ 0:28N\\x00xj\\x000:50;0:25\\x01\\n+ 0:43N\\x00xj3:64;1:63\\x01:(11.57)\\nWe applied the EM algorithm to the two-dimensional dataset shown\\nin Figure 11.1 with K= 3 mixture components. Figure 11.9 illustrates\\nsome steps of the EM algorithm and shows the negative log-likelihood as\\na function of the EM iteration (Figure 11.9(b)). Figure 11.10(a) shows\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.4 Latent-Variable Perspective 363\\nFigure 11.10 GMM\\nﬁt and\\nresponsibilities\\nwhen EM converges.\\n(a) GMM ﬁt when\\nEM converges;\\n(b) each data point\\nis colored according\\nto the\\nresponsibilities of\\nthe mixture\\ncomponents.\\n−5 0 5\\nx1−6−4−20246x2\\n(a) GMM ﬁt after 62iterations.\\n−5 0 5\\nx1−6−4−20246x2 (b) Dataset colored according to the respon-\\nsibilities of the mixture components.\\nthe corresponding ﬁnal GMM ﬁt. Figure 11.10(b) visualizes the ﬁnal re-\\nsponsibilities of the mixture components for the data points. The dataset is\\ncolored according to the responsibilities of the mixture components when\\nEM converges. While a single mixture component is clearly responsible\\nfor the data on the left, the overlap of the two data clusters on the right\\ncould have been generated by two mixture components. It becomes clear\\nthat there are data points that cannot be uniquely assigned to a single\\ncomponent (either blue or yellow), such that the responsibilities of these\\ntwo clusters for those points are around 0:5.\\n11.4 Latent-Variable Perspective\\nWe can look at the GMM from the perspective of a discrete latent-variable\\nmodel, i.e., where the latent variable zcan attain only a ﬁnite set of val-\\nues. This is in contrast to PCA, where the latent variables were continuous-\\nvalued numbers in RM.\\nThe advantages of the probabilistic perspective are that (i) it will jus-\\ntify some ad hoc decisions we made in the previous sections, (ii) it allows\\nfor a concrete interpretation of the responsibilities as posterior probabil-\\nities, and (iii) the iterative algorithm for updating the model parameters\\ncan be derived in a principled manner as the EM algorithm for maximum\\nlikelihood parameter estimation in latent-variable models.\\n11.4.1 Generative Process and Probabilistic Model\\nTo derive the probabilistic model for GMMs, it is useful to think about the\\ngenerative process, i.e., the process that allows us to generate data, using\\na probabilistic model.\\nWe assume a mixture model with Kcomponents and that a data point\\nxcan be generated by exactly one mixture component. We introduce a\\nbinary indicator variable zk2f0;1gwith two states (see Section 6.2) that\\nindicates whether the kth mixture component generated that data point\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n364 Density Estimation with Gaussian Mixture Models\\nso that\\np(xjzk= 1) =N\\x00xj\\x16k;\\x06k\\x01: (11.58)\\nWe deﬁnez:= [z1;:::;zK]>2RKas a probability vector consisting of\\nK\\x001many 0s and exactly one 1. For example, for K= 3, a validzwould\\nbez= [z1;z2;z3]>= [0;1;0]>, which would select the second mixture\\ncomponent since z2= 1.\\nRemark. Sometimes this kind of probability distribution is called “multi-\\nnoulli”, a generalization of the Bernoulli distribution to more than two\\nvalues (Murphy, 2012). }\\nThe properties of zimply thatPK\\nk=1zk= 1. Therefore,zis aone-hot one-hot encoding\\nencoding (also: 1-of-Krepresentation ). 1-of-K\\nrepresentation Thus far, we assumed that the indicator variables zkare known. How-\\never, in practice, this is not the case, and we place a prior distribution\\np(z) =\\x19= [\\x191;:::;\\x19K]>;KX\\nk=1\\x19k= 1; (11.59)\\non the latent variable z. Then thekth entry\\n\\x19k=p(zk= 1) (11.60)\\nof this probability vector describes the probability that the kth mixture\\ncomponent generated data point x. Figure 11.11\\nGraphical model for\\na GMM with a single\\ndata point.\\n\\x19\\nz\\nx \\x06k\\x16k\\nk= 1;:::;KRemark (Sampling from a GMM) .The construction of this latent-variable\\nmodel (see the corresponding graphical model in Figure 11.11) lends it-\\nself to a very simple sampling procedure (generative process) to generate\\ndata:\\n1. Samplez(i)\\x18p(z).\\n2. Samplex(i)\\x18p(xjz(i)= 1).\\nIn the ﬁrst step, we select a mixture component i(via the one-hot encod-\\ningz) at random according to p(z) =\\x19; in the second step we draw a\\nsample from the corresponding mixture component. When we discard the\\nsamples of the latent variable so that we are left with the x(i), we have\\nvalid samples from the GMM. This kind of sampling, where samples of\\nrandom variables depend on samples from the variable’s parents in the\\ngraphical model, is called ancestral sampling . } ancestral sampling\\nGenerally, a probabilistic model is deﬁned by the joint distribution of\\nthe data and the latent variables (see Section 8.4). With the prior p(z)\\ndeﬁned in (11.59) and (11.60) and the conditional p(xjz)from (11.58),\\nwe obtain all Kcomponents of this joint distribution via\\np(x;zk= 1) =p(xjzk= 1)p(zk= 1) =\\x19kN\\x00xj\\x16k;\\x06k\\x01\\n(11.61)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.4 Latent-Variable Perspective 365\\nfork= 1;:::;K , so that\\np(x;z) =2\\n64p(x;z1= 1)\\n...\\np(x;zK= 1)3\\n75=2\\n64\\x191N\\x00xj\\x161;\\x061\\x01\\n...\\n\\x19KN\\x00xj\\x16K;\\x06K\\x013\\n75; (11.62)\\nwhich fully speciﬁes the probabilistic model.\\n11.4.2 Likelihood\\nTo obtain the likelihood p(xj\\x12)in a latent-variable model, we need to\\nmarginalize out the latent variables (see Section 8.4.3). In our case, this\\ncan be done by summing out all latent variables from the joint p(x;z)\\nin (11.62) so that\\np(xj\\x12) =X\\nzp(xj\\x12;z)p(zj\\x12);\\x12:=f\\x16k;\\x06k;\\x19k:k= 1;:::;Kg:\\n(11.63)\\nWe now explicitly condition on the parameters \\x12of the probabilistic model,\\nwhich we previously omitted. In (11.63), we sum over all Kpossible one-\\nhot encodings of z, which is denoted byP\\nz. Since there is only a single\\nnonzero single entry in each zthere are only Kpossible conﬁgurations/\\nsettings ofz. For example, if K= 3, thenzcan have the conﬁgurations\\n2\\n41\\n0\\n03\\n5;2\\n40\\n1\\n03\\n5;2\\n40\\n0\\n13\\n5: (11.64)\\nSumming over all possible conﬁgurations of zin (11.63) is equivalent to\\nlooking at the nonzero entry of the z-vector and writing\\np(xj\\x12) =X\\nzp(xj\\x12;z)p(zj\\x12) (11.65a)\\n=KX\\nk=1p(xj\\x12;zk= 1)p(zk= 1j\\x12) (11.65b)\\nso that the desired marginal distribution is given as\\np(xj\\x12)(11.65b)=KX\\nk=1p(xj\\x12;zk= 1)p(zk= 1j\\x12) (11.66a)\\n=KX\\nk=1\\x19kN\\x00xj\\x16k;\\x06k\\x01; (11.66b)\\nwhich we identify as the GMM model from (11.3). Given a dataset X, we\\nimmediately obtain the likelihood\\np(Xj\\x12) =NY\\nn=1p(xnj\\x12)(11.66b)=NY\\nn=1KX\\nk=1\\x19kN\\x00xnj\\x16k;\\x06k\\x01;(11.67)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n366 Density Estimation with Gaussian Mixture Models\\nFigure 11.12\\nGraphical model for\\na GMM with Ndata\\npoints.\\x19\\nzn\\nxn \\x06k\\x16k\\nn= 1;:::;Nk= 1;:::;K\\nwhich is exactly the GMM likelihood from (11.9). Therefore, the latent-\\nvariable model with latent indicators zkis an equivalent way of thinking\\nabout a Gaussian mixture model.\\n11.4.3 Posterior Distribution\\nLet us have a brief look at the posterior distribution on the latent variable\\nz. According to Bayes’ theorem, the posterior of the kth component having\\ngenerated data point x\\np(zk= 1jx) =p(zk= 1)p(xjzk= 1)\\np(x); (11.68)\\nwhere the marginal p(x)is given in (11.66b). This yields the posterior\\ndistribution for the kth indicator variable zk\\np(zk= 1jx) =p(zk= 1)p(xjzk= 1)\\nPK\\nj=1p(zj= 1)p(xjzj= 1)=\\x19kN\\x00xj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xj\\x16j;\\x06j\\x01;\\n(11.69)\\nwhich we identify as the responsibility of the kth mixture component for\\ndata pointx. Note that we omitted the explicit conditioning on the GMM\\nparameters \\x19k;\\x16k;\\x06kwherek= 1;:::;K .\\n11.4.4 Extension to a Full Dataset\\nThus far, we have only discussed the case where the dataset consists only\\nof a single data point x. However, the concepts of the prior and posterior\\ncan be directly extended to the case of Ndata pointsX:=fx1;:::;xNg.\\nIn the probabilistic interpretation of the GMM, every data point xnpos-\\nsesses its own latent variable\\nzn= [zn1;:::;znK]>2RK: (11.70)\\nPreviously (when we only considered a single data point x), we omitted\\nthe indexn, but now this becomes important.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.4 Latent-Variable Perspective 367\\nWe share the same prior distribution \\x19across all latent variables zn.\\nThe corresponding graphical model is shown in Figure 11.12, where we\\nuse the plate notation.\\nThe conditional distribution p(x1;:::;xNjz1;:::;zN)factorizes over\\nthe data points and is given as\\np(x1;:::;xNjz1;:::;zN) =NY\\nn=1p(xnjzn): (11.71)\\nTo obtain the posterior distribution p(znk= 1jxn), we follow the same\\nreasoning as in Section 11.4.3 and apply Bayes’ theorem to obtain\\np(znk= 1jxn) =p(xnjznk= 1)p(znk= 1)\\nPK\\nj=1p(xnjznj= 1)p(znj= 1)(11.72a)\\n=\\x19kN\\x00xnj\\x16k;\\x06k\\x01\\nPK\\nj=1\\x19jN\\x00xnj\\x16j;\\x06j\\x01=rnk: (11.72b)\\nThis means that p(zk= 1jxn)is the (posterior) probability that the kth\\nmixture component generated data point xnand corresponds to the re-\\nsponsibility rnkwe introduced in (11.17). Now the responsibilities also\\nhave not only an intuitive but also a mathematically justiﬁed interpreta-\\ntion as posterior probabilities.\\n11.4.5 EM Algorithm Revisited\\nThe EM algorithm that we introduced as an iterative scheme for maximum\\nlikelihood estimation can be derived in a principled way from the latent-\\nvariable perspective. Given a current setting \\x12(t)of model parameters, the\\nE-step calculates the expected log-likelihood\\nQ(\\x12j\\x12(t)) =Ezjx;\\x12(t)[logp(x;zj\\x12)] (11.73a)\\n=Z\\nlogp(x;zj\\x12)p(zjx;\\x12(t))dz; (11.73b)\\nwhere the expectation of logp(x;zj\\x12)is taken with respect to the poste-\\nriorp(zjx;\\x12(t))of the latent variables. The M-step selects an updated set\\nof model parameters \\x12(t+1)by maximizing (11.73b).\\nAlthough an EM iteration does increase the log-likelihood, there are\\nno guarantees that EM converges to the maximum likelihood solution.\\nIt is possible that the EM algorithm converges to a local maximum of\\nthe log-likelihood. Different initializations of the parameters \\x12could be\\nused in multiple EM runs to reduce the risk of ending up in a bad local\\noptimum. We do not go into further details here, but refer to the excellent\\nexpositions by Rogers and Girolami (2016) and Bishop (2006).\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n368 Density Estimation with Gaussian Mixture Models\\n11.5 Further Reading\\nThe GMM can be considered a generative model in the sense that it is\\nstraightforward to generate new data using ancestral sampling (Bishop,\\n2006). For given GMM parameters \\x19k;\\x16k;\\x06k,k= 1;:::;K , we sample\\nan indexkfrom the probability vector [\\x191;:::;\\x19K]>and then sample a\\ndata pointx\\x18N\\x00\\x16k;\\x06k\\x01\\n. If we repeat this Ntimes, we obtain a dataset\\nthat has been generated by a GMM. Figure 11.1 was generated using this\\nprocedure.\\nThroughout this chapter, we assumed that the number of components\\nKis known. In practice, this is often not the case. However, we could use\\nnested cross-validation, as discussed in Section 8.6.1, to ﬁnd good models.\\nGaussian mixture models are closely related to the K-means clustering\\nalgorithm.K-means also uses the EM algorithm to assign data points to\\nclusters. If we treat the means in the GMM as cluster centers and ignore\\nthe covariances (or set them to I), we arrive at K-means. As also nicely\\ndescribed by MacKay (2003), K-means makes a “hard” assignment of data\\npoints to cluster centers \\x16k, whereas a GMM makes a “soft” assignment\\nvia the responsibilities.\\nWe only touched upon the latent-variable perspective of GMMs and the\\nEM algorithm. Note that EM can be used for parameter learning in general\\nlatent-variable models, e.g., nonlinear state-space models (Ghahramani\\nand Roweis, 1999; Roweis and Ghahramani, 1999) and for reinforcement\\nlearning as discussed by Barber (2012). Therefore, the latent-variable per-\\nspective of a GMM is useful to derive the corresponding EM algorithm in\\na principled way (Bishop, 2006; Barber, 2012; Murphy, 2012).\\nWe only discussed maximum likelihood estimation (via the EM algo-\\nrithm) for ﬁnding GMM parameters. The standard criticisms of maximum\\nlikelihood also apply here:\\nAs in linear regression, maximum likelihood can suffer from severe\\noverﬁtting. In the GMM case, this happens when the mean of a mix-\\nture component is identical to a data point and the covariance tends to\\n0. Then, the likelihood approaches inﬁnity. Bishop (2006) and Barber\\n(2012) discuss this issue in detail.\\nWe only obtain a point estimate of the parameters \\x19k;\\x16k;\\x06kfork=\\n1;:::;K , which does not give any indication of uncertainty in the pa-\\nrameter values. A Bayesian approach would place a prior on the param-\\neters, which can be used to obtain a posterior distribution on the param-\\neters. This posterior allows us to compute the model evidence (marginal\\nlikelihood), which can be used for model comparison, which gives us a\\nprincipled way to determine the number of mixture components. Un-\\nfortunately, closed-form inference is not possible in this setting because\\nthere is no conjugate prior for this model. However, approximations,\\nsuch as variational inference, can be used to obtain an approximate\\nposterior (Bishop, 2006).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n11.5 Further Reading 369\\nFigure 11.13\\nHistogram (orange\\nbars) and kernel\\ndensity estimation\\n(blue line). The\\nkernel density\\nestimator produces\\na smooth estimate\\nof the underlying\\ndensity, whereas the\\nhistogram is an\\nunsmoothed count\\nmeasure of how\\nmany data points\\n(black) fall into a\\nsingle bin.\\n\\x004\\x002 0 2 4 6 8\\nx0:000:050:100:150:200:250:30p(x)\\nData\\nKDE\\nHistogram In this chapter, we discussed mixture models for density estimation.\\nThere is a plethora of density estimation techniques available. In practice,\\nwe often use histograms and kernel density estimation. histogram\\nHistograms provide a nonparametric way to represent continuous den-\\nsities and have been proposed by Pearson (1895). A histogram is con-\\nstructed by “binning” the data space and count, how many data points fall\\ninto each bin. Then a bar is drawn at the center of each bin, and the height\\nof the bar is proportional to the number of data points within that bin. The\\nbin size is a critical hyperparameter, and a bad choice can lead to overﬁt-\\nting and underﬁtting. Cross-validation, as discussed in Section 8.2.4, can\\nbe used to determine a good bin size. kernel density\\nestimation Kernel density estimation , independently proposed by Rosenblatt (1956)\\nand Parzen (1962), is a nonparametric way for density estimation. Given\\nNi.i.d. samples, the kernel density estimator represents the underlying\\ndistribution as\\np(x) =1\\nNhNX\\nn=1k\\x12x\\x00xn\\nh\\x13\\n; (11.74)\\nwherekis a kernel function, i.e., a nonnegative function that integrates to\\n1andh >0is a smoothing/bandwidth parameter, which plays a similar\\nrole as the bin size in histograms. Note that we place a kernel on every\\nsingle data point xnin the dataset. Commonly used kernel functions are\\nthe uniform distribution and the Gaussian distribution. Kernel density esti-\\nmates are closely related to histograms, but by choosing a suitable kernel,\\nwe can guarantee smoothness of the density estimate. Figure 11.13 illus-\\ntrates the difference between a histogram and a kernel density estimator\\n(with a Gaussian-shaped kernel) for a given dataset of 250data points.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n12\\nClassiﬁcation with Support Vector Machines\\nIn many situations, we want our machine learning algorithm to predict\\none of a number of (discrete) outcomes. For example, an email client sorts\\nmail into personal mail and junk mail, which has two outcomes. Another\\nexample is a telescope that identiﬁes whether an object in the night sky\\nis a galaxy, star, or planet. There are usually a small number of outcomes,\\nand more importantly there is usually no additional structure on these\\noutcomes. In this chapter, we consider predictors that output binary val- An example of\\nstructure is if the\\noutcomes were\\nordered, like in the\\ncase of small,\\nmedium, and large\\nt-shirts.ues, i.e., there are only two possible outcomes. This machine learning task\\nis called binary classiﬁcation . This is in contrast to Chapter 9, where we\\nbinary classiﬁcationconsidered a prediction problem with continuous-valued outputs.\\nFor binary classiﬁcation, the set of possible values that the label/output\\ncan attain is binary, and for this chapter we denote them by f+1;\\x001g. In\\nother words, we consider predictors of the form\\nf:RD!f+1;\\x001g: (12.1)\\nRecall from Chapter 8 that we represent each example (data point) xn\\nas a feature vector of Dreal numbers. The labels are often referred to as Input example xn\\nmay also be referred\\nto as inputs, data\\npoints, features, or\\ninstances.the positive and negative classes , respectively. One should be careful not\\nclassto infer intuitive attributes of positiveness of the +1class. For example,\\nin a cancer detection task, a patient with cancer is often labeled +1. In\\nprinciple, any two distinct values can be used, e.g., fTrue;Falseg,f0;1g\\norfred;blueg. The problem of binary classiﬁcation is well studied, and For probabilistic\\nmodels, it is\\nmathematically\\nconvenient to use\\nf0;1gas a binary\\nrepresentation; see\\nthe remark after\\nExample 6.12.we defer a survey of other approaches to Section 12.6.\\nWe present an approach known as the support vector machine (SVM),\\nwhich solves the binary classiﬁcation task. As in regression, we have a su-\\npervised learning task, where we have a set of examples xn2RDalong\\nwith their corresponding (binary) labels yn2f+1;\\x001g. Given a train-\\ning data set consisting of example–label pairs f(x1;y1);:::; (xN;yN)g, we\\nwould like to estimate parameters of the model that will give the smallest\\nclassiﬁcation error. Similar to Chapter 9, we consider a linear model, and\\nhide away the nonlinearity in a transformation \\x1eof the examples (9.13).\\nWe will revisit \\x1ein Section 12.4.\\nThe SVM provides state-of-the-art results in many applications, with\\nsound theoretical guarantees (Steinwart and Christmann, 2008). There\\nare two main reasons why we chose to illustrate binary classiﬁcation using\\n370\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\nClassiﬁcation with Support Vector Machines 371\\nFigure 12.1\\nExample 2D data,\\nillustrating the\\nintuition of data\\nwhere we can ﬁnd a\\nlinear classiﬁer that\\nseparates orange\\ncrosses from blue\\ndiscs.\\nx(1)x(2)\\nSVMs. First, the SVM allows for a geometric way to think about supervised\\nmachine learning. While in Chapter 9 we considered the machine learning\\nproblem in terms of probabilistic models and attacked it using maximum\\nlikelihood estimation and Bayesian inference, here we will consider an\\nalternative approach where we reason geometrically about the machine\\nlearning task. It relies heavily on concepts, such as inner products and\\nprojections, which we discussed in Chapter 3. The second reason why we\\nﬁnd SVMs instructive is that in contrast to Chapter 9, the optimization\\nproblem for SVM does not admit an analytic solution so that we need to\\nresort to a variety of optimization tools introduced in Chapter 7.\\nThe SVM view of machine learning is subtly different from the max-\\nimum likelihood view of Chapter 9. The maximum likelihood view pro-\\nposes a model based on a probabilistic view of the data distribution, from\\nwhich an optimization problem is derived. In contrast, the SVM view starts\\nby designing a particular function that is to be optimized during training,\\nbased on geometric intuitions. We have seen something similar already\\nin Chapter 10, where we derived PCA from geometric principles. In the\\nSVM case, we start by designing a loss function that is to be minimized\\non training data, following the principles of empirical risk minimization\\n(Section 8.2).\\nLet us derive the optimization problem corresponding to training an\\nSVM on example–label pairs. Intuitively, we imagine binary classiﬁcation\\ndata, which can be separated by a hyperplane as illustrated in Figure 12.1.\\nHere, every example xn(a vector of dimension 2) is a two-dimensional\\nlocation (x(1)\\nnandx(2)\\nn), and the corresponding binary label ynis one of\\ntwo different symbols (orange cross or blue disc). “Hyperplane” is a word\\nthat is commonly used in machine learning, and we encountered hyper-\\nplanes already in Section 2.8. A hyperplane is an afﬁne subspace of di-\\nmensionD\\x001(if the corresponding vector space is of dimension D).\\nThe examples consist of two classes (there are two possible labels) that\\nhave features (the components of the vector representing the example)\\narranged in such a way as to allow us to separate/classify them by draw-\\ning a straight line.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n372 Classiﬁcation with Support Vector Machines\\nIn the following, we formalize the idea of ﬁnding a linear separator\\nof the two classes. We introduce the idea of the margin and then extend\\nlinear separators to allow for examples to fall on the “wrong” side, incur-\\nring a classiﬁcation error. We present two equivalent ways of formalizing\\nthe SVM: the geometric view (Section 12.2.4) and the loss function view\\n(Section 12.2.5). We derive the dual version of the SVM using Lagrange\\nmultipliers (Section 7.2). The dual SVM allows us to observe a third way\\nof formalizing the SVM: in terms of the convex hulls of the examples of\\neach class (Section 12.3.2). We conclude by brieﬂy describing kernels and\\nhow to numerically solve the nonlinear kernel-SVM optimization problem.\\n12.1 Separating Hyperplanes\\nGiven two examples represented as vectors xiandxj, one way to compute\\nthe similarity between them is using an inner product hxi;xji. Recall from\\nSection 3.2 that inner products are closely related to the angle between\\ntwo vectors. The value of the inner product between two vectors depends\\non the length (norm) of each vector. Furthermore, inner products allow\\nus to rigorously deﬁne geometric concepts such as orthogonality and pro-\\njections.\\nThe main idea behind many classiﬁcation algorithms is to represent\\ndata in RDand then partition this space, ideally in a way that examples\\nwith the same label (and no other examples) are in the same partition.\\nIn the case of binary classiﬁcation, the space would be divided into two\\nparts corresponding to the positive and negative classes, respectively. We\\nconsider a particularly convenient partition, which is to (linearly) split\\nthe space into two halves using a hyperplane. Let example x2RDbe an\\nelement of the data space. Consider a function\\nf:RD!R (12.2a)\\nx7!f(x) :=hw;xi+b; (12.2b)\\nparametrized by w2RDandb2R. Recall from Section 2.8 that hy-\\nperplanes are afﬁne subspaces. Therefore, we deﬁne the hyperplane that\\nseparates the two classes in our binary classiﬁcation problem as\\n\\x08x2RD:f(x) = 0\\t: (12.3)\\nAn illustration of the hyperplane is shown in Figure 12.2, where the\\nvectorwis a vector normal to the hyperplane and bthe intercept. We can\\nderive thatwis a normal vector to the hyperplane in (12.3) by choosing\\nany two examples xaandxbon the hyperplane and showing that the\\nvector between them is orthogonal to w. In the form of an equation,\\nf(xa)\\x00f(xb) =hw;xai+b\\x00(hw;xbi+b) (12.4a)\\n=hw;xa\\x00xbi; (12.4b)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.1 Separating Hyperplanes 373\\nFigure 12.2\\nEquation of a\\nseparating\\nhyperplane (12.3).\\n(a) The standard\\nway of representing\\nthe equation in 3D.\\n(b) For ease of\\ndrawing, we look at\\nthe hyperplane edge\\non.w\\n(a) Separating hyperplane in 3Dw\\n.\\n0.Positive\\n.\\nNegativeb\\n(b) Projection of the setting in (a) onto\\na plane\\nwhere the second line is obtained by the linearity of the inner product\\n(Section 3.2). Since we have chosen xaandxbto be on the hyperplane,\\nthis implies that f(xa) = 0 andf(xb) = 0 and hencehw;xa\\x00xbi= 0.\\nRecall that two vectors are orthogonal when their inner product is zero. wis orthogonal to\\nany vector on the\\nhyperplane.Therefore, we obtain that wis orthogonal to any vector on the hyperplane.\\nRemark. Recall from Chapter 2 that we can think of vectors in different\\nways. In this chapter, we think of the parameter vector was an arrow\\nindicating a direction, i.e., we consider wto be a geometric vector. In\\ncontrast, we think of the example vector xas a data point (as indicated\\nby its coordinates), i.e., we consider xto be the coordinates of a vector\\nwith respect to the standard basis. }\\nWhen presented with a test example, we classify the example as pos-\\nitive or negative depending on the side of the hyperplane on which it\\noccurs. Note that (12.3) not only deﬁnes a hyperplane; it additionally de-\\nﬁnes a direction. In other words, it deﬁnes the positive and negative side\\nof the hyperplane. Therefore, to classify a test example xtest, we calcu-\\nlate the value of the function f(xtest)and classify the example as +1if\\nf(xtest)>0and\\x001otherwise. Thinking geometrically, the positive ex-\\namples lie “above” the hyperplane and the negative examples “below” the\\nhyperplane.\\nWhen training the classiﬁer, we want to ensure that the examples with\\npositive labels are on the positive side of the hyperplane, i.e.,\\nhw;xni+b>0 whenyn= +1 (12.5)\\nand the examples with negative labels are on the negative side, i.e.,\\nhw;xni+b<0 whenyn=\\x001: (12.6)\\nRefer to Figure 12.2 for a geometric intuition of positive and negative\\nexamples. These two conditions are often presented in a single equation\\nyn(hw;xni+b)>0: (12.7)\\nEquation (12.7) is equivalent to (12.5) and (12.6) when we multiply both\\nsides of (12.5) and (12.6) with yn= 1andyn=\\x001, respectively.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n374 Classiﬁcation with Support Vector Machines\\nFigure 12.3\\nPossible separating\\nhyperplanes. There\\nare many linear\\nclassiﬁers (green\\nlines) that separate\\norange crosses from\\nblue discs.\\nx(1)x(2)\\n12.2 Primal Support Vector Machine\\nBased on the concept of distances from points to a hyperplane, we now\\nare in a position to discuss the support vector machine. For a dataset\\nf(x1;y1);:::; (xN;yN)gthat is linearly separable, we have inﬁnitely many\\ncandidate hyperplanes (refer to Figure 12.3), and therefore classiﬁers,\\nthat solve our classiﬁcation problem without any (training) errors. To ﬁnd\\na unique solution, one idea is to choose the separating hyperplane that\\nmaximizes the margin between the positive and negative examples. In\\nother words, we want the positive and negative examples to be separated\\nby a large margin (Section 12.2.1). In the following, we compute the dis- A classiﬁer with\\nlarge margin turns\\nout to generalize\\nwell (Steinwart and\\nChristmann, 2008).tance between an example and a hyperplane to derive the margin. Recall\\nthat the closest point on the hyperplane to a given point (example xn) is\\nobtained by the orthogonal projection (Section 3.8).\\n12.2.1 Concept of the Margin\\nThe concept of the margin is intuitively simple: It is the distance of the margin\\nseparating hyperplane to the closest examples in the dataset, assuming There could be two\\nor more closest\\nexamples to a\\nhyperplane.that the dataset is linearly separable. However, when trying to formalize\\nthis distance, there is a technical wrinkle that may be confusing. The tech-\\nnical wrinkle is that we need to deﬁne a scale at which to measure the\\ndistance. A potential scale is to consider the scale of the data, i.e., the raw\\nvalues ofxn. There are problems with this, as we could change the units\\nof measurement of xnand change the values in xn, and, hence, change\\nthe distance to the hyperplane. As we will see shortly, we deﬁne the scale\\nbased on the equation of the hyperplane (12.3) itself.\\nConsider a hyperplane hw;xi+b, and an example xaas illustrated in\\nFigure 12.4. Without loss of generality, we can consider the example xa\\nto be on the positive side of the hyperplane, i.e., hw;xai+b > 0. We\\nwould like to compute the distance r >0ofxafrom the hyperplane. We\\ndo so by considering the orthogonal projection (Section 3.8) of xaonto\\nthe hyperplane, which we denote by x0\\na. Sincewis orthogonal to the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.2 Primal Support Vector Machine 375\\nFigure 12.4 Vector\\naddition to express\\ndistance to\\nhyperplane:\\nxa=x0\\na+rw\\nkwk.\\n.0.xa\\nw.x0\\nar\\nhyperplane, we know that the distance ris just a scaling of this vector w.\\nIf the length of wis known, then we can use this scaling factor rfactor\\nto work out the absolute distance between xaandx0\\na. For convenience,\\nwe choose to use a vector of unit length (its norm is 1) and obtain this\\nby dividingwby its norm,w\\nkwk. Using vector addition (Section 2.4), we\\nobtain\\nxa=x0\\na+rw\\nkwk: (12.8)\\nAnother way of thinking about ris that it is the coordinate of xain the\\nsubspace spanned by w=kwk. We have now expressed the distance of xa\\nfrom the hyperplane as r, and if we choose xato be the point closest to\\nthe hyperplane, this distance ris the margin.\\nRecall that we would like the positive examples to be further than r\\nfrom the hyperplane, and the negative examples to be further than dis-\\ntancer(in the negative direction) from the hyperplane. Analogously to\\nthe combination of (12.5) and (12.6) into (12.7), we formulate this ob-\\njective as\\nyn(hw;xni+b)>r: (12.9)\\nIn other words, we combine the requirements that examples are at least\\nraway from the hyperplane (in the positive and negative direction) into\\none single inequality.\\nSince we are interested only in the direction, we add an assumption to\\nour model that the parameter vector wis of unit length, i.e., kwk= 1,\\nwhere we use the Euclidean norm kwk=p\\nw>w(Section 3.1). This We will see other\\nchoices of inner\\nproducts\\n(Section 3.2) in\\nSection 12.4.assumption also allows a more intuitive interpretation of the distance r\\n(12.8) since it is the scaling factor of a vector of length 1.\\nRemark. A reader familiar with other presentations of the margin would\\nnotice that our deﬁnition of kwk= 1 is different from the standard\\npresentation if the SVM was the one provided by Sch ¨olkopf and Smola\\n(2002), for example. In Section 12.2.3, we will show the equivalence of\\nboth approaches. }\\nCollecting the three requirements into a single constrained optimization\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n376 Classiﬁcation with Support Vector Machines\\nFigure 12.5\\nDerivation of the\\nmargin:r=1\\nkwk..xa\\nw\\nhw;xi+\\nb= 0hw;xi+\\nb= 1.x0\\nar\\nproblem, we obtain the objective\\nmax\\nw;b;rr|{z}\\nmargin\\nsubject to yn(hw;xni+b)>r|{z}\\ndata ﬁtting;kwk= 1|{z}\\nnormalization; r> 0;(12.10)\\nwhich says that we want to maximize the margin rwhile ensuring that\\nthe data lies on the correct side of the hyperplane.\\nRemark. The concept of the margin turns out to be highly pervasive in ma-\\nchine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis\\nto show that when the margin is large, the “complexity” of the function\\nclass is low, and hence learning is possible (Vapnik, 2000). It turns out\\nthat the concept is useful for various different approaches for theoret-\\nically analyzing generalization error (Steinwart and Christmann, 2008;\\nShalev-Shwartz and Ben-David, 2014). }\\n12.2.2 Traditional Derivation of the Margin\\nIn the previous section, we derived (12.10) by making the observation that\\nwe are only interested in the direction of wand not its length, leading to\\nthe assumption that kwk= 1. In this section, we derive the margin max-\\nimization problem by making a different assumption. Instead of choosing\\nthat the parameter vector is normalized, we choose a scale for the data.\\nWe choose this scale such that the value of the predictor hw;xi+bis1at\\nthe closest example. Let us also denote the example in the dataset that is Recall that we\\ncurrently consider\\nlinearly separable\\ndata.closest to the hyperplane by xa.\\nFigure 12.5 is identical to Figure 12.4, except that now we rescaled the\\naxes, such that the example xalies exactly on the margin, i.e., hw;xai+\\nb= 1. Sincex0\\nais the orthogonal projection of xaonto the hyperplane, it\\nmust by deﬁnition lie on the hyperplane, i.e.,\\nhw;x0\\nai+b= 0: (12.11)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.2 Primal Support Vector Machine 377\\nBy substituting (12.8) into (12.11), we obtain\\n\\x1c\\nw;xa\\x00rw\\nkwk\\x1d\\n+b= 0: (12.12)\\nExploiting the bilinearity of the inner product (see Section 3.2), we get\\nhw;xai+b\\x00rhw;wi\\nkwk= 0: (12.13)\\nObserve that the ﬁrst term is 1by our assumption of scale, i.e., hw;xai+\\nb= 1. From (3.16) in Section 3.1, we know that hw;wi=kwk2. Hence,\\nthe second term reduces to rkwk. Using these simpliﬁcations, we obtain\\nr=1\\nkwk: (12.14)\\nThis means we derived the distance rin terms of the normal vector w\\nof the hyperplane. At ﬁrst glance, this equation is counterintuitive as we We can also think of\\nthe distance as the\\nprojection error that\\nincurs when\\nprojectingxaonto\\nthe hyperplane.seem to have derived the distance from the hyperplane in terms of the\\nlength of the vector w, but we do not yet know this vector. One way to\\nthink about it is to consider the distance rto be a temporary variable\\nthat we only use for this derivation. Therefore, for the rest of this section\\nwe will denote the distance to the hyperplane by1\\nkwk. In Section 12.2.3,\\nwe will see that the choice that the margin equals 1is equivalent to our\\nprevious assumption of kwk= 1in Section 12.2.1.\\nSimilar to the argument to obtain (12.9), we want the positive and\\nnegative examples to be at least 1away from the hyperplane, which yields\\nthe condition\\nyn(hw;xni+b)>1: (12.15)\\nCombining the margin maximization with the fact that examples need to\\nbe on the correct side of the hyperplane (based on their labels) gives us\\nmax\\nw;b1\\nkwk(12.16)\\nsubject toyn(hw;xni+b)>1 for all n= 1;:::;N: (12.17)\\nInstead of maximizing the reciprocal of the norm as in (12.16), we often\\nminimize the squared norm. We also often include a constant1\\n2that does The squared norm\\nresults in a convex\\nquadratic\\nprogramming\\nproblem for the\\nSVM (Section 12.5).not affect the optimal w;bbut yields a tidier form when we compute the\\ngradient. Then, our objective becomes\\nmin\\nw;b1\\n2kwk2(12.18)\\nsubject toyn(hw;xni+b)>1 for all n= 1;:::;N: (12.19)\\nEquation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM\\nexpression “hard” is because the formulation does not allow for any vi-\\nolations of the margin condition. We will see in Section 12.2.4 that this\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n378 Classiﬁcation with Support Vector Machines\\n“hard” condition can be relaxed to accommodate violations if the data is\\nnot linearly separable.\\n12.2.3 Why We Can Set the Margin to 1\\nIn Section 12.2.1, we argued that we would like to maximize some value\\nr, which represents the distance of the closest example to the hyperplane.\\nIn Section 12.2.2, we scaled the data such that the closest example is of\\ndistance 1to the hyperplane. In this section, we relate the two derivations,\\nand show that they are equivalent.\\nTheorem 12.1. Maximizing the margin r, where we consider normalized\\nweights as in (12.10) ,\\nmax\\nw;b;rr|{z}\\nmargin\\nsubject to yn(hw;xni+b)>r|{z}\\ndata ﬁtting;kwk= 1|{z}\\nnormalization; r> 0;(12.20)\\nis equivalent to scaling the data, such that the margin is unity:\\nmin\\nw;b1\\n2kwk2\\n|{z}\\nmargin\\nsubject to yn(hw;xni+b)>1|{z}\\ndata ﬁtting:(12.21)\\nProof Consider (12.20). Since the square is a strictly monotonic trans-\\nformation for non-negative arguments, the maximum stays the same if we\\nconsiderr2in the objective. Since kwk= 1 we can reparametrize the\\nequation with a new weight vector w0that is not normalized by explicitly\\nusingw0\\nkw0k. We obtain\\nmax\\nw0;b;rr2\\nsubject to yn\\x12\\x1cw0\\nkw0k;xn\\x1d\\n+b\\x13\\n>r; r> 0:(12.22)\\nEquation (12.22) explicitly states that the distance ris positive. Therefore,\\nwe can divide the ﬁrst constraint by r, which yields Note thatr>0\\nbecause we\\nassumed linear\\nseparability, and\\nhence there is no\\nissue to divide by r.max\\nw0;b;rr2\\nsubject to yn0\\nBBB@*\\nw0\\nkw0kr|{z}\\nw00;xn+\\n+b\\nr|{z}\\nb001\\nCCCA>1; r> 0(12.23)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.2 Primal Support Vector Machine 379\\nFigure 12.6\\n(a) Linearly\\nseparable and\\n(b) non-linearly\\nseparable data.\\nx(1)x(2)\\n(a) Linearly separable data, with a large\\nmargin\\nx(1)x(2)(b) Non-linearly separable data\\nrenaming the parameters to w00andb00. Sincew00=w0\\nkw0kr, rearranging for\\nrgives\\nkw00k=\\r\\r\\r\\rw0\\nkw0kr\\r\\r\\r\\r=1\\nr\\x01\\r\\r\\r\\rw0\\nkw0k\\r\\r\\r\\r=1\\nr: (12.24)\\nBy substituting this result into (12.23), we obtain\\nmax\\nw00;b001\\nkw00k2\\nsubject to yn(hw00;xni+b00)>1:(12.25)\\nThe ﬁnal step is to observe that maximizing1\\nkw00k2yields the same solution\\nas minimizing1\\n2kw00k2, which concludes the proof of Theorem 12.1.\\n12.2.4 Soft Margin SVM: Geometric View\\nIn the case where data is not linearly separable, we may wish to allow\\nsome examples to fall within the margin region, or even to be on the\\nwrong side of the hyperplane as illustrated in Figure 12.6.\\nThe model that allows for some classiﬁcation errors is called the soft soft margin SVM\\nmargin SVM . In this section, we derive the resulting optimization problem\\nusing geometric arguments. In Section 12.2.5, we will derive an equiv-\\nalent optimization problem using the idea of a loss function. Using La-\\ngrange multipliers (Section 7.2), we will derive the dual optimization\\nproblem of the SVM in Section 12.3. This dual optimization problem al-\\nlows us to observe a third interpretation of the SVM: as a hyperplane that\\nbisects the line between convex hulls corresponding to the positive and\\nnegative data examples (Section 12.3.2).\\nThe key geometric idea is to introduce a slack variable \\x18ncorresponding slack variable\\nto each example–label pair (xn;yn)that allows a particular example to be\\nwithin the margin or even on the wrong side of the hyperplane (refer to\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n380 Classiﬁcation with Support Vector Machines\\nFigure 12.7 Soft\\nmargin SVM allows\\nexamples to be\\nwithin the margin or\\non the wrong side of\\nthe hyperplane. The\\nslack variable \\x18\\nmeasures the\\ndistance of a\\npositive example\\nx+to the positive\\nmargin hyperplane\\nhw;xi+b= 1\\nwhenx+is on the\\nwrong side..x+w\\nhw;xi+\\nb= 0hw;xi+\\nb= 1.\\n\\x18\\nFigure 12.7). We subtract the value of \\x18nfrom the margin, constraining\\n\\x18nto be non-negative. To encourage correct classiﬁcation of the samples,\\nwe add\\x18nto the objective\\nmin\\nw;b;\\x181\\n2kwk2+CNX\\nn=1\\x18n (12.26a)\\nsubject to yn(hw;xni+b)>1\\x00\\x18n (12.26b)\\n\\x18n>0 (12.26c)\\nforn= 1;:::;N . In contrast to the optimization problem (12.18) for the\\nhard margin SVM, this one is called the soft margin SVM . The parameter soft margin SVM\\nC > 0trades off the size of the margin and the total amount of slack that\\nwe have. This parameter is called the regularization parameter since, as regularization\\nparameter we will see in the following section, the margin term in the objective func-\\ntion (12.26a) is a regularization term. The margin term kwk2is called\\ntheregularizer , and in many books on numerical optimization, the reg- regularizer\\nularization parameter is multiplied with this term (Section 8.2.3). This\\nis in contrast to our formulation in this section. Here a large value of C\\nimplies low regularization, as we give the slack variables larger weight,\\nhence giving more priority to examples that do not lie on the correct side\\nof the margin. There are\\nalternative\\nparametrizations of\\nthis regularization,\\nwhich is\\nwhy (12.26a) is also\\noften referred to as\\ntheC-SVM.Remark. In the formulation of the soft margin SVM (12.26a) wis reg-\\nularized, but bis not regularized. We can see this by observing that the\\nregularization term does not contain b. The unregularized term bcom-\\nplicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\\nand decreases computational efﬁciency (Fan et al., 2008). }\\n12.2.5 Soft Margin SVM: Loss Function View\\nLet us consider a different approach for deriving the SVM, following the\\nprinciple of empirical risk minimization (Section 8.2). For the SVM, we\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.2 Primal Support Vector Machine 381\\nchoose hyperplanes as the hypothesis class, that is\\nf(x) =hw;xi+b: (12.27)\\nWe will see in this section that the margin corresponds to the regulariza-\\ntion term. The remaining question is, what is the loss function ? In con- loss function\\ntrast to Chapter 9, where we consider regression problems (the output\\nof the predictor is a real number), in this chapter, we consider binary\\nclassiﬁcation problems (the output of the predictor is one of two labels\\nf+1;\\x001g). Therefore, the error/loss function for each single example–\\nlabel pair needs to be appropriate for binary classiﬁcation. For example,\\nthe squared loss that is used for regression (9.10b) is not suitable for bi-\\nnary classiﬁcation.\\nRemark. The ideal loss function between binary labels is to count the num-\\nber of mismatches between the prediction and the label. This means that\\nfor a predictor fapplied to an example xn, we compare the output f(xn)\\nwith the label yn. We deﬁne the loss to be zero if they match, and one if\\nthey do not match. This is denoted by 1(f(xn)6=yn)and is called the\\nzero-one loss . Unfortunately, the zero-one loss results in a combinatorial zero-one loss\\noptimization problem for ﬁnding the best parameters w;b. Combinatorial\\noptimization problems (in contrast to continuous optimization problems\\ndiscussed in Chapter 7) are in general more challenging to solve. }\\nWhat is the loss function corresponding to the SVM? Consider the error\\nbetween the output of a predictor f(xn)and the label yn. The loss de-\\nscribes the error that is made on the training data. An equivalent way to\\nderive (12.26a) is to use the hinge loss hinge loss\\n`(t) = maxf0;1\\x00tgwheret=yf(x) =y(hw;xi+b):(12.28)\\nIff(x)is on the correct side (based on the corresponding label y) of the\\nhyperplane, and further than distance 1, this means that t>1and the\\nhinge loss returns a value of zero. If f(x)is on the correct side but too\\nclose to the hyperplane ( 0<t< 1), the example xis within the margin,\\nand the hinge loss returns a positive value. When the example is on the\\nwrong side of the hyperplane ( t<0), the hinge loss returns an even larger\\nvalue, which increases linearly. In other words, we pay a penalty once we\\nare closer than the margin to the hyperplane, even if the prediction is\\ncorrect, and the penalty increases linearly. An alternative way to express\\nthe hinge loss is by considering it as two linear pieces\\n`(t) =(\\n0 ift>1\\n1\\x00tift<1; (12.29)\\nas illustrated in Figure 12.8. The loss corresponding to the hard margin\\nSVM 12.18 is deﬁned as\\n`(t) =(\\n0 ift>1\\n1ift<1: (12.30)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n382 Classiﬁcation with Support Vector Machines\\nFigure 12.8 The\\nhinge loss is a\\nconvex upper bound\\nof zero-one loss.\\n−2 0 2\\nt024max{0,1−t}Zero-one loss\\nHinge loss\\nThis loss can be interpreted as never allowing any examples inside the\\nmargin.\\nFor a given training set f(x1;y1);:::; (xN;yN)g, we seek to minimize\\nthe total loss, while regularizing the objective with `2-regularization (see\\nSection 8.2.3). Using the hinge loss (12.28) gives us the unconstrained\\noptimization problem\\nmin\\nw;b1\\n2kwk2\\n|{z}\\nregularizer+CNX\\nn=1maxf0;1\\x00yn(hw;xni+b)g\\n| {z }\\nerror term: (12.31)\\nThe ﬁrst term in (12.31) is called the regularization term or the regularizer regularizer\\n(see Section 8.2.3), and the second term is called the loss term or the error loss term\\nerror termterm. Recall from Section 12.2.4 that the term1\\n2kwk2arises directly from\\nthe margin. In other words, margin maximization can be interpreted as\\nregularization . regularization\\nIn principle, the unconstrained optimization problem in (12.31) can\\nbe directly solved with (sub-)gradient descent methods as described in\\nSection 7.1. To see that (12.31) and (12.26a) are equivalent, observe that\\nthe hinge loss (12.28) essentially consists of two linear parts, as expressed\\nin (12.29). Consider the hinge loss for a single example-label pair (12.28).\\nWe can equivalently replace minimization of the hinge loss over twith a\\nminimization of a slack variable \\x18with two constraints. In equation form,\\nmin\\ntmaxf0;1\\x00tg (12.32)\\nis equivalent to\\nmin\\n\\x18;t\\x18\\nsubject to \\x18>0; \\x18>1\\x00t:(12.33)\\nBy substituting this expression into (12.31) and rearranging one of the\\nconstraints, we obtain exactly the soft margin SVM (12.26a).\\nRemark. Let us contrast our choice of the loss function in this section to the\\nloss function for linear regression in Chapter 9. Recall from Section 9.2.1\\nthat for ﬁnding maximum likelihood estimators, we usually minimize the\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.3 Dual Support Vector Machine 383\\nnegative log-likelihood. Furthermore, since the likelihood term for linear\\nregression with Gaussian noise is Gaussian, the negative log-likelihood for\\neach example is a squared error function. The squared error function is the\\nloss function that is minimized when looking for the maximum likelihood\\nsolution. }\\n12.3 Dual Support Vector Machine\\nThe description of the SVM in the previous sections, in terms of the vari-\\nableswandb, is known as the primal SVM. Recall that we consider inputs\\nx2RDwithDfeatures. Since wis of the same dimension as x, this\\nmeans that the number of parameters (the dimension of w) of the opti-\\nmization problem grows linearly with the number of features.\\nIn the following, we consider an equivalent optimization problem (the\\nso-called dual view), which is independent of the number of features. In-\\nstead, the number of parameters increases with the number of examples\\nin the training set. We saw a similar idea appear in Chapter 10, where we\\nexpressed the learning problem in a way that does not scale with the num-\\nber of features. This is useful for problems where we have more features\\nthan the number of examples in the training dataset. The dual SVM also\\nhas the additional advantage that it easily allows kernels to be applied,\\nas we shall see at the end of this chapter. The word “dual” appears often\\nin mathematical literature, and in this particular case it refers to convex\\nduality. The following subsections are essentially an application of convex\\nduality, which we discussed in Section 7.2.\\n12.3.1 Convex Duality via Lagrange Multipliers\\nRecall the primal soft margin SVM (12.26a). We call the variables w,b,\\nand\\x18corresponding to the primal SVM the primal variables. We use \\x0bn> In Chapter 7, we\\nused\\x15as Lagrange\\nmultipliers. In this\\nsection, we follow\\nthe notation\\ncommonly chosen in\\nSVM literature, and\\nuse\\x0band\\r.0as the Lagrange multiplier corresponding to the constraint (12.26b) that\\nthe examples are classiﬁed correctly and \\rn>0as the Lagrange multi-\\nplier corresponding to the non-negativity constraint of the slack variable;\\nsee (12.26c). The Lagrangian is then given by\\nL(w;b;\\x18;\\x0b;\\r ) =1\\n2kwk2+CNX\\nn=1\\x18n (12.34)\\n\\x00NX\\nn=1\\x0bn(yn(hw;xni+b)\\x001 +\\x18n)\\n| {z }\\nconstraint (12.26b)\\x00NX\\nn=1\\rn\\x18n\\n|{z}\\nconstraint (12.26c):\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n384 Classiﬁcation with Support Vector Machines\\nBy differentiating the Lagrangian (12.34) with respect to the three primal\\nvariablesw,b, and\\x18respectively, we obtain\\n@L\\n@w=w>\\x00NX\\nn=1\\x0bnynxn>; (12.35)\\n@L\\n@b=\\x00NX\\nn=1\\x0bnyn; (12.36)\\n@L\\n@\\x18n=C\\x00\\x0bn\\x00\\rn: (12.37)\\nWe now ﬁnd the maximum of the Lagrangian by setting each of these\\npartial derivatives to zero. By setting (12.35) to zero, we ﬁnd\\nw=NX\\nn=1\\x0bnynxn; (12.38)\\nwhich is a particular instance of the representer theorem (Kimeldorf and representer theorem\\nWahba, 1970). Equation (12.38) states that the optimal weight vector in The representer\\ntheorem is actually\\na collection of\\ntheorems saying\\nthat the solution of\\nminimizing\\nempirical risk lies in\\nthe subspace\\n(Section 2.4.3)\\ndeﬁned by the\\nexamples.the primal is a linear combination of the examples xn. Recall from Sec-\\ntion 2.6.1 that this means that the solution of the optimization problem\\nlies in the span of training data. Additionally, the constraint obtained by\\nsetting (12.36) to zero implies that the optimal weight vector is an afﬁne\\ncombination of the examples. The representer theorem turns out to hold\\nfor very general settings of regularized empirical risk minimization (Hof-\\nmann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\\ngeneral versions (Sch ¨olkopf et al., 2001), and necessary and sufﬁcient\\nconditions on its existence can be found in Yu et al. (2013).\\nRemark. The representer theorem (12.38) also provides an explanation\\nof the name “support vector machine.” The examples xn, for which the\\ncorresponding parameters \\x0bn= 0, do not contribute to the solution wat\\nall. The other examples, where \\x0bn>0, are called support vectors since support vector\\nthey “support” the hyperplane. }\\nBy substituting the expression for winto the Lagrangian (12.34), we\\nobtain the dual\\nD(\\x18;\\x0b;\\r ) =1\\n2NX\\ni=1NX\\nj=1yiyj\\x0bi\\x0bjhxi;xji\\x00NX\\ni=1yi\\x0bi*NX\\nj=1yj\\x0bjxj;xi+\\n+CNX\\ni=1\\x18i\\x00bNX\\ni=1yi\\x0bi+NX\\ni=1\\x0bi\\x00NX\\ni=1\\x0bi\\x18i\\x00NX\\ni=1\\ri\\x18i:\\n(12.39)\\nNote that there are no longer any terms involving the primal variable w.\\nBy setting (12.36) to zero, we obtainPN\\nn=1yn\\x0bn= 0. Therefore, the term\\ninvolvingbalso vanishes. Recall that inner products are symmetric and\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.3 Dual Support Vector Machine 385\\nbilinear (see Section 3.2). Therefore, the ﬁrst two terms in (12.39) are\\nover the same objects. These terms (colored blue) can be simpliﬁed, and\\nwe obtain the Lagrangian\\nD(\\x18;\\x0b;\\r ) =\\x001\\n2NX\\ni=1NX\\nj=1yiyj\\x0bi\\x0bjhxi;xji+NX\\ni=1\\x0bi+NX\\ni=1(C\\x00\\x0bi\\x00\\ri)\\x18i:\\n(12.40)\\nThe last term in this equation is a collection of all terms that contain slack\\nvariables\\x18i. By setting (12.37) to zero, we see that the last term in (12.40)\\nis also zero. Furthermore, by using the same equation and recalling that\\nthe Lagrange multiplers \\riare non-negative, we conclude that \\x0bi6C.\\nWe now obtain the dual optimization problem of the SVM, which is ex-\\npressed exclusively in terms of the Lagrange multipliers \\x0bi. Recall from\\nLagrangian duality (Deﬁnition 7.1) that we maximize the dual problem.\\nThis is equivalent to minimizing the negative dual problem, such that we\\nend up with the dual SVM dual SVM\\nmin\\n\\x0b1\\n2NX\\ni=1NX\\nj=1yiyj\\x0bi\\x0bjhxi;xji\\x00NX\\ni=1\\x0bi\\nsubject toNX\\ni=1yi\\x0bi= 0\\n06\\x0bi6Cfor alli= 1;:::;N:(12.41)\\nThe equality constraint in (12.41) is obtained from setting (12.36) to\\nzero. The inequality constraint \\x0bi>0is the condition imposed on La-\\ngrange multipliers of inequality constraints (Section 7.2). The inequality\\nconstraint\\x0bi6Cis discussed in the previous paragraph.\\nThe set of inequality constraints in the SVM are called “box constraints”\\nbecause they limit the vector \\x0b= [\\x0b1;\\x01\\x01\\x01;\\x0bN]>2RNof Lagrange mul-\\ntipliers to be inside the box deﬁned by 0andCon each axis. These\\naxis-aligned boxes are particularly efﬁcient to implement in numerical\\nsolvers (Dost ´al, 2009, chapter 5). It turns out that\\nexamples that lie\\nexactly on the\\nmargin are\\nexamples whose\\ndual parameters lie\\nstrictly inside the\\nbox constraints,\\n0<\\x0bi<C. This is\\nderived using the\\nKarush Kuhn Tucker\\nconditions, for\\nexample in\\nSch¨olkopf and\\nSmola (2002).Once we obtain the dual parameters \\x0b, we can recover the primal pa-\\nrameterswby using the representer theorem (12.38). Let us call the op-\\ntimal primal parameter w\\x03. However, there remains the question on how\\nto obtain the parameter b\\x03. Consider an example xnthat lies exactly on\\nthe margin’s boundary, i.e., hw\\x03;xni+b=yn. Recall that ynis either +1\\nor\\x001. Therefore, the only unknown is b, which can be computed by\\nb\\x03=yn\\x00hw\\x03;xni: (12.42)\\nRemark. In principle, there may be no examples that lie exactly on the\\nmargin. In this case, we should compute jyn\\x00hw\\x03;xnijfor all support\\nvectors and take the median value of this absolute value difference to be\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n386 Classiﬁcation with Support Vector Machines\\nFigure 12.9 Convex\\nhulls. (a) Convex\\nhull of points, some\\nof which lie within\\nthe boundary;\\n(b) convex hulls\\naround positive and\\nnegative examples.\\n(a) Convex hull.\\nc\\nd (b) Convex hulls around positive (blue) and\\nnegative (orange) examples. The distance be-\\ntween the two convex sets is the length of the\\ndifference vector c\\x00d.\\nthe value of b\\x03. A derivation of this can be found in http://fouryears.\\neu/2012/06/07/the-svm-bias-term-conspiracy/ .}\\n12.3.2 Dual SVM: Convex Hull View\\nAnother approach to obtain the dual SVM is to consider an alternative\\ngeometric argument. Consider the set of examples xnwith the same label.\\nWe would like to build a convex set that contains all the examples such\\nthat it is the smallest possible set. This is called the convex hull and is\\nillustrated in Figure 12.9.\\nLet us ﬁrst build some intuition about a convex combination of points.\\nConsider two points x1andx2and corresponding non-negative weights\\n\\x0b1;\\x0b2>0such that\\x0b1+\\x0b2= 1. The equation \\x0b1x1+\\x0b2x2describes each\\npoint on a line between x1andx2. Consider what happens when we add\\na third point x3along with a weight \\x0b3>0such thatP3\\nn=1\\x0bn= 1.\\nThe convex combination of these three points x1;x2;x3spans a two-\\ndimensional area. The convex hull of this area is the triangle formed by convex hull\\nthe edges corresponding to each pair of of points. As we add more points,\\nand the number of points becomes greater than the number of dimen-\\nsions, some of the points will be inside the convex hull, as we can see in\\nFigure 12.9(a).\\nIn general, building a convex convex hull can be done by introducing\\nnon-negative weights \\x0bn>0corresponding to each example xn. Then\\nthe convex hull can be described as the set\\nconv (X) =(NX\\nn=1\\x0bnxn)\\nwithNX\\nn=1\\x0bn= 1 and\\x0bn>0;(12.43)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.3 Dual Support Vector Machine 387\\nfor alln= 1;:::;N . If the two clouds of points corresponding to the\\npositive and negative classes are separated, then the convex hulls do not\\noverlap. Given the training data (x1;y1);:::; (xN;yN), we form two con-\\nvex hulls, corresponding to the positive and negative classes respectively.\\nWe pick a point c, which is in the convex hull of the set of positive exam-\\nples, and is closest to the negative class distribution. Similarly, we pick a\\npointdin the convex hull of the set of negative examples and is closest to\\nthe positive class distribution; see Figure 12.9(b). We deﬁne a difference\\nvector between dandcas\\nw:=c\\x00d: (12.44)\\nPicking the points canddas in the preceding cases, and requiring them\\nto be closest to each other is equivalent to minimizing the length/norm of\\nw, so that we end up with the corresponding optimization problem\\narg min\\nwkwk= arg min\\nw1\\n2kwk2: (12.45)\\nSincecmust be in the positive convex hull, it can be expressed as a convex\\ncombination of the positive examples, i.e., for non-negative coefﬁcients\\n\\x0b+\\nn\\nc=X\\nn:yn=+1\\x0b+\\nnxn: (12.46)\\nIn (12.46), we use the notation n:yn= +1 to indicate the set of indices\\nnfor whichyn= +1 . Similarly, for the examples with negative labels, we\\nobtain\\nd=X\\nn:yn=\\x001\\x0b\\x00\\nnxn: (12.47)\\nBy substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the\\nobjective\\nmin\\n\\x0b1\\n2\\r\\r\\r\\r\\rX\\nn:yn=+1\\x0b+\\nnxn\\x00X\\nn:yn=\\x001\\x0b\\x00\\nnxn\\r\\r\\r\\r\\r2\\n: (12.48)\\nLet\\x0bbe the set of all coefﬁcients, i.e., the concatenation of \\x0b+and\\x0b\\x00.\\nRecall that we require that for each convex hull that their coefﬁcients sum\\nto one,\\nX\\nn:yn=+1\\x0b+\\nn= 1 andX\\nn:yn=\\x001\\x0b\\x00\\nn= 1: (12.49)\\nThis implies the constraint\\nNX\\nn=1yn\\x0bn= 0: (12.50)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n388 Classiﬁcation with Support Vector Machines\\nThis result can be seen by multiplying out the individual classes\\nNX\\nn=1yn\\x0bn=X\\nn:yn=+1(+1)\\x0b+\\nn+X\\nn:yn=\\x001(\\x001)\\x0b\\x00\\nn (12.51a)\\n=X\\nn:yn=+1\\x0b+\\nn\\x00X\\nn:yn=\\x001\\x0b\\x00\\nn= 1\\x001 = 0: (12.51b)\\nThe objective function (12.48) and the constraint (12.50), along with the\\nassumption that \\x0b>0, give us a constrained (convex) optimization prob-\\nlem. This optimization problem can be shown to be the same as that of\\nthe dual hard margin SVM (Bennett and Bredensteiner, 2000a).\\nRemark. To obtain the soft margin dual, we consider the reduced hull. The\\nreduced hull is similar to the convex hull but has an upper bound to the reduced hull\\nsize of the coefﬁcients \\x0b. The maximum possible value of the elements\\nof\\x0brestricts the size that the convex hull can take. In other words, the\\nbound on\\x0bshrinks the convex hull to a smaller volume (Bennett and\\nBredensteiner, 2000b). }\\n12.4 Kernels\\nConsider the formulation of the dual SVM (12.41). Notice that the in-\\nner product in the objective occurs only between examples xiandxj.\\nThere are no inner products between the examples and the parameters.\\nTherefore, if we consider a set of features \\x1e(xi)to representxi, the only\\nchange in the dual SVM will be to replace the inner product. This mod-\\nularity, where the choice of the classiﬁcation method (the SVM) and the\\nchoice of the feature representation \\x1e(x)can be considered separately,\\nprovides ﬂexibility for us to explore the two problems independently. In\\nthis section, we discuss the representation \\x1e(x)and brieﬂy introduce the\\nidea of kernels, but do not go into the technical details.\\nSince\\x1e(x)could be a non-linear function, we can use the SVM (which\\nassumes a linear classiﬁer) to construct classiﬁers that are nonlinear in\\nthe examples xn. This provides a second avenue, in addition to the soft\\nmargin, for users to deal with a dataset that is not linearly separable. It\\nturns out that there are many algorithms and statistical methods that have\\nthis property that we observed in the dual SVM: the only inner products\\nare those that occur between examples. Instead of explicitly deﬁning a\\nnon-linear feature map \\x1e(\\x01)and computing the resulting inner product\\nbetween examples xiandxj, we deﬁne a similarity function k(xi;xj)be-\\ntweenxiandxj. For a certain class of similarity functions, called kernels , kernel\\nthe similarity function implicitly deﬁnes a non-linear feature map \\x1e(\\x01).\\nKernels are by deﬁnition functions k:X\\x02X! Rfor which there exists The inputsXof the\\nkernel function can\\nbe very general and\\nare not necessarily\\nrestricted to RD.a Hilbert spaceHand\\x1e:X!H a feature map such that\\nk(xi;xj) =h\\x1e(xi);\\x1e(xj)iH: (12.52)\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.4 Kernels 389\\nFigure 12.10 SVM\\nwith different\\nkernels. Note that\\nwhile the decision\\nboundary is\\nnonlinear, the\\nunderlying problem\\nbeing solved is for a\\nlinear separating\\nhyperplane (albeit\\nwith a nonlinear\\nkernel).\\nFirst featureSecond feature\\n(a) SVM with linear kernel\\nFirst featureSecond feature (b) SVM with RBF kernel\\nFirst featureSecond feature\\n(c) SVM with polynomial (degree 2) kernel\\nFirst featureSecond feature (d) SVM with polynomial (degree 3) kernel\\nThere is a unique reproducing kernel Hilbert space associated with every\\nkernelk(Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this\\nunique association, \\x1e(x) =k(\\x01;x)is called the canonical feature map .canonical feature\\nmap The generalization from an inner product to a kernel function (12.52) is\\nknown as the kernel trick (Sch¨olkopf and Smola, 2002; Shawe-Taylor and kernel trick\\nCristianini, 2004), as it hides away the explicit non-linear feature map.\\nThe matrixK2RN\\x02N, resulting from the inner products or the appli-\\ncation ofk(\\x01;\\x01)to a dataset, is called the Gram matrix , and is often just Gram matrix\\nreferred to as the kernel matrix . Kernels must be symmetric and positive kernel matrix\\nsemideﬁnite functions so that every kernel matrix Kis symmetric and\\npositive semideﬁnite (Section 3.2.3):\\n8z2RN:z>Kz>0: (12.53)\\nSome popular examples of kernels for multivariate real-valued data xi2\\nRDare the polynomial kernel, the Gaussian radial basis function kernel,\\nand the rational quadratic kernel (Sch ¨olkopf and Smola, 2002; Rasmussen\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n390 Classiﬁcation with Support Vector Machines\\nand Williams, 2006). Figure 12.10 illustrates the effect of different kernels\\non separating hyperplanes on an example dataset. Note that we are still\\nsolving for hyperplanes, that is, the hypothesis class of functions are still\\nlinear. The non-linear surfaces are due to the kernel function.\\nRemark. Unfortunately for the ﬂedgling machine learner, there are mul-\\ntiple meanings of the word “kernel.” In this chapter, the word “kernel”\\ncomes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-\\nszajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-\\near algebra (Section 2.7.3), where the kernel is another word for the null\\nspace. The third common use of the word “kernel” in machine learning is\\nthe smoothing kernel in kernel density estimation (Section 11.5). }\\nSince the explicit representation \\x1e(x)is mathematically equivalent to\\nthe kernel representation k(xi;xj), a practitioner will often design the\\nkernel function such that it can be computed more efﬁciently than the\\ninner product between explicit feature maps. For example, consider the\\npolynomial kernel (Sch ¨olkopf and Smola, 2002), where the number of\\nterms in the explicit expansion grows very quickly (even for polynomials\\nof low degree) when the input dimension is large. The kernel function\\nonly requires one multiplication per input dimension, which can provide\\nsigniﬁcant computational savings. Another example is the Gaussian ra-\\ndial basis function kernel (Sch ¨olkopf and Smola, 2002; Rasmussen and\\nWilliams, 2006), where the corresponding feature space is inﬁnite dimen-\\nsional. In this case, we cannot explicitly represent the feature space but\\ncan still compute similarities between a pair of examples using the kernel. The choice of\\nkernel, as well as\\nthe parameters of\\nthe kernel, is often\\nchosen using nested\\ncross-validation\\n(Section 8.6.1).Another useful aspect of the kernel trick is that there is no need for\\nthe original data to be already represented as multivariate real-valued\\ndata. Note that the inner product is deﬁned on the output of the function\\n\\x1e(\\x01), but does not restrict the input to real numbers. Hence, the function\\n\\x1e(\\x01)and the kernel function k(\\x01;\\x01)can be deﬁned on any object, e.g.,\\nsets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;\\nG¨artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan\\net al., 2010).\\n12.5 Numerical Solution\\nWe conclude our discussion of SVMs by looking at how to express the\\nproblems derived in this chapter in terms of the concepts presented in\\nChapter 7. We consider two different approaches for ﬁnding the optimal\\nsolution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-\\npress this as an unconstrained optimization problem. Then we express the\\nconstrained versions of the primal and dual SVMs as quadratic programs\\nin standard form 7.3.2.\\nConsider the loss function view of the SVM (12.31). This is a convex\\nunconstrained optimization problem, but the hinge loss (12.28) is not dif-\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.5 Numerical Solution 391\\nferentiable. Therefore, we apply a subgradient approach for solving it.\\nHowever, the hinge loss is differentiable almost everywhere, except for\\none single point at the hinge t= 1. At this point, the gradient is a set of\\npossible values that lie between 0and\\x001. Therefore, the subgradient gof\\nthe hinge loss is given by\\ng(t) =8\\n><\\n>:\\x001t<1\\n[\\x001;0]t= 1\\n0t>1: (12.54)\\nUsing this subgradient, we can apply the optimization methods presented\\nin Section 7.1.\\nBoth the primal and the dual SVM result in a convex quadratic pro-\\ngramming problem (constrained optimization). Note that the primal SVM\\nin (12.26a) has optimization variables that have the size of the dimen-\\nsionDof the input examples. The dual SVM in (12.41) has optimization\\nvariables that have the size of the number Nof examples.\\nTo express the primal SVM in the standard form (7.45) for quadratic\\nprogramming, let us assume that we use the dot product (3.5) as the\\ninner product. We rearrange the equation for the primal SVM (12.26a), Recall from\\nSection 3.2 that we\\nuse the phrase dot\\nproduct to mean the\\ninner product on\\nEuclidean vector\\nspace.such that the optimization variables are all on the right and the inequality\\nof the constraint matches the standard form. This yields the optimization\\nmin\\nw;b;\\x181\\n2kwk2+CNX\\nn=1\\x18n\\nsubject to\\x00ynx>\\nnw\\x00ynb\\x00\\x18n6\\x001\\n\\x00\\x18n60(12.55)\\nn= 1;:::;N . By concatenating the variables w;b;xninto a single vector,\\nand carefully collecting the terms, we obtain the following matrix form of\\nthe soft margin SVM:\\nmin\\nw;b;\\x181\\n22\\n4w\\nb\\n\\x183\\n5>\\x14ID 0D;N+1\\n0N+1;D0N+1;N+1\\x152\\n4w\\nb\\n\\x183\\n5+\\x020D+1;1C1N;1\\x03>2\\n4w\\nb\\n\\x183\\n5\\nsubject to\\x14\\x00YX\\x00y\\x00IN\\n0N;D+1\\x00IN\\x152\\n4w\\nb\\n\\x183\\n56\\x14\\x001N;1\\n0N;1\\x15\\n:\\n(12.56)\\nIn the preceding optimization problem, the minimization is over the pa-\\nrameters [w>;b;\\x18>]>2RD+1+N, and we use the notation: Imto rep-\\nresent the identity matrix of size m\\x02m,0m;nto represent the matrix\\nof zeros of size m\\x02n, and 1m;nto represent the matrix of ones of size\\nm\\x02n. In addition, yis the vector of labels [y1;\\x01\\x01\\x01;yN]>,Y= diag(y)\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n392 Classiﬁcation with Support Vector Machines\\nis anNbyNmatrix where the elements of the diagonal are from y, and\\nX2RN\\x02Dis the matrix obtained by concatenating all the examples.\\nWe can similarly perform a collection of terms for the dual version of the\\nSVM (12.41). To express the dual SVM in standard form, we ﬁrst have to\\nexpress the kernel matrix Ksuch that each entry is Kij=k(xi;xj). If we\\nhave an explicit feature representation xithen we deﬁne Kij=hxi;xji.\\nFor convenience of notation we introduce a matrix with zeros everywhere\\nexcept on the diagonal, where we store the labels, that is, Y= diag(y).\\nThe dual SVM can be written as\\nmin\\n\\x0b1\\n2\\x0b>YKY\\x0b\\x001>\\nN;1\\x0b\\nsubject to2\\n664y>\\n\\x00y>\\n\\x00IN\\nIN3\\n775\\x0b6\\x140N+2;1\\nC1N;1\\x15\\n:(12.57)\\nRemark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms\\nof the constraints to be inequality constraints. We will express the dual\\nSVM’s equality constraint as two inequality constraints, i.e.,\\nAx=bis replaced by Ax6bandAx>b: (12.58)\\nParticular software implementations of convex optimization methods may\\nprovide the ability to express equality constraints. }\\nSince there are many different possible views of the SVM, there are\\nmany approaches for solving the resulting optimization problem. The ap-\\nproach presented here, expressing the SVM problem in standard convex\\noptimization form, is not often used in practice. The two main implemen-\\ntations of SVM solvers are Chang and Lin (2011) (which is open source)\\nand Joachims (1999). Since SVMs have a clear and well-deﬁned optimiza-\\ntion problem, many approaches based on numerical optimization tech-\\nniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and\\nSun, 2011).\\n12.6 Further Reading\\nThe SVM is one of many approaches for studying binary classiﬁcation.\\nOther approaches include the perceptron, logistic regression, Fisher dis-\\ncriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\\nMurphy, 2012). A short tutorial on SVMs and kernels on discrete se-\\nquences can be found in Ben-Hur et al. (2008). The development of SVMs\\nis closely linked to empirical risk minimization, discussed in Section 8.2.\\nHence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-\\nwart and Christmann, 2008). The book about kernel methods (Sch ¨olkopf\\nand Smola, 2002) includes many details of support vector machines and\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\n12.6 Further Reading 393\\nhow to optimize them. A broader book about kernel methods (Shawe-\\nTaylor and Cristianini, 2004) also includes many linear algebra approaches\\nfor different machine learning problems.\\nAn alternative derivation of the dual SVM can be obtained using the\\nidea of the Legendre–Fenchel transform (Section 7.3.3). The derivation\\nconsiders each term of the unconstrained formulation of the SVM (12.31)\\nseparately and calculates their convex conjugates (Rifkin and Lippert,\\n2007). Readers interested in the functional analysis view (also the reg-\\nularization methods view) of SVMs are referred to the work by Wahba\\n(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,\\n1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic ground-\\ning in linear operators (Akhiezer and Glazman, 1993). The idea of kernels\\nhave been generalized to Banach spaces (Zhang et al., 2009) and Kre ˘ın\\nspaces (Ong et al., 2004; Loosli et al., 2016).\\nObserve that the hinge loss has three equivalent representations, as\\nshown in (12.28) and (12.29), as well as the constrained optimization\\nproblem in (12.33). The formulation (12.28) is often used when compar-\\ning the SVM loss function with other loss functions (Steinwart, 2007).\\nThe two-piece formulation (12.29) is convenient for computing subgra-\\ndients, as each piece is linear. The third formulation (12.33), as seen\\nin Section 12.5, enables the use of convex quadratic programming (Sec-\\ntion 7.3.2) tools.\\nSince binary classiﬁcation is a well-studied task in machine learning,\\nother words are also sometimes used, such as discrimination, separation,\\nand decision. Furthermore, there are three quantities that can be the out-\\nput of a binary classiﬁer. First is the output of the linear function itself\\n(often called the score), which can take any real value. This output can be\\nused for ranking the examples, and binary classiﬁcation can be thought\\nof as picking a threshold on the ranked examples (Shawe-Taylor and Cris-\\ntianini, 2004). The second quantity that is often considered the output\\nof a binary classiﬁer is the output determined after it is passed through\\na non-linear function to constrain its value to a bounded range, for ex-\\nample in the interval [0;1]. A common non-linear function is the sigmoid\\nfunction (Bishop, 2006). When the non-linearity results in well-calibrated\\nprobabilities (Gneiting and Raftery, 2007; Reid and Williamson, 2011),\\nthis is called class probability estimation. The third output of a binary\\nclassiﬁer is the ﬁnal binary decision f+1;\\x001g, which is the one most com-\\nmonly assumed to be the output of the classiﬁer.\\nThe SVM is a binary classiﬁer that does not naturally lend itself to a\\nprobabilistic interpretation. There are several approaches for converting\\nthe raw output of the linear function (the score) into a calibrated class\\nprobability estimate ( P(Y= 1jX=x)) that involve an additional cal-\\nibration step (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007).\\nFrom the training perspective, there are many related probabilistic ap-\\nproaches. We mentioned at the end of Section 12.2.5 that there is a re-\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n394 Classiﬁcation with Support Vector Machines\\nlationship between loss function and the likelihood (also compare Sec-\\ntions 8.2 and 8.3). The maximum likelihood approach corresponding to\\na well-calibrated transformation during training is called logistic regres-\\nsion, which comes from a class of methods called generalized linear mod-\\nels. Details of logistic regression from this point of view can be found in\\nAgresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).\\nNaturally, one could take a more Bayesian view of the classiﬁer output by\\nestimating a posterior distribution using Bayesian logistic regression. The\\nBayesian view also includes the speciﬁcation of the prior, which includes\\ndesign choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-\\nditionally, one could consider latent functions as priors, which results in\\nGaussian process classiﬁcation (Rasmussen and Williams, 2006, chapter\\n3).\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nReferences\\nAbel, Niels H. 1826. D´emonstration de l’Impossibilit ´e de la R ´esolution Alg ´ebrique des\\n´Equations G ´en´erales qui Passent le Quatri `eme Degr ´e. Grøndahl and Søn.\\nAdhikari, Ani, and DeNero, John. 2018. Computational and Inferential Thinking: The\\nFoundations of Data Science . Gitbooks.\\nAgarwal, Arvind, and Daum ´e III, Hal. 2010. A Geometric View of Conjugate Priors.\\nMachine Learning ,81(1), 99–113.\\nAgresti, A. 2002. Categorical Data Analysis . Wiley.\\nAkaike, Hirotugu. 1974. A New Look at the Statistical Model Identiﬁcation. IEEE\\nTransactions on Automatic Control ,19(6), 716–723.\\nAkhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert\\nSpace . Dover Publications.\\nAlpaydin, Ethem. 2010. Introduction to Machine Learning . MIT Press.\\nAmari, Shun-ichi. 2016. Information Geometry and Its Applications . Springer.\\nArgyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer\\nTheorems. In: Proceedings of the International Conference on Machine Learning .\\nAronszajn, Nachman. 1950. Theory of Reproducing Kernels. Transactions of the Amer-\\nican Mathematical Society ,68, 337–404.\\nAxler, Sheldon. 2015. Linear Algebra Done Right . Springer.\\nBakir, G ¨okhan, Hofmann, Thomas, Sch ¨olkopf, Bernhard, Smola, Alexander J., Taskar,\\nBen, and Vishwanathan, S. V. N. (eds). 2007. Predicting Structured Data . MIT Press.\\nBarber, David. 2012. Bayesian Reasoning and Machine Learning . Cambridge University\\nPress.\\nBarndorff-Nielsen, Ole. 2014. Information and Exponential Families: In Statistical The-\\nory. Wiley.\\nBartholomew, David, Knott, Martin, and Moustaki, Irini. 2011. Latent Variable Models\\nand Factor Analysis: A Uniﬁed Approach . Wiley.\\nBaydin, Atılım G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M.\\n2018. Automatic Differentiation in Machine Learning: A Survey. Journal of Machine\\nLearning Research ,18, 1–43.\\nBeck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgra-\\ndient Methods for Convex Optimization. Operations Research Letters ,31(3), 167–\\n175.\\nBelabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine\\nLearning and New Strategies for Very Large Datasets. Proceedings of the National\\nAcademy of Sciences , 0810600105.\\nBelkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality\\nReduction and Data Representation. Neural Computation ,15(6), 1373–1396.\\nBen-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S ¨oren, Sch ¨olkopf, Bernhard, and R ¨atsch,\\nGunnar. 2008. Support Vector Machines and Kernels for Computational Biology.\\nPLoS Computational Biology ,4(10), e1000173.\\n395\\nThis material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\\n396 References\\nBennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM\\nClassiﬁers. In: Proceedings of the International Conference on Machine Learning .\\nBennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pages\\n132–145 of: Geometry at Work . Mathematical Association of America.\\nBerlinet, Alain, and Thomas-Agnan, Christine. 2004. Reproducing Kernel Hilbert Spaces\\nin Probability and Statistics . Springer.\\nBertsekas, Dimitri P. 1999. Nonlinear Programming . Athena Scientiﬁc.\\nBertsekas, Dimitri P. 2009. Convex Optimization Theory . Athena Scientiﬁc.\\nBickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and\\nSelected Topics . Vol. 1. Prentice Hall.\\nBickson, Danny, Dolev, Danny, Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007.\\nLinear Detection via Belief Propagation. In: Proceedings of the Annual Allerton Con-\\nference on Communication, Control, and Computing .\\nBillingsley, Patrick. 1995. Probability and Measure . Wiley.\\nBishop, Christopher M. 1995. Neural Networks for Pattern Recognition . Clarendon\\nPress.\\nBishop, Christopher M. 1999. Bayesian PCA. In: Advances in Neural Information Pro-\\ncessing Systems .\\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning . Springer.\\nBlei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A\\nReview for Statisticians. Journal of the American Statistical Association ,112(518),\\n859–877.\\nBlum, Arvim, and Hardt, Moritz. 2015. The Ladder: A Reliable Leaderboard for Ma-\\nchine Learning Competitions. In: International Conference on Machine Learning .\\nBonnans, J. Fr ´ed´eric, Gilbert, J. Charles, Lemar ´echal, Claude, and Sagastiz ´abal, Clau-\\ndia A. 2006. Numerical Optimization: Theoretical and Practical Aspects . Springer.\\nBorwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear\\nOptimization . 2nd edn. Canadian Mathematical Society.\\nBottou, L ´eon. 1998. Online Algorithms and Stochastic Approximations. Pages 9–42\\nof:Online Learning and Neural Networks . Cambridge University Press.\\nBottou, L ´eon, Curtis, Frank E., and Nocedal, Jorge. 2018. Optimization Methods for\\nLarge-Scale Machine Learning. SIAM Review ,60(2), 223–311.\\nBoucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration In-\\nequalities: A Nonasymptotic Theory of Independence. Oxford University Press.\\nBoyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization . Cambridge\\nUniversity Press.\\nBoyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Alge-\\nbra. Cambridge University Press.\\nBrochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian\\nOptimization of Expensive Cost Functions, with Application to Active User Modeling\\nand Hierarchical Reinforcement Learning . Tech. rept. TR-2009-023. Department of\\nComputer Science, University of British Columbia.\\nBrooks, Steve, Gelman, Andrew, Jones, Galin L., and Meng, Xiao-Li (eds). 2011. Hand-\\nbook of Markov Chain Monte Carlo . Chapman and Hall/CRC.\\nBrown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Ap-\\nplications in Statistical Decision Theory . Institute of Mathematical Statistics.\\nBryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation\\nProcesses. In: Proceedings of the Harvard University Symposium on Digital Computers\\nand Their Applications .\\nBubeck, S ´ebastien. 2015. Convex Optimization: Algorithms and Complexity. Founda-\\ntions and Trends in Machine Learning ,8(3-4), 231–357.\\nB¨uhlmann, Peter, and Van De Geer, Sara. 2011. Statistics for High-Dimensional Data .\\nSpringer.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nReferences 397\\nBurges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and\\nTrends in Machine Learning ,2(4), 275–365.\\nCarroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in\\nMultidimensional Scaling via an N-Way Generalization of “Eckart-Young” Decom-\\nposition. Psychometrika ,35(3), 283–319.\\nCasella, George, and Berger, Roger L. 2002. Statistical Inference . Duxbury.\\nC ¸inlar, Erhan. 2011. Probability and Stochastics . Springer.\\nChang, Chih-Chung, and Lin, Chih-Jen. 2011. LIBSVM: A Library for Support Vector\\nMachines. ACM Transactions on Intelligent Systems and Technology ,2, 27:1–27:27.\\nCheeseman, Peter. 1985. In Defense of Probability. In: Proceedings of the International\\nJoint Conference on Artiﬁcial Intelligence .\\nChollet, Francois, and Allaire, J. J. 2018. Deep Learning with R . Manning Publications.\\nCodd, Edgar F. 1990. The Relational Model for Database Management . Addison-Wesley\\nLongman Publishing.\\nCunningham, John P., and Ghahramani, Zoubin. 2015. Linear Dimensionality Reduc-\\ntion: Survey, Insights, and Generalizations. Journal of Machine Learning Research ,\\n16, 2859–2900.\\nDatta, Biswa N. 2010. Numerical Linear Algebra and Applications . SIAM.\\nDavidson, Anthony C., and Hinkley, David V. 1997. Bootstrap Methods and Their Appli-\\ncation . Cambridge University Press.\\nDean, Jeffrey, Corrado, Greg S., Monga, Rajat, and Chen, et al. 2012. Large Scale\\nDistributed Deep Networks. In: Advances in Neural Information Processing Systems .\\nDeisenroth, Marc P., and Mohamed, Shakir. 2012. Expectation Propagation in Gaus-\\nsian Process Dynamical Systems. Pages 2618–2626 of: Advances in Neural Informa-\\ntion Processing Systems .\\nDeisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian\\nFiltering and Smoothing: Explaining Current and Deriving New Algorithms. In:\\nProceedings of the American Control Conference .\\nDeisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes\\nfor Data-Efﬁcient Learning in Robotics and Control. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence ,37(2), 408–423.\\nDempster, Arthur P., Laird, Nan M., and Rubin, Donald B. 1977. Maximum Likelihood\\nfrom Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society ,\\n39(1), 1–38.\\nDeng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and\\nHinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep\\nAuto-Encoder. In: Proceedings of Interspeech .\\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation . Springer.\\nDonoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear\\nEmbedding Techniques for High-Dimensional Data. Proceedings of the National\\nAcademy of Sciences ,100(10), 5591–5596.\\nDost´al, Zden ˘ek. 2009. Optimal Quadratic Programming Algorithms: With Applications\\nto Variational Inequalities . Springer.\\nDouven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy . Meta-\\nphysics Research Lab, Stanford University.\\nDowney, Allen B. 2014. Think Stats: Exploratory Data Analysis . 2nd edn. O’Reilly\\nMedia.\\nDreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of\\nMathematical Analysis and Applications ,5(1), 30–45.\\nDrumm, Volker, and Weil, Wolfgang. 2001. Lineare Algebra und Analytische Geometrie .\\nLecture Notes, Universit ¨at Karlsruhe (TH).\\nDudley, Richard M. 2002. Real Analysis and Probability . Cambridge University Press.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n398 References\\nEaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach . Institute of\\nMathematical Statistics Lecture Notes.\\nEckart, Carl, and Young, Gale. 1936. The Approximation of One Matrix by Another of\\nLower Rank. Psychometrika ,1(3), 211–218.\\nEfron, Bradley, and Hastie, Trevor. 2016. Computer Age Statistical Inference: Algorithms,\\nEvidence and Data Science . Cambridge University Press.\\nEfron, Bradley, and Tibshirani, Robert J. 1993. An Introduction to the Bootstrap . Chap-\\nman and Hall/CRC.\\nElliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Func-\\ntional Programming .\\nEvgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical\\nLearning Theory: A Primer. International Journal of Computer Vision ,38(1), 9–13.\\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen.\\n2008. LIBLINEAR: A Library for Large Linear Classiﬁcation. Journal of Machine\\nLearning Research ,9, 1871–1874.\\nGal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. 2014. Distributed Variational\\nInference in Sparse Gaussian Process Regression and Latent Variable Models. In:\\nAdvances in Neural Information Processing Systems .\\nG¨artner, Thomas. 2008. Kernels for Structured Data . World Scientiﬁc.\\nGavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Singular\\nValues is 4p\\n3.IEEE Transactions on Information Theory ,60(8), 5040–5053.\\nGelman, Andrew, Carlin, John B., Stern, Hal S., and Rubin, Donald B. 2004. Bayesian\\nData Analysis . Chapman and Hall/CRC.\\nGentle, James E. 2004. Random Number Generation and Monte Carlo Methods .\\nSpringer.\\nGhahramani, Zoubin. 2015. Probabilistic Machine Learning and Artiﬁcial Intelligence.\\nNature ,521, 452–459.\\nGhahramani, Zoubin, and Roweis, Sam T. 1999. Learning Nonlinear Dynamical Sys-\\ntems Using an EM Algorithm. In: Advances in Neural Information Processing Systems .\\nMIT Press.\\nGilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain\\nMonte Carlo in Practice . Chapman and Hall/CRC.\\nGneiting, Tilmann, and Raftery, Adrian E. 2007. Strictly Proper Scoring Rules, Pre-\\ndiction, and Estimation. Journal of the American Statistical Association ,102(477),\\n359–378.\\nGoh, Gabriel. 2017. Why Momentum Really Works. Distill .\\nGohberg, Israel, Goldberg, Seymour, and Krupnik, Nahum. 2012. Traces and Determi-\\nnants of Linear Operators . Birkh ¨auser.\\nGolan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to\\nKnow . Springer.\\nGolub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations . JHU Press.\\nGoodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning . MIT\\nPress.\\nGraepel, Thore, Candela, Joaquin Qui ˜nonero-Candela, Borchert, Thomas, and Her-\\nbrich, Ralf. 2010. Web-Scale Bayesian Click-through Rate Prediction for Sponsored\\nSearch Advertising in Microsoft’s Bing Search Engine. In: Proceedings of the Interna-\\ntional Conference on Machine Learning .\\nGriewank, Andreas, and Walther, Andrea. 2003. Introduction to Automatic Differenti-\\nation. In: Proceedings in Applied Mathematics and Mechanics .\\nGriewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and\\nTechniques of Algorithmic Differentiation . SIAM.\\nGrimmett, Geoffrey R., and Welsh, Dominic. 2014. Probability: An Introduction . Oxford\\nUniversity Press.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nReferences 399\\nGrinstead, Charles M., and Snell, J. Laurie. 1997. Introduction to Probability . American\\nMathematical Society.\\nHacking, Ian. 2001. Probability and Inductive Logic . Cambridge University Press.\\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion . Springer.\\nHallin, Marc, Paindaveine, Davy, and ˇSiman, Miroslav. 2010. Multivariate Quan-\\ntiles and Multiple-Output Regression Quantiles: From `1Optimization to Halfspace\\nDepth. Annals of Statistics ,38, 635–669.\\nHasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a\\nPanorama of Recent Developments . Cambridge University Press.\\nHastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Sta-\\ntistical Learning – Data Mining, Inference, and Prediction . Springer.\\nHausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller,\\nMartin. 2018. Learning an Embedding Space for Transferable Robot Skills. In:\\nProceedings of the International Conference on Learning Representations .\\nHazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and\\nTrends in Optimization ,2(3–4), 157–325.\\nHensman, James, Fusi, Nicol `o, and Lawrence, Neil D. 2013. Gaussian Processes for\\nBig Data. In: Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence .\\nHerbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian\\nSkill Rating System. In: Advances in Neural Information Processing Systems .\\nHiriart-Urruty, Jean-Baptiste, and Lemar ´echal, Claude. 2001. Fundamentals of Convex\\nAnalysis . Springer.\\nHoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for\\nLatent Dirichlet Allocation. Advances in Neural Information Processing Systems .\\nHoffman, Matthew D., Blei, David M., Wang, Chong, and Paisley, John. 2013. Stochas-\\ntic Variational Inference. Journal of Machine Learning Research ,14(1), 1303–1347.\\nHofmann, Thomas, Sch ¨olkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Meth-\\nods in Machine Learning. Annals of Statistics ,36(3), 1171–1220.\\nHogben, Leslie. 2013. Handbook of Linear Algebra . Chapman and Hall/CRC.\\nHorn, Roger A., and Johnson, Charles R. 2013. Matrix Analysis . Cambridge University\\nPress.\\nHotelling, Harold. 1933. Analysis of a Complex of Statistical Variables into Principal\\nComponents. Journal of Educational Psychology ,24, 417–441.\\nHyvarinen, Aapo, Oja, Erkki, and Karhunen, Juha. 2001. Independent Component Anal-\\nysis. Wiley.\\nImbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social\\nand Biomedical Sciences . Cambridge University Press.\\nJacod, Jean, and Protter, Philip. 2004. Probability Essentials . Springer.\\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science . Cambridge University\\nPress.\\nJefferys, William H., and Berger, James O. 1992. Ockham’s Razor and Bayesian Anal-\\nysis. American Scientist ,80, 64–72.\\nJeffreys, Harold. 1961. Theory of Probability . Oxford University Press.\\nJimenez Rezende, Danilo, and Mohamed, Shakir. 2015. Variational Inference with Nor-\\nmalizing Flows. In: Proceedings of the International Conference on Machine Learning .\\nJimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic\\nBackpropagation and Approximate Inference in Deep Generative Models. In: Pro-\\nceedings of the International Conference on Machine Learning .\\nJoachims, Thorsten. 1999. Advances in Kernel Methods – Support Vector Learning . MIT\\nPress. Chap. Making Large-Scale SVM Learning Practical, pages 169–184.\\nJordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., and Saul, Lawrence K.\\n1999. An Introduction to Variational Methods for Graphical Models. Machine Learn-\\ning,37, 183–233.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n400 References\\nJulier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter\\nto Nonlinear Systems. In: Proceedings of AeroSense Symposium on Aerospace/Defense\\nSensing, Simulation and Controls .\\nKaiser, Marcus, and Hilgetag, Claus C. 2006. Nonoptimal Component Placement, but\\nShort Processing Paths, Due to Long-Distance Projections in Neural Systems. PLoS\\nComputational Biology ,2(7), e95.\\nKalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. Col-\\nlege Mathematics Journal ,27(1), 2–23.\\nKalman, Rudolf E. 1960. A New Approach to Linear Filtering and Prediction Problems.\\nTransactions of the ASME – Journal of Basic Engineering ,82(Series D), 35–45.\\nKamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efﬁcient Reinforcement Learning\\nwith Probabilistic Model Predictive Control. In: Proceedings of the International\\nConference on Artiﬁcial Intelligence and Statistics .\\nKatz, Victor J. 2004. A History of Mathematics . Pearson/Addison-Wesley.\\nKelley, Henry J. 1960. Gradient Theory of Optimal Flight Paths. Ars Journal ,30(10),\\n947–954.\\nKimeldorf, George S., and Wahba, Grace. 1970. A Correspondence between Bayesian\\nEstimation on Stochastic Processes and Smoothing by Splines. Annals of Mathemat-\\nical Statistics ,41(2), 495–502.\\nKingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:\\nProceedings of the International Conference on Learning Representations .\\nKittler, Josef, and F ¨oglein, Janos. 1984. Contextual Classiﬁcation of Multispectral Pixel\\nData. Image and Vision Computing ,2(1), 13–29.\\nKolda, Tamara G., and Bader, Brett W. 2009. Tensor Decompositions and Applications.\\nSIAM Review ,51(3), 455–500.\\nKoller, Daphne, and Friedman, Nir. 2009. Probabilistic Graphical Models . MIT Press.\\nKong, Linglong, and Mizera, Ivan. 2012. Quantile Tomography: Using Quantiles with\\nMultivariate Data. Statistica Sinica ,22, 1598–1610.\\nLang, Serge. 1987. Linear Algebra . Springer.\\nLawrence, Neil D. 2005. Probabilistic Non-Linear Principal Component Analysis with\\nGaussian Process Latent Variable Models. Journal of Machine Learning Research ,\\n6(Nov.), 1783–1816.\\nLeemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution\\nRelationships. American Statistician ,62(1), 45–53.\\nLehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses .\\nSpringer.\\nLehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation . Springer.\\nLiesen, J ¨org, and Mehrmann, Volker. 2015. Linear Algebra . Springer.\\nLin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt’s Probabilistic\\nOutputs for Support Vector Machines. Machine Learning ,68, 267–276.\\nLjung, Lennart. 1999. System Identiﬁcation: Theory for the User . Prentice Hall.\\nLoosli, Ga ¨elle, Canu, St ´ephane, and Ong, Cheng Soon. 2016. Learning SVM in Kre ˘ın\\nSpaces. IEEE Transactions of Pattern Analysis and Machine Intelligence ,38(6), 1204–\\n1216.\\nLuenberger, David G. 1969. Optimization by Vector Space Methods . Wiley.\\nMacKay, David J. C. 1992. Bayesian Interpolation. Neural Computation ,4, 415–447.\\nMacKay, David J. C. 1998. Introduction to Gaussian Processes. Pages 133–165 of:\\nBishop, C. M. (ed), Neural Networks and Machine Learning . Springer.\\nMacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms .\\nCambridge University Press.\\nMagnus, Jan R., and Neudecker, Heinz. 2007. Matrix Differential Calculus with Appli-\\ncations in Statistics and Econometrics . Wiley.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nReferences 401\\nManton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing\\nKernel Hilbert Spaces. Foundations and Trends in Signal Processing ,8(1–2), 1–126.\\nMarkovsky, Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Appli-\\ncations . Springer.\\nMaybeck, Peter S. 1979. Stochastic Models, Estimation, and Control . Academic Press.\\nMcCullagh, Peter, and Nelder, John A. 1989. Generalized Linear Models . CRC Press.\\nMcEliece, Robert J., MacKay, David J. C., and Cheng, Jung-Fu. 1998. Turbo Decoding\\nas an Instance of Pearl’s “Belief Propagation” Algorithm. IEEE Journal on Selected\\nAreas in Communications ,16(2), 140–152.\\nMika, Sebastian, R ¨atsch, Gunnar, Weston, Jason, Sch ¨olkopf, Bernhard, and M ¨uller,\\nKlaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41–48 of:\\nProceedings of the Workshop on Neural Networks for Signal Processing .\\nMinka, Thomas P. 2001a. A Family of Algorithms for Approximate Bayesian Inference .\\nPh.D. thesis, Massachusetts Institute of Technology.\\nMinka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in\\nNeural Information Processing Systems .\\nMitchell, Tom. 1997. Machine Learning . McGraw-Hill.\\nMnih, Volodymyr, Kavukcuoglu, Koray, and Silver, David, et al. 2015. Human-Level\\nControl through Deep Reinforcement Learning. Nature ,518, 529–533.\\nMoonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms,\\nArchitectures and Applications . Elsevier.\\nMoustaki, Irini, Knott, Martin, and Bartholomew, David J. 2015. Latent-Variable Mod-\\neling. American Cancer Society. Pages 1–10.\\nM¨uller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with\\nPython: A Guide for Data Scientists . O’Reilly Publishing.\\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective . MIT Press.\\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks . Ph.D. thesis, Depart-\\nment of Computer Science, University of Toronto.\\nNeal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that\\nJustiﬁes Incremental, Sparse, and Other Variants. Pages 355–368 of: Learning in\\nGraphical Models . MIT Press.\\nNelsen, Roger. 2006. An Introduction to Copulas . Springer.\\nNesterov, Yuri. 2018. Lectures on Convex Optimization . Springer.\\nNeumaier, Arnold. 1998. Solving Ill-Conditioned and Singular Linear Systems: A Tu-\\ntorial on Regularization. SIAM Review ,40, 636–666.\\nNocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization . Springer.\\nNowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H.\\n(eds). 2014. Advanced Structured Prediction . MIT Press.\\nO’Hagan, Anthony. 1991. Bayes-Hermite Quadrature. Journal of Statistical Planning\\nand Inference ,29, 245–260.\\nOng, Cheng Soon, Mary, Xavier, Canu, St ´ephane, and Smola, Alexander J. 2004. Learn-\\ning with Non-Positive Kernels. In: Proceedings of the International Conference on\\nMachine Learning .\\nOrmoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001.\\nLearning and Tracking Cyclic Human Motion. In: Advances in Neural Information\\nProcessing Systems .\\nPage, Lawrence, Brin, Sergey, Motwani, Rajeev, and Winograd, Terry. 1999. The\\nPageRank Citation Ranking: Bringing Order to the Web . Tech. rept. Stanford Info-\\nLab.\\nPaquet, Ulrich. 2008. Bayesian Inference for Latent Variable Models . Ph.D. thesis, Uni-\\nversity of Cambridge.\\nParzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode.\\nAnnals of Mathematical Statistics ,33(3), 1065–1076.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n402 References\\nPearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\\nInference . Morgan Kaufmann.\\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference . 2nd edn. Cambridge\\nUniversity Press.\\nPearson, Karl. 1895. Contributions to the Mathematical Theory of Evolution. II. Skew\\nVariation in Homogeneous Material. Philosophical Transactions of the Royal Society\\nA: Mathematical, Physical and Engineering Sciences ,186, 343–414.\\nPearson, Karl. 1901. On Lines and Planes of Closest Fit to Systems of Points in Space.\\nPhilosophical Magazine ,2(11), 559–572.\\nPeters, Jonas, Janzing, Dominik, and Sch ¨olkopf, Bernhard. 2017. Elements of Causal\\nInference: Foundations and Learning Algorithms . MIT Press.\\nPetersen, Kaare B., and Pedersen, Michael S. 2012. The Matrix Cookbook . Tech. rept.\\nTechnical University of Denmark.\\nPlatt, John C. 2000. Probabilistic Outputs for Support Vector Machines and Compar-\\nisons to Regularized Likelihood Methods. In: Advances in Large Margin Classiﬁers .\\nPollard, David. 2002. A User’s Guide to Measure Theoretic Probability . Cambridge\\nUniversity Press.\\nPolyak, Roman A. 2016. The Legendre Transformation in Modern Optimization. Pages\\n437–507 of: Goldengorin, B. (ed), Optimization and Its Applications in Control and\\nData Sciences . Springer.\\nPress, William H., Teukolsky, Saul A., Vetterling, William T., and Flannery, Brian P.\\n2007. Numerical Recipes: The Art of Scientiﬁc Computing . Cambridge University\\nPress.\\nProschan, Michael A., and Presnell, Brett. 1998. Expect the Unexpected from Condi-\\ntional Expectation. American Statistician ,52(3), 248–252.\\nRaschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine\\nLearning and Deep Learning with Python, scikit-learn, and TensorFlow . Packt Publish-\\ning.\\nRasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam’s Razor. In: Advances in\\nNeural Information Processing Systems .\\nRasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-\\nvances in Neural Information Processing Systems .\\nRasmussen, Carl E., and Williams, Christopher K. I. 2006. Gaussian Processes for Ma-\\nchine Learning . MIT Press.\\nReid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for\\nBinary Experiments. Journal of Machine Learning Research ,12, 731–817.\\nRifkin, Ryan M., and Lippert, Ross A. 2007. Value Regularization and Fenchel Duality.\\nJournal of Machine Learning Research ,8, 441–479.\\nRockafellar, Ralph T. 1970. Convex Analysis . Princeton University Press.\\nRogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning . Chap-\\nman and Hall/CRC.\\nRosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal\\nInference . Harvard University Press.\\nRosenblatt, Murray. 1956. Remarks on Some Nonparametric Estimates of a Density\\nFunction. Annals of Mathematical Statistics ,27(3), 832–837.\\nRoweis, Sam T. 1998. EM Algorithms for PCA and SPCA. Pages 626–632 of: Advances\\nin Neural Information Processing Systems .\\nRoweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gaussian\\nModels. Neural Computation ,11(2), 305–345.\\nRoy, Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for\\nStatistics . Chapman and Hall/CRC.\\nRubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo\\nMethod . Wiley.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nReferences 403\\nRufﬁni, Paolo. 1799. Teoria Generale delle Equazioni, in cui si Dimostra Impossibile la\\nSoluzione Algebraica delle Equazioni Generali di Grado Superiore al Quarto . Stampe-\\nria di S. Tommaso d’Aquino.\\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning\\nRepresentations by Back-Propagating Errors. Nature ,323(6088), 533–536.\\nSæmundsson, Steind ´or, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein-\\nforcement Learning with Latent Variable Gaussian Processes. In: Proceedings of the\\nConference on Uncertainty in Artiﬁcial Intelligence .\\nSaitoh, Saburou. 1988. Theory of Reproducing Kernels and its Applications . Longman\\nScientiﬁc and Technical.\\nS¨arkk¨a, Simo. 2013. Bayesian Filtering and Smoothing . Cambridge University Press.\\nSch¨olkopf, Bernhard, and Smola, Alexander J. 2002. Learning with Kernels – Support\\nVector Machines, Regularization, Optimization, and Beyond . MIT Press.\\nSch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1997. Kernel\\nPrincipal Component Analysis. In: Proceedings of the International Conference on\\nArtiﬁcial Neural Networks .\\nSch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1998. Nonlinear\\nComponent Analysis as a Kernel Eigenvalue Problem. Neural Computation ,10(5),\\n1299–1319.\\nSch¨olkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized\\nRepresenter Theorem. In: Proceedings of the International Conference on Computa-\\ntional Learning Theory .\\nSchwartz, Laurent. 1964. Sous Espaces Hilbertiens d’Espaces Vectoriels Topologiques\\net Noyaux Associ ´es.Journal d’Analyse Math ´ematique ,13, 115–256.\\nSchwarz, Gideon E. 1978. Estimating the Dimension of a Model. Annals of Statistics ,\\n6(2), 461–464.\\nShahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams, Ryan P., and De Freitas, Nando.\\n2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.\\nProceedings of the IEEE ,104(1), 148–175.\\nShalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning:\\nFrom Theory to Algorithms . Cambridge University Press.\\nShawe-Taylor, John, and Cristianini, Nello. 2004. Kernel Methods for Pattern Analysis .\\nCambridge University Press.\\nShawe-Taylor, John, and Sun, Shiliang. 2011. A Review of Optimization Methodologies\\nin Support Vector Machines. Neurocomputing ,74(17), 3609–3618.\\nShental, Ori, Siegel, Paul H., Wolf, Jack K., Bickson, Danny, and Dolev, Danny. 2008.\\nGaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863–\\n1867 of: Proceedings of the International Symposium on Information Theory .\\nShewchuk, Jonathan R. 1994. An Introduction to the Conjugate Gradient Method with-\\nout the Agonizing Pain .\\nShi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence ,22(8), 888–905.\\nShi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J.,\\nand Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of\\nMachine Learning Research , 2615–2637.\\nShiryayev, Albert N. 1984. Probability . Springer.\\nShor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions . Springer.\\nShotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton-\\nBoost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recog-\\nnition and Segmentation. In: Proceedings of the European Conference on Computer\\nVision .\\nSmith, Adrian F. M., and Spiegelhalter, David. 1980. Bayes Factors and Choice Criteria\\nfor Linear Models. Journal of the Royal Statistical Society B ,42(2), 213–220.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n404 References\\nSnoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op-\\ntimization of Machine Learning Algorithms. In: Advances in Neural Information\\nProcessing Systems .\\nSpearman, Charles. 1904. “General Intelligence,” Objectively Determined and Mea-\\nsured. American Journal of Psychology ,15(2), 201–292.\\nSriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Sch ¨olkopf, Bernhard,\\nand Lanckriet, Gert R. G. 2010. Hilbert Space Embeddings and Metrics on Proba-\\nbility Measures. Journal of Machine Learning Research ,11, 1517–1561.\\nSteinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks.\\nConstructive Approximation ,26, 225–287.\\nSteinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines . Springer.\\nStoer, Josef, and Burlirsch, Roland. 2002. Introduction to Numerical Analysis . Springer.\\nStrang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. The American\\nMathematical Monthly ,100(9), 848–855.\\nStrang, Gilbert. 2003. Introduction to Linear Algebra . Wellesley-Cambridge Press.\\nStray, Jonathan. 2016. The Curious Journalist’s Guide to Data . Tow Center for Digital\\nJournalism at Columbia’s Graduate School of Journalism.\\nStrogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.\\nNotices of the American Mathematical Society ,61(3), 286–291.\\nSucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level\\nVision. Image and Vision Computing ,12(1), 42–60.\\nSzeliski, Richard, Zabih, Ramin, and Scharstein, Daniel, et al. 2008. A Compar-\\native Study of Energy Minimization Methods for Markov Random Fields with\\nSmoothness-Based Priors. IEEE Transactions on Pattern Analysis and Machine In-\\ntelligence ,30(6), 1068–1080.\\nTandra, Haryono. 2014. The Relationship between the Change of Variable Theorem\\nand the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of\\nMathematics ,17(2), 76–83.\\nTenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geometric\\nFramework for Nonlinear Dimensionality Reduction. Science ,290(5500), 2319–\\n2323.\\nTibshirani, Robert. 1996. Regression Selection and Shrinkage via the Lasso. Journal\\nof the Royal Statistical Society B ,58(1), 267–288.\\nTipping, Michael E., and Bishop, Christopher M. 1999. Probabilistic Principal Compo-\\nnent Analysis. Journal of the Royal Statistical Society: Series B ,61(3), 611–622.\\nTitsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent\\nVariable Model. In: Proceedings of the International Conference on Artiﬁcial Intelli-\\ngence and Statistics .\\nToussaint, Marc. 2012. Some Notes on Gradient Descent . https://ipvs.informatik.uni-\\nstuttgart.de/mlr/marc/notes/gradientDescent.pdf.\\nTrefethen, Lloyd N., and Bau III, David. 1997. Numerical Linear Algebra . SIAM.\\nTucker, Ledyard R. 1966. Some Mathematical Notes on Three-Mode Factor Analysis.\\nPsychometrika ,31(3), 279–311.\\nVapnik, Vladimir N. 1998. Statistical Learning Theory . Wiley.\\nVapnik, Vladimir N. 1999. An Overview of Statistical Learning Theory. IEEE Transac-\\ntions on Neural Networks ,10(5), 988–999.\\nVapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory . Springer.\\nVishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt,\\nKarsten M. 2010. Graph Kernels. Journal of Machine Learning Research ,11, 1201–\\n1242.\\nvon Luxburg, Ulrike, and Sch ¨olkopf, Bernhard. 2011. Statistical Learning Theory:\\nModels, Concepts, and Results. Pages 651–706 of: D. M. Gabbay, S. Hartmann,\\nJ. Woods (ed), Handbook of the History of Logic , vol. 10. Elsevier.\\nDraft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\\nReferences 405\\nWahba, Grace. 1990. Spline Models for Observational Data . Society for Industrial and\\nApplied Mathematics.\\nWalpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Ye, Keying. 2011.\\nProbability and Statistics for Engineers and Scientists . Prentice Hall.\\nWasserman, Larry. 2004. All of Statistics . Springer.\\nWasserman, Larry. 2007. All of Nonparametric Statistics . Springer.\\nWhittle, Peter. 2000. Probability via Expectation . Springer.\\nWickham, Hadley. 2014. Tidy Data. Journal of Statistical Software ,59, 1–23.\\nWilliams, Christopher K. I. 1997. Computing with Inﬁnite Networks. In: Advances in\\nNeural Information Processing Systems .\\nYu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv ´ari, Csaba. 2013. Charac-\\nterizing the Representer Theorem. In: Proceedings of the International Conference on\\nMachine Learning .\\nZadrozny, Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti-\\nmates from Decision Trees and Naive Bayesian Classiﬁers. In: Proceedings of the\\nInternational Conference on Machine Learning .\\nZhang, Haizhang, Xu, Yuesheng, and Zhang, Jun. 2009. Reproducing Kernel Banach\\nSpaces for Machine Learning. Journal of Machine Learning Research ,10, 2741–2775.\\nZia, Royce K. P., Redish, Edward F., and McKay, Susan R. 2009. Making Sense of the\\nLegendre Transform. American Journal of Physics ,77(614), 614–622.\\n©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('LD-inference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b3dd287303ef379cd3bfc1446cef400144e42385eabfb9b315f60147448249f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a927fd66",
   "metadata": {},
   "source": [
    "# PDF to text\n",
    "This notebook experiments how to extract text from PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a820fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"../dat/books/mml-book.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95295c96",
   "metadata": {},
   "source": [
    "What is the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ec20a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859745"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079e4db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATHEMATICS  FOR \n",
      "MACHINE LEARNING\n",
      "Marc Peter DeisenrothA. Aldo FaisalCheng Soon Ong\n",
      "MATHEMATICS FOR MACHINE LEARNING DEISENROTH ET AL.\n",
      "The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efﬁ  ciently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the ﬁ  rst time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book’s web site.\n",
      "MARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine Learning at the Department of Computing, Împerial College London.\n",
      "A. ALDO FAISAL  leads the Brain & Behaviour Lab at Imperial College London, where he is also Reader in Neurotechnology at the Department of Bioengineering and the Department of Computing.\n",
      "CHENG SOON ONG  is Principal Research Scientist at the Machine Learning Research Group, Data61, CSIRO. He is also Adjunct Associate Professor at Australian National University.\n",
      "Cover image courtesy of Daniel Bosma / Moment / Getty ImagesCover design by Holly Johnson\n",
      "Deisenrith et al. 9781108455145 Cover. C M Y K\n",
      "\n",
      "Contents\n",
      "Foreword 1\n",
      "Part I Mathematical Foundations 9\n",
      "1 Introduction and Motivation 11\n",
      "1.1 Finding Words for Intuitions 12\n",
      "1.2 Two Ways to Read This Book 13\n",
      "1.3 Exercises and Feedback 16\n",
      "2 Linear Algebra 17\n",
      "2.1 Systems of Linear Equations 19\n",
      "2.2 Matrices 22\n",
      "2.3 Solving Systems of Linear Equations 27\n",
      "2.4 Vector Spaces 35\n",
      "2.5 Linear Independence 40\n",
      "2.6 Basis and Rank 44\n",
      "2.7 Linear Mappings 48\n",
      "2.8 Afﬁne Spaces 61\n",
      "2.9 Further Reading 63\n",
      "Exercises 64\n",
      "3 Analytic Geometry 70\n",
      "3.1 Norms 71\n",
      "3.2 Inner Products 72\n",
      "3.3 Lengths and Distances 75\n",
      "3.4 Angles and Orthogonality 76\n",
      "3.5 Orthonormal Basis 78\n",
      "3.6 Orthogonal Complement 79\n",
      "3.7 Inner Product of Functions 80\n",
      "3.8 Orthogonal Projections 81\n",
      "3.9 Rotations 91\n",
      "3.10 Further Reading 94\n",
      "Exercises 96\n",
      "4 Matrix Decompositions 98\n",
      "4.1 Determinant and Trace 99\n",
      "i\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "ii Contents\n",
      "4.2 Eigenvalues and Eigenvectors 105\n",
      "4.3 Cholesky Decomposition 114\n",
      "4.4 Eigendecomposition and Diagonalization 115\n",
      "4.5 Singular Value Decomposition 119\n",
      "4.6 Matrix Approximation 129\n",
      "4.7 Matrix Phylogeny 134\n",
      "4.8 Further Reading 135\n",
      "Exercises 137\n",
      "5 Vector Calculus 139\n",
      "5.1 Differentiation of Univariate Functions 141\n",
      "5.2 Partial Differentiation and Gradients 146\n",
      "5.3 Gradients of Vector-Valued Functions 149\n",
      "5.4 Gradients of Matrices 155\n",
      "5.5 Useful Identities for Computing Gradients 158\n",
      "5.6 Backpropagation and Automatic Differentiation 159\n",
      "5.7 Higher-Order Derivatives 164\n",
      "5.8 Linearization and Multivariate Taylor Series 165\n",
      "5.9 Further Reading 170\n",
      "Exercises 170\n",
      "6 Probability and Distributions 172\n",
      "6.1 Construction of a Probability Space 172\n",
      "6.2 Discrete and Continuous Probabilities 178\n",
      "6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\n",
      "6.4 Summary Statistics and Independence 186\n",
      "6.5 Gaussian Distribution 197\n",
      "6.6 Conjugacy and the Exponential Family 205\n",
      "6.7 Change of Variables/Inverse Transform 214\n",
      "6.8 Further Reading 221\n",
      "Exercises 222\n",
      "7 Continuous Optimization 225\n",
      "7.1 Optimization Using Gradient Descent 227\n",
      "7.2 Constrained Optimization and Lagrange Multipliers 233\n",
      "7.3 Convex Optimization 236\n",
      "7.4 Further Reading 246\n",
      "Exercises 247\n",
      "Part II Central Machine Learning Problems 249\n",
      "8 When Models Meet Data 251\n",
      "8.1 Data, Models, and Learning 251\n",
      "8.2 Empirical Risk Minimization 258\n",
      "8.3 Parameter Estimation 265\n",
      "8.4 Probabilistic Modeling and Inference 272\n",
      "8.5 Directed Graphical Models 278\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Contents iii\n",
      "8.6 Model Selection 283\n",
      "9 Linear Regression 289\n",
      "9.1 Problem Formulation 291\n",
      "9.2 Parameter Estimation 292\n",
      "9.3 Bayesian Linear Regression 303\n",
      "9.4 Maximum Likelihood as Orthogonal Projection 313\n",
      "9.5 Further Reading 315\n",
      "10 Dimensionality Reduction with Principal Component Analysis 317\n",
      "10.1 Problem Setting 318\n",
      "10.2 Maximum Variance Perspective 320\n",
      "10.3 Projection Perspective 325\n",
      "10.4 Eigenvector Computation and Low-Rank Approximations 333\n",
      "10.5 PCA in High Dimensions 335\n",
      "10.6 Key Steps of PCA in Practice 336\n",
      "10.7 Latent Variable Perspective 339\n",
      "10.8 Further Reading 343\n",
      "11 Density Estimation with Gaussian Mixture Models 348\n",
      "11.1 Gaussian Mixture Model 349\n",
      "11.2 Parameter Learning via Maximum Likelihood 350\n",
      "11.3 EM Algorithm 360\n",
      "11.4 Latent-Variable Perspective 363\n",
      "11.5 Further Reading 368\n",
      "12 Classiﬁcation with Support Vector Machines 370\n",
      "12.1 Separating Hyperplanes 372\n",
      "12.2 Primal Support Vector Machine 374\n",
      "12.3 Dual Support Vector Machine 383\n",
      "12.4 Kernels 388\n",
      "12.5 Numerical Solution 390\n",
      "12.6 Further Reading 392\n",
      "References 395\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "\n",
      "Foreword\n",
      "Machine learning is the latest in a long line of attempts to distill human\n",
      "knowledge and reasoning into a form that is suitable for constructing ma-\n",
      "chines and engineering automated systems. As machine learning becomes\n",
      "more ubiquitous and its software packages become easier to use, it is nat-\n",
      "ural and desirable that the low-level technical details are abstracted away\n",
      "and hidden from the practitioner. However, this brings with it the danger\n",
      "that a practitioner becomes unaware of the design decisions and, hence,\n",
      "the limits of machine learning algorithms.\n",
      "The enthusiastic practitioner who is interested to learn more about the\n",
      "magic behind successful machine learning algorithms currently faces a\n",
      "daunting set of pre-requisite knowledge:\n",
      "Programming languages and data analysis tools\n",
      "Large-scale computation and the associated frameworks\n",
      "Mathematics and statistics and how machine learning builds on it\n",
      "At universities, introductory courses on machine learning tend to spend\n",
      "early parts of the course covering some of these pre-requisites. For histori-\n",
      "cal reasons, courses in machine learning tend to be taught in the computer\n",
      "science department, where students are often trained in the ﬁrst two areas\n",
      "of knowledge, but not so much in mathematics and statistics.\n",
      "Current machine learning textbooks primarily focus on machine learn-\n",
      "ing algorithms and methodologies and assume that the reader is com-\n",
      "petent in mathematics and statistics. Therefore, these books only spend\n",
      "one or two chapters on background mathematics, either at the beginning\n",
      "of the book or as appendices. We have found many people who want to\n",
      "delve into the foundations of basic machine learning methods who strug-\n",
      "gle with the mathematical knowledge required to read a machine learning\n",
      "textbook. Having taught undergraduate and graduate courses at universi-\n",
      "ties, we ﬁnd that the gap between high school mathematics and the math-\n",
      "ematics level required to read a standard machine learning textbook is too\n",
      "big for many people.\n",
      "This book brings the mathematical foundations of basic machine learn-\n",
      "ing concepts to the fore and collects the information in a single place so\n",
      "that this skills gap is narrowed or even closed.\n",
      "1\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "2 Foreword\n",
      "Why Another Book on Machine Learning?\n",
      "Machine learning builds upon the language of mathematics to express\n",
      "concepts that seem intuitively obvious but that are surprisingly difﬁcult\n",
      "to formalize. Once formalized properly, we can gain insights into the task\n",
      "we want to solve. One common complaint of students of mathematics\n",
      "around the globe is that the topics covered seem to have little relevance\n",
      "to practical problems. We believe that machine learning is an obvious and\n",
      "direct motivation for people to learn mathematics.\n",
      "This book is intended to be a guidebook to the vast mathematical lit-\n",
      "erature that forms the foundations of modern machine learning. We mo- “Math is linked in\n",
      "the popular mind\n",
      "with phobia and\n",
      "anxiety. You’d think\n",
      "we’re discussing\n",
      "spiders.” (Strogatz,\n",
      "2014, page 281)tivate the need for mathematical concepts by directly pointing out their\n",
      "usefulness in the context of fundamental machine learning problems. In\n",
      "the interest of keeping the book short, many details and more advanced\n",
      "concepts have been left out. Equipped with the basic concepts presented\n",
      "here, and how they ﬁt into the larger context of machine learning, the\n",
      "reader can ﬁnd numerous resources for further study, which we provide at\n",
      "the end of the respective chapters. For readers with a mathematical back-\n",
      "ground, this book provides a brief but precisely stated glimpse of machine\n",
      "learning. In contrast to other books that focus on methods and models\n",
      "of machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar-\n",
      "ber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers\n",
      "and Girolami, 2016) or programmatic aspects of machine learning (M ¨uller\n",
      "and Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018),\n",
      "we provide only four representative examples of machine learning algo-\n",
      "rithms. Instead, we focus on the mathematical concepts behind the models\n",
      "themselves. We hope that readers will be able to gain a deeper understand-\n",
      "ing of the basic questions in machine learning and connect practical ques-\n",
      "tions arising from the use of machine learning with fundamental choices\n",
      "in the mathematical model.\n",
      "We do not aim to write a classical machine learning book. Instead, our\n",
      "intention is to provide the mathematical background, applied to four cen-\n",
      "tral machine learning problems, to make it easier to read other machine\n",
      "learning textbooks.\n",
      "Who Is the Target Audience?\n",
      "As applications of machine learning become widespread in society, we\n",
      "believe that everybody should have some understanding of its underlying\n",
      "principles. This book is written in an academic mathematical style, which\n",
      "enables us to be precise about the concepts behind machine learning. We\n",
      "encourage readers unfamiliar with this seemingly terse style to persevere\n",
      "and to keep the goals of each topic in mind. We sprinkle comments and\n",
      "remarks throughout the text, in the hope that it provides useful guidance\n",
      "with respect to the big picture.\n",
      "The book assumes the reader to have mathematical knowledge commonly\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Foreword 3\n",
      "covered in high school mathematics and physics. For example, the reader\n",
      "should have seen derivatives and integrals before, and geometric vectors\n",
      "in two or three dimensions. Starting from there, we generalize these con-\n",
      "cepts. Therefore, the target audience of the book includes undergraduate\n",
      "university students, evening learners and learners participating in online\n",
      "machine learning courses.\n",
      "In analogy to music, there are three types of interaction that people\n",
      "have with machine learning:\n",
      "Astute Listener The democratization of machine learning by the pro-\n",
      "vision of open-source software, online tutorials and cloud-based tools al-\n",
      "lows users to not worry about the speciﬁcs of pipelines. Users can focus on\n",
      "extracting insights from data using off-the-shelf tools. This enables non-\n",
      "tech-savvy domain experts to beneﬁt from machine learning. This is sim-\n",
      "ilar to listening to music; the user is able to choose and discern between\n",
      "different types of machine learning, and beneﬁts from it. More experi-\n",
      "enced users are like music critics, asking important questions about the\n",
      "application of machine learning in society such as ethics, fairness, and pri-\n",
      "vacy of the individual. We hope that this book provides a foundation for\n",
      "thinking about the certiﬁcation and risk management of machine learning\n",
      "systems, and allows them to use their domain expertise to build better\n",
      "machine learning systems.\n",
      "Experienced Artist Skilled practitioners of machine learning can plug\n",
      "and play different tools and libraries into an analysis pipeline. The stereo-\n",
      "typical practitioner would be a data scientist or engineer who understands\n",
      "machine learning interfaces and their use cases, and is able to perform\n",
      "wonderful feats of prediction from data. This is similar to a virtuoso play-\n",
      "ing music, where highly skilled practitioners can bring existing instru-\n",
      "ments to life and bring enjoyment to their audience. Using the mathe-\n",
      "matics presented here as a primer, practitioners would be able to under-\n",
      "stand the beneﬁts and limits of their favorite method, and to extend and\n",
      "generalize existing machine learning algorithms. We hope that this book\n",
      "provides the impetus for more rigorous and principled development of\n",
      "machine learning methods.\n",
      "Fledgling Composer As machine learning is applied to new domains,\n",
      "developers of machine learning need to develop new methods and extend\n",
      "existing algorithms. They are often researchers who need to understand\n",
      "the mathematical basis of machine learning and uncover relationships be-\n",
      "tween different tasks. This is similar to composers of music who, within\n",
      "the rules and structure of musical theory, create new and amazing pieces.\n",
      "We hope this book provides a high-level overview of other technical books\n",
      "for people who want to become composers of machine learning. There is\n",
      "a great need in society for new researchers who are able to propose and\n",
      "explore novel approaches for attacking the many challenges of learning\n",
      "from data.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "4 Foreword\n",
      "Acknowledgments\n",
      "We are grateful to many people who looked at early drafts of the book\n",
      "and suffered through painful expositions of concepts. We tried to imple-\n",
      "ment their ideas that we did not vehemently disagree with. We would\n",
      "like to especially acknowledge Christfried Webers for his careful reading\n",
      "of many parts of the book, and his detailed suggestions on structure and\n",
      "presentation. Many friends and colleagues have also been kind enough\n",
      "to provide their time and energy on different versions of each chapter.\n",
      "We have been lucky to beneﬁt from the generosity of the online commu-\n",
      "nity, who have suggested improvements via https://github.com , which\n",
      "greatly improved the book.\n",
      "The following people have found bugs, proposed clariﬁcations and sug-\n",
      "gested relevant literature, either via https://github.com or personal\n",
      "communication. Their names are sorted alphabetically.\n",
      "Abdul-Ganiy Usman\n",
      "Adam Gaier\n",
      "Adele Jackson\n",
      "Aditya Menon\n",
      "Alasdair Tran\n",
      "Aleksandar Krnjaic\n",
      "Alexander Makrigiorgos\n",
      "Alfredo Canziani\n",
      "Ali Shafti\n",
      "Amr Khalifa\n",
      "Andrew Tanggara\n",
      "Angus Gruen\n",
      "Antal A. Buss\n",
      "Antoine Toisoul Le Cann\n",
      "Areg Sarvazyan\n",
      "Artem Artemev\n",
      "Artyom Stepanov\n",
      "Bill Kromydas\n",
      "Bob Williamson\n",
      "Boon Ping Lim\n",
      "Chao Qu\n",
      "Cheng Li\n",
      "Chris Sherlock\n",
      "Christopher Gray\n",
      "Daniel McNamara\n",
      "Daniel Wood\n",
      "Darren Siegel\n",
      "David Johnston\n",
      "Dawei ChenEllen Broad\n",
      "Fengkuangtian Zhu\n",
      "Fiona Condon\n",
      "Georgios Theodorou\n",
      "He Xin\n",
      "Irene Raissa Kameni\n",
      "Jakub Nabaglo\n",
      "James Hensman\n",
      "Jamie Liu\n",
      "Jean Kaddour\n",
      "Jean-Paul Ebejer\n",
      "Jerry Qiang\n",
      "Jitesh Sindhare\n",
      "John Lloyd\n",
      "Jonas Ngnawe\n",
      "Jon Martin\n",
      "Justin Hsi\n",
      "Kai Arulkumaran\n",
      "Kamil Dreczkowski\n",
      "Lily Wang\n",
      "Lionel Tondji Ngoupeyou\n",
      "Lydia Kn ¨uﬁng\n",
      "Mahmoud Aslan\n",
      "Mark Hartenstein\n",
      "Mark van der Wilk\n",
      "Markus Hegland\n",
      "Martin Hewing\n",
      "Matthew Alger\n",
      "Matthew Lee\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Foreword 5\n",
      "Maximus McCann\n",
      "Mengyan Zhang\n",
      "Michael Bennett\n",
      "Michael Pedersen\n",
      "Minjeong Shin\n",
      "Mohammad Malekzadeh\n",
      "Naveen Kumar\n",
      "Nico Montali\n",
      "Oscar Armas\n",
      "Patrick Henriksen\n",
      "Patrick Wieschollek\n",
      "Pattarawat Chormai\n",
      "Paul Kelly\n",
      "Petros Christodoulou\n",
      "Piotr Januszewski\n",
      "Pranav Subramani\n",
      "Quyu Kong\n",
      "Ragib Zaman\n",
      "Rui Zhang\n",
      "Ryan-Rhys Grifﬁths\n",
      "Salomon Kabongo\n",
      "Samuel Ogunmola\n",
      "Sandeep Mavadia\n",
      "Sarvesh Nikumbh\n",
      "Sebastian Raschka\n",
      "Senanayak Sesh Kumar Karri\n",
      "Seung-Heon Baek\n",
      "Shahbaz ChaudharyShakir Mohamed\n",
      "Shawn Berry\n",
      "Sheikh Abdul Raheem Ali\n",
      "Sheng Xue\n",
      "Sridhar Thiagarajan\n",
      "Syed Nouman Hasany\n",
      "Szymon Brych\n",
      "Thomas B ¨uhler\n",
      "Timur Sharapov\n",
      "Tom Melamed\n",
      "Vincent Adam\n",
      "Vincent Dutordoir\n",
      "Vu Minh\n",
      "Wasim Aftab\n",
      "Wen Zhi\n",
      "Wojciech Stokowiec\n",
      "Xiaonan Chong\n",
      "Xiaowei Zhang\n",
      "Yazhou Hao\n",
      "Yicheng Luo\n",
      "Young Lee\n",
      "Yu Lu\n",
      "Yun Cheng\n",
      "Yuxiao Huang\n",
      "Zac Cranko\n",
      "Zijian Cao\n",
      "Zoe Nolan\n",
      "Contributors through GitHub, whose real names were not listed on their\n",
      "GitHub proﬁle, are:\n",
      "SamDataMad\n",
      "bumptiousmonkey\n",
      "idoamihai\n",
      "deepakiiminsad\n",
      "HorizonP\n",
      "cs-maillist\n",
      "kudo23empet\n",
      "victorBigand\n",
      "17SKYE\n",
      "jessjing1995\n",
      "We are also very grateful to Parameswaran Raman and the many anony-\n",
      "mous reviewers, organized by Cambridge University Press, who read one\n",
      "or more chapters of earlier versions of the manuscript, and provided con-\n",
      "structive criticism that led to considerable improvements. A special men-\n",
      "tion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt\n",
      "advice about LATEX-related issues. Last but not least, we are very grateful\n",
      "to our editor Lauren Cowles, who has been patiently guiding us through\n",
      "the gestation process of this book.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "6 Foreword\n",
      "Table of Symbols\n",
      "Symbol Typical meaning\n",
      " Scalars are lowercase\n",
      "x;y;z Vectors are bold lowercase\n",
      "A;B;C Matrices are bold uppercase\n",
      "x>;A>Transpose of a vector or matrix\n",
      "A\u00001Inverse of a matrix\n",
      "hx;yi Inner product of xandy\n",
      "x>y Dot product of xandy\n",
      "B= (b1;b2;b3)(Ordered) tuple\n",
      "B= [b1;b2;b3]Matrix of column vectors stacked horizontally\n",
      "B=fb1;b2;b3gSet of vectors (unordered)\n",
      "Z;N Integers and natural numbers, respectively\n",
      "R;C Real and complex numbers, respectively\n",
      "Rnn-dimensional vector space of real numbers\n",
      "8x Universal quantiﬁer: for all x\n",
      "9x Existential quantiﬁer: there exists x\n",
      "a:=b a is deﬁned as b\n",
      "a=:b b is deﬁned as a\n",
      "a/b a is proportional to b, i.e.,a=constant\u0001b\n",
      "g\u000ef Function composition: “ gafterf”\n",
      "() If and only if\n",
      "=) Implies\n",
      "A;C Sets\n",
      "a2A ais an element of set A\n",
      "; Empty set\n",
      "AnB A withoutB: the set of elements in Abut not inB\n",
      "D Number of dimensions; indexed by d= 1;:::;D\n",
      "N Number of data points; indexed by n= 1;:::;N\n",
      "Im Identity matrix of size m\u0002m\n",
      "0m;n Matrix of zeros of size m\u0002n\n",
      "1m;n Matrix of ones of size m\u0002n\n",
      "ei Standard/canonical vector (where iis the component that is 1)\n",
      "dim Dimensionality of vector space\n",
      "rk(A) Rank of matrix A\n",
      "Im) Image of linear mapping\n",
      "ker) Kernel (null space) of a linear mapping\n",
      "span[b1] Span (generating set) of b1\n",
      "tr(A) Trace ofA\n",
      "det(A) Determinant of A\n",
      "j\u0001j Absolute value or determinant (depending on context)\n",
      "k\u0001k Norm; Euclidean, unless speciﬁed\n",
      "\u0015 Eigenvalue or Lagrange multiplier\n",
      "E\u0015 Eigenspace corresponding to eigenvalue \u0015\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Foreword 7\n",
      "Symbol Typical meaning\n",
      "x?y Vectorsxandyare orthogonal\n",
      "V Vector space\n",
      "V?Orthogonal complement of vector space VPN\n",
      "n=1xn Sum of thexn:x1+:::+xNQN\n",
      "n=1xn Product of the xn:x1\u0001:::\u0001xN\n",
      "\u0012 Parameter vector\n",
      "@f\n",
      "@xPartial derivative of fwith respect to x\n",
      "df\n",
      "dxTotal derivative of fwith respect to x\n",
      "r Gradient\n",
      "f\u0003= minxf(x) The smallest function value of f\n",
      "x\u00032arg minxf(x)The valuex\u0003that minimizes f(note: arg min returns a set of values)\n",
      "L Lagrangian\n",
      "L Negative log-likelihood\u0000n\n",
      "k\u0001\n",
      "Binomial coefﬁcient, nchoosek\n",
      "VX[x] Variance ofxwith respect to the random variable X\n",
      "EX[x] Expectation of xwith respect to the random variable X\n",
      "CovX;Y[x;y] Covariance between xandy.\n",
      "X? ?YjZ X is conditionally independent of YgivenZ\n",
      "X\u0018p Random variable Xis distributed according to p\n",
      "N\u0000\u0016;\u0006\u0001\n",
      "Gaussian distribution with mean \u0016and covariance \u0006\n",
      "Ber(\u0016) Bernoulli distribution with parameter \u0016\n",
      "Bin(N;\u0016) Binomial distribution with parameters N;\u0016\n",
      "Beta(\u000b;\f) Beta distribution with parameters \u000b;\f\n",
      "Table of Abbreviations and Acronyms\n",
      "Acronym Meaning\n",
      "e.g. Exempli gratia (Latin: for example)\n",
      "GMM Gaussian mixture model\n",
      "i.e. Id est (Latin: this means)\n",
      "i.i.d. Independent, identically distributed\n",
      "MAP Maximum a posteriori\n",
      "MLE Maximum likelihood estimation/estimator\n",
      "ONB Orthonormal basis\n",
      "PCA Principal component analysis\n",
      "PPCA Probabilistic principal component analysis\n",
      "REF Row-echelon form\n",
      "SPD Symmetric, positive deﬁnite\n",
      "SVM Support vector machine\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "\n",
      "Part I\n",
      "Mathematical Foundations\n",
      "9\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "\n",
      "1\n",
      "Introduction and Motivation\n",
      "Machine learning is about designing algorithms that automatically extract\n",
      "valuable information from data. The emphasis here is on “automatic”, i.e.,\n",
      "machine learning is concerned about general-purpose methodologies that\n",
      "can be applied to many datasets, while producing something that is mean-\n",
      "ingful. There are three concepts that are at the core of machine learning:\n",
      "data, a model, and learning.\n",
      "Since machine learning is inherently data driven, data is at the core data\n",
      "of machine learning. The goal of machine learning is to design general-\n",
      "purpose methodologies to extract valuable patterns from data, ideally\n",
      "without much domain-speciﬁc expertise. For example, given a large corpus\n",
      "of documents (e.g., books in many libraries), machine learning methods\n",
      "can be used to automatically ﬁnd relevant topics that are shared across\n",
      "documents (Hoffman et al., 2010). To achieve this goal, we design mod-\n",
      "elsthat are typically related to the process that generates data, similar to model\n",
      "the dataset we are given. For example, in a regression setting, the model\n",
      "would describe a function that maps inputs to real-valued outputs. To\n",
      "paraphrase Mitchell (1997): A model is said to learn from data if its per-\n",
      "formance on a given task improves after the data is taken into account.\n",
      "The goal is to ﬁnd good models that generalize well to yet unseen data,\n",
      "which we may care about in the future. Learning can be understood as a learning\n",
      "way to automatically ﬁnd patterns and structure in data by optimizing the\n",
      "parameters of the model.\n",
      "While machine learning has seen many success stories, and software is\n",
      "readily available to design and train rich and ﬂexible machine learning\n",
      "systems, we believe that the mathematical foundations of machine learn-\n",
      "ing are important in order to understand fundamental principles upon\n",
      "which more complicated machine learning systems are built. Understand-\n",
      "ing these principles can facilitate creating new machine learning solutions,\n",
      "understanding and debugging existing approaches, and learning about the\n",
      "inherent assumptions and limitations of the methodologies we are work-\n",
      "ing with.\n",
      "11\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "12 Introduction and Motivation\n",
      "1.1 Finding Words for Intuitions\n",
      "A challenge we face regularly in machine learning is that concepts and\n",
      "words are slippery, and a particular component of the machine learning\n",
      "system can be abstracted to different mathematical concepts. For example,\n",
      "the word “algorithm” is used in at least two different senses in the con-\n",
      "text of machine learning. In the ﬁrst sense, we use the phrase “machine\n",
      "learning algorithm” to mean a system that makes predictions based on in-\n",
      "put data. We refer to these algorithms as predictors . In the second sense, predictor\n",
      "we use the exact same phrase “machine learning algorithm” to mean a\n",
      "system that adapts some internal parameters of the predictor so that it\n",
      "performs well on future unseen input data. Here we refer to this adapta-\n",
      "tion as training a system. training\n",
      "This book will not resolve the issue of ambiguity, but we want to high-\n",
      "light upfront that, depending on the context, the same expressions can\n",
      "mean different things. However, we attempt to make the context sufﬁ-\n",
      "ciently clear to reduce the level of ambiguity.\n",
      "The ﬁrst part of this book introduces the mathematical concepts and\n",
      "foundations needed to talk about the three main components of a machine\n",
      "learning system: data, models, and learning. We will brieﬂy outline these\n",
      "components here, and we will revisit them again in Chapter 8 once we\n",
      "have discussed the necessary mathematical concepts.\n",
      "While not all data is numerical, it is often useful to consider data in\n",
      "a number format. In this book, we assume that data has already been\n",
      "appropriately converted into a numerical representation suitable for read-\n",
      "ing into a computer program. Therefore, we think of data as vectors. As data as vectors\n",
      "another illustration of how subtle words are, there are (at least) three\n",
      "different ways to think about vectors: a vector as an array of numbers (a\n",
      "computer science view), a vector as an arrow with a direction and magni-\n",
      "tude (a physics view), and a vector as an object that obeys addition and\n",
      "scaling (a mathematical view).\n",
      "Amodel is typically used to describe a process for generating data, sim- model\n",
      "ilar to the dataset at hand. Therefore, good models can also be thought\n",
      "of as simpliﬁed versions of the real (unknown) data-generating process,\n",
      "capturing aspects that are relevant for modeling the data and extracting\n",
      "hidden patterns from it. A good model can then be used to predict what\n",
      "would happen in the real world without performing real-world experi-\n",
      "ments.\n",
      "We now come to the crux of the matter, the learning component of learning\n",
      "machine learning. Assume we are given a dataset and a suitable model.\n",
      "Training the model means to use the data available to optimize some pa-\n",
      "rameters of the model with respect to a utility function that evaluates how\n",
      "well the model predicts the training data. Most training methods can be\n",
      "thought of as an approach analogous to climbing a hill to reach its peak.\n",
      "In this analogy, the peak of the hill corresponds to a maximum of some\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "1.2 Two Ways to Read This Book 13\n",
      "desired performance measure. However, in practice, we are interested in\n",
      "the model to perform well on unseen data. Performing well on data that\n",
      "we have already seen (training data) may only mean that we found a\n",
      "good way to memorize the data. However, this may not generalize well to\n",
      "unseen data, and, in practical applications, we often need to expose our\n",
      "machine learning system to situations that it has not encountered before.\n",
      "Let us summarize the main concepts of machine learning that we cover\n",
      "in this book:\n",
      "We represent data as vectors.\n",
      "We choose an appropriate model, either using the probabilistic or opti-\n",
      "mization view.\n",
      "We learn from available data by using numerical optimization methods\n",
      "with the aim that the model performs well on data not used for training.\n",
      "1.2 Two Ways to Read This Book\n",
      "We can consider two strategies for understanding the mathematics for\n",
      "machine learning:\n",
      "Bottom-up: Building up the concepts from foundational to more ad-\n",
      "vanced. This is often the preferred approach in more technical ﬁelds,\n",
      "such as mathematics. This strategy has the advantage that the reader\n",
      "at all times is able to rely on their previously learned concepts. Unfor-\n",
      "tunately, for a practitioner many of the foundational concepts are not\n",
      "particularly interesting by themselves, and the lack of motivation means\n",
      "that most foundational deﬁnitions are quickly forgotten.\n",
      "Top-down: Drilling down from practical needs to more basic require-\n",
      "ments. This goal-driven approach has the advantage that the readers\n",
      "know at all times why they need to work on a particular concept, and\n",
      "there is a clear path of required knowledge. The downside of this strat-\n",
      "egy is that the knowledge is built on potentially shaky foundations, and\n",
      "the readers have to remember a set of words that they do not have any\n",
      "way of understanding.\n",
      "We decided to write this book in a modular way to separate foundational\n",
      "(mathematical) concepts from applications so that this book can be read\n",
      "in both ways. The book is split into two parts, where Part I lays the math-\n",
      "ematical foundations and Part II applies the concepts from Part I to a set\n",
      "of fundamental machine learning problems, which form four pillars of\n",
      "machine learning as illustrated in Figure 1.1: regression, dimensionality\n",
      "reduction, density estimation, and classiﬁcation. Chapters in Part I mostly\n",
      "build upon the previous ones, but it is possible to skip a chapter and work\n",
      "backward if necessary. Chapters in Part II are only loosely coupled and\n",
      "can be read in any order. There are many pointers forward and backward\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "14 Introduction and Motivation\n",
      "Figure 1.1 The\n",
      "foundations and\n",
      "four pillars of\n",
      "machine learning.\n",
      "Classiﬁcation  \n",
      "Density  \n",
      "Estimation  \n",
      "Regression  \n",
      "Dimensionality  \n",
      "Reduction  \n",
      "Machine Learning\n",
      "Vector Calculus Probability & Distributions Optimization\n",
      "Analytic Geometry Matrix Decomposition Linear Algebra\n",
      "between the two parts of the book to link mathematical concepts with\n",
      "machine learning algorithms.\n",
      "Of course there are more than two ways to read this book. Most readers\n",
      "learn using a combination of top-down and bottom-up approaches, some-\n",
      "times building up basic mathematical skills before attempting more com-\n",
      "plex concepts, but also choosing topics based on applications of machine\n",
      "learning.\n",
      "Part I Is about Mathematics\n",
      "The four pillars of machine learning we cover in this book (see Figure 1.1)\n",
      "require a solid mathematical foundation, which is laid out in Part I.\n",
      "We represent numerical data as vectors and represent a table of such\n",
      "data as a matrix. The study of vectors and matrices is called linear algebra ,\n",
      "which we introduce in Chapter 2. The collection of vectors as a matrix is linear algebra\n",
      "also described there.\n",
      "Given two vectors representing two objects in the real world, we want\n",
      "to make statements about their similarity. The idea is that vectors that\n",
      "are similar should be predicted to have similar outputs by our machine\n",
      "learning algorithm (our predictor). To formalize the idea of similarity be-\n",
      "tween vectors, we need to introduce operations that take two vectors as\n",
      "input and return a numerical value representing their similarity. The con-\n",
      "struction of similarity and distances is central to analytic geometry and is analytic geometry\n",
      "discussed in Chapter 3.\n",
      "In Chapter 4, we introduce some fundamental concepts about matri-\n",
      "ces and matrix decomposition . Some operations on matrices are extremely matrix\n",
      "decomposition useful in machine learning, and they allow for an intuitive interpretation\n",
      "of the data and more efﬁcient learning.\n",
      "We often consider data to be noisy observations of some true underly-\n",
      "ing signal. We hope that by applying machine learning we can identify the\n",
      "signal from the noise. This requires us to have a language for quantify-\n",
      "ing what “noise” means. We often would also like to have predictors that\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "1.2 Two Ways to Read This Book 15\n",
      "allow us to express some sort of uncertainty, e.g., to quantify the conﬁ-\n",
      "dence we have about the value of the prediction at a particular test data\n",
      "point. Quantiﬁcation of uncertainty is the realm of probability theory and probability theory\n",
      "is covered in Chapter 6.\n",
      "To train machine learning models, we typically ﬁnd parameters that\n",
      "maximize some performance measure. Many optimization techniques re-\n",
      "quire the concept of a gradient, which tells us the direction in which to\n",
      "search for a solution. Chapter 5 is about vector calculus and details the vector calculus\n",
      "concept of gradients, which we subsequently use in Chapter 7, where we\n",
      "talk about optimization to ﬁnd maxima/minima of functions. optimization\n",
      "Part II Is about Machine Learning\n",
      "The second part of the book introduces four pillars of machine learning\n",
      "as shown in Figure 1.1. We illustrate how the mathematical concepts in-\n",
      "troduced in the ﬁrst part of the book are the foundation for each pillar.\n",
      "Broadly speaking, chapters are ordered by difﬁculty (in ascending order).\n",
      "In Chapter 8, we restate the three components of machine learning\n",
      "(data, models, and parameter estimation) in a mathematical fashion. In\n",
      "addition, we provide some guidelines for building experimental set-ups\n",
      "that guard against overly optimistic evaluations of machine learning sys-\n",
      "tems. Recall that the goal is to build a predictor that performs well on\n",
      "unseen data.\n",
      "In Chapter 9, we will have a close look at linear regression , where our linear regression\n",
      "objective is to ﬁnd functions that map inputs x2RDto corresponding ob-\n",
      "served function values y2R, which we can interpret as the labels of their\n",
      "respective inputs. We will discuss classical model ﬁtting (parameter esti-\n",
      "mation) via maximum likelihood and maximum a posteriori estimation,\n",
      "as well as Bayesian linear regression, where we integrate the parameters\n",
      "out instead of optimizing them.\n",
      "Chapter 10 focuses on dimensionality reduction , the second pillar in Fig- dimensionality\n",
      "reduction ure 1.1, using principal component analysis. The key objective of dimen-\n",
      "sionality reduction is to ﬁnd a compact, lower-dimensional representation\n",
      "of high-dimensional data x2RD, which is often easier to analyze than\n",
      "the original data. Unlike regression, dimensionality reduction is only con-\n",
      "cerned about modeling the data – there are no labels associated with a\n",
      "data pointx.\n",
      "In Chapter 11, we will move to our third pillar: density estimation . The density estimation\n",
      "objective of density estimation is to ﬁnd a probability distribution that de-\n",
      "scribes a given dataset. We will focus on Gaussian mixture models for this\n",
      "purpose, and we will discuss an iterative scheme to ﬁnd the parameters of\n",
      "this model. As in dimensionality reduction, there are no labels associated\n",
      "with the data points x2RD. However, we do not seek a low-dimensional\n",
      "representation of the data. Instead, we are interested in a density model\n",
      "that describes the data.\n",
      "Chapter 12 concludes the book with an in-depth discussion of the fourth\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "16 Introduction and Motivation\n",
      "pillar: classiﬁcation . We will discuss classiﬁcation in the context of support classiﬁcation\n",
      "vector machines. Similar to regression (Chapter 9), we have inputs xand\n",
      "corresponding labels y. However, unlike regression, where the labels were\n",
      "real-valued, the labels in classiﬁcation are integers, which requires special\n",
      "care.\n",
      "1.3 Exercises and Feedback\n",
      "We provide some exercises in Part I, which can be done mostly by pen and\n",
      "paper. For Part II, we provide programming tutorials (jupyter notebooks)\n",
      "to explore some properties of the machine learning algorithms we discuss\n",
      "in this book.\n",
      "We appreciate that Cambridge University Press strongly supports our\n",
      "aim to democratize education and learning by making this book freely\n",
      "available for download at\n",
      "https://mml-book.com\n",
      "where tutorials, errata, and additional materials can be found. Mistakes\n",
      "can be reported and feedback provided using the preceding URL.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2\n",
      "Linear Algebra\n",
      "When formalizing intuitive concepts, a common approach is to construct a\n",
      "set of objects (symbols) and a set of rules to manipulate these objects. This\n",
      "is known as an algebra . Linear algebra is the study of vectors and certain algebra\n",
      "rules to manipulate vectors. The vectors many of us know from school are\n",
      "called “geometric vectors”, which are usually denoted by a small arrow\n",
      "above the letter, e.g.,\u0000 !xand\u0000 !y. In this book, we discuss more general\n",
      "concepts of vectors and use a bold letter to represent them, e.g., xandy.\n",
      "In general, vectors are special objects that can be added together and\n",
      "multiplied by scalars to produce another object of the same kind. From\n",
      "an abstract mathematical viewpoint, any object that satisﬁes these two\n",
      "properties can be considered a vector. Here are some examples of such\n",
      "vector objects:\n",
      "1. Geometric vectors. This example of a vector may be familiar from high\n",
      "school mathematics and physics. Geometric vectors – see Figure 2.1(a)\n",
      "– are directed segments, which can be drawn (at least in two dimen-\n",
      "sions). Two geometric vectors!x;!ycan be added, such that!x+!y=!z\n",
      "is another geometric vector. Furthermore, multiplication by a scalar\n",
      "\u0015!x,\u00152R, is also a geometric vector. In fact, it is the original vector\n",
      "scaled by\u0015. Therefore, geometric vectors are instances of the vector\n",
      "concepts introduced previously. Interpreting vectors as geometric vec-\n",
      "tors enables us to use our intuitions about direction and magnitude to\n",
      "reason about mathematical operations.\n",
      "2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can\n",
      "Figure 2.1\n",
      "Different types of\n",
      "vectors. Vectors can\n",
      "be surprising\n",
      "objects, including\n",
      "(a) geometric\n",
      "vectors\n",
      "and (b) polynomials.!x!y!x+!y\n",
      "(a) Geometric vectors.\n",
      "−2 0 2\n",
      "x−6−4−2024y (b) Polynomials.\n",
      "17\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "18 Linear Algebra\n",
      "be added together, which results in another polynomial; and they can\n",
      "be multiplied by a scalar \u00152R, and the result is a polynomial as\n",
      "well. Therefore, polynomials are (rather unusual) instances of vectors.\n",
      "Note that polynomials are very different from geometric vectors. While\n",
      "geometric vectors are concrete “drawings”, polynomials are abstract\n",
      "concepts. However, they are both vectors in the sense previously de-\n",
      "scribed.\n",
      "3. Audio signals are vectors. Audio signals are represented as a series of\n",
      "numbers. We can add audio signals together, and their sum is a new\n",
      "audio signal. If we scale an audio signal, we also obtain an audio signal.\n",
      "Therefore, audio signals are a type of vector, too.\n",
      "4. Elements of Rn(tuples ofnreal numbers) are vectors. Rnis more\n",
      "abstract than polynomials, and it is the concept we focus on in this\n",
      "book. For instance,\n",
      "a=2\n",
      "41\n",
      "2\n",
      "33\n",
      "52R3(2.1)\n",
      "is an example of a triplet of numbers. Adding two vectors a;b2Rn\n",
      "component-wise results in another vector: a+b=c2Rn. Moreover,\n",
      "multiplyinga2Rnby\u00152Rresults in a scaled vector \u0015a2Rn.\n",
      "Considering vectors as elements of Rnhas an additional beneﬁt that Be careful to check\n",
      "whether array\n",
      "operations actually\n",
      "perform vector\n",
      "operations when\n",
      "implementing on a\n",
      "computer.it loosely corresponds to arrays of real numbers on a computer. Many\n",
      "programming languages support array operations, which allow for con-\n",
      "venient implementation of algorithms that involve vector operations.\n",
      "Linear algebra focuses on the similarities between these vector concepts.\n",
      "We can add them together and multiply them by scalars. We will largelyPavel Grinfeld’s\n",
      "series on linear\n",
      "algebra:\n",
      "http://tinyurl.\n",
      "com/nahclwm\n",
      "Gilbert Strang’s\n",
      "course on linear\n",
      "algebra:\n",
      "http://tinyurl.\n",
      "com/29p5q8j\n",
      "3Blue1Brown series\n",
      "on linear algebra:\n",
      "https://tinyurl.\n",
      "com/h5g4kpsfocus on vectors in Rnsince most algorithms in linear algebra are for-\n",
      "mulated in Rn. We will see in Chapter 8 that we often consider data to\n",
      "be represented as vectors in Rn. In this book, we will focus on ﬁnite-\n",
      "dimensional vector spaces, in which case there is a 1:1correspondence\n",
      "between any kind of vector and Rn. When it is convenient, we will use\n",
      "intuitions about geometric vectors and consider array-based algorithms.\n",
      "One major idea in mathematics is the idea of “closure”. This is the ques-\n",
      "tion: What is the set of all things that can result from my proposed oper-\n",
      "ations? In the case of vectors: What is the set of vectors that can result by\n",
      "starting with a small set of vectors, and adding them to each other and\n",
      "scaling them? This results in a vector space (Section 2.4). The concept of\n",
      "a vector space and its properties underlie much of machine learning. The\n",
      "concepts introduced in this chapter are summarized in Figure 2.2.\n",
      "This chapter is mostly based on the lecture notes and books by Drumm\n",
      "and Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann\n",
      "(2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.1 Systems of Linear Equations 19\n",
      "Figure 2.2 A mind\n",
      "map of the concepts\n",
      "introduced in this\n",
      "chapter, along with\n",
      "where they are used\n",
      "in other parts of the\n",
      "book.Vector\n",
      "Vector spaceMatrixChapter 5\n",
      "Vector calculus\n",
      "Group\n",
      "System of\n",
      "linear equations\n",
      "Matrix\n",
      "inverse\n",
      "Gaussian\n",
      "eliminationLinear/afﬁne\n",
      "mappingLinear\n",
      "independence\n",
      "Basis\n",
      "Chapter 10\n",
      "Dimensionality\n",
      "reductionChapter 12\n",
      "ClassiﬁcationChapter 3\n",
      "Analytic geometrycomposesclosure\n",
      "Abelian\n",
      "with +represents\n",
      "represents\n",
      "solved by solvesproperty ofmaximal set\n",
      "resources are Gilbert Strang’s Linear Algebra course at MIT and the Linear\n",
      "Algebra Series by 3Blue1Brown.\n",
      "Linear algebra plays an important role in machine learning and gen-\n",
      "eral mathematics. The concepts introduced in this chapter are further ex-\n",
      "panded to include the idea of geometry in Chapter 3. In Chapter 5, we\n",
      "will discuss vector calculus, where a principled knowledge of matrix op-\n",
      "erations is essential. In Chapter 10, we will use projections (to be intro-\n",
      "duced in Section 3.8) for dimensionality reduction with principal compo-\n",
      "nent analysis (PCA). In Chapter 9, we will discuss linear regression, where\n",
      "linear algebra plays a central role for solving least-squares problems.\n",
      "2.1 Systems of Linear Equations\n",
      "Systems of linear equations play a central part of linear algebra. Many\n",
      "problems can be formulated as systems of linear equations, and linear\n",
      "algebra gives us the tools for solving them.\n",
      "Example 2.1\n",
      "A company produces products N1;:::;Nnfor which resources\n",
      "R1;:::;Rmare required. To produce a unit of product Nj,aijunits of\n",
      "resourceRiare needed, where i= 1;:::;m andj= 1;:::;n .\n",
      "The objective is to ﬁnd an optimal production plan, i.e., a plan of how\n",
      "many units xjof productNjshould be produced if a total of biunits of\n",
      "resourceRiare available and (ideally) no resources are left over.\n",
      "If we produce x1;:::;xnunits of the corresponding products, we need\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "20 Linear Algebra\n",
      "a total of\n",
      "ai1x1+\u0001\u0001\u0001+ainxn (2.2)\n",
      "many units of resource Ri. An optimal production plan (x1;:::;xn)2Rn,\n",
      "therefore, has to satisfy the following system of equations:\n",
      "a11x1+\u0001\u0001\u0001+a1nxn=b1\n",
      "...\n",
      "am1x1+\u0001\u0001\u0001+amnxn=bm; (2.3)\n",
      "whereaij2Randbi2R.\n",
      "Equation (2.3) is the general form of a system of linear equations , and system of linear\n",
      "equations x1;:::;xnare the unknowns of this system. Every n-tuple (x1;:::;xn)2\n",
      "Rnthat satisﬁes (2.3) is a solution of the linear equation system. solution\n",
      "Example 2.2\n",
      "The system of linear equations\n",
      "x1+x2+x3= 3 (1)\n",
      "x1\u0000x2+ 2x3= 2 (2)\n",
      "2x1 + 3x3= 1 (3)(2.4)\n",
      "hasno solution: Adding the ﬁrst two equations yields 2x1+3x3= 5, which\n",
      "contradicts the third equation (3).\n",
      "Let us have a look at the system of linear equations\n",
      "x1+x2+x3= 3 (1)\n",
      "x1\u0000x2+ 2x3= 2 (2)\n",
      "x2+x3= 2 (3): (2.5)\n",
      "From the ﬁrst and third equation, it follows that x1= 1. From (1) +(2),\n",
      "we get 2x1+ 3x3= 5, i.e.,x3= 1. From (3), we then get that x2= 1.\n",
      "Therefore, (1;1;1)is the only possible and unique solution (verify that\n",
      "(1;1;1)is a solution by plugging in).\n",
      "As a third example, we consider\n",
      "x1+x2+x3= 3 (1)\n",
      "x1\u0000x2+ 2x3= 2 (2)\n",
      "2x1 + 3x3= 5 (3): (2.6)\n",
      "Since (1) +(2)=(3), we can omit the third equation (redundancy). From\n",
      "(1) and (2), we get 2x1= 5\u00003x3and2x2= 1+x3. We deﬁnex3=a2R\n",
      "as a free variable, such that any triplet\n",
      "\u00125\n",
      "2\u00003\n",
      "2a;1\n",
      "2+1\n",
      "2a;a\u0013\n",
      "; a2R (2.7)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.1 Systems of Linear Equations 21\n",
      "Figure 2.3 The\n",
      "solution space of a\n",
      "system of two linear\n",
      "equations with two\n",
      "variables can be\n",
      "geometrically\n",
      "interpreted as the\n",
      "intersection of two\n",
      "lines. Every linear\n",
      "equation represents\n",
      "a line.\n",
      "2x1\u00004x2= 1\n",
      "4x1+ 4x2= 5\n",
      "x1\n",
      "x2\n",
      "is a solution of the system of linear equations, i.e., we obtain a solution\n",
      "set that contains inﬁnitely many solutions.\n",
      "In general, for a real-valued system of linear equations we obtain either\n",
      "no, exactly one, or inﬁnitely many solutions. Linear regression (Chapter 9)\n",
      "solves a version of Example 2.1 when we cannot solve the system of linear\n",
      "equations.\n",
      "Remark (Geometric Interpretation of Systems of Linear Equations) .In a\n",
      "system of linear equations with two variables x1;x2, each linear equation\n",
      "deﬁnes a line on the x1x2-plane. Since a solution to a system of linear\n",
      "equations must satisfy all equations simultaneously, the solution set is the\n",
      "intersection of these lines. This intersection set can be a line (if the linear\n",
      "equations describe the same line), a point, or empty (when the lines are\n",
      "parallel). An illustration is given in Figure 2.3 for the system\n",
      "4x1+ 4x2= 5\n",
      "2x1\u00004x2= 1(2.8)\n",
      "where the solution space is the point (x1;x2) = (1;1\n",
      "4). Similarly, for three\n",
      "variables, each linear equation determines a plane in three-dimensional\n",
      "space. When we intersect these planes, i.e., satisfy all linear equations at\n",
      "the same time, we can obtain a solution set that is a plane, a line, a point\n",
      "or empty (when the planes have no common intersection). }\n",
      "For a systematic approach to solving systems of linear equations, we\n",
      "will introduce a useful compact notation. We collect the coefﬁcients aij\n",
      "into vectors and collect the vectors into matrices. In other words, we write\n",
      "the system from (2.3) in the following form:\n",
      "2\n",
      "64a11\n",
      "...\n",
      "am13\n",
      "75x1+2\n",
      "64a12\n",
      "...\n",
      "am23\n",
      "75x2+\u0001\u0001\u0001+2\n",
      "64a1n\n",
      "...\n",
      "amn3\n",
      "75xn=2\n",
      "64b1\n",
      "...\n",
      "bm3\n",
      "75 (2.9)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "22 Linear Algebra\n",
      "()2\n",
      "64a11\u0001\u0001\u0001a1n\n",
      "......\n",
      "am1\u0001\u0001\u0001amn3\n",
      "752\n",
      "64x1\n",
      "...\n",
      "xn3\n",
      "75=2\n",
      "64b1\n",
      "...\n",
      "bm3\n",
      "75: (2.10)\n",
      "In the following, we will have a close look at these matrices and de-\n",
      "ﬁne computation rules. We will return to solving linear equations in Sec-\n",
      "tion 2.3.\n",
      "2.2 Matrices\n",
      "Matrices play a central role in linear algebra. They can be used to com-\n",
      "pactly represent systems of linear equations, but they also represent linear\n",
      "functions (linear mappings) as we will see later in Section 2.7. Before we\n",
      "discuss some of these interesting topics, let us ﬁrst deﬁne what a matrix\n",
      "is and what kind of operations we can do with matrices. We will see more\n",
      "properties of matrices in Chapter 4.\n",
      "Deﬁnition 2.1 (Matrix) .Withm;n2Na real-valued (m;n)matrixAis matrix\n",
      "anm\u0001n-tuple of elements aij,i= 1;:::;m ,j= 1;:::;n , which is ordered\n",
      "according to a rectangular scheme consisting of mrows andncolumns:\n",
      "A=2\n",
      "6664a11a12\u0001\u0001\u0001a1n\n",
      "a21a22\u0001\u0001\u0001a2n\n",
      ".........\n",
      "am1am2\u0001\u0001\u0001amn3\n",
      "7775; aij2R: (2.11)\n",
      "By convention (1;n)-matrices are called rows and(m;1)-matrices are called row\n",
      "columns . These special matrices are also called row/column vectors . column\n",
      "row vector\n",
      "column vector\n",
      "Figure 2.4 By\n",
      "stacking its\n",
      "columns, a matrix A\n",
      "can be represented\n",
      "as a long vector a.\n",
      "re-shapeA2R4\u00022a2R8Rm\u0002nis the set of all real-valued (m;n)-matrices.A2Rm\u0002ncan be\n",
      "equivalently represented as a2Rmnby stacking all ncolumns of the\n",
      "matrix into a long vector; see Figure 2.4.\n",
      "2.2.1 Matrix Addition and Multiplication\n",
      "The sum of two matrices A2Rm\u0002n,B2Rm\u0002nis deﬁned as the element-\n",
      "wise sum, i.e.,\n",
      "A+B:=2\n",
      "64a11+b11\u0001\u0001\u0001a1n+b1n\n",
      "......\n",
      "am1+bm1\u0001\u0001\u0001amn+bmn3\n",
      "752Rm\u0002n: (2.12)\n",
      "For matricesA2Rm\u0002n,B2Rn\u0002k, the elements cijof the product Note the size of the\n",
      "matrices. C=AB2Rm\u0002kare computed as\n",
      "C =\n",
      "np.einsum('il,\n",
      "lj', A, B) cij=nX\n",
      "l=1ailblj; i = 1;:::;m; j = 1;:::;k: (2.13)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.2 Matrices 23\n",
      "This means, to compute element cijwe multiply the elements of the ith There arencolumns\n",
      "inAandnrows in\n",
      "Bso that we can\n",
      "computeailbljfor\n",
      "l= 1;:::;n .\n",
      "Commonly, the dot\n",
      "product between\n",
      "two vectorsa;bis\n",
      "denoted bya>bor\n",
      "ha;bi.row ofAwith thejth column ofBand sum them up. Later in Section 3.2,\n",
      "we will call this the dot product of the corresponding row and column. In\n",
      "cases, where we need to be explicit that we are performing multiplication,\n",
      "we use the notation A\u0001Bto denote multiplication (explicitly showing\n",
      "“\u0001”).\n",
      "Remark. Matrices can only be multiplied if their “neighboring” dimensions\n",
      "match. For instance, an n\u0002k-matrixAcan be multiplied with a k\u0002m-\n",
      "matrixB, but only from the left side:\n",
      "A|{z}\n",
      "n\u0002kB|{z}\n",
      "k\u0002m=C|{z}\n",
      "n\u0002m(2.14)\n",
      "The productBAis not deﬁned if m6=nsince the neighboring dimensions\n",
      "do not match. }\n",
      "Remark. Matrix multiplication is notdeﬁned as an element-wise operation\n",
      "on matrix elements, i.e., cij6=aijbij(even if the size of A;Bwas cho-\n",
      "sen appropriately). This kind of element-wise multiplication often appears\n",
      "in programming languages when we multiply (multi-dimensional) arrays\n",
      "with each other, and is called a Hadamard product . } Hadamard product\n",
      "Example 2.3\n",
      "ForA=\u00141 2 3\n",
      "3 2 1\u0015\n",
      "2R2\u00023,B=2\n",
      "40 2\n",
      "1\u00001\n",
      "0 13\n",
      "52R3\u00022, we obtain\n",
      "AB=\u00141 2 3\n",
      "3 2 1\u00152\n",
      "40 2\n",
      "1\u00001\n",
      "0 13\n",
      "5=\u00142 3\n",
      "2 5\u0015\n",
      "2R2\u00022; (2.15)\n",
      "BA=2\n",
      "40 2\n",
      "1\u00001\n",
      "0 13\n",
      "5\u00141 2 3\n",
      "3 2 1\u0015\n",
      "=2\n",
      "46 4 2\n",
      "\u00002 0 2\n",
      "3 2 13\n",
      "52R3\u00023: (2.16)\n",
      "Figure 2.5 Even if\n",
      "both matrix\n",
      "multiplications AB\n",
      "andBA are\n",
      "deﬁned, the\n",
      "dimensions of the\n",
      "results can be\n",
      "different.\n",
      "From this example, we can already see that matrix multiplication is not\n",
      "commutative, i.e., AB6=BA; see also Figure 2.5 for an illustration.\n",
      "Deﬁnition 2.2 (Identity Matrix) .InRn\u0002n, we deﬁne the identity matrix\n",
      "identity matrixIn:=2\n",
      "6666666641 0\u0001\u0001\u00010\u0001\u0001\u00010\n",
      "0 1\u0001\u0001\u00010\u0001\u0001\u00010\n",
      "..................\n",
      "0 0\u0001\u0001\u00011\u0001\u0001\u00010\n",
      "..................\n",
      "0 0\u0001\u0001\u00010\u0001\u0001\u000113\n",
      "7777777752Rn\u0002n(2.17)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "24 Linear Algebra\n",
      "as then\u0002n-matrix containing 1on the diagonal and 0everywhere else.\n",
      "Now that we deﬁned matrix multiplication, matrix addition and the\n",
      "identity matrix, let us have a look at some properties of matrices:\n",
      "associativity\n",
      "Associativity:\n",
      "8A2Rm\u0002n;B2Rn\u0002p;C2Rp\u0002q: (AB)C=A(BC) (2.18)\n",
      "distributivity\n",
      "Distributivity:\n",
      "8A;B2Rm\u0002n;C;D2Rn\u0002p: (A+B)C=AC+BC (2.19a)\n",
      "A(C+D) =AC+AD (2.19b)\n",
      "Multiplication with the identity matrix:\n",
      "8A2Rm\u0002n:ImA=AIn=A (2.20)\n",
      "Note thatIm6=Inform6=n.\n",
      "2.2.2 Inverse and Transpose\n",
      "Deﬁnition 2.3 (Inverse) .Consider a square matrix A2Rn\u0002n. Let matrix A square matrix\n",
      "possesses the same\n",
      "number of columns\n",
      "and rows.B2Rn\u0002nhave the property that AB =In=BA.Bis called the\n",
      "inverse ofAand denoted by A\u00001.\n",
      "inverseUnfortunately, not every matrix Apossesses an inverse A\u00001. If this\n",
      "inverse does exist, Ais called regular /invertible /nonsingular , otherwise regular\n",
      "invertible\n",
      "nonsingularsingular /noninvertible . When the matrix inverse exists, it is unique. In Sec-\n",
      "singular\n",
      "noninvertibletion 2.3, we will discuss a general way to compute the inverse of a matrix\n",
      "by solving a system of linear equations.\n",
      "Remark (Existence of the Inverse of a 2\u00022-matrix) .Consider a matrix\n",
      "A:=\u0014a11a12\n",
      "a21a22\u0015\n",
      "2R2\u00022: (2.21)\n",
      "If we multiply Awith\n",
      "A0:=\u0014a22\u0000a12\n",
      "\u0000a21a11\u0015\n",
      "(2.22)\n",
      "we obtain\n",
      "AA0=\u0014a11a22\u0000a12a21 0\n",
      "0a11a22\u0000a12a21\u0015\n",
      "= (a11a22\u0000a12a21)I:\n",
      "(2.23)\n",
      "Therefore,\n",
      "A\u00001=1\n",
      "a11a22\u0000a12a21\u0014a22\u0000a12\n",
      "\u0000a21a11\u0015\n",
      "(2.24)\n",
      "if and only if a11a22\u0000a12a216= 0. In Section 4.1, we will see that a11a22\u0000\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.2 Matrices 25\n",
      "a12a21is the determinant of a 2\u00022-matrix. Furthermore, we can generally\n",
      "use the determinant to check whether a matrix is invertible. }\n",
      "Example 2.4 (Inverse Matrix)\n",
      "The matrices\n",
      "A=2\n",
      "41 2 1\n",
      "4 4 5\n",
      "6 7 73\n",
      "5;B=2\n",
      "4\u00007\u00007 6\n",
      "2 1\u00001\n",
      "4 5\u000043\n",
      "5 (2.25)\n",
      "are inverse to each other since AB=I=BA.\n",
      "Deﬁnition 2.4 (Transpose) .ForA2Rm\u0002nthe matrixB2Rn\u0002mwith\n",
      "bij=ajiis called the transpose ofA. We writeB=A>. transpose\n",
      "The main diagonal\n",
      "(sometimes called\n",
      "“principal diagonal”,\n",
      "“primary diagonal”,\n",
      "“leading diagonal”,\n",
      "or “major diagonal”)\n",
      "of a matrixAis the\n",
      "collection of entries\n",
      "Aijwherei=j.In general,A>can be obtained by writing the columns of Aas the rows\n",
      "ofA>. The following are important properties of inverses and transposes:\n",
      "The scalar case of\n",
      "(2.28) is\n",
      "1\n",
      "2+4=1\n",
      "66=1\n",
      "2+1\n",
      "4.AA\u00001=I=A\u00001A (2.26)\n",
      "(AB)\u00001=B\u00001A\u00001(2.27)\n",
      "(A+B)\u000016=A\u00001+B\u00001(2.28)\n",
      "(A>)>=A (2.29)\n",
      "(A+B)>=A>+B>(2.30)\n",
      "(AB)>=B>A>(2.31)\n",
      "Deﬁnition 2.5 (Symmetric Matrix) .A matrixA2Rn\u0002nissymmetric if symmetric matrix\n",
      "A=A>.\n",
      "Note that only (n;n)-matrices can be symmetric. Generally, we call\n",
      "(n;n)-matrices also square matrices because they possess the same num- square matrix\n",
      "ber of rows and columns. Moreover, if Ais invertible, then so is A>, and\n",
      "(A\u00001)>= (A>)\u00001=:A\u0000>.\n",
      "Remark (Sum and Product of Symmetric Matrices) .The sum of symmet-\n",
      "ric matricesA;B2Rn\u0002nis always symmetric. However, although their\n",
      "product is always deﬁned, it is generally not symmetric:\n",
      "\u00141 0\n",
      "0 0\u0015\u00141 1\n",
      "1 1\u0015\n",
      "=\u00141 1\n",
      "0 0\u0015\n",
      ": (2.32)\n",
      "}\n",
      "2.2.3 Multiplication by a Scalar\n",
      "Let us look at what happens to matrices when they are multiplied by a\n",
      "scalar\u00152R. LetA2Rm\u0002nand\u00152R. Then\u0015A=K,Kij=\u0015aij.\n",
      "Practically,\u0015scales each element of A. For\u0015; 2R, the following holds:\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "26 Linear Algebra\n",
      "associativityAssociativity:\n",
      "(\u0015 )C=\u0015( C);C2Rm\u0002n\n",
      "\u0015(BC) = (\u0015B)C=B(\u0015C) = (BC)\u0015;B2Rm\u0002n;C2Rn\u0002k.\n",
      "Note that this allows us to move scalar values around.\n",
      "(\u0015C)>=C>\u0015>=C>\u0015=\u0015C>since\u0015=\u0015>for all\u00152R.distributivity\n",
      "Distributivity:\n",
      "(\u0015+ )C=\u0015C+ C;C2Rm\u0002n\n",
      "\u0015(B+C) =\u0015B+\u0015C;B;C2Rm\u0002n\n",
      "Example 2.5 (Distributivity)\n",
      "If we deﬁne\n",
      "C:=\u00141 2\n",
      "3 4\u0015\n",
      "; (2.33)\n",
      "then for any \u0015; 2Rwe obtain\n",
      "(\u0015+ )C=\u0014(\u0015+ )1 (\u0015+ )2\n",
      "(\u0015+ )3 (\u0015+ )4\u0015\n",
      "=\u0014\u0015+ 2\u0015+ 2 \n",
      "3\u0015+ 3 4\u0015+ 4 \u0015\n",
      "(2.34a)\n",
      "=\u0014\u00152\u0015\n",
      "3\u00154\u0015\u0015\n",
      "+\u0014 2 \n",
      "3 4 \u0015\n",
      "=\u0015C+ C: (2.34b)\n",
      "2.2.4 Compact Representations of Systems of Linear Equations\n",
      "If we consider the system of linear equations\n",
      "2x1+ 3x2+ 5x3= 1\n",
      "4x1\u00002x2\u00007x3= 8\n",
      "9x1+ 5x2\u00003x3= 2(2.35)\n",
      "and use the rules for matrix multiplication, we can write this equation\n",
      "system in a more compact form as\n",
      "2\n",
      "42 3 5\n",
      "4\u00002\u00007\n",
      "9 5\u000033\n",
      "52\n",
      "4x1\n",
      "x2\n",
      "x33\n",
      "5=2\n",
      "41\n",
      "8\n",
      "23\n",
      "5: (2.36)\n",
      "Note thatx1scales the ﬁrst column, x2the second one, and x3the third\n",
      "one.\n",
      "Generally, a system of linear equations can be compactly represented in\n",
      "their matrix form as Ax=b; see (2.3), and the product Axis a (linear)\n",
      "combination of the columns of A. We will discuss linear combinations in\n",
      "more detail in Section 2.5.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.3 Solving Systems of Linear Equations 27\n",
      "2.3 Solving Systems of Linear Equations\n",
      "In (2.3), we introduced the general form of an equation system, i.e.,\n",
      "a11x1+\u0001\u0001\u0001+a1nxn=b1\n",
      "...\n",
      "am1x1+\u0001\u0001\u0001+amnxn=bm;(2.37)\n",
      "whereaij2Randbi2Rare known constants and xjare unknowns,\n",
      "i= 1;:::;m ,j= 1;:::;n . Thus far, we saw that matrices can be used as\n",
      "a compact way of formulating systems of linear equations so that we can\n",
      "writeAx=b, see (2.10). Moreover, we deﬁned basic matrix operations,\n",
      "such as addition and multiplication of matrices. In the following, we will\n",
      "focus on solving systems of linear equations and provide an algorithm for\n",
      "ﬁnding the inverse of a matrix.\n",
      "2.3.1 Particular and General Solution\n",
      "Before discussing how to generally solve systems of linear equations, let\n",
      "us have a look at an example. Consider the system of equations\n",
      "\u00141 0 8\u00004\n",
      "0 1 2 12\u00152\n",
      "664x1\n",
      "x2\n",
      "x3\n",
      "x43\n",
      "775=\u001442\n",
      "8\u0015\n",
      ": (2.38)\n",
      "The system has two equations and four unknowns. Therefore, in general\n",
      "we would expect inﬁnitely many solutions. This system of equations is\n",
      "in a particularly easy form, where the ﬁrst two columns consist of a 1\n",
      "and a 0. Remember that we want to ﬁnd scalars x1;:::;x 4, such thatP4\n",
      "i=1xici=b, where we deﬁne cito be theith column of the matrix and\n",
      "bthe right-hand-side of (2.38). A solution to the problem in (2.38) can\n",
      "be found immediately by taking 42times the ﬁrst column and 8times the\n",
      "second column so that\n",
      "b=\u001442\n",
      "8\u0015\n",
      "= 42\u00141\n",
      "0\u0015\n",
      "+ 8\u00140\n",
      "1\u0015\n",
      ": (2.39)\n",
      "Therefore, a solution is [42;8;0;0]>. This solution is called a particular particular solution\n",
      "solution orspecial solution . However, this is not the only solution of this special solution\n",
      "system of linear equations. To capture all the other solutions, we need\n",
      "to be creative in generating 0in a non-trivial way using the columns of\n",
      "the matrix: Adding 0to our special solution does not change the special\n",
      "solution. To do so, we express the third column using the ﬁrst two columns\n",
      "(which are of this very simple form)\n",
      "\u00148\n",
      "2\u0015\n",
      "= 8\u00141\n",
      "0\u0015\n",
      "+ 2\u00140\n",
      "1\u0015\n",
      "(2.40)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "28 Linear Algebra\n",
      "so that 0= 8c1+ 2c2\u00001c3+ 0c4and(x1;x2;x3;x4) = (8;2;\u00001;0). In\n",
      "fact, any scaling of this solution by \u001512Rproduces the 0vector, i.e.,\n",
      "\u00141 0 8\u00004\n",
      "0 1 2 12\u00150\n",
      "BB@\u001512\n",
      "6648\n",
      "2\n",
      "\u00001\n",
      "03\n",
      "7751\n",
      "CCA=\u00151(8c1+ 2c2\u0000c3) =0: (2.41)\n",
      "Following the same line of reasoning, we express the fourth column of the\n",
      "matrix in (2.38) using the ﬁrst two columns and generate another set of\n",
      "non-trivial versions of 0as\n",
      "\u00141 0 8\u00004\n",
      "0 1 2 12\u00150\n",
      "BB@\u001522\n",
      "664\u00004\n",
      "12\n",
      "0\n",
      "\u000013\n",
      "7751\n",
      "CCA=\u00152(\u00004c1+ 12c2\u0000c4) =0 (2.42)\n",
      "for any\u001522R. Putting everything together, we obtain all solutions of the\n",
      "equation system in (2.38), which is called the general solution , as the set general solution\n",
      "8\n",
      ">><\n",
      ">>:x2R4:x=2\n",
      "66442\n",
      "8\n",
      "0\n",
      "03\n",
      "775+\u001512\n",
      "6648\n",
      "2\n",
      "\u00001\n",
      "03\n",
      "775+\u001522\n",
      "664\u00004\n",
      "12\n",
      "0\n",
      "\u000013\n",
      "775;\u00151;\u001522R9\n",
      ">>=\n",
      ">>;:(2.43)\n",
      "Remark. The general approach we followed consisted of the following\n",
      "three steps:\n",
      "1. Find a particular solution to Ax=b.\n",
      "2. Find all solutions to Ax=0.\n",
      "3. Combine the solutions from steps 1. and 2. to the general solution.\n",
      "Neither the general nor the particular solution is unique. }\n",
      "The system of linear equations in the preceding example was easy to\n",
      "solve because the matrix in (2.38) has this particularly convenient form,\n",
      "which allowed us to ﬁnd the particular and the general solution by in-\n",
      "spection. However, general equation systems are not of this simple form.\n",
      "Fortunately, there exists a constructive algorithmic way of transforming\n",
      "any system of linear equations into this particularly simple form: Gaussian\n",
      "elimination. Key to Gaussian elimination are elementary transformations\n",
      "of systems of linear equations, which transform the equation system into\n",
      "a simple form. Then, we can apply the three steps to the simple form that\n",
      "we just discussed in the context of the example in (2.38).\n",
      "2.3.2 Elementary Transformations\n",
      "Key to solving a system of linear equations are elementary transformations elementary\n",
      "transformations that keep the solution set the same, but that transform the equation system\n",
      "into a simpler form:\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.3 Solving Systems of Linear Equations 29\n",
      "Exchange of two equations (rows in the matrix representing the system\n",
      "of equations)\n",
      "Multiplication of an equation (row) with a constant \u00152Rnf0g\n",
      "Addition of two equations (rows)\n",
      "Example 2.6\n",
      "Fora2R, we seek all solutions of the following system of equations:\n",
      "\u00002x1+ 4x2\u00002x3\u0000x4+ 4x5=\u00003\n",
      "4x1\u00008x2+ 3x3\u00003x4+x5= 2\n",
      "x1\u00002x2+x3\u0000x4+x5= 0\n",
      "x1\u00002x2\u00003x4+ 4x5=a: (2.44)\n",
      "We start by converting this system of equations into the compact matrix\n",
      "notationAx=b. We no longer mention the variables xexplicitly and\n",
      "build the augmented matrix (in the form\u0002Ajb\u0003\n",
      ") augmented matrix\n",
      "2\n",
      "664\u00002 4\u00002\u00001 4\u00003\n",
      "4\u00008 3\u00003 1 2\n",
      "1\u00002 1\u00001 1 0\n",
      "1\u00002 0\u00003 4 a3\n",
      "775Swap withR3\n",
      "Swap withR1\n",
      "where we used the vertical line to separate the left-hand side from the\n",
      "right-hand side in (2.44). We use  to indicate a transformation of the\n",
      "augmented matrix using elementary transformations. The augmented\n",
      "matrix\u0002\n",
      "Ajb\u0003\n",
      "compactly\n",
      "represents the\n",
      "system of linear\n",
      "equationsAx=b.Swapping Rows 1and3leads to\n",
      "2\n",
      "6641\u00002 1\u00001 1 0\n",
      "4\u00008 3\u00003 1 2\n",
      "\u00002 4\u00002\u00001 4\u00003\n",
      "1\u00002 0\u00003 4 a3\n",
      "775\u00004R1\n",
      "+2R1\n",
      "\u0000R1\n",
      "When we now apply the indicated transformations (e.g., subtract Row 1\n",
      "four times from Row 2), we obtain\n",
      "2\n",
      "6641\u00002 1\u00001 1 0\n",
      "0 0\u00001 1\u00003 2\n",
      "0 0 0 \u00003 6\u00003\n",
      "0 0\u00001\u00002 3 a3\n",
      "775\n",
      "\u0000R2\u0000R3\n",
      " 2\n",
      "6641\u00002 1\u00001 1 0\n",
      "0 0\u00001 1\u00003 2\n",
      "0 0 0 \u00003 6\u00003\n",
      "0 0 0 0 0 a+13\n",
      "775\u0001(\u00001)\n",
      "\u0001(\u00001\n",
      "3)\n",
      " 2\n",
      "6641\u00002 1\u00001 1 0\n",
      "0 0 1 \u00001 3\u00002\n",
      "0 0 0 1 \u00002 1\n",
      "0 0 0 0 0 a+13\n",
      "775\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "30 Linear Algebra\n",
      "This (augmented) matrix is in a convenient form, the row-echelon form row-echelon form\n",
      "(REF). Reverting this compact notation back into the explicit notation with\n",
      "the variables we seek, we obtain\n",
      "x1\u00002x2+x3\u0000x4+x5= 0\n",
      "x3\u0000x4+ 3x5=\u00002\n",
      "x4\u00002x5= 1\n",
      "0 =a+ 1: (2.45)\n",
      "Only fora=\u00001this system can be solved. A particular solution is particular solution\n",
      "2\n",
      "66664x1\n",
      "x2\n",
      "x3\n",
      "x4\n",
      "x53\n",
      "77775=2\n",
      "666642\n",
      "0\n",
      "\u00001\n",
      "1\n",
      "03\n",
      "77775: (2.46)\n",
      "Thegeneral solution , which captures the set of all possible solutions, is general solution\n",
      "8\n",
      ">>>><\n",
      ">>>>:x2R5:x=2\n",
      "666642\n",
      "0\n",
      "\u00001\n",
      "1\n",
      "03\n",
      "77775+\u001512\n",
      "666642\n",
      "1\n",
      "0\n",
      "0\n",
      "03\n",
      "77775+\u001522\n",
      "666642\n",
      "0\n",
      "\u00001\n",
      "2\n",
      "13\n",
      "77775; \u0015 1;\u001522R9\n",
      ">>>>=\n",
      ">>>>;:(2.47)\n",
      "In the following, we will detail a constructive way to obtain a particular\n",
      "and general solution of a system of linear equations.\n",
      "Remark (Pivots and Staircase Structure) .The leading coefﬁcient of a row\n",
      "(ﬁrst nonzero number from the left) is called the pivot and is always pivot\n",
      "strictly to the right of the pivot of the row above it. Therefore, any equa-\n",
      "tion system in row-echelon form always has a “staircase” structure. }\n",
      "Deﬁnition 2.6 (Row-Echelon Form) .A matrix is in row-echelon form if row-echelon form\n",
      "All rows that contain only zeros are at the bottom of the matrix; corre-\n",
      "spondingly, all rows that contain at least one nonzero element are on\n",
      "top of rows that contain only zeros.\n",
      "Looking at nonzero rows only, the ﬁrst nonzero number from the left\n",
      "(also called the pivot or the leading coefﬁcient ) is always strictly to the pivot\n",
      "leading coefﬁcient right of the pivot of the row above it.\n",
      "In other texts, it is\n",
      "sometimes required\n",
      "that the pivot is 1.Remark (Basic and Free Variables) .The variables corresponding to the\n",
      "pivots in the row-echelon form are called basic variables and the other\n",
      "basic variable variables are free variables . For example, in (2.45), x1;x3;x4are basic\n",
      "free variable variables, whereas x2;x5are free variables. }\n",
      "Remark (Obtaining a Particular Solution) .The row-echelon form makes\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.3 Solving Systems of Linear Equations 31\n",
      "our lives easier when we need to determine a particular solution. To do\n",
      "this, we express the right-hand side of the equation system using the pivot\n",
      "columns, such that b=PP\n",
      "i=1\u0015ipi, wherepi; i= 1;:::;P , are the pivot\n",
      "columns. The \u0015iare determined easiest if we start with the rightmost pivot\n",
      "column and work our way to the left.\n",
      "In the previous example, we would try to ﬁnd \u00151;\u00152;\u00153so that\n",
      "\u001512\n",
      "6641\n",
      "0\n",
      "0\n",
      "03\n",
      "775+\u001522\n",
      "6641\n",
      "1\n",
      "0\n",
      "03\n",
      "775+\u001532\n",
      "664\u00001\n",
      "\u00001\n",
      "1\n",
      "03\n",
      "775=2\n",
      "6640\n",
      "\u00002\n",
      "1\n",
      "03\n",
      "775: (2.48)\n",
      "From here, we ﬁnd relatively directly that \u00153= 1;\u00152=\u00001;\u00151= 2. When\n",
      "we put everything together, we must not forget the non-pivot columns\n",
      "for which we set the coefﬁcients implicitly to 0. Therefore, we get the\n",
      "particular solution x= [2;0;\u00001;1;0]>. }\n",
      "Remark (Reduced Row Echelon Form) .An equation system is in reduced reduced\n",
      "row-echelon form row-echelon form (also: row-reduced echelon form orrow canonical form ) if\n",
      "It is in row-echelon form.\n",
      "Every pivot is 1.\n",
      "The pivot is the only nonzero entry in its column.\n",
      "}\n",
      "The reduced row-echelon form will play an important role later in Sec-\n",
      "tion 2.3.3 because it allows us to determine the general solution of a sys-\n",
      "tem of linear equations in a straightforward way.Gaussian\n",
      "elimination Remark (Gaussian Elimination) . Gaussian elimination is an algorithm that\n",
      "performs elementary transformations to bring a system of linear equations\n",
      "into reduced row-echelon form. }\n",
      "Example 2.7 (Reduced Row Echelon Form)\n",
      "Verify that the following matrix is in reduced row-echelon form (the pivots\n",
      "are in bold):\n",
      "A=2\n",
      "413 0 0 3\n",
      "0 0 10 9\n",
      "0 0 0 1\u000043\n",
      "5: (2.49)\n",
      "The key idea for ﬁnding the solutions of Ax=0is to look at the non-\n",
      "pivot columns , which we will need to express as a (linear) combination of\n",
      "the pivot columns. The reduced row echelon form makes this relatively\n",
      "straightforward, and we express the non-pivot columns in terms of sums\n",
      "and multiples of the pivot columns that are on their left: The second col-\n",
      "umn is 3times the ﬁrst column (we can ignore the pivot columns on the\n",
      "right of the second column). Therefore, to obtain 0, we need to subtract\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "32 Linear Algebra\n",
      "the second column from three times the ﬁrst column. Now, we look at the\n",
      "ﬁfth column, which is our second non-pivot column. The ﬁfth column can\n",
      "be expressed as 3times the ﬁrst pivot column, 9times the second pivot\n",
      "column, and\u00004times the third pivot column. We need to keep track of\n",
      "the indices of the pivot columns and translate this into 3times the ﬁrst col-\n",
      "umn, 0times the second column (which is a non-pivot column), 9times\n",
      "the third column (which is our second pivot column), and \u00004times the\n",
      "fourth column (which is the third pivot column). Then we need to subtract\n",
      "the ﬁfth column to obtain 0. In the end, we are still solving a homogeneous\n",
      "equation system.\n",
      "To summarize, all solutions of Ax=0;x2R5are given by\n",
      "8\n",
      ">>>><\n",
      ">>>>:x2R5:x=\u001512\n",
      "666643\n",
      "\u00001\n",
      "0\n",
      "0\n",
      "03\n",
      "77775+\u001522\n",
      "666643\n",
      "0\n",
      "9\n",
      "\u00004\n",
      "\u000013\n",
      "77775; \u0015 1;\u001522R9\n",
      ">>>>=\n",
      ">>>>;: (2.50)\n",
      "2.3.3 The Minus-1 Trick\n",
      "In the following, we introduce a practical trick for reading out the solu-\n",
      "tionsxof a homogeneous system of linear equations Ax=0, where\n",
      "A2Rk\u0002n;x2Rn.\n",
      "To start, we assume that Ais in reduced row-echelon form without any\n",
      "rows that just contain zeros, i.e.,\n",
      "A=2\n",
      "666666640\u0001\u0001\u000101\u0003 \u0001\u0001\u0001 \u0003 0\u0003 \u0001\u0001\u0001 \u0003 0\u0003 \u0001\u0001\u0001 \u0003\n",
      "......0 0\u0001\u0001\u000101\u0003 \u0001\u0001\u0001 \u0003.........\n",
      "...............0...............\n",
      "........................0......\n",
      "0\u0001\u0001\u00010 0 0\u0001\u0001\u00010 0 0\u0001\u0001\u000101\u0003 \u0001\u0001\u0001 \u00033\n",
      "77777775;\n",
      "(2.51)\n",
      "where\u0003can be an arbitrary real number, with the constraints that the ﬁrst\n",
      "nonzero entry per row must be 1and all other entries in the corresponding\n",
      "column must be 0. The columns j1;:::;jkwith the pivots (marked in\n",
      "bold) are the standard unit vectors e1;:::;ek2Rk. We extend this matrix\n",
      "to ann\u0002n-matrix ~Aby addingn\u0000krows of the form\n",
      "\u00020\u0001\u0001\u00010\u00001 0\u0001\u0001\u00010\u0003\n",
      "(2.52)\n",
      "so that the diagonal of the augmented matrix ~Acontains either 1or\u00001.\n",
      "Then, the columns of ~Athat contain the\u00001as pivots are solutions of\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.3 Solving Systems of Linear Equations 33\n",
      "the homogeneous equation system Ax=0. To be more precise, these\n",
      "columns form a basis (Section 2.6.1) of the solution space of Ax=0,\n",
      "which we will later call the kernel ornull space (see Section 2.7.3). kernel\n",
      "null space\n",
      "Example 2.8 (Minus-1 Trick)\n",
      "Let us revisit the matrix in (2.49), which is already in reduced REF:\n",
      "A=2\n",
      "41 3 0 0 3\n",
      "0 0 1 0 9\n",
      "0 0 0 1\u000043\n",
      "5: (2.53)\n",
      "We now augment this matrix to a 5\u00025matrix by adding rows of the\n",
      "form (2.52) at the places where the pivots on the diagonal are missing\n",
      "and obtain\n",
      "~A=2\n",
      "666641 3 0 0 3\n",
      "0\u000010 0 0\n",
      "0 0 1 0 9\n",
      "0 0 0 1\u00004\n",
      "0 0 0 0\u000013\n",
      "77775: (2.54)\n",
      "From this form, we can immediately read out the solutions of Ax=0by\n",
      "taking the columns of ~A, which contain\u00001on the diagonal:\n",
      "8\n",
      ">>>><\n",
      ">>>>:x2R5:x=\u001512\n",
      "666643\n",
      "\u00001\n",
      "0\n",
      "0\n",
      "03\n",
      "77775+\u001522\n",
      "666643\n",
      "0\n",
      "9\n",
      "\u00004\n",
      "\u000013\n",
      "77775; \u0015 1;\u001522R9\n",
      ">>>>=\n",
      ">>>>;; (2.55)\n",
      "which is identical to the solution in (2.50) that we obtained by “insight”.\n",
      "Calculating the Inverse\n",
      "To compute the inverse A\u00001ofA2Rn\u0002n, we need to ﬁnd a matrix X\n",
      "that satisﬁesAX =In. Then,X=A\u00001. We can write this down as\n",
      "a set of simultaneous linear equations AX =In, where we solve for\n",
      "X= [x1j\u0001\u0001\u0001jxn]. We use the augmented matrix notation for a compact\n",
      "representation of this set of systems of linear equations and obtain\n",
      "\u0002AjIn\u0003\n",
      " \u0001\u0001\u0001 \u0002InjA\u00001\u0003: (2.56)\n",
      "This means that if we bring the augmented equation system into reduced\n",
      "row-echelon form, we can read out the inverse on the right-hand side of\n",
      "the equation system. Hence, determining the inverse of a matrix is equiv-\n",
      "alent to solving systems of linear equations.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "34 Linear Algebra\n",
      "Example 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)\n",
      "To determine the inverse of\n",
      "A=2\n",
      "6641 0 2 0\n",
      "1 1 0 0\n",
      "1 2 0 1\n",
      "1 1 1 13\n",
      "775(2.57)\n",
      "we write down the augmented matrix\n",
      "2\n",
      "6641 0 2 0 1 0 0 0\n",
      "1 1 0 0 0 1 0 0\n",
      "1 2 0 1 0 0 1 0\n",
      "1 1 1 1 0 0 0 13\n",
      "775\n",
      "and use Gaussian elimination to bring it into reduced row-echelon form\n",
      "2\n",
      "6641 0 0 0\u00001 2\u00002 2\n",
      "0 1 0 0 1\u00001 2\u00002\n",
      "0 0 1 0 1\u00001 1\u00001\n",
      "0 0 0 1\u00001 0\u00001 23\n",
      "775;\n",
      "such that the desired inverse is given as its right-hand side:\n",
      "A\u00001=2\n",
      "664\u00001 2\u00002 2\n",
      "1\u00001 2\u00002\n",
      "1\u00001 1\u00001\n",
      "\u00001 0\u00001 23\n",
      "775: (2.58)\n",
      "We can verify that (2.58) is indeed the inverse by performing the multi-\n",
      "plicationAA\u00001and observing that we recover I4.\n",
      "2.3.4 Algorithms for Solving a System of Linear Equations\n",
      "In the following, we brieﬂy discuss approaches to solving a system of lin-\n",
      "ear equations of the form Ax=b. We make the assumption that a solu-\n",
      "tion exists. Should there be no solution, we need to resort to approximate\n",
      "solutions, which we do not cover in this chapter. One way to solve the ap-\n",
      "proximate problem is using the approach of linear regression, which we\n",
      "discuss in detail in Chapter 9.\n",
      "In special cases, we may be able to determine the inverse A\u00001, such\n",
      "that the solution of Ax=bis given asx=A\u00001b. However, this is\n",
      "only possible if Ais a square matrix and invertible, which is often not the\n",
      "case. Otherwise, under mild assumptions (i.e., Aneeds to have linearly\n",
      "independent columns) we can use the transformation\n",
      "Ax=b()A>Ax=A>b()x= (A>A)\u00001A>b (2.59)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.4 Vector Spaces 35\n",
      "and use the Moore-Penrose pseudo-inverse (A>A)\u00001A>to determine the Moore-Penrose\n",
      "pseudo-inverse solution (2.59) that solves Ax=b, which also corresponds to the mini-\n",
      "mum norm least-squares solution. A disadvantage of this approach is that\n",
      "it requires many computations for the matrix-matrix product and comput-\n",
      "ing the inverse of A>A. Moreover, for reasons of numerical precision it\n",
      "is generally not recommended to compute the inverse or pseudo-inverse.\n",
      "In the following, we therefore brieﬂy discuss alternative approaches to\n",
      "solving systems of linear equations.\n",
      "Gaussian elimination plays an important role when computing deter-\n",
      "minants (Section 4.1), checking whether a set of vectors is linearly inde-\n",
      "pendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2),\n",
      "computing the rank of a matrix (Section 2.6.2), and determining a basis\n",
      "of a vector space (Section 2.6.1). Gaussian elimination is an intuitive and\n",
      "constructive way to solve a system of linear equations with thousands of\n",
      "variables. However, for systems with millions of variables, it is impracti-\n",
      "cal as the required number of arithmetic operations scales cubically in the\n",
      "number of simultaneous equations.\n",
      "In practice, systems of many linear equations are solved indirectly, by ei-\n",
      "ther stationary iterative methods, such as the Richardson method, the Ja-\n",
      "cobi method, the Gauß-Seidel method, and the successive over-relaxation\n",
      "method, or Krylov subspace methods, such as conjugate gradients, gener-\n",
      "alized minimal residual, or biconjugate gradients. We refer to the books\n",
      "by Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann\n",
      "(2015) for further details.\n",
      "Letx\u0003be a solution of Ax=b. The key idea of these iterative methods\n",
      "is to set up an iteration of the form\n",
      "x(k+1)=Cx(k)+d (2.60)\n",
      "for suitableCanddthat reduces the residual error kx(k+1)\u0000x\u0003kin every\n",
      "iteration and converges to x\u0003. We will introduce norms k\u0001k, which allow\n",
      "us to compute similarities between vectors, in Section 3.1.\n",
      "2.4 Vector Spaces\n",
      "Thus far, we have looked at systems of linear equations and how to solve\n",
      "them (Section 2.3). We saw that systems of linear equations can be com-\n",
      "pactly represented using matrix-vector notation (2.10). In the following,\n",
      "we will have a closer look at vector spaces, i.e., a structured space in which\n",
      "vectors live.\n",
      "In the beginning of this chapter, we informally characterized vectors as\n",
      "objects that can be added together and multiplied by a scalar, and they\n",
      "remain objects of the same type. Now, we are ready to formalize this,\n",
      "and we will start by introducing the concept of a group, which is a set\n",
      "of elements and an operation deﬁned on these elements that keeps some\n",
      "structure of the set intact.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "36 Linear Algebra\n",
      "2.4.1 Groups\n",
      "Groups play an important role in computer science. Besides providing a\n",
      "fundamental framework for operations on sets, they are heavily used in\n",
      "cryptography, coding theory, and graphics.\n",
      "Deﬁnition 2.7 (Group) .Consider a setGand an operation\n",
      ":G\u0002G!G\n",
      "deﬁned onG. ThenG:= (G;\n",
      ")is called a group if the following hold: group\n",
      "closure1.Closure ofGunder\n",
      ":8x;y2G:x\n",
      "y2Gassociativity\n",
      "2.Associativity:8x;y;z2G: (x\n",
      "y)\n",
      "z=x\n",
      "(y\n",
      "z)neutral element\n",
      "3.Neutral element:9e2G8x2G:x\n",
      "e=xande\n",
      "x=x inverse element\n",
      "4.Inverse element:8x2G9y2G:x\n",
      "y=eandy\n",
      "x=e, whereeis\n",
      "the neutral element. We often write x\u00001to denote the inverse element\n",
      "ofx.\n",
      "Remark. The inverse element is deﬁned with respect to the operation \n",
      "and does not necessarily mean1\n",
      "x. }\n",
      "If additionally8x;y2G:x\n",
      "y=y\n",
      "x, thenG= (G;\n",
      ")is an Abelian Abelian group\n",
      "group (commutative).\n",
      "Example 2.10 (Groups)\n",
      "Let us have a look at some examples of sets with associated operations\n",
      "and see whether they are groups:\n",
      "(Z;+)is an Abelian group.\n",
      "(N0;+)is not a group: Although (N0;+)possesses a neutral element N0:=N[f0g\n",
      "(0), the inverse elements are missing.\n",
      "(Z;\u0001)is not a group: Although (Z;\u0001)contains a neutral element ( 1), the\n",
      "inverse elements for any z2Z;z6=\u00061, are missing.\n",
      "(R;\u0001)is not a group since 0does not possess an inverse element.\n",
      "(Rnf0g;\u0001)is Abelian.\n",
      "(Rn;+);(Zn;+);n2Nare Abelian if +is deﬁned componentwise, i.e.,\n",
      "(x1;\u0001\u0001\u0001;xn) + (y1;\u0001\u0001\u0001;yn) = (x1+y1;\u0001\u0001\u0001;xn+yn): (2.61)\n",
      "Then, (x1;\u0001\u0001\u0001;xn)\u00001:= (\u0000x1;\u0001\u0001\u0001;\u0000xn)is the inverse element and\n",
      "e= (0;\u0001\u0001\u0001;0)is the neutral element.\n",
      "(Rm\u0002n;+), the set ofm\u0002n-matrices is Abelian (with componentwise\n",
      "addition as deﬁned in (2.61)).\n",
      "Let us have a closer look at (Rn\u0002n;\u0001), i.e., the set of n\u0002n-matrices with\n",
      "matrix multiplication as deﬁned in (2.13).\n",
      "–Closure and associativity follow directly from the deﬁnition of matrix\n",
      "multiplication.\n",
      "–Neutral element: The identity matrix Inis the neutral element with\n",
      "respect to matrix multiplication “ \u0001” in(Rn\u0002n;\u0001).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.4 Vector Spaces 37\n",
      "–Inverse element: If the inverse exists ( Ais regular), then A\u00001is the\n",
      "inverse element of A2Rn\u0002n, and in exactly this case (Rn\u0002n;\u0001)is a\n",
      "group, called the general linear group .\n",
      "Deﬁnition 2.8 (General Linear Group) .The set of regular (invertible)\n",
      "matricesA2Rn\u0002nis a group with respect to matrix multiplication as\n",
      "deﬁned in (2.13) and is called general linear group GL(n;R). However, general linear group\n",
      "since matrix multiplication is not commutative, the group is not Abelian.\n",
      "2.4.2 Vector Spaces\n",
      "When we discussed groups, we looked at sets Gand inner operations on\n",
      "G, i.e., mappingsG\u0002G!G that only operate on elements in G. In the\n",
      "following, we will consider sets that in addition to an inner operation +\n",
      "also contain an outer operation \u0001, the multiplication of a vector x2Gby\n",
      "a scalar\u00152R. We can think of the inner operation as a form of addition,\n",
      "and the outer operation as a form of scaling. Note that the inner/outer\n",
      "operations have nothing to do with inner/outer products.\n",
      "Deﬁnition 2.9 (Vector Space) .A real-valued vector space V= (V;+;\u0001)is vector space\n",
      "a setVwith two operations\n",
      "+ :V\u0002V!V (2.62)\n",
      "\u0001:R\u0002V!V (2.63)\n",
      "where\n",
      "1.(V;+)is an Abelian group\n",
      "2. Distributivity:\n",
      "1.8\u00152R;x;y2V:\u0015\u0001(x+y) =\u0015\u0001x+\u0015\u0001y\n",
      "2.8\u0015; 2R;x2V: (\u0015+ )\u0001x=\u0015\u0001x+ \u0001x\n",
      "3. Associativity (outer operation): 8\u0015; 2R;x2V:\u0015\u0001( \u0001x) = (\u0015 )\u0001x\n",
      "4. Neutral element with respect to the outer operation: 8x2V: 1\u0001x=x\n",
      "The elements x2Vare called vectors . The neutral element of (V;+)is vector\n",
      "the zero vector 0= [0;:::; 0]>, and the inner operation +is called vector vector addition\n",
      "addition . The elements \u00152Rare called scalars and the outer operation scalar\n",
      "\u0001is amultiplication by scalars . Note that a scalar product is something multiplication by\n",
      "scalars different, and we will get to this in Section 3.2.\n",
      "Remark. A “vector multiplication” ab,a;b2Rn, is not deﬁned. Theoret-\n",
      "ically, we could deﬁne an element-wise multiplication, such that c=ab\n",
      "withcj=ajbj. This “array multiplication” is common to many program-\n",
      "ming languages but makes mathematically limited sense using the stan-\n",
      "dard rules for matrix multiplication: By treating vectors as n\u00021matrices\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "38 Linear Algebra\n",
      "(which we usually do), we can use the matrix multiplication as deﬁned\n",
      "in (2.13). However, then the dimensions of the vectors do not match. Only\n",
      "the following multiplications for vectors are deﬁned: ab>2Rn\u0002n(outer outer product\n",
      "product ),a>b2R(inner/scalar/dot product). }\n",
      "Example 2.11 (Vector Spaces)\n",
      "Let us have a look at some important examples:\n",
      "V=Rn;n2Nis a vector space with operations deﬁned as follows:\n",
      "–Addition:x+y= (x1;:::;xn)+(y1;:::;yn) = (x1+y1;:::;xn+yn)\n",
      "for allx;y2Rn\n",
      "–Multiplication by scalars: \u0015x=\u0015(x1;:::;xn) = (\u0015x1;:::;\u0015xn)for\n",
      "all\u00152R;x2Rn\n",
      "V=Rm\u0002n;m;n2Nis a vector space with\n",
      "–Addition:A+B=2\n",
      "64a11+b11\u0001\u0001\u0001a1n+b1n\n",
      "......\n",
      "am1+bm1\u0001\u0001\u0001amn+bmn3\n",
      "75is deﬁned ele-\n",
      "mentwise for all A;B2V\n",
      "–Multiplication by scalars: \u0015A=2\n",
      "64\u0015a11\u0001\u0001\u0001\u0015a1n\n",
      "......\n",
      "\u0015am1\u0001\u0001\u0001\u0015amn3\n",
      "75as deﬁned in\n",
      "Section 2.2. Remember that Rm\u0002nis equivalent to Rmn.\n",
      "V=C, with the standard deﬁnition of addition of complex numbers.\n",
      "Remark. In the following, we will denote a vector space (V;+;\u0001)byV\n",
      "when +and\u0001are the standard vector addition and scalar multiplication.\n",
      "Moreover, we will use the notation x2Vfor vectors inVto simplify\n",
      "notation. }\n",
      "Remark. The vector spaces Rn;Rn\u00021;R1\u0002nare only different in the way\n",
      "we write vectors. In the following, we will not make a distinction between\n",
      "RnandRn\u00021, which allows us to write n-tuples as column vectors column vector\n",
      "x=2\n",
      "64x1\n",
      "...\n",
      "xn3\n",
      "75: (2.64)\n",
      "This simpliﬁes the notation regarding vector space operations. However,\n",
      "we do distinguish between Rn\u00021andR1\u0002n(therow vectors ) to avoid con- row vector\n",
      "fusion with matrix multiplication. By default, we write xto denote a col-\n",
      "umn vector, and a row vector is denoted by x>, the transpose ofx.} transpose\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.4 Vector Spaces 39\n",
      "2.4.3 Vector Subspaces\n",
      "In the following, we will introduce vector subspaces. Intuitively, they are\n",
      "sets contained in the original vector space with the property that when\n",
      "we perform vector space operations on elements within this subspace, we\n",
      "will never leave it. In this sense, they are “closed”. Vector subspaces are a\n",
      "key idea in machine learning. For example, Chapter 10 demonstrates how\n",
      "to use vector subspaces for dimensionality reduction.\n",
      "Deﬁnition 2.10 (Vector Subspace) .LetV= (V;+;\u0001)be a vector space\n",
      "andU\u0012V ,U6=;. ThenU= (U;+;\u0001)is called vector subspace ofV(or vector subspace\n",
      "linear subspace ) ifUis a vector space with the vector space operations + linear subspace\n",
      "and\u0001restricted toU\u0002U andR\u0002U. We writeU\u0012Vto denote a subspace\n",
      "UofV.\n",
      "IfU\u0012V andVis a vector space, then Unaturally inherits many prop-\n",
      "erties directly from Vbecause they hold for all x2V, and in particular for\n",
      "allx2U\u0012V . This includes the Abelian group properties, the distribu-\n",
      "tivity, the associativity and the neutral element. To determine whether\n",
      "(U;+;\u0001)is a subspace of Vwe still do need to show\n",
      "1.U6=;, in particular: 02U\n",
      "2. Closure of U:\n",
      "a. With respect to the outer operation: 8\u00152R8x2U:\u0015x2U.\n",
      "b. With respect to the inner operation: 8x;y2U:x+y2U.\n",
      "Example 2.12 (Vector Subspaces)\n",
      "Let us have a look at some examples:\n",
      "For every vector space V, the trivial subspaces are Vitself andf0g.\n",
      "Only example Din Figure 2.6 is a subspace of R2(with the usual inner/\n",
      "outer operations). In AandC, the closure property is violated; Bdoes\n",
      "not contain 0.\n",
      "The solution set of a homogeneous system of linear equations Ax=0\n",
      "withnunknownsx= [x1;:::;xn]>is a subspace of Rn.\n",
      "The solution of an inhomogeneous system of linear equations Ax=\n",
      "b;b6=0is not a subspace of Rn.\n",
      "The intersection of arbitrarily many subspaces is a subspace itself.\n",
      "Figure 2.6 Not all\n",
      "subsets of R2are\n",
      "subspaces. In Aand\n",
      "C, the closure\n",
      "property is violated;\n",
      "Bdoes not contain\n",
      "0. OnlyDis a\n",
      "subspace.0 0 0 0AB\n",
      "CD\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "40 Linear Algebra\n",
      "Remark. Every subspace U\u0012(Rn;+;\u0001)is the solution space of a homo-\n",
      "geneous system of linear equations Ax=0forx2Rn.}\n",
      "2.5 Linear Independence\n",
      "In the following, we will have a close look at what we can do with vectors\n",
      "(elements of the vector space). In particular, we can add vectors together\n",
      "and multiply them with scalars. The closure property guarantees that we\n",
      "end up with another vector in the same vector space. It is possible to ﬁnd\n",
      "a set of vectors with which we can represent every vector in the vector\n",
      "space by adding them together and scaling them. This set of vectors is\n",
      "abasis, and we will discuss them in Section 2.6.1. Before we get there,\n",
      "we will need to introduce the concepts of linear combinations and linear\n",
      "independence.\n",
      "Deﬁnition 2.11 (Linear Combination) .Consider a vector space Vand a\n",
      "ﬁnite number of vectors x1;:::;xk2V. Then, every v2Vof the form\n",
      "v=\u00151x1+\u0001\u0001\u0001+\u0015kxk=kX\n",
      "i=1\u0015ixi2V (2.65)\n",
      "with\u00151;:::;\u0015k2Ris alinear combination of the vectors x1;:::;xk. linear combination\n",
      "The0-vector can always be written as the linear combination of kvec-\n",
      "torsx1;:::;xkbecause 0=Pk\n",
      "i=10xiis always true. In the following,\n",
      "we are interested in non-trivial linear combinations of a set of vectors to\n",
      "represent 0, i.e., linear combinations of vectors x1;:::;xk, where not all\n",
      "coefﬁcients \u0015iin (2.65) are 0.\n",
      "Deﬁnition 2.12 (Linear (In)dependence) .Let us consider a vector space\n",
      "Vwithk2Nandx1;:::;xk2V. If there is a non-trivial linear com-\n",
      "bination, such that 0=Pk\n",
      "i=1\u0015ixiwith at least one \u0015i6= 0, the vectors\n",
      "x1;:::;xkarelinearly dependent . If only the trivial solution exists, i.e., linearly dependent\n",
      "\u00151=:::=\u0015k= 0the vectorsx1;:::;xkarelinearly independent . linearly\n",
      "independent\n",
      "Linear independence is one of the most important concepts in linear\n",
      "algebra. Intuitively, a set of linearly independent vectors consists of vectors\n",
      "that have no redundancy, i.e., if we remove any of those vectors from\n",
      "the set, we will lose something. Throughout the next sections, we will\n",
      "formalize this intuition more.\n",
      "Example 2.13 (Linearly Dependent Vectors)\n",
      "A geographic example may help to clarify the concept of linear indepen-\n",
      "dence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is\n",
      "might say ,“You can get to Kigali by ﬁrst going 506 km Northwest to Kam-\n",
      "pala (Uganda) and then 374 km Southwest.”. This is sufﬁcient information\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.5 Linear Independence 41\n",
      "to describe the location of Kigali because the geographic coordinate sys-\n",
      "tem may be considered a two-dimensional vector space (ignoring altitude\n",
      "and the Earth’s curved surface). The person may add, “It is about 751 km\n",
      "West of here.” Although this last statement is true, it is not necessary to\n",
      "ﬁnd Kigali given the previous information (see Figure 2.7 for an illus-\n",
      "tration). In this example, the “ 506 km Northwest” vector (blue) and the\n",
      "“374 km Southwest” vector (purple) are linearly independent. This means\n",
      "the Southwest vector cannot be described in terms of the Northwest vec-\n",
      "tor, and vice versa. However, the third “ 751 km West” vector (black) is a\n",
      "linear combination of the other two vectors, and it makes the set of vec-\n",
      "tors linearly dependent. Equivalently, given “ 751 km West” and “ 374 km\n",
      "Southwest” can be linearly combined to obtain “ 506 km Northwest”.\n",
      "Figure 2.7\n",
      "Geographic example\n",
      "(with crude\n",
      "approximations to\n",
      "cardinal directions)\n",
      "of linearly\n",
      "dependent vectors\n",
      "in a\n",
      "two-dimensional\n",
      "space (plane).\n",
      "506 km Northwest\n",
      "751 km West\n",
      "374 km Southwest\n",
      "374 km Southwest\n",
      "Kampala\n",
      "Nairobi\n",
      "Kigali\n",
      "Remark. The following properties are useful to ﬁnd out whether vectors\n",
      "are linearly independent:\n",
      "kvectors are either linearly dependent or linearly independent. There\n",
      "is no third option.\n",
      "If at least one of the vectors x1;:::;xkis0then they are linearly de-\n",
      "pendent. The same holds if two vectors are identical.\n",
      "The vectorsfx1;:::;xk:xi6=0;i= 1;:::;kg,k>2, are linearly\n",
      "dependent if and only if (at least) one of them is a linear combination\n",
      "of the others. In particular, if one vector is a multiple of another vector,\n",
      "i.e.,xi=\u0015xj; \u00152Rthen the setfx1;:::;xk:xi6=0;i= 1;:::;kg\n",
      "is linearly dependent.\n",
      "A practical way of checking whether vectors x1;:::;xk2Vare linearly\n",
      "independent is to use Gaussian elimination: Write all vectors as columns\n",
      "of a matrixAand perform Gaussian elimination until the matrix is in\n",
      "row echelon form (the reduced row-echelon form is unnecessary here):\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "42 Linear Algebra\n",
      "–The pivot columns indicate the vectors, which are linearly indepen-\n",
      "dent of the vectors on the left. Note that there is an ordering of vec-\n",
      "tors when the matrix is built.\n",
      "–The non-pivot columns can be expressed as linear combinations of\n",
      "the pivot columns on their left. For instance, the row-echelon form\n",
      "\u00141 3 0\n",
      "0 0 2\u0015\n",
      "(2.66)\n",
      "tells us that the ﬁrst and third columns are pivot columns. The sec-\n",
      "ond column is a non-pivot column because it is three times the ﬁrst\n",
      "column.\n",
      "All column vectors are linearly independent if and only if all columns\n",
      "are pivot columns. If there is at least one non-pivot column, the columns\n",
      "(and, therefore, the corresponding vectors) are linearly dependent.\n",
      "}\n",
      "Example 2.14\n",
      "Consider R4with\n",
      "x1=2\n",
      "6641\n",
      "2\n",
      "\u00003\n",
      "43\n",
      "775;x2=2\n",
      "6641\n",
      "1\n",
      "0\n",
      "23\n",
      "775;x3=2\n",
      "664\u00001\n",
      "\u00002\n",
      "1\n",
      "13\n",
      "775: (2.67)\n",
      "To check whether they are linearly dependent, we follow the general ap-\n",
      "proach and solve\n",
      "\u00151x1+\u00152x2+\u00153x3=\u001512\n",
      "6641\n",
      "2\n",
      "\u00003\n",
      "43\n",
      "775+\u001522\n",
      "6641\n",
      "1\n",
      "0\n",
      "23\n",
      "775+\u001532\n",
      "664\u00001\n",
      "\u00002\n",
      "1\n",
      "13\n",
      "775=0 (2.68)\n",
      "for\u00151;:::;\u0015 3. We write the vectors xi,i= 1;2;3, as the columns of a\n",
      "matrix and apply elementary row operations until we identify the pivot\n",
      "columns:\n",
      "2\n",
      "6641 1\u00001\n",
      "2 1\u00002\n",
      "\u00003 0 1\n",
      "4 2 13\n",
      "775 \u0001\u0001\u0001 2\n",
      "6641 1\u00001\n",
      "0 1 0\n",
      "0 0 1\n",
      "0 0 03\n",
      "775: (2.69)\n",
      "Here, every column of the matrix is a pivot column. Therefore, there is no\n",
      "non-trivial solution, and we require \u00151= 0;\u00152= 0;\u00153= 0to solve the\n",
      "equation system. Hence, the vectors x1;x2;x3are linearly independent.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.5 Linear Independence 43\n",
      "Remark. Consider a vector space Vwithklinearly independent vectors\n",
      "b1;:::;bkandmlinear combinations\n",
      "x1=kX\n",
      "i=1\u0015i1bi;\n",
      "...\n",
      "xm=kX\n",
      "i=1\u0015imbi:(2.70)\n",
      "DeﬁningB= [b1;:::;bk]as the matrix whose columns are the linearly\n",
      "independent vectors b1;:::;bk, we can write\n",
      "xj=B\u0015j;\u0015j=2\n",
      "64\u00151j\n",
      "...\n",
      "\u0015kj3\n",
      "75; j = 1;:::;m; (2.71)\n",
      "in a more compact form.\n",
      "We want to test whether x1;:::;xmare linearly independent. For this\n",
      "purpose, we follow the general approach of testing whenPm\n",
      "j=1 jxj=0.\n",
      "With (2.71), we obtain\n",
      "mX\n",
      "j=1 jxj=mX\n",
      "j=1 jB\u0015j=BmX\n",
      "j=1 j\u0015j: (2.72)\n",
      "This means thatfx1;:::;xmgare linearly independent if and only if the\n",
      "column vectorsf\u00151;:::;\u0015mgare linearly independent.\n",
      "}\n",
      "Remark. In a vector space V,mlinear combinations of kvectorsx1;:::;xk\n",
      "are linearly dependent if m>k . }\n",
      "Example 2.15\n",
      "Consider a set of linearly independent vectors b1;b2;b3;b42Rnand\n",
      "x1=b1\u00002b2+b3\u0000b4\n",
      "x2=\u00004b1\u00002b2 + 4b4\n",
      "x3= 2b1 + 3b2\u0000b3\u00003b4\n",
      "x4= 17b1\u000010b2+ 11b3+b4: (2.73)\n",
      "Are the vectors x1;:::;x42Rnlinearly independent? To answer this\n",
      "question, we investigate whether the column vectors\n",
      "8\n",
      ">><\n",
      ">>:2\n",
      "6641\n",
      "\u00002\n",
      "1\n",
      "\u000013\n",
      "775;2\n",
      "664\u00004\n",
      "\u00002\n",
      "0\n",
      "43\n",
      "775;2\n",
      "6642\n",
      "3\n",
      "\u00001\n",
      "\u000033\n",
      "775;2\n",
      "66417\n",
      "\u000010\n",
      "11\n",
      "13\n",
      "7759\n",
      ">>=\n",
      ">>;(2.74)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "44 Linear Algebra\n",
      "are linearly independent. The reduced row-echelon form of the corre-\n",
      "sponding linear equation system with coefﬁcient matrix\n",
      "A=2\n",
      "6641\u00004 2 17\n",
      "\u00002\u00002 3\u000010\n",
      "1 0\u00001 11\n",
      "\u00001 4\u00003 13\n",
      "775(2.75)\n",
      "is given as\n",
      "2\n",
      "6641 0 0\u00007\n",
      "0 1 0\u000015\n",
      "0 0 1\u000018\n",
      "0 0 0 03\n",
      "775: (2.76)\n",
      "We see that the corresponding linear equation system is non-trivially solv-\n",
      "able: The last column is not a pivot column, and x4=\u00007x1\u000015x2\u000018x3.\n",
      "Therefore,x1;:::;x4are linearly dependent as x4can be expressed as a\n",
      "linear combination of x1;:::;x3.\n",
      "2.6 Basis and Rank\n",
      "In a vector space V, we are particularly interested in sets of vectors Athat\n",
      "possess the property that any vector v2Vcan be obtained by a linear\n",
      "combination of vectors in A. These vectors are special vectors, and in the\n",
      "following, we will characterize them.\n",
      "2.6.1 Generating Set and Basis\n",
      "Deﬁnition 2.13 (Generating Set and Span) .Consider a vector space V=\n",
      "(V;+;\u0001)and set of vectors A=fx1;:::;xkg\u0012V . If every vector v2\n",
      "Vcan be expressed as a linear combination of x1;:::;xk,Ais called a\n",
      "generating set ofV. The set of all linear combinations of vectors in Ais generating set\n",
      "called the span ofA. IfAspans the vector space V, we writeV= span[A] span\n",
      "orV= span[x1;:::;xk].\n",
      "Generating sets are sets of vectors that span vector (sub)spaces, i.e.,\n",
      "every vector can be represented as a linear combination of the vectors\n",
      "in the generating set. Now, we will be more speciﬁc and characterize the\n",
      "smallest generating set that spans a vector (sub)space.\n",
      "Deﬁnition 2.14 (Basis) .Consider a vector space V= (V;+;\u0001)andA\u0012\n",
      "V. A generating set AofVis called minimal if there exists no smaller set minimal\n",
      "~A(A\u0012V that spansV. Every linearly independent generating set of V\n",
      "is minimal and is called a basis ofV. basis\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.6 Basis and Rank 45\n",
      "LetV= (V;+;\u0001)be a vector space and B \u0012 V;B 6=;. Then, the\n",
      "following statements are equivalent: A basis is a minimal\n",
      "generating set and a\n",
      "maximal linearly\n",
      "independent set of\n",
      "vectors.Bis a basis of V.\n",
      "Bis a minimal generating set.\n",
      "Bis a maximal linearly independent set of vectors in V, i.e., adding any\n",
      "other vector to this set will make it linearly dependent.\n",
      "Every vectorx2Vis a linear combination of vectors from B, and every\n",
      "linear combination is unique, i.e., with\n",
      "x=kX\n",
      "i=1\u0015ibi=kX\n",
      "i=1 ibi (2.77)\n",
      "and\u0015i; i2R,bi2Bit follows that \u0015i= i; i= 1;:::;k .\n",
      "Example 2.16\n",
      "InR3, the canonical/standard basis is canonical basis\n",
      "B=8\n",
      "<\n",
      ":2\n",
      "41\n",
      "0\n",
      "03\n",
      "5;2\n",
      "40\n",
      "1\n",
      "03\n",
      "5;2\n",
      "40\n",
      "0\n",
      "13\n",
      "59\n",
      "=\n",
      ";: (2.78)\n",
      "Different bases in R3are\n",
      "B1=8\n",
      "<\n",
      ":2\n",
      "41\n",
      "0\n",
      "03\n",
      "5;2\n",
      "41\n",
      "1\n",
      "03\n",
      "5;2\n",
      "41\n",
      "1\n",
      "13\n",
      "59\n",
      "=\n",
      ";;B2=8\n",
      "<\n",
      ":2\n",
      "40:5\n",
      "0:8\n",
      "0:43\n",
      "5;2\n",
      "41:8\n",
      "0:3\n",
      "0:33\n",
      "5;2\n",
      "4\u00002:2\n",
      "\u00001:3\n",
      "3:53\n",
      "59\n",
      "=\n",
      ";:(2.79)\n",
      "The set\n",
      "A=8\n",
      ">><\n",
      ">>:2\n",
      "6641\n",
      "2\n",
      "3\n",
      "43\n",
      "775;2\n",
      "6642\n",
      "\u00001\n",
      "0\n",
      "23\n",
      "775;2\n",
      "6641\n",
      "1\n",
      "0\n",
      "\u000043\n",
      "7759\n",
      ">>=\n",
      ">>;(2.80)\n",
      "is linearly independent, but not a generating set (and no basis) of R4:\n",
      "For instance, the vector [1;0;0;0]>cannot be obtained by a linear com-\n",
      "bination of elements in A.\n",
      "Remark. Every vector space Vpossesses a basisB. The preceding exam-\n",
      "ples show that there can be many bases of a vector space V, i.e., there is\n",
      "no unique basis. However, all bases possess the same number of elements,\n",
      "thebasis vectors . } basis vector\n",
      "We only consider ﬁnite-dimensional vector spaces V. In this case, the\n",
      "dimension ofVis the number of basis vectors of V, and we write dim(V).dimension\n",
      "IfU\u0012Vis a subspace of V, then dim(U)6dim(V)anddim(U) =\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "46 Linear Algebra\n",
      "dim(V)if and only if U=V. Intuitively, the dimension of a vector space\n",
      "can be thought of as the number of independent directions in this vector\n",
      "space. The dimension of a\n",
      "vector space\n",
      "corresponds to the\n",
      "number of its basis\n",
      "vectors.Remark. The dimension of a vector space is not necessarily the number\n",
      "of elements in a vector. For instance, the vector space V= span[\u00140\n",
      "1\u0015\n",
      "]is\n",
      "one-dimensional, although the basis vector possesses two elements. }\n",
      "Remark. A basis of a subspace U= span[x1;:::;xm]\u0012Rncan be found\n",
      "by executing the following steps:\n",
      "1. Write the spanning vectors as columns of a matrix A\n",
      "2. Determine the row-echelon form of A.\n",
      "3. The spanning vectors associated with the pivot columns are a basis of\n",
      "U.\n",
      "}\n",
      "Example 2.17 (Determining a Basis)\n",
      "For a vector subspace U\u0012R5, spanned by the vectors\n",
      "x1=2\n",
      "666641\n",
      "2\n",
      "\u00001\n",
      "\u00001\n",
      "\u000013\n",
      "77775;x2=2\n",
      "666642\n",
      "\u00001\n",
      "1\n",
      "2\n",
      "\u000023\n",
      "77775;x3=2\n",
      "666643\n",
      "\u00004\n",
      "3\n",
      "5\n",
      "\u000033\n",
      "77775;x4=2\n",
      "66664\u00001\n",
      "8\n",
      "\u00005\n",
      "\u00006\n",
      "13\n",
      "777752R5;(2.81)\n",
      "we are interested in ﬁnding out which vectors x1;:::;x4are a basis for U.\n",
      "For this, we need to check whether x1;:::;x4are linearly independent.\n",
      "Therefore, we need to solve\n",
      "4X\n",
      "i=1\u0015ixi=0; (2.82)\n",
      "which leads to a homogeneous system of equations with matrix\n",
      "\u0002x1;x2;x3;x4\u0003=2\n",
      "666641 2 3\u00001\n",
      "2\u00001\u00004 8\n",
      "\u00001 1 3\u00005\n",
      "\u00001 2 5\u00006\n",
      "\u00001\u00002\u00003 13\n",
      "77775: (2.83)\n",
      "With the basic transformation rules for systems of linear equations, we\n",
      "obtain the row-echelon form2\n",
      "666641 2 3\u00001\n",
      "2\u00001\u00004 8\n",
      "\u00001 1 3\u00005\n",
      "\u00001 2 5\u00006\n",
      "\u00001\u00002\u00003 13\n",
      "77775 \u0001\u0001\u0001 2\n",
      "666641 2 3\u00001\n",
      "0 1 2\u00002\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 03\n",
      "77775:\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.6 Basis and Rank 47\n",
      "Since the pivot columns indicate which set of vectors is linearly indepen-\n",
      "dent, we see from the row-echelon form that x1;x2;x4are linearly inde-\n",
      "pendent (because the system of linear equations \u00151x1+\u00152x2+\u00154x4=0\n",
      "can only be solved with \u00151=\u00152=\u00154= 0). Therefore,fx1;x2;x4gis a\n",
      "basis ofU.\n",
      "2.6.2 Rank\n",
      "The number of linearly independent columns of a matrix A2Rm\u0002n\n",
      "equals the number of linearly independent rows and is called the rank rank\n",
      "ofAand is denoted by rk(A).\n",
      "Remark. The rank of a matrix has some important properties:\n",
      "rk(A) = rk(A>), i.e., the column rank equals the row rank.\n",
      "The columns of A2Rm\u0002nspan a subspace U\u0012Rmwith dim(U) =\n",
      "rk(A). Later we will call this subspace the image orrange . A basis of\n",
      "Ucan be found by applying Gaussian elimination to Ato identify the\n",
      "pivot columns.\n",
      "The rows ofA2Rm\u0002nspan a subspace W\u0012Rnwith dim(W) =\n",
      "rk(A). A basis ofWcan be found by applying Gaussian elimination to\n",
      "A>.\n",
      "For allA2Rn\u0002nit holds thatAis regular (invertible) if and only if\n",
      "rk(A) =n.\n",
      "For allA2Rm\u0002nand allb2Rmit holds that the linear equation\n",
      "systemAx=bcan be solved if and only if rk(A) = rk(Ajb), where\n",
      "Ajbdenotes the augmented system.\n",
      "ForA2Rm\u0002nthe subspace of solutions for Ax=0possesses dimen-\n",
      "sionn\u0000rk(A). Later, we will call this subspace the kernel or the null kernel\n",
      "null space space .\n",
      "A matrixA2Rm\u0002nhasfull rank if its rank equals the largest possible full rank\n",
      "rank for a matrix of the same dimensions. This means that the rank of\n",
      "a full-rank matrix is the lesser of the number of rows and columns, i.e.,\n",
      "rk(A) = min(m;n). A matrix is said to be rank deﬁcient if it does not rank deﬁcient\n",
      "have full rank.\n",
      "}\n",
      "Example 2.18 (Rank)\n",
      "A=2\n",
      "41 0 1\n",
      "0 1 1\n",
      "0 0 03\n",
      "5.\n",
      "Ahas two linearly independent rows/columns so that rk(A) = 2 .\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "48 Linear Algebra\n",
      "A=2\n",
      "41 2 1\n",
      "\u00002\u00003 1\n",
      "3 5 03\n",
      "5:\n",
      "We use Gaussian elimination to determine the rank:\n",
      "2\n",
      "41 2 1\n",
      "\u00002\u00003 1\n",
      "3 5 03\n",
      "5 \u0001\u0001\u0001 2\n",
      "41 2 1\n",
      "0 1 3\n",
      "0 0 03\n",
      "5: (2.84)\n",
      "Here, we see that the number of linearly independent rows and columns\n",
      "is 2, such that rk(A) = 2 .\n",
      "2.7 Linear Mappings\n",
      "In the following, we will study mappings on vector spaces that preserve\n",
      "their structure, which will allow us to deﬁne the concept of a coordinate.\n",
      "In the beginning of the chapter, we said that vectors are objects that can be\n",
      "added together and multiplied by a scalar, and the resulting object is still\n",
      "a vector. We wish to preserve this property when applying the mapping:\n",
      "Consider two real vector spaces V;W . A mapping :V!Wpreserves\n",
      "the structure of the vector space if\n",
      "\b(x+y) =(x) +(y) (2.85)\n",
      "\b(\u0015x) =(x) (2.86)\n",
      "for allx;y2Vand\u00152R. We can summarize this in the following\n",
      "deﬁnition:\n",
      "Deﬁnition 2.15 (Linear Mapping) .For vector spaces V;W , a mapping\n",
      "\b :V!Wis called a linear mapping (orvector space homomorphism / linear mapping\n",
      "vector space\n",
      "homomorphismlinear transformation ) if\n",
      "linear\n",
      "transformation8x;y2V8\u0015; 2R:(\u0015x+ y) =(x) +(y): (2.87)\n",
      "It turns out that we can represent linear mappings as matrices (Sec-\n",
      "tion 2.7.1). Recall that we can also collect a set of vectors as columns of a\n",
      "matrix. When working with matrices, we have to keep in mind what the\n",
      "matrix represents: a linear mapping or a collection of vectors. We will see\n",
      "more about linear mappings in Chapter 4. Before we continue, we will\n",
      "brieﬂy introduce special mappings.\n",
      "Deﬁnition 2.16 (Injective, Surjective, Bijective) .Consider a mapping :\n",
      "V!W , whereV;Wcan be arbitrary sets. Thenis called\n",
      "injective\n",
      "Injective if8x;y2V:(x) =(y) =)x=y.surjective\n",
      "Surjective i(V) =W.bijective\n",
      "Bijective if it is injective and surjective.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.7 Linear Mappings 49\n",
      "Iis surjective, then every element in Wcan be “reached” from V\n",
      "using. A bijectivecan be “undone”, i.e., there exists a mapping \t :\n",
      "W!V so that \t(x) =x. This mapping \tis then called the inverse\n",
      "oand normally denoted by\u00001.\n",
      "With these deﬁnitions, we introduce the following special cases of linear\n",
      "mappings between vector spaces VandW:\n",
      "isomorphism\n",
      "Isomorphism: :V!Wlinear and bijective endomorphism\n",
      "Endomorphism: :V!Vlinear automorphism\n",
      "Automorphism: :V!Vlinear and bijective\n",
      "We deﬁne idV:V!V,x7!xas the identity mapping oridentity identity mapping\n",
      "identity\n",
      "automorphismautomorphism inV.\n",
      "Example 2.19 (Homomorphism)\n",
      "The mapping :R2!C(x) =x1+ix2, is a homomorphism:\n",
      "\b\u0012\u0014x1\n",
      "x2\u0015\n",
      "+\u0014y1\n",
      "y2\u0015\u0013\n",
      "= (x1+y1) +i(x2+y2) =x1+ix2+y1+iy2\n",
      "=\u0012\u0014x1\n",
      "x2\u0015\u0013\n",
      "+\u0012\u0014y1\n",
      "y2\u0015\u0013\n",
      "\b\u0012\n",
      "\u0015\u0014x1\n",
      "x2\u0015\u0013\n",
      "=\u0015x1+\u0015ix2=\u0015(x1+ix2) =\u0012\u0014x1\n",
      "x2\u0015\u0013\n",
      ":\n",
      "(2.88)\n",
      "This also justiﬁes why complex numbers can be represented as tuples in\n",
      "R2: There is a bijective linear mapping that converts the elementwise addi-\n",
      "tion of tuples in R2into the set of complex numbers with the correspond-\n",
      "ing addition. Note that we only showed linearity, but not the bijection.\n",
      "Theorem 2.17 (Theorem 3.59 in Axler (2015)) .Finite-dimensional vector\n",
      "spacesVandWare isomorphic if and only if dim(V) = dim(W).\n",
      "Theorem 2.17 states that there exists a linear, bijective mapping be-\n",
      "tween two vector spaces of the same dimension. Intuitively, this means\n",
      "that vector spaces of the same dimension are kind of the same thing, as\n",
      "they can be transformed into each other without incurring any loss.\n",
      "Theorem 2.17 also gives us the justiﬁcation to treat Rm\u0002n(the vector\n",
      "space ofm\u0002n-matrices) and Rmn(the vector space of vectors of length\n",
      "mn) the same, as their dimensions are mn, and there exists a linear, bi-\n",
      "jective mapping that transforms one into the other.\n",
      "Remark. Consider vector spaces V;W;X . Then:\n",
      "For linear mappings :V!Wand\t :W!X, the mapping\n",
      "\t :V!Xis also linear.\n",
      "I :V!Wis an isomorphism, then\u00001:W!Vis an isomor-\n",
      "phism, too.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "50 Linear Algebra\n",
      "Figure 2.8 Two\n",
      "different coordinate\n",
      "systems deﬁned by\n",
      "two sets of basis\n",
      "vectors. A vector x\n",
      "has different\n",
      "coordinate\n",
      "representations\n",
      "depending on which\n",
      "coordinate system is\n",
      "chosen.x x\n",
      "e1e2\n",
      "b1b2\n",
      "I :V!W;\t :V!Ware linear, then + \t and; \u00152R, are\n",
      "linear, too.\n",
      "}\n",
      "2.7.1 Matrix Representation of Linear Mappings\n",
      "Anyn-dimensional vector space is isomorphic to Rn(Theorem 2.17). We\n",
      "consider a basisfb1;:::;bngof ann-dimensional vector space V. In the\n",
      "following, the order of the basis vectors will be important. Therefore, we\n",
      "write\n",
      "B= (b1;:::;bn) (2.89)\n",
      "and call this n-tuple an ordered basis ofV. ordered basis\n",
      "Remark (Notation) .We are at the point where notation gets a bit tricky.\n",
      "Therefore, we summarize some parts here. B= (b1;:::;bn)is an ordered\n",
      "basis,B=fb1;:::;bngis an (unordered) basis, and B= [b1;:::;bn]is a\n",
      "matrix whose columns are the vectors b1;:::;bn. }\n",
      "Deﬁnition 2.18 (Coordinates) .Consider a vector space Vand an ordered\n",
      "basisB= (b1;:::;bn)ofV. For anyx2Vwe obtain a unique represen-\n",
      "tation (linear combination)\n",
      "x=\u000b1b1+:::+\u000bnbn (2.90)\n",
      "ofxwith respect to B. Then\u000b1;:::;\u000bnare the coordinates ofxwith coordinate\n",
      "respect toB, and the vector\n",
      "\u000b=2\n",
      "64\u000b1\n",
      "...\n",
      "\u000bn3\n",
      "752Rn(2.91)\n",
      "is the coordinate vector /coordinate representation ofxwith respect to the coordinate vector\n",
      "coordinate\n",
      "representationordered basis B.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.7 Linear Mappings 51\n",
      "A basis effectively deﬁnes a coordinate system. We are familiar with the\n",
      "Cartesian coordinate system in two dimensions, which is spanned by the\n",
      "canonical basis vectors e1;e2. In this coordinate system, a vector x2R2\n",
      "has a representation that tells us how to linearly combine e1ande2to\n",
      "obtainx. However, any basis of R2deﬁnes a valid coordinate system,\n",
      "and the same vector xfrom before may have a different coordinate rep-\n",
      "resentation in the (b1;b2)basis. In Figure 2.8, the coordinates of xwith\n",
      "respect to the standard basis (e1;e2)is[2;2]>. However, with respect to\n",
      "the basis (b1;b2)the same vector xis represented as [1:09;0:72]>, i.e.,\n",
      "x= 1:09b1+ 0:72b2. In the following sections, we will discover how to\n",
      "obtain this representation.\n",
      "Example 2.20\n",
      "Let us have a look at a geometric vector x2R2with coordinates [2;3]>Figure 2.9\n",
      "Different coordinate\n",
      "representations of a\n",
      "vectorx, depending\n",
      "on the choice of\n",
      "basis.\n",
      "e1e2b2\n",
      "b1x=\u00001\n",
      "2b1+5\n",
      "2b2x= 2e1+ 3e2with respect to the standard basis (e1;e2)ofR2. This means, we can write\n",
      "x= 2e1+ 3e2. However, we do not have to choose the standard basis to\n",
      "represent this vector. If we use the basis vectors b1= [1;\u00001]>;b2= [1;1]>\n",
      "we will obtain the coordinates1\n",
      "2[\u00001;5]>to represent the same vector with\n",
      "respect to (b1;b2)(see Figure 2.9).\n",
      "Remark. For ann-dimensional vector space Vand an ordered basis B\n",
      "ofV, the mapping :Rn!V(ei) =bi,i= 1;:::;n; is linear\n",
      "(and because of Theorem 2.17 an isomorphism), where (e1;:::;en)is\n",
      "the standard basis of Rn.\n",
      "}\n",
      "Now we are ready to make an explicit connection between matrices and\n",
      "linear mappings between ﬁnite-dimensional vector spaces.\n",
      "Deﬁnition 2.19 (Transformation Matrix) .Consider vector spaces V;W\n",
      "with corresponding (ordered) bases B= (b1;:::;bn)andC= (c1;:::;cm).\n",
      "Moreover, we consider a linear mapping :V!W. Forj2f1;:::;ng,\n",
      "\b(bj) =\u000b1jc1+\u0001\u0001\u0001+\u000bmjcm=mX\n",
      "i=1\u000bijci (2.92)\n",
      "is the unique representation of(bj)with respect to C. Then, we call the\n",
      "m\u0002n-matrix, whose elements are given by\n",
      "(i;j) =\u000bij; (2.93)\n",
      "thetransformation matrix o(with respect to the ordered bases BofV transformation\n",
      "matrix andCofW).\n",
      "The coordinates of(bj)with respect to the ordered basis CofW\n",
      "are thej-th column of . Consider (ﬁnite-dimensional) vector spaces\n",
      "V;W with ordered bases B;C and a linear mapping :V!Wwith\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "52 Linear Algebra\n",
      "transformation matrix . If^xis the coordinate vector of x2Vwith\n",
      "respect toBand^ythe coordinate vector of y=(x)2Wwith respect\n",
      "toC, then\n",
      "^y=^x: (2.94)\n",
      "This means that the transformation matrix can be used to map coordinates\n",
      "with respect to an ordered basis in Vto coordinates with respect to an\n",
      "ordered basis in W.\n",
      "Example 2.21 (Transformation Matrix)\n",
      "Consider a homomorphism :V!Wand ordered bases B=\n",
      "(b1;:::;b3)ofVandC= (c1;:::;c4)ofW. With\n",
      "\b(b1) =c1\u0000c2+ 3c3\u0000c4\n",
      "\b(b2) = 2c1+c2+ 7c3+ 2c4\n",
      "\b(b3) = 3c2+c3+ 4c4(2.95)\n",
      "the transformation matrix with respect to BandCsatisﬁes(bk) =P4\n",
      "i=1\u000bikcifork= 1;:::; 3and is given as\n",
      "= [\u000b1;\u000b2;\u000b3] =2\n",
      "6641 2 0\n",
      "\u00001 1 3\n",
      "3 7 1\n",
      "\u00001 2 43\n",
      "775; (2.96)\n",
      "where the\u000bj; j= 1;2;3;are the coordinate vectors of(bj)with respect\n",
      "toC.\n",
      "Example 2.22 (Linear Transformations of Vectors)\n",
      "Figure 2.10 Three\n",
      "examples of linear\n",
      "transformations of\n",
      "the vectors shown\n",
      "as dots in (a);\n",
      "(b) Rotation by 45\u000e;\n",
      "(c) Stretching of the\n",
      "horizontal\n",
      "coordinates by 2;\n",
      "(d) Combination of\n",
      "reﬂection, rotation\n",
      "and stretching.\n",
      "(a) Original data.\n",
      " (b) Rotation by 45\u000e.\n",
      "(c) Stretch along the\n",
      "horizontal axis.\n",
      "(d) General linear\n",
      "mapping.\n",
      "We consider three linear transformations of a set of vectors in R2with\n",
      "the transformation matrices\n",
      "A1=\u0014cos(\u0019\n",
      "4)\u0000sin(\u0019\n",
      "4)\n",
      "sin(\u0019\n",
      "4) cos(\u0019\n",
      "4)\u0015\n",
      ";A2=\u00142 0\n",
      "0 1\u0015\n",
      ";A3=1\n",
      "2\u00143\u00001\n",
      "1\u00001\u0015\n",
      ":(2.97)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.7 Linear Mappings 53\n",
      "Figure 2.10 gives three examples of linear transformations of a set of vec-\n",
      "tors. Figure 2.10(a) shows 400vectors in R2, each of which is represented\n",
      "by a dot at the corresponding (x1;x2)-coordinates. The vectors are ar-\n",
      "ranged in a square. When we use matrix A1in (2.97) to linearly transform\n",
      "each of these vectors, we obtain the rotated square in Figure 2.10(b). If we\n",
      "apply the linear mapping represented by A2, we obtain the rectangle in\n",
      "Figure 2.10(c) where each x1-coordinate is stretched by 2. Figure 2.10(d)\n",
      "shows the original square from Figure 2.10(a) when linearly transformed\n",
      "usingA3, which is a combination of a reﬂection, a rotation, and a stretch.\n",
      "2.7.2 Basis Change\n",
      "In the following, we will have a closer look at how transformation matrices\n",
      "of a linear mapping :V!Wchange if we change the bases in Vand\n",
      "W. Consider two ordered bases\n",
      "B= (b1;:::;bn);~B= (~b1;:::; ~bn) (2.98)\n",
      "ofVand two ordered bases\n",
      "C= (c1;:::;cm);~C= (~c1;:::; ~cm) (2.99)\n",
      "ofW. Moreover,2Rm\u0002nis the transformation matrix of the linear\n",
      "mapping :V!Wwith respect to the bases BandC, and ~2Rm\u0002n\n",
      "is the corresponding transformation mapping with respect to ~Band ~C.\n",
      "In the following, we will investigate how Aand~Aare related, i.e., how/\n",
      "whether we can transform into ~if we choose to perform a basis\n",
      "change from B;C to~B;~C.\n",
      "Remark. We effectively get different coordinate representations of the\n",
      "identity mapping idV. In the context of Figure 2.9, this would mean to\n",
      "map coordinates with respect to (e1;e2)onto coordinates with respect to\n",
      "(b1;b2)without changing the vector x. By changing the basis and corre-\n",
      "spondingly the representation of vectors, the transformation matrix with\n",
      "respect to this new basis can have a particularly simple form that allows\n",
      "for straightforward computation. }\n",
      "Example 2.23 (Basis Change)\n",
      "Consider a transformation matrix\n",
      "A=\u00142 1\n",
      "1 2\u0015\n",
      "(2.100)\n",
      "with respect to the canonical basis in R2. If we deﬁne a new basis\n",
      "B= (\u00141\n",
      "1\u0015\n",
      ";\u00141\n",
      "\u00001\u0015\n",
      ") (2.101)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "54 Linear Algebra\n",
      "we obtain a diagonal transformation matrix\n",
      "~A=\u00143 0\n",
      "0 1\u0015\n",
      "(2.102)\n",
      "with respect to B, which is easier to work with than A.\n",
      "In the following, we will look at mappings that transform coordinate\n",
      "vectors with respect to one basis into coordinate vectors with respect to\n",
      "a different basis. We will state our main result ﬁrst and then provide an\n",
      "explanation.\n",
      "Theorem 2.20 (Basis Change) .For a linear mapping :V!W, ordered\n",
      "bases\n",
      "B= (b1;:::;bn);~B= (~b1;:::; ~bn) (2.103)\n",
      "ofVand\n",
      "C= (c1;:::;cm);~C= (~c1;:::; ~cm) (2.104)\n",
      "ofW, and a transformation matrix owith respect to BandC, the\n",
      "corresponding transformation matrix ~with respect to the bases ~Band~C\n",
      "is given as\n",
      "~=T\u00001S: (2.105)\n",
      "Here,S2Rn\u0002nis the transformation matrix of idVthat maps coordinates\n",
      "with respect to ~Bonto coordinates with respect to B, andT2Rm\u0002mis the\n",
      "transformation matrix of idWthat maps coordinates with respect to ~Conto\n",
      "coordinates with respect to C.\n",
      "Proof Following Drumm and Weil (2001), we can write the vectors of\n",
      "the new basis ~BofVas a linear combination of the basis vectors of B,\n",
      "such that\n",
      "~bj=s1jb1+\u0001\u0001\u0001+snjbn=nX\n",
      "i=1sijbi; j = 1;:::;n: (2.106)\n",
      "Similarly, we write the new basis vectors ~CofWas a linear combination\n",
      "of the basis vectors of C, which yields\n",
      "~ck=t1kc1+\u0001\u0001\u0001+tmkcm=mX\n",
      "l=1tlkcl; k = 1;:::;m: (2.107)\n",
      "We deﬁneS= ((sij))2Rn\u0002nas the transformation matrix that maps\n",
      "coordinates with respect to ~Bonto coordinates with respect to Band\n",
      "T= ((tlk))2Rm\u0002mas the transformation matrix that maps coordinates\n",
      "with respect to ~Conto coordinates with respect to C. In particular, the jth\n",
      "column ofSis the coordinate representation of ~bjwith respect to Band\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.7 Linear Mappings 55\n",
      "thekth column ofTis the coordinate representation of ~ckwith respect to\n",
      "C. Note that both SandTare regular.\n",
      "We are going to look at(~bj)from two perspectives. First, applying the\n",
      "mapping, we get that for all j= 1;:::;n\n",
      "\b(~bj) =mX\n",
      "k=1~akj~ck|{z}\n",
      "2W(2.107)=mX\n",
      "k=1~akjmX\n",
      "l=1tlkcl=mX\n",
      "l=1 mX\n",
      "k=1tlk~akj!\n",
      "cl;(2.108)\n",
      "where we ﬁrst expressed the new basis vectors ~ck2Was linear com-\n",
      "binations of the basis vectors cl2Wand then swapped the order of\n",
      "summation.\n",
      "Alternatively, when we express the ~bj2Vas linear combinations of\n",
      "bj2V, we arrive at\n",
      "\b(~bj)(2.106)= nX\n",
      "i=1sijbi!\n",
      "=nX\n",
      "i=1si(bi) =nX\n",
      "i=1sijmX\n",
      "l=1alicl(2.109a)\n",
      "=mX\n",
      "l=1 nX\n",
      "i=1alisij!\n",
      "cl; j = 1;:::;n; (2.109b)\n",
      "where we exploited the linearity of. Comparing (2.108) and (2.109b),\n",
      "it follows for all j= 1;:::;n andl= 1;:::;m that\n",
      "mX\n",
      "k=1tlk~akj=nX\n",
      "i=1alisij (2.110)\n",
      "and, therefore,\n",
      "T~=S2Rm\u0002n; (2.111)\n",
      "such that\n",
      "~=T\u00001S; (2.112)\n",
      "which proves Theorem 2.20.\n",
      "Theorem 2.20 tells us that with a basis change in V(Bis replaced with\n",
      "~B) andW(Cis replaced with ~C), the transformation matrix of a\n",
      "linear mapping :V!Wis replaced by an equivalent matrix ~with\n",
      "~=T\u00001S: (2.113)\n",
      "Figure 2.11 illustrates this relation: Consider a homomorphism :V!\n",
      "Wand ordered bases B;~BofVandC;~CofW. The mappingCBis an\n",
      "instantiation ofand maps basis vectors of Bonto linear combinations\n",
      "of basis vectors of C. Assume that we know the transformation matrix \n",
      "oCBwith respect to the ordered bases B;C . When we perform a basis\n",
      "change from Bto~BinVand fromCto~CinW, we can determine the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "56 Linear Algebra\n",
      "Figure 2.11 For a\n",
      "homomorphism\n",
      "\b :V!Wand\n",
      "ordered bases B;~B\n",
      "ofVandC;~CofW\n",
      "(marked in blue),\n",
      "we can express the\n",
      "mapping~C~Bwith\n",
      "respect to the bases\n",
      "~B;~Cequivalently as\n",
      "a composition of the\n",
      "homomorphisms\n",
      "\b~C~B=\n",
      "\u0004~CCCB\u000e\tB~B\n",
      "with respect to the\n",
      "bases in the\n",
      "subscripts. The\n",
      "corresponding\n",
      "transformation\n",
      "matrices are in red.V W\n",
      "B\n",
      "~B ~C\n",
      "\bCB\n",
      "\b~C~B\tB~B \u0004C~C S T\n",
      "~V W\n",
      "B\n",
      "~B ~C\n",
      "\bCB\n",
      "\b~C~B\tB~B \u0004~CC= \u0004\u00001\n",
      "C~CST\u00001\n",
      "~Vector spaces\n",
      "Ordered bases\n",
      "corresponding transformation matrix ~as follows: First, we ﬁnd the ma-\n",
      "trix representation of the linear mapping \tB~B:V!Vthat maps coordi-\n",
      "nates with respect to the new basis ~Bonto the (unique) coordinates with\n",
      "respect to the “old” basis B(inV). Then, we use the transformation ma-\n",
      "trixoCB:V!Wto map these coordinates onto the coordinates\n",
      "with respect to CinW. Finally, we use a linear mapping \u0004~CC:W!W\n",
      "to map the coordinates with respect to Conto coordinates with respect to\n",
      "~C. Therefore, we can express the linear mapping~C~Bas a composition of\n",
      "linear mappings that involve the “old” basis:\n",
      "\b~C~B= \u0004 ~CCCB\u000e\tB~B= \u0004\u00001\n",
      "C~CCB\u000e\tB~B: (2.114)\n",
      "Concretely, we use \tB~B= idVand\u0004C~C= idW, i.e., the identity mappings\n",
      "that map vectors onto themselves, but with respect to a different basis.\n",
      "Deﬁnition 2.21 (Equivalence) .Two matrices A;~A2Rm\u0002nareequivalent equivalent\n",
      "if there exist regular matrices S2Rn\u0002nandT2Rm\u0002m, such that\n",
      "~A=T\u00001AS.\n",
      "Deﬁnition 2.22 (Similarity) .Two matrices A;~A2Rn\u0002naresimilar if similar\n",
      "there exists a regular matrix S2Rn\u0002nwith ~A=S\u00001AS\n",
      "Remark. Similar matrices are always equivalent. However, equivalent ma-\n",
      "trices are not necessarily similar. }\n",
      "Remark. Consider vector spaces V;W;X . From the remark that follows\n",
      "Theorem 2.17, we already know that for linear mappings :V!W\n",
      "and\t :W!Xthe mapping \t :V!Xis also linear. With\n",
      "transformation matrices andA\tof the corresponding mappings, the\n",
      "overall transformation matrix is A\t=A\t. }\n",
      "In light of this remark, we can look at basis changes from the perspec-\n",
      "tive of composing linear mappings:\n",
      "is the transformation matrix of a linear mappingCB:V!W\n",
      "with respect to the bases B;C .\n",
      "~is the transformation matrix of the linear mapping~C~B:V!W\n",
      "with respect to the bases ~B;~C.\n",
      "Sis the transformation matrix of a linear mapping \tB~B:V!V\n",
      "(automorphism) that represents ~Bin terms ofB. Normally, \t = idVis\n",
      "the identity mapping in V.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.7 Linear Mappings 57\n",
      "Tis the transformation matrix of a linear mapping \u0004C~C:W!W\n",
      "(automorphism) that represents ~Cin terms ofC. Normally, \u0004 = idWis\n",
      "the identity mapping in W.\n",
      "If we (informally) write down the transformations just in terms of bases,\n",
      "then:B!C,~:~B!~C,S:~B!B,T:~C!Cand\n",
      "T\u00001:C!~C, and\n",
      "~B!~C=~B!B!C!~C (2.115)\n",
      "~=T\u00001S: (2.116)\n",
      "Note that the execution order in (2.116) is from right to left because vec-\n",
      "tors are multiplied at the right-hand side so that x7!Sx7!(Sx)7!\n",
      "T\u00001\u0000(Sx)\u0001=~x.\n",
      "Example 2.24 (Basis Change)\n",
      "Consider a linear mapping :R3!R4whose transformation matrix is\n",
      "=2\n",
      "6641 2 0\n",
      "\u00001 1 3\n",
      "3 7 1\n",
      "\u00001 2 43\n",
      "775(2.117)\n",
      "with respect to the standard bases\n",
      "B= (2\n",
      "41\n",
      "0\n",
      "03\n",
      "5;2\n",
      "40\n",
      "1\n",
      "03\n",
      "5;2\n",
      "40\n",
      "0\n",
      "13\n",
      "5); C = (2\n",
      "6641\n",
      "0\n",
      "0\n",
      "03\n",
      "775;2\n",
      "6640\n",
      "1\n",
      "0\n",
      "03\n",
      "775;2\n",
      "6640\n",
      "0\n",
      "1\n",
      "03\n",
      "775;2\n",
      "6640\n",
      "0\n",
      "0\n",
      "13\n",
      "775): (2.118)\n",
      "We seek the transformation matrix ~owith respect to the new bases\n",
      "~B= (2\n",
      "41\n",
      "1\n",
      "03\n",
      "5;2\n",
      "40\n",
      "1\n",
      "13\n",
      "5;2\n",
      "41\n",
      "0\n",
      "13\n",
      "5)2R3;~C= (2\n",
      "6641\n",
      "1\n",
      "0\n",
      "03\n",
      "775;2\n",
      "6641\n",
      "0\n",
      "1\n",
      "03\n",
      "775;2\n",
      "6640\n",
      "1\n",
      "1\n",
      "03\n",
      "775;2\n",
      "6641\n",
      "0\n",
      "0\n",
      "13\n",
      "775):(2.119)\n",
      "Then,\n",
      "S=2\n",
      "41 0 1\n",
      "1 1 0\n",
      "0 1 13\n",
      "5;T=2\n",
      "6641 1 0 1\n",
      "1 0 1 0\n",
      "0 1 1 0\n",
      "0 0 0 13\n",
      "775; (2.120)\n",
      "where the ith column of Sis the coordinate representation of ~biin\n",
      "terms of the basis vectors of B. SinceBis the standard basis, the co-\n",
      "ordinate representation is straightforward to ﬁnd. For a general basis B,\n",
      "we would need to solve a linear equation system to ﬁnd the \u0015isuch that\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "58 Linear Algebra\n",
      "P3\n",
      "i=1\u0015ibi=~bj,j= 1;:::; 3. Similarly, the jth column ofTis the coordi-\n",
      "nate representation of ~cjin terms of the basis vectors of C.\n",
      "Therefore, we obtain\n",
      "~=T\u00001S=1\n",
      "22\n",
      "6641 1\u00001\u00001\n",
      "1\u00001 1\u00001\n",
      "\u00001 1 1 1\n",
      "0 0 0 23\n",
      "7752\n",
      "6643 2 1\n",
      "0 4 2\n",
      "10 8 4\n",
      "1 6 33\n",
      "775(2.121a)\n",
      "=2\n",
      "664\u00004\u00004\u00002\n",
      "6 0 0\n",
      "4 8 4\n",
      "1 6 33\n",
      "775: (2.121b)\n",
      "In Chapter 4, we will be able to exploit the concept of a basis change\n",
      "to ﬁnd a basis with respect to which the transformation matrix of an en-\n",
      "domorphism has a particularly simple (diagonal) form. In Chapter 10, we\n",
      "will look at a data compression problem and ﬁnd a convenient basis onto\n",
      "which we can project the data while minimizing the compression loss.\n",
      "2.7.3 Image and Kernel\n",
      "The image and kernel of a linear mapping are vector subspaces with cer-\n",
      "tain important properties. In the following, we will characterize them\n",
      "more carefully.\n",
      "Deﬁnition 2.23 (Image and Kernel) .\n",
      "Fo :V!W, we deﬁne the kernel /null space kernel\n",
      "null space\n",
      "ker) :=\u00001(0W) =fv2V:(v) =0Wg (2.122)\n",
      "and the image /range image\n",
      "range\n",
      "Im) :=( V) =fw2Wj9v2V:(v) =wg: (2.123)\n",
      "We also call VandWthedomain andcodomain o, respectively. domain\n",
      "codomain\n",
      "Intuitively, the kernel is the set of vectors v2Vthamaps onto the\n",
      "neutral element 0W2W. The image is the set of vectors w2Wthat\n",
      "can be “reached” byfrom any vector in V. An illustration is given in\n",
      "Figure 2.12.\n",
      "Remark. Consider a linear mapping :V!W, whereV;W are vector\n",
      "spaces.\n",
      "It always holds that(0V) =0Wand, therefore, 0V2ker) . In\n",
      "particular, the null space is never empty.\n",
      "Im)\u0012Wis a subspace of W, and ker)\u0012Vis a subspace of V.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.7 Linear Mappings 59\n",
      "Figure 2.12 Kernel\n",
      "and image of a\n",
      "linear mapping\n",
      "\b :V!W.\n",
      "Im)\n",
      "0Wker)\n",
      "0 :V!WV W\n",
      "\bis injective (one-to-one) if and only if ker) =f0g.\n",
      "}\n",
      "Remark (Null Space and Column Space) .Let us consider A2Rm\u0002nand\n",
      "a linear mapping :Rn!Rm;x7!Ax.\n",
      "ForA= [a1;:::;an], whereaiare the columns of A, we obtain\n",
      "Im) =fAx:x2Rng=(nX\n",
      "i=1xiai:x1;:::;xn2R)\n",
      "(2.124a)\n",
      "= span[a1;:::;an]\u0012Rm; (2.124b)\n",
      "i.e., the image is the span of the columns of A, also called the column column space\n",
      "space . Therefore, the column space (image) is a subspace of Rm, where\n",
      "mis the “height” of the matrix.\n",
      "rk(A) = dim(Im)) .\n",
      "The kernel/null space ker) is the general solution to the homoge-\n",
      "neous system of linear equations Ax=0and captures all possible\n",
      "linear combinations of the elements in Rnthat produce 02Rm.\n",
      "The kernel is a subspace of Rn, wherenis the “width” of the matrix.\n",
      "The kernel focuses on the relationship among the columns, and we can\n",
      "use it to determine whether/how we can express a column as a linear\n",
      "combination of other columns.\n",
      "}\n",
      "Example 2.25 (Image and Kernel of a Linear Mapping)\n",
      "The mapping\n",
      "\b :R4!R2;2\n",
      "664x1\n",
      "x2\n",
      "x3\n",
      "x43\n",
      "7757!\u00141 2\u00001 0\n",
      "1 0 0 1\u00152\n",
      "664x1\n",
      "x2\n",
      "x3\n",
      "x43\n",
      "775=\u0014x1+ 2x2\u0000x3\n",
      "x1+x4\u0015\n",
      "(2.125a)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "60 Linear Algebra\n",
      "=x1\u00141\n",
      "1\u0015\n",
      "+x2\u00142\n",
      "0\u0015\n",
      "+x3\u0014\u00001\n",
      "0\u0015\n",
      "+x4\u00140\n",
      "1\u0015\n",
      "(2.125b)\n",
      "is linear. To determine Im) , we can take the span of the columns of the\n",
      "transformation matrix and obtain\n",
      "Im) = span[\u00141\n",
      "1\u0015\n",
      ";\u00142\n",
      "0\u0015\n",
      ";\u0014\u00001\n",
      "0\u0015\n",
      ";\u00140\n",
      "1\u0015\n",
      "]: (2.126)\n",
      "To compute the kernel (null space) of, we need to solve Ax=0, i.e.,\n",
      "we need to solve a homogeneous equation system. To do this, we use\n",
      "Gaussian elimination to transform Ainto reduced row-echelon form:\n",
      "\u00141 2\u00001 0\n",
      "1 0 0 1\u0015\n",
      " \u0001\u0001\u0001 \u00141 0 0 1\n",
      "0 1\u00001\n",
      "2\u00001\n",
      "2\u0015\n",
      ": (2.127)\n",
      "This matrix is in reduced row-echelon form, and we can use the Minus-\n",
      "1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively,\n",
      "we can express the non-pivot columns (columns 3and4) as linear com-\n",
      "binations of the pivot columns (columns 1and2). The third column a3is\n",
      "equivalent to\u00001\n",
      "2times the second column a2. Therefore, 0=a3+1\n",
      "2a2. In\n",
      "the same way, we see that a4=a1\u00001\n",
      "2a2and, therefore, 0=a1\u00001\n",
      "2a2\u0000a4.\n",
      "Overall, this gives us the kernel (null space) as\n",
      "ker) = span[2\n",
      "6640\n",
      "1\n",
      "2\n",
      "1\n",
      "03\n",
      "775;2\n",
      "664\u00001\n",
      "1\n",
      "2\n",
      "0\n",
      "13\n",
      "775]: (2.128)\n",
      "rank-nullity\n",
      "theorem Theorem 2.24 (Rank-Nullity Theorem) .For vector spaces V;W and a lin-\n",
      "ear mapping :V!Wit holds that\n",
      "dim(ker)) + dim(Im)) = dim( V): (2.129)\n",
      "The rank-nullity theorem is also referred to as the fundamental theorem fundamental\n",
      "theorem of linear\n",
      "mappingsof linear mappings (Axler, 2015, theorem 3.22). The following are direct\n",
      "consequences of Theorem 2.24:\n",
      "Ifdim(Im)) <dim(V), then ker) is non-trivial, i.e., the kernel\n",
      "contains more than 0Vanddim(ker))>1.\n",
      "Ifis the transformation matrix ofwith respect to an ordered basis\n",
      "anddim(Im)) <dim(V), then the system of linear equations x=\n",
      "0has inﬁnitely many solutions.\n",
      "Ifdim(V) = dim(W), then the following three-way equivalence holds:\n",
      "is injective\n",
      "is surjective\n",
      "is bijective\n",
      "since Im)\u0012W.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.8 Afﬁne Spaces 61\n",
      "2.8 Afﬁne Spaces\n",
      "In the following, we will have a closer look at spaces that are offset from\n",
      "the origin, i.e., spaces that are no longer vector subspaces. Moreover, we\n",
      "will brieﬂy discuss properties of mappings between these afﬁne spaces,\n",
      "which resemble linear mappings.\n",
      "Remark. In the machine learning literature, the distinction between linear\n",
      "and afﬁne is sometimes not clear so that we can ﬁnd references to afﬁne\n",
      "spaces/mappings as linear spaces/mappings. }\n",
      "2.8.1 Afﬁne Subspaces\n",
      "Deﬁnition 2.25 (Afﬁne Subspace) .LetVbe a vector space, x02Vand\n",
      "U\u0012Va subspace. Then the subset\n",
      "L=x0+U:=fx0+u:u2Ug (2.130a)\n",
      "=fv2Vj9u2U:v=x0+ug\u0012V (2.130b)\n",
      "is called afﬁne subspace orlinear manifold ofV.Uis called direction or afﬁne subspace\n",
      "linear manifold\n",
      "directiondirection space , andx0is called support point . In Chapter 12, we refer to\n",
      "direction space\n",
      "support pointsuch a subspace as a hyperplane .\n",
      "hyperplaneNote that the deﬁnition of an afﬁne subspace excludes 0ifx0=2U.\n",
      "Therefore, an afﬁne subspace is not a (linear) subspace (vector subspace)\n",
      "ofVforx0=2U.\n",
      "Examples of afﬁne subspaces are points, lines, and planes in R3, which\n",
      "do not (necessarily) go through the origin.\n",
      "Remark. Consider two afﬁne subspaces L=x0+Uand~L=~x0+~Uof a\n",
      "vector space V. Then,L\u0012~Lif and only if U\u0012~Uandx0\u0000~x02~U.\n",
      "Afﬁne subspaces are often described by parameters : Consider a k-dimen-\n",
      "sional afﬁne space L=x0+UofV. If(b1;:::;bk)is an ordered basis of\n",
      "U, then every element x2Lcan be uniquely described as\n",
      "x=x0+\u00151b1+:::+\u0015kbk; (2.131)\n",
      "where\u00151;:::;\u0015k2R. This representation is called parametric equation parametric equation\n",
      "ofLwith directional vectors b1;:::;bkandparameters\u00151;:::;\u0015k.} parameters\n",
      "Example 2.26 (Afﬁne Subspaces)\n",
      "One-dimensional afﬁne subspaces are called lines and can be written line\n",
      "asy=x0+\u0015b1, where\u00152RandU= span[b1]\u0012Rnis a one-\n",
      "dimensional subspace of Rn. This means that a line is deﬁned by a sup-\n",
      "port pointx0and a vectorb1that deﬁnes the direction. See Figure 2.13\n",
      "for an illustration.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "62 Linear Algebra\n",
      "Two-dimensional afﬁne subspaces of Rnare called planes . The para- plane\n",
      "metric equation for planes is y=x0+\u00151b1+\u00152b2, where\u00151;\u001522R\n",
      "andU= span[b1;b2]\u0012Rn. This means that a plane is deﬁned by a\n",
      "support point x0and two linearly independent vectors b1;b2that span\n",
      "the direction space.\n",
      "InRn, the (n\u00001)-dimensional afﬁne subspaces are called hyperplanes , hyperplane\n",
      "and the corresponding parametric equation is y=x0+Pn\u00001\n",
      "i=1\u0015ibi,\n",
      "whereb1;:::;bn\u00001form a basis of an (n\u00001)-dimensional subspace\n",
      "UofRn. This means that a hyperplane is deﬁned by a support point\n",
      "x0and(n\u00001)linearly independent vectors b1;:::;bn\u00001that span the\n",
      "direction space. In R2, a line is also a hyperplane. In R3, a plane is also\n",
      "a hyperplane.\n",
      "Figure 2.13 Lines\n",
      "are afﬁne subspaces.\n",
      "Vectorsyon a line\n",
      "x0+\u0015b1lie in an\n",
      "afﬁne subspace L\n",
      "with support point\n",
      "x0and direction b1.\n",
      "0x0\n",
      "b1yL=x0+\u0015b1\n",
      "Remark (Inhomogeneous systems of linear equations and afﬁne subspaces) .\n",
      "ForA2Rm\u0002nandx2Rm, the solution of the system of linear equa-\n",
      "tionsA\u0015 =xis either the empty set or an afﬁne subspace of Rnof\n",
      "dimensionn\u0000rk(A). In particular, the solution of the linear equation\n",
      "\u00151b1+:::+\u0015nbn=x, where (\u00151;:::;\u0015n)6= (0;:::; 0), is a hyperplane\n",
      "inRn.\n",
      "InRn, everyk-dimensional afﬁne subspace is the solution of an inho-\n",
      "mogeneous system of linear equations Ax=b, whereA2Rm\u0002n;b2\n",
      "Rmandrk(A) =n\u0000k. Recall that for homogeneous equation systems\n",
      "Ax=0the solution was a vector subspace, which we can also think of\n",
      "as a special afﬁne space with support point x0=0. }\n",
      "2.8.2 Afﬁne Mappings\n",
      "Similar to linear mappings between vector spaces, which we discussed\n",
      "in Section 2.7, we can deﬁne afﬁne mappings between two afﬁne spaces.\n",
      "Linear and afﬁne mappings are closely related. Therefore, many properties\n",
      "that we already know from linear mappings, e.g., that the composition of\n",
      "linear mappings is a linear mapping, also hold for afﬁne mappings.\n",
      "Deﬁnition 2.26 (Afﬁne Mapping) .For two vector spaces V;W , a linear\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "2.9 Further Reading 63\n",
      "mapping :V!W, anda2W, the mapping\n",
      "\u001e:V!W (2.132)\n",
      "x7!a+(x) (2.133)\n",
      "is an afﬁne mapping fromVtoW. The vectorais called the translation afﬁne mapping\n",
      "translation vector vector of\u001e.\n",
      "Every afﬁne mapping \u001e:V!Wis also the composition of a linear\n",
      "mapping :V!Wand a translation \u001c:W!WinW, such that\n",
      "\u001e=\u001c. The mappingsand\u001care uniquely determined.\n",
      "The composition \u001e0\u000e\u001eof afﬁne mappings \u001e:V!W,\u001e0:W!Xis\n",
      "afﬁne.\n",
      "Afﬁne mappings keep the geometric structure invariant. They also pre-\n",
      "serve the dimension and parallelism.\n",
      "2.9 Further Reading\n",
      "There are many resources for learning linear algebra, including the text-\n",
      "books by Strang (2003), Golan (2007), Axler (2015), and Liesen and\n",
      "Mehrmann (2015). There are also several online resources that we men-\n",
      "tioned in the introduction to this chapter. We only covered Gaussian elim-\n",
      "ination here, but there are many other approaches for solving systems of\n",
      "linear equations, and we refer to numerical linear algebra textbooks by\n",
      "Stoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and\n",
      "Johnson (2013) for an in-depth discussion.\n",
      "In this book, we distinguish between the topics of linear algebra (e.g.,\n",
      "vectors, matrices, linear independence, basis) and topics related to the\n",
      "geometry of a vector space. In Chapter 3, we will introduce the inner\n",
      "product, which induces a norm. These concepts allow us to deﬁne angles,\n",
      "lengths and distances, which we will use for orthogonal projections. Pro-\n",
      "jections turn out to be key in many machine learning algorithms, such as\n",
      "linear regression and principal component analysis, both of which we will\n",
      "cover in Chapters 9 and 10, respectively.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "64 Linear Algebra\n",
      "Exercises\n",
      "2.1 We consider (Rnf\u00001g;?), where\n",
      "a?b :=ab+a+b; a;b2Rnf\u00001g (2.134)\n",
      "a. Show that (Rnf\u00001g;?)is an Abelian group.\n",
      "b. Solve\n",
      "3?x?x = 15\n",
      "in the Abelian group (Rnf\u00001g;?), where?is deﬁned in (2.134).\n",
      "2.2 Letnbe inNnf0g. Letk;xbe inZ. We deﬁne the congruence class \u0016kof the\n",
      "integerkas the set\n",
      "k=fx2Zjx\u0000k= 0 (modn)g\n",
      "=fx2Zj9a2Z: (x\u0000k=n\u0001a)g:\n",
      "We now deﬁne Z=nZ(sometimes written Zn) as the set of all congruence\n",
      "classes modulo n. Euclidean division implies that this set is a ﬁnite set con-\n",
      "tainingnelements:\n",
      "Zn=f0;1;:::;n\u00001g\n",
      "For alla;b2Zn, we deﬁne\n",
      "b:=a+b\n",
      "a. Show that (Zn)is a group. Is it Abelian?\n",
      "b. We now deﬁne another operation \n",
      "for allaandbinZnas\n",
      "a\n",
      "b=a\u0002b; (2.135)\n",
      "wherea\u0002brepresents the usual multiplication in Z.\n",
      "Letn= 5. Draw the times table of the elements of Z5nf0gunder\n",
      ", i.e.,\n",
      "calculate the products a\n",
      "bfor allaandbinZ5nf0g.\n",
      "Hence, show that Z5nf0gis closed under\n",
      "and possesses a neutral\n",
      "element for\n",
      ". Display the inverse of all elements in Z5nf0gunder\n",
      ".\n",
      "Conclude that (Z5nf0g;\n",
      ")is an Abelian group.\n",
      "c. Show that (Z8nf0g;\n",
      ")is not a group.\n",
      "d. We recall that the B ´ezout theorem states that two integers aandbare\n",
      "relatively prime (i.e., gcd(a;b) = 1 ) if and only if there exist two integers\n",
      "uandvsuch thatau+bv= 1. Show that (Znnf0g;\n",
      ")is a group if and\n",
      "only ifn2Nnf0gis prime.\n",
      "2.3 Consider the set Gof3\u00023matrices deﬁned as follows:\n",
      "G=8\n",
      "<\n",
      ":2\n",
      "41x z\n",
      "0 1y\n",
      "0 0 13\n",
      "52R3\u00023\f\f\f\f\f\fx;y;z2R9\n",
      "=\n",
      ";\n",
      "We deﬁne\u0001as the standard matrix multiplication.\n",
      "Is(G;\u0001)a group? If yes, is it Abelian? Justify your answer.\n",
      "2.4 Compute the following matrix products, if possible:\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 65\n",
      "a.\n",
      "2\n",
      "41 2\n",
      "4 5\n",
      "7 83\n",
      "52\n",
      "41 1 0\n",
      "0 1 1\n",
      "1 0 13\n",
      "5\n",
      "b.\n",
      "2\n",
      "41 2 3\n",
      "4 5 6\n",
      "7 8 93\n",
      "52\n",
      "41 1 0\n",
      "0 1 1\n",
      "1 0 13\n",
      "5\n",
      "c.\n",
      "2\n",
      "41 1 0\n",
      "0 1 1\n",
      "1 0 13\n",
      "52\n",
      "41 2 3\n",
      "4 5 6\n",
      "7 8 93\n",
      "5\n",
      "d.\n",
      "\u0014\n",
      "1 2 1 2\n",
      "4 1\u00001\u00004\u00152\n",
      "6640 3\n",
      "1\u00001\n",
      "2 1\n",
      "5 23\n",
      "775\n",
      "e.\n",
      "2\n",
      "6640 3\n",
      "1\u00001\n",
      "2 1\n",
      "5 23\n",
      "775\u0014\n",
      "1 2 1 2\n",
      "4 1\u00001\u00004\u0015\n",
      "2.5 Find the set Sof all solutions in xof the following inhomogeneous linear\n",
      "systemsAx=b, whereAandbare deﬁned as follows:\n",
      "a.\n",
      "A=2\n",
      "6641 1\u00001\u00001\n",
      "2 5\u00007\u00005\n",
      "2\u00001 1 3\n",
      "5 2\u00004 23\n",
      "775;b=2\n",
      "6641\n",
      "\u00002\n",
      "4\n",
      "63\n",
      "775\n",
      "b.\n",
      "A=2\n",
      "6641\u00001 0 0 1\n",
      "1 1 0\u00003 0\n",
      "2\u00001 0 1\u00001\n",
      "\u00001 2 0\u00002\u000013\n",
      "775;b=2\n",
      "6643\n",
      "6\n",
      "5\n",
      "\u000013\n",
      "775\n",
      "2.6 Using Gaussian elimination, ﬁnd all solutions of the inhomogeneous equa-\n",
      "tion systemAx=bwith\n",
      "A=2\n",
      "40 1 0 0 1 0\n",
      "0 0 0 1 1 0\n",
      "0 1 0 0 0 13\n",
      "5;b=2\n",
      "42\n",
      "\u00001\n",
      "13\n",
      "5:\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "66 Linear Algebra\n",
      "2.7 Find all solutions in x=2\n",
      "4x1\n",
      "x2\n",
      "x33\n",
      "52R3of the equation system Ax= 12x,\n",
      "where\n",
      "A=2\n",
      "46 4 3\n",
      "6 0 9\n",
      "0 8 03\n",
      "5\n",
      "andP3\n",
      "i=1xi= 1.\n",
      "2.8 Determine the inverses of the following matrices if possible:\n",
      "a.\n",
      "A=2\n",
      "42 3 4\n",
      "3 4 5\n",
      "4 5 63\n",
      "5\n",
      "b.\n",
      "A=2\n",
      "6641 0 1 0\n",
      "0 1 1 0\n",
      "1 1 0 1\n",
      "1 1 1 03\n",
      "775\n",
      "2.9 Which of the following sets are subspaces of R3?\n",
      "a.A=f(\u0015;\u0015+\u00163;\u0015\u0000\u00163)j\u0015;\u00162Rg\n",
      "b.B=f(\u00152;\u0000\u00152;0)j\u00152Rg\n",
      "be inR.\n",
      "g=f(\u00181;\u00182;\u00183)2R3j\u00181\u00002\u00182+ 3\u00183=\n",
      "d.D=f(\u00181;\u00182;\u00183)2R3j\u001822Zg\n",
      "2.10 Are the following sets of vectors linearly independent?\n",
      "a.\n",
      "x1=2\n",
      "42\n",
      "\u00001\n",
      "33\n",
      "5;x2=2\n",
      "41\n",
      "1\n",
      "\u000023\n",
      "5;x3=2\n",
      "43\n",
      "\u00003\n",
      "83\n",
      "5\n",
      "b.\n",
      "x1=2\n",
      "666641\n",
      "2\n",
      "1\n",
      "0\n",
      "03\n",
      "77775;x2=2\n",
      "666641\n",
      "1\n",
      "0\n",
      "1\n",
      "13\n",
      "77775;x3=2\n",
      "666641\n",
      "0\n",
      "0\n",
      "1\n",
      "13\n",
      "77775\n",
      "2.11 Write\n",
      "y=2\n",
      "41\n",
      "\u00002\n",
      "53\n",
      "5\n",
      "as linear combination of\n",
      "x1=2\n",
      "41\n",
      "1\n",
      "13\n",
      "5;x2=2\n",
      "41\n",
      "2\n",
      "33\n",
      "5;x3=2\n",
      "42\n",
      "\u00001\n",
      "13\n",
      "5\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 67\n",
      "2.12 Consider two subspaces of R4:\n",
      "U1= span[2\n",
      "6641\n",
      "1\n",
      "\u00003\n",
      "13\n",
      "775;2\n",
      "6642\n",
      "\u00001\n",
      "0\n",
      "\u000013\n",
      "775;2\n",
      "664\u00001\n",
      "1\n",
      "\u00001\n",
      "13\n",
      "775]; U 2= span[2\n",
      "664\u00001\n",
      "\u00002\n",
      "2\n",
      "13\n",
      "775;2\n",
      "6642\n",
      "\u00002\n",
      "0\n",
      "03\n",
      "775;2\n",
      "664\u00003\n",
      "6\n",
      "\u00002\n",
      "\u000013\n",
      "775]:\n",
      "Determine a basis of U1\\U2.\n",
      "2.13 Consider two subspaces U1andU2, whereU1is the solution space of the\n",
      "homogeneous equation system A1x=0andU2is the solution space of the\n",
      "homogeneous equation system A2x=0with\n",
      "A1=2\n",
      "6641 0 1\n",
      "1\u00002\u00001\n",
      "2 1 3\n",
      "1 0 13\n",
      "775;A2=2\n",
      "6643\u00003 0\n",
      "1 2 3\n",
      "7\u00005 2\n",
      "3\u00001 23\n",
      "775:\n",
      "a. Determine the dimension of U1;U2.\n",
      "b. Determine bases of U1andU2.\n",
      "c. Determine a basis of U1\\U2.\n",
      "2.14 Consider two subspaces U1andU2, whereU1is spanned by the columns of\n",
      "A1andU2is spanned by the columns of A2with\n",
      "A1=2\n",
      "6641 0 1\n",
      "1\u00002\u00001\n",
      "2 1 3\n",
      "1 0 13\n",
      "775;A2=2\n",
      "6643\u00003 0\n",
      "1 2 3\n",
      "7\u00005 2\n",
      "3\u00001 23\n",
      "775:\n",
      "a. Determine the dimension of U1;U2\n",
      "b. Determine bases of U1andU2\n",
      "c. Determine a basis of U1\\U2\n",
      "2.15 LetF=f(x;y;z )2R3jx+y\u0000z= 0gandG=f(a\u0000b;a+b;a\u00003b)ja;b2Rg.\n",
      "a. Show that FandGare subspaces of R3.\n",
      "b. Calculate F\\Gwithout resorting to any basis vector.\n",
      "c. Find one basis for Fand one for G, calculateF\\Gusing the basis vectors\n",
      "previously found and check your result with the previous question.\n",
      "2.16 Are the following mappings linear?\n",
      "a. Leta;b2R.\n",
      "\b :L1([a;b])!R\n",
      "f7(f) =Zb\n",
      "af(x)dx;\n",
      "whereL1([a;b])denotes the set of integrable functions on [a;b].\n",
      "b.\n",
      "\b :C1!C0\n",
      "f7(f) =f0;\n",
      "where fork>1,Ckdenotes the set of ktimes continuously differen-\n",
      "tiable functions, and C0denotes the set of continuous functions.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "68 Linear Algebra\n",
      "c.\n",
      "\b :R!R\n",
      "x7(x) = cos(x)\n",
      "d.\n",
      "\b :R3!R2\n",
      "x7!\u0014\n",
      "1 2 3\n",
      "1 4 3\u0015\n",
      "x\n",
      "e. Let\u0012be in [0;2\u0019[and\n",
      "\b :R2!R2\n",
      "x7!\u0014\n",
      "cos(\u0012) sin(\u0012)\n",
      "\u0000sin(\u0012) cos(\u0012)\u0015\n",
      "x\n",
      "2.17 Consider the linear mapping\n",
      "\b :R3!R4\n",
      "\b0\n",
      "@2\n",
      "4x1\n",
      "x2\n",
      "x33\n",
      "51\n",
      "A=2\n",
      "6643x1+ 2x2+x3\n",
      "x1+x2+x3\n",
      "x1\u00003x2\n",
      "2x1+ 3x2+x33\n",
      "775\n",
      "Find the transformation matrix .\n",
      "Determine rk().\n",
      "Compute the kernel and image of. What are dim(ker)) anddim(Im)) ?\n",
      "2.18 LetEbe a vector space. Let fandgbe two automorphisms on Esuch that\n",
      "f\u000eg= idE(i.e.,f\u000egis the identity mapping idE). Show that ker(f) =\n",
      "ker(g\u000ef),Im(g) = Im(g\u000ef)and that ker(f)\\Im(g) =f0Eg.\n",
      "2.19 Consider an endomorphism :R3!R3whose transformation matrix\n",
      "(with respect to the standard basis in R3) is\n",
      "=2\n",
      "41 1 0\n",
      "1\u00001 0\n",
      "1 1 13\n",
      "5:\n",
      "a. Determine ker) andIm) .\n",
      "b. Determine the transformation matrix ~with respect to the basis\n",
      "B= (2\n",
      "41\n",
      "1\n",
      "13\n",
      "5;2\n",
      "41\n",
      "2\n",
      "13\n",
      "5;2\n",
      "41\n",
      "0\n",
      "03\n",
      "5);\n",
      "i.e., perform a basis change toward the new basis B.\n",
      "2.20 Let us consider b1;b2;b0\n",
      "1;b0\n",
      "2,4vectors of R2expressed in the standard basis\n",
      "ofR2as\n",
      "b1=\u0014\n",
      "2\n",
      "1\u0015\n",
      ";b2=\u0014\n",
      "\u00001\n",
      "\u00001\u0015\n",
      ";b0\n",
      "1=\u0014\n",
      "2\n",
      "\u00002\u0015\n",
      ";b0\n",
      "2=\u0014\n",
      "1\n",
      "1\u0015\n",
      "and let us deﬁne two ordered bases B= (b1;b2)andB0= (b0\n",
      "1;b0\n",
      "2)ofR2.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 69\n",
      "a. Show that BandB0are two bases of R2and draw those basis vectors.\n",
      "b. Compute the matrix P1that performs a basis change from B0toB.\n",
      "c. We consider c1;c2;c3, three vectors of R3deﬁned in the standard basis\n",
      "ofR3as\n",
      "c1=2\n",
      "41\n",
      "2\n",
      "\u000013\n",
      "5;c2=2\n",
      "40\n",
      "\u00001\n",
      "23\n",
      "5;c3=2\n",
      "41\n",
      "0\n",
      "\u000013\n",
      "5\n",
      "and we deﬁne C= (c1;c2;c3).\n",
      "(i) Show that Cis a basis of R3, e.g., by using determinants (see\n",
      "Section 4.1).\n",
      "(ii) Let us call C0= (c0\n",
      "1;c0\n",
      "2;c0\n",
      "3)the standard basis of R3. Determine\n",
      "the matrixP2that performs the basis change from CtoC0.\n",
      "d. We consider a homomorphism :R2\u0000!R3, such that\n",
      "\b(b1+b2) =c2+c3\n",
      "\b(b1\u0000b2) = 2c1\u0000c2+ 3c3\n",
      "whereB= (b1;b2)andC= (c1;c2;c3)are ordered bases of R2andR3,\n",
      "respectively.\n",
      "Determine the transformation matrix owith respect to the or-\n",
      "dered bases BandC.\n",
      "e. Determine A0, the transformation matrix ofwith respect to the bases\n",
      "B0andC0.\n",
      "f. Let us consider the vector x2R2whose coordinates in B0are[2;3]>.\n",
      "In other words, x= 2b0\n",
      "1+ 3b0\n",
      "2.\n",
      "(i) Calculate the coordinates of xinB.\n",
      "(ii) Based on that, compute the coordinates of(x)expressed in C.\n",
      "(iii) Then, write(x)in terms ofc0\n",
      "1;c0\n",
      "2;c0\n",
      "3.\n",
      "(iv) Use the representation of xinB0and the matrix A0to ﬁnd this\n",
      "result directly.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "3\n",
      "Analytic Geometry\n",
      "In Chapter 2, we studied vectors, vector spaces, and linear mappings at\n",
      "a general but abstract level. In this chapter, we will add some geomet-\n",
      "ric interpretation and intuition to all of these concepts. In particular, we\n",
      "will look at geometric vectors and compute their lengths and distances\n",
      "or angles between two vectors. To be able to do this, we equip the vec-\n",
      "tor space with an inner product that induces the geometry of the vector\n",
      "space. Inner products and their corresponding norms and metrics capture\n",
      "the intuitive notions of similarity and distances, which we use to develop\n",
      "the support vector machine in Chapter 12. We will then use the concepts\n",
      "of lengths and angles between vectors to discuss orthogonal projections,\n",
      "which will play a central role when we discuss principal component anal-\n",
      "ysis in Chapter 10 and regression via maximum likelihood estimation in\n",
      "Chapter 9. Figure 3.1 gives an overview of how concepts in this chapter\n",
      "are related and how they are connected to other chapters of the book.\n",
      "Figure 3.1 A mind\n",
      "map of the concepts\n",
      "introduced in this\n",
      "chapter, along with\n",
      "when they are used\n",
      "in other parts of the\n",
      "book.Inner product\n",
      "Norm\n",
      "LengthsOrthogonal\n",
      "projectionAngles Rotations\n",
      "Chapter 4\n",
      "Matrix\n",
      "decompositionChapter 10\n",
      "Dimensionality\n",
      "reductionChapter 9\n",
      "RegressionChapter 12\n",
      "Classiﬁcationinduces\n",
      "70\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "3.1 Norms 71\n",
      "Figure 3.3 For\n",
      "different norms, the\n",
      "red lines indicate\n",
      "the set of vectors\n",
      "with norm 1. Left:\n",
      "Manhattan norm;\n",
      "Right: Euclidean\n",
      "distance.\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "kxk1= 1\n",
      "kxk2= 1\n",
      "3.1 Norms\n",
      "When we think of geometric vectors, i.e., directed line segments that start\n",
      "at the origin, then intuitively the length of a vector is the distance of the\n",
      "“end” of this directed line segment from the origin. In the following, we\n",
      "will discuss the notion of the length of vectors using the concept of a norm.\n",
      "Deﬁnition 3.1 (Norm) .Anorm on a vector space Vis a function norm\n",
      "k\u0001k:V!R; (3.1)\n",
      "x7!kxk; (3.2)\n",
      "which assigns each vector xitslengthkxk2R, such that for all \u00152R length\n",
      "andx;y2Vthe following hold:\n",
      "absolutely\n",
      "homogeneous Absolutely homogeneous: k\u0015xk=j\u0015jkxk\n",
      "triangle inequality Triangle inequality: kx+yk6kxk+kyk\n",
      "positive deﬁnite Positive deﬁnite:kxk>0andkxk= 0()x=0\n",
      "Figure 3.2 Triangle\n",
      "inequality.\n",
      "a\n",
      "b\n",
      "c\u0014a+b In geometric terms, the triangle inequality states that for any triangle,\n",
      "the sum of the lengths of any two sides must be greater than or equal\n",
      "to the length of the remaining side; see Figure 3.2 for an illustration.\n",
      "Deﬁnition 3.1 is in terms of a general vector space V(Section 2.4), but\n",
      "in this book we will only consider a ﬁnite-dimensional vector space Rn.\n",
      "Recall that for a vector x2Rnwe denote the elements of the vector using\n",
      "a subscript, that is, xiis theithelement of the vector x.\n",
      "Example 3.1 (Manhattan Norm)\n",
      "The Manhattan norm onRnis deﬁned for x2Rnas Manhattan norm\n",
      "kxk1:=nX\n",
      "i=1jxij; (3.3)\n",
      "wherej\u0001jis the absolute value. The left panel of Figure 3.3 shows all\n",
      "vectorsx2R2withkxk1= 1. The Manhattan norm is also called `1`1norm\n",
      "norm .\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "72 Analytic Geometry\n",
      "Example 3.2 (Euclidean Norm)\n",
      "The Euclidean norm ofx2Rnis deﬁned as Euclidean norm\n",
      "kxk2:=vuutnX\n",
      "i=1x2\n",
      "i=p\n",
      "x>x (3.4)\n",
      "and computes the Euclidean distance ofxfrom the origin. The right panel Euclidean distance\n",
      "of Figure 3.3 shows all vectors x2R2withkxk2= 1. The Euclidean\n",
      "norm is also called `2norm . `2norm\n",
      "Remark. Throughout this book, we will use the Euclidean norm (3.4) by\n",
      "default if not stated otherwise. }\n",
      "3.2 Inner Products\n",
      "Inner products allow for the introduction of intuitive geometrical con-\n",
      "cepts, such as the length of a vector and the angle or distance between\n",
      "two vectors. A major purpose of inner products is to determine whether\n",
      "vectors are orthogonal to each other.\n",
      "3.2.1 Dot Product\n",
      "We may already be familiar with a particular type of inner product, the\n",
      "scalar product /dot product inRn, which is given by scalar product\n",
      "dot product\n",
      "x>y=nX\n",
      "i=1xiyi: (3.5)\n",
      "We will refer to this particular inner product as the dot product in this\n",
      "book. However, inner products are more general concepts with speciﬁc\n",
      "properties, which we will now introduce.\n",
      "3.2.2 General Inner Products\n",
      "Recall the linear mapping from Section 2.7, where we can rearrange the\n",
      "mapping with respect to addition and multiplication with a scalar. A bi- bilinear mapping\n",
      "linear mapping \n",
      "is a mapping with two arguments, and it is linear in\n",
      "each argument, i.e., when we look at a vector space Vthen it holds that\n",
      "for allx;y;z2V; \u0015; 2Rthat\n",
      "\n",
      "(\u0015x+ y;z) =\u0015\n",
      "(x;z) + \n",
      "(y;z) (3.6)\n",
      "\n",
      "(x;\u0015y+ z) =\u0015\n",
      "(x;y) + \n",
      "(x;z): (3.7)\n",
      "Here, (3.6) asserts that \n",
      "is linear in the ﬁrst argument, and (3.7) asserts\n",
      "that\n",
      "is linear in the second argument (see also (2.87)).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.2 Inner Products 73\n",
      "Deﬁnition 3.2. LetVbe a vector space and \n",
      " :V\u0002V!Rbe a bilinear\n",
      "mapping that takes two vectors and maps them onto a real number. Then\n",
      "\n",
      "is called symmetric if\n",
      "(x;y) = \n",
      "(y;x)for allx;y2V, i.e., the symmetric\n",
      "order of the arguments does not matter.\n",
      "\n",
      "is called positive deﬁnite if positive deﬁnite\n",
      "8x2Vnf0g: \n",
      "(x;x)>0;\n",
      "(0;0) = 0: (3.8)\n",
      "Deﬁnition 3.3. LetVbe a vector space and \n",
      " :V\u0002V!Rbe a bilinear\n",
      "mapping that takes two vectors and maps them onto a real number. Then\n",
      "A positive deﬁnite, symmetric bilinear mapping \n",
      " :V\u0002V!Ris called\n",
      "aninner product onV. We typically write hx;yiinstead of \n",
      "(x;y). inner product\n",
      "The pair (V;h\u0001;\u0001i)is called an inner product space or (real) vector space inner product space\n",
      "vector space with\n",
      "inner productwith inner product . If we use the dot product deﬁned in (3.5), we call\n",
      "(V;h\u0001;\u0001i)aEuclidean vector space .\n",
      "Euclidean vector\n",
      "space We will refer to these spaces as inner product spaces in this book.\n",
      "Example 3.3 (Inner Product That Is Not the Dot Product)\n",
      "ConsiderV=R2. If we deﬁne\n",
      "hx;yi:=x1y1\u0000(x1y2+x2y1) + 2x2y2 (3.9)\n",
      "thenh\u0001;\u0001iis an inner product but different from the dot product. The proof\n",
      "will be an exercise.\n",
      "3.2.3 Symmetric, Positive Deﬁnite Matrices\n",
      "Symmetric, positive deﬁnite matrices play an important role in machine\n",
      "learning, and they are deﬁned via the inner product. In Section 4.3, we\n",
      "will return to symmetric, positive deﬁnite matrices in the context of matrix\n",
      "decompositions. The idea of symmetric positive semideﬁnite matrices is\n",
      "key in the deﬁnition of kernels (Section 12.4).\n",
      "Consider an n-dimensional vector space Vwith an inner product h\u0001;\u0001i:\n",
      "V\u0002V!R(see Deﬁnition 3.3) and an ordered basis B= (b1;:::;bn)of\n",
      "V. Recall from Section 2.6.1 that any vectors x;y2Vcan be written as\n",
      "linear combinations of the basis vectors so that x=Pn\n",
      "i=1 ibi2Vand\n",
      "y=Pn\n",
      "j=1\u0015jbj2Vfor suitable  i;\u0015j2R. Due to the bilinearity of the\n",
      "inner product, it holds for all x;y2Vthat\n",
      "hx;yi=*nX\n",
      "i=1 ibi;nX\n",
      "j=1\u0015jbj+\n",
      "=nX\n",
      "i=1nX\n",
      "j=1 ihbi;bji\u0015j=^x>A^y;(3.10)\n",
      "whereAij:=hbi;bjiand^x;^yare the coordinates of xandywith respect\n",
      "to the basis B. This implies that the inner product h\u0001;\u0001iis uniquely deter-\n",
      "mined through A. The symmetry of the inner product also means that A\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "74 Analytic Geometry\n",
      "is symmetric. Furthermore, the positive deﬁniteness of the inner product\n",
      "implies that\n",
      "8x2Vnf0g:x>Ax>0: (3.11)\n",
      "Deﬁnition 3.4 (Symmetric, Positive Deﬁnite Matrix) .A symmetric matrix\n",
      "A2Rn\u0002nthat satisﬁes (3.11) is called symmetric, positive deﬁnite , or symmetric, positive\n",
      "deﬁnite justpositive deﬁnite . If only>holds in (3.11), then Ais called symmetric,\n",
      "positive deﬁnite\n",
      "symmetric, positive\n",
      "semideﬁnitepositive semideﬁnite .\n",
      "Example 3.4 (Symmetric, Positive Deﬁnite Matrices)\n",
      "Consider the matrices\n",
      "A1=\u00149 6\n",
      "6 5\u0015\n",
      ";A2=\u00149 6\n",
      "6 3\u0015\n",
      ": (3.12)\n",
      "A1is positive deﬁnite because it is symmetric and\n",
      "x>A1x=\u0002x1x2\u0003\u00149 6\n",
      "6 5\u0015\u0014x1\n",
      "x2\u0015\n",
      "(3.13a)\n",
      "= 9x2\n",
      "1+ 12x1x2+ 5x2\n",
      "2= (3x1+ 2x2)2+x2\n",
      "2>0 (3.13b)\n",
      "for allx2Vnf0g. In contrast,A2is symmetric but not positive deﬁnite\n",
      "becausex>A2x= 9x2\n",
      "1+ 12x1x2+ 3x2\n",
      "2= (3x1+ 2x2)2\u0000x2\n",
      "2can be less\n",
      "than 0, e.g., forx= [2;\u00003]>.\n",
      "IfA2Rn\u0002nis symmetric, positive deﬁnite, then\n",
      "hx;yi=^x>A^y (3.14)\n",
      "deﬁnes an inner product with respect to an ordered basis B, where ^xand\n",
      "^yare the coordinate representations of x;y2Vwith respect to B.\n",
      "Theorem 3.5. For a real-valued, ﬁnite-dimensional vector space Vand an\n",
      "ordered basis BofV, it holds thath\u0001;\u0001i:V\u0002V!Ris an inner product if\n",
      "and only if there exists a symmetric, positive deﬁnite matrix A2Rn\u0002nwith\n",
      "hx;yi=^x>A^y: (3.15)\n",
      "The following properties hold if A2Rn\u0002nis symmetric and positive\n",
      "deﬁnite:\n",
      "The null space (kernel) of Aconsists only of 0becausex>Ax>0for\n",
      "allx6=0. This implies that Ax6=0ifx6=0.\n",
      "The diagonal elements aiiofAare positive because aii=e>\n",
      "iAei>0,\n",
      "whereeiis theith vector of the standard basis in Rn.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.3 Lengths and Distances 75\n",
      "3.3 Lengths and Distances\n",
      "In Section 3.1, we already discussed norms that we can use to compute\n",
      "the length of a vector. Inner products and norms are closely related in the\n",
      "sense that any inner product induces a norm Inner products\n",
      "induce norms.\n",
      "kxk:=q\n",
      "hx;xi (3.16)\n",
      "in a natural way, such that we can compute lengths of vectors using the in-\n",
      "ner product. However, not every norm is induced by an inner product. The\n",
      "Manhattan norm (3.3) is an example of a norm without a corresponding\n",
      "inner product. In the following, we will focus on norms that are induced\n",
      "by inner products and introduce geometric concepts, such as lengths, dis-\n",
      "tances, and angles.\n",
      "Remark (Cauchy-Schwarz Inequality) .For an inner product vector space\n",
      "(V;h\u0001;\u0001i)the induced norm k\u0001ksatisﬁes the Cauchy-Schwarz inequality Cauchy-Schwarz\n",
      "inequality\n",
      "jhx;yij6kxkkyk: (3.17)\n",
      "}\n",
      "Example 3.5 (Lengths of Vectors Using Inner Products)\n",
      "In geometry, we are often interested in lengths of vectors. We can now use\n",
      "an inner product to compute them using (3.16). Let us take x= [1;1]>2\n",
      "R2. If we use the dot product as the inner product, with (3.16) we obtain\n",
      "kxk=p\n",
      "x>x=p\n",
      "12+ 12=p\n",
      "2 (3.18)\n",
      "as the length of x. Let us now choose a different inner product:\n",
      "hx;yi:=x>\u00141\u00001\n",
      "2\n",
      "\u00001\n",
      "21\u0015\n",
      "y=x1y1\u00001\n",
      "2(x1y2+x2y1) +x2y2:(3.19)\n",
      "If we compute the norm of a vector, then this inner product returns smaller\n",
      "values than the dot product if x1andx2have the same sign (and x1x2>\n",
      "0); otherwise, it returns greater values than the dot product. With this\n",
      "inner product, we obtain\n",
      "hx;xi=x2\n",
      "1\u0000x1x2+x2\n",
      "2= 1\u00001 + 1 = 1 =) kxk=p\n",
      "1 = 1;(3.20)\n",
      "such thatxis “shorter” with this inner product than with the dot product.\n",
      "Deﬁnition 3.6 (Distance and Metric) .Consider an inner product space\n",
      "(V;h\u0001;\u0001i). Then\n",
      "d(x;y) :=kx\u0000yk=q\n",
      "hx\u0000y;x\u0000yi (3.21)\n",
      "is called the distance betweenxandyforx;y2V. If we use the dot distance\n",
      "product as the inner product, then the distance is called Euclidean distance .Euclidean distance\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "76 Analytic Geometry\n",
      "The mapping\n",
      "d:V\u0002V!R (3.22)\n",
      "(x;y)7!d(x;y) (3.23)\n",
      "is called a metric . metric\n",
      "Remark. Similar to the length of a vector, the distance between vectors\n",
      "does not require an inner product: a norm is sufﬁcient. If we have a norm\n",
      "induced by an inner product, the distance may vary depending on the\n",
      "choice of the inner product. }\n",
      "A metricdsatisﬁes the following:\n",
      "1.dispositive deﬁnite , i.e.,d(x;y)>0for allx;y2Vandd(x;y) = positive deﬁnite\n",
      "0()x=y.\n",
      "2.dissymmetric , i.e.,d(x;y) =d(y;x)for allx;y2V. symmetric\n",
      "triangle inequality 3.Triangle inequality: d(x;z)6d(x;y) +d(y;z)for allx;y;z2V.\n",
      "Remark. At ﬁrst glance, the lists of properties of inner products and met-\n",
      "rics look very similar. However, by comparing Deﬁnition 3.3 with Deﬁni-\n",
      "tion 3.6 we observe that hx;yiandd(x;y)behave in opposite directions.\n",
      "Very similarxandywill result in a large value for the inner product and\n",
      "a small value for the metric. }\n",
      "3.4 Angles and Orthogonality\n",
      "Figure 3.4 When\n",
      "restricted to [0;\u0019]\n",
      "thenf(!) = cos(!)\n",
      "returns a unique\n",
      "number in the\n",
      "interval [\u00001;1].\n",
      "0π/2π\n",
      "ω−101cos(ω)In addition to enabling the deﬁnition of lengths of vectors, as well as the\n",
      "distance between two vectors, inner products also capture the geometry\n",
      "of a vector space by deﬁning the angle !between two vectors. We use\n",
      "the Cauchy-Schwarz inequality (3.17) to deﬁne angles !in inner prod-\n",
      "uct spaces between two vectors x;y, and this notion coincides with our\n",
      "intuition in R2andR3. Assume that x6=0;y6=0. Then\n",
      "\u000016hx;yi\n",
      "kxkkyk61: (3.24)\n",
      "Therefore, there exists a unique !2[0;\u0019], illustrated in Figure 3.4, with\n",
      "cos!=hx;yi\n",
      "kxkkyk: (3.25)\n",
      "The number !is the angle between the vectors xandy. Intuitively, the angle\n",
      "angle between two vectors tells us how similar their orientations are. For\n",
      "example, using the dot product, the angle between xandy= 4x, i.e.,y\n",
      "is a scaled version of x, is0: Their orientation is the same.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.4 Angles and Orthogonality 77\n",
      "Example 3.6 (Angle between Vectors)\n",
      "Let us compute the angle between x= [1;1]>2R2andy= [1;2]>2R2;Figure 3.5 The\n",
      "angle!between\n",
      "two vectorsx;yis\n",
      "computed using the\n",
      "inner product.\n",
      "y\n",
      "x\n",
      "1 01\n",
      "!see Figure 3.5, where we use the dot product as the inner product. Then\n",
      "we get\n",
      "cos!=hx;yip\n",
      "hx;xihy;yi=x>yp\n",
      "x>xy>y=3p\n",
      "10; (3.26)\n",
      "and the angle between the two vectors is arccos(3p\n",
      "10)\u00190:32 rad , which\n",
      "corresponds to about 18\u000e.\n",
      "A key feature of the inner product is that it also allows us to characterize\n",
      "vectors that are orthogonal.\n",
      "Deﬁnition 3.7 (Orthogonality) .Two vectorsxandyareorthogonal if and orthogonal\n",
      "only ifhx;yi= 0, and we write x?y. If additionallykxk= 1 =kyk,\n",
      "i.e., the vectors are unit vectors, then xandyareorthonormal . orthonormal\n",
      "An implication of this deﬁnition is that the 0-vector is orthogonal to\n",
      "every vector in the vector space.\n",
      "Remark. Orthogonality is the generalization of the concept of perpendic-\n",
      "ularity to bilinear forms that do not have to be the dot product. In our\n",
      "context, geometrically, we can think of orthogonal vectors as having a\n",
      "right angle with respect to a speciﬁc inner product. }\n",
      "Example 3.7 (Orthogonal Vectors)\n",
      "Figure 3.6 The\n",
      "angle!between\n",
      "two vectorsx;ycan\n",
      "change depending\n",
      "on the inner\n",
      "product.y x\n",
      "\u00001 1 01\n",
      "!\n",
      "Consider two vectors x= [1;1]>;y= [\u00001;1]>2R2; see Figure 3.6.\n",
      "We are interested in determining the angle !between them using two\n",
      "different inner products. Using the dot product as the inner product yields\n",
      "an angle!betweenxandyof90\u000e, such thatx?y. However, if we\n",
      "choose the inner product\n",
      "hx;yi=x>\u00142 0\n",
      "0 1\u0015\n",
      "y; (3.27)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "78 Analytic Geometry\n",
      "we get that the angle !betweenxandyis given by\n",
      "cos!=hx;yi\n",
      "kxkkyk=\u00001\n",
      "3=)!\u00191:91 rad\u0019109:5\u000e; (3.28)\n",
      "andxandyare not orthogonal. Therefore, vectors that are orthogonal\n",
      "with respect to one inner product do not have to be orthogonal with re-\n",
      "spect to a different inner product.\n",
      "Deﬁnition 3.8 (Orthogonal Matrix) .A square matrix A2Rn\u0002nis an\n",
      "orthogonal matrix if and only if its columns are orthonormal so that orthogonal matrix\n",
      "AA>=I=A>A; (3.29)\n",
      "which implies that\n",
      "A\u00001=A>; (3.30)\n",
      "i.e., the inverse is obtained by simply transposing the matrix. It is convention to\n",
      "call these matrices\n",
      "“orthogonal” but a\n",
      "more precise\n",
      "description would\n",
      "be “orthonormal”.Transformations by orthogonal matrices are special because the length\n",
      "of a vectorxis not changed when transforming it using an orthogonal\n",
      "matrixA. For the dot product, we obtain\n",
      "Transformations\n",
      "with orthogonal\n",
      "matrices preserve\n",
      "distances and\n",
      "angles.kAxk2= (Ax)>(Ax) =x>A>Ax=x>Ix=x>x=kxk2:(3.31)\n",
      "Moreover, the angle between any two vectors x;y, as measured by their\n",
      "inner product, is also unchanged when transforming both of them using\n",
      "an orthogonal matrix A. Assuming the dot product as the inner product,\n",
      "the angle of the images AxandAyis given as\n",
      "cos!=(Ax)>(Ay)\n",
      "kAxkkAyk=x>A>Ayq\n",
      "x>A>Axy>A>Ay=x>y\n",
      "kxkkyk;(3.32)\n",
      "which gives exactly the angle between xandy. This means that orthog-\n",
      "onal matrices AwithA>=A\u00001preserve both angles and distances. It\n",
      "turns out that orthogonal matrices deﬁne transformations that are rota-\n",
      "tions (with the possibility of ﬂips). In Section 3.9, we will discuss more\n",
      "details about rotations.\n",
      "3.5 Orthonormal Basis\n",
      "In Section 2.6.1, we characterized properties of basis vectors and found\n",
      "that in ann-dimensional vector space, we need nbasis vectors, i.e., n\n",
      "vectors that are linearly independent. In Sections 3.3 and 3.4, we used\n",
      "inner products to compute the length of vectors and the angle between\n",
      "vectors. In the following, we will discuss the special case where the basis\n",
      "vectors are orthogonal to each other and where the length of each basis\n",
      "vector is 1. We will call this basis then an orthonormal basis.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.6 Orthogonal Complement 79\n",
      "Let us introduce this more formally.\n",
      "Deﬁnition 3.9 (Orthonormal Basis) .Consider an n-dimensional vector\n",
      "spaceVand a basisfb1;:::;bngofV. If\n",
      "hbi;bji= 0 fori6=j (3.33)\n",
      "hbi;bii= 1 (3.34)\n",
      "for alli;j= 1;:::;n then the basis is called an orthonormal basis (ONB). orthonormal basis\n",
      "ONB If only (3.33) is satisﬁed, then the basis is called an orthogonal basis . Note\n",
      "orthogonal basisthat (3.34) implies that every basis vector has length/norm 1.\n",
      "Recall from Section 2.6.1 that we can use Gaussian elimination to ﬁnd a\n",
      "basis for a vector space spanned by a set of vectors. Assume we are given\n",
      "a setf~b1;:::; ~bngof non-orthogonal and unnormalized basis vectors. We\n",
      "concatenate them into a matrix ~B= [~b1;:::; ~bn]and apply Gaussian elim-\n",
      "ination to the augmented matrix (Section 2.3.2) [~B~B>j~B]to obtain an\n",
      "orthonormal basis. This constructive way to iteratively build an orthonor-\n",
      "mal basisfb1;:::;bngis called the Gram-Schmidt process (Strang, 2003).\n",
      "Example 3.8 (Orthonormal Basis)\n",
      "The canonical/standard basis for a Euclidean vector space Rnis an or-\n",
      "thonormal basis, where the inner product is the dot product of vectors.\n",
      "InR2, the vectors\n",
      "b1=1p\n",
      "2\u00141\n",
      "1\u0015\n",
      ";b2=1p\n",
      "2\u00141\n",
      "\u00001\u0015\n",
      "(3.35)\n",
      "form an orthonormal basis since b>\n",
      "1b2= 0andkb1k= 1 =kb2k.\n",
      "We will exploit the concept of an orthonormal basis in Chapter 12 and\n",
      "Chapter 10 when we discuss support vector machines and principal com-\n",
      "ponent analysis.\n",
      "3.6 Orthogonal Complement\n",
      "Having deﬁned orthogonality, we will now look at vector spaces that are\n",
      "orthogonal to each other. This will play an important role in Chapter 10,\n",
      "when we discuss linear dimensionality reduction from a geometric per-\n",
      "spective.\n",
      "Consider aD-dimensional vector space Vand anM-dimensional sub-\n",
      "spaceU\u0012V. Then its orthogonal complement U?is a(D\u0000M)-dimensional orthogonal\n",
      "complement subspace of Vand contains all vectors in Vthat are orthogonal to every\n",
      "vector inU. Furthermore, U\\U?=f0gso that any vector x2Vcan be\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "80 Analytic Geometry\n",
      "Figure 3.7 A plane\n",
      "Uin a\n",
      "three-dimensional\n",
      "vector space can be\n",
      "described by its\n",
      "normal vector,\n",
      "which spans its\n",
      "orthogonal\n",
      "complement U?.e3\n",
      "e1e2w\n",
      "U\n",
      "uniquely decomposed into\n",
      "x=MX\n",
      "m=1\u0015mbm+D\u0000MX\n",
      "j=1 jb?\n",
      "j; \u0015m;  j2R; (3.36)\n",
      "where (b1;:::;bM)is a basis of Uand(b?\n",
      "1;:::;b?\n",
      "D\u0000M)is a basis of U?.\n",
      "Therefore, the orthogonal complement can also be used to describe a\n",
      "planeU(two-dimensional subspace) in a three-dimensional vector space.\n",
      "More speciﬁcally, the vector wwithkwk= 1, which is orthogonal to the\n",
      "planeU, is the basis vector of U?. Figure 3.7 illustrates this setting. All\n",
      "vectors that are orthogonal to wmust (by construction) lie in the plane\n",
      "U. The vectorwis called the normal vector ofU. normal vector\n",
      "Generally, orthogonal complements can be used to describe hyperplanes\n",
      "inn-dimensional vector and afﬁne spaces.\n",
      "3.7 Inner Product of Functions\n",
      "Thus far, we looked at properties of inner products to compute lengths,\n",
      "angles and distances. We focused on inner products of ﬁnite-dimensional\n",
      "vectors. In the following, we will look at an example of inner products of\n",
      "a different type of vectors: inner products of functions.\n",
      "The inner products we discussed so far were deﬁned for vectors with a\n",
      "ﬁnite number of entries. We can think of a vector x2Rnas a function\n",
      "withnfunction values. The concept of an inner product can be generalized\n",
      "to vectors with an inﬁnite number of entries (countably inﬁnite) and also\n",
      "continuous-valued functions (uncountably inﬁnite). Then the sum over\n",
      "individual components of vectors (see Equation (3.5) for example) turns\n",
      "into an integral.\n",
      "An inner product of two functions u:R!Randv:R!Rcan be\n",
      "deﬁned as the deﬁnite integral\n",
      "hu;vi:=Zb\n",
      "au(x)v(x)dx (3.37)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.8 Orthogonal Projections 81\n",
      "for lower and upper limits a;b<1, respectively. As with our usual inner\n",
      "product, we can deﬁne norms and orthogonality by looking at the inner\n",
      "product. If (3.37) evaluates to 0, the functions uandvare orthogonal. To\n",
      "make the preceding inner product mathematically precise, we need to take\n",
      "care of measures and the deﬁnition of integrals, leading to the deﬁnition of\n",
      "a Hilbert space. Furthermore, unlike inner products on ﬁnite-dimensional\n",
      "vectors, inner products on functions may diverge (have inﬁnite value). All\n",
      "this requires diving into some more intricate details of real and functional\n",
      "analysis, which we do not cover in this book.\n",
      "Example 3.9 (Inner Product of Functions)\n",
      "If we choose u= sin(x)andv= cos(x), the integrand f(x) =u(x)v(x)Figure 3.8f(x) =\n",
      "sin(x) cos(x).\n",
      "−2.5 0.0 2.5\n",
      "x−0.50.00.5sin(x) cos(x) of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e.,\n",
      "f(\u0000x) =\u0000f(x). Therefore, the integral with limits a=\u0000\u0019;b=\u0019of this\n",
      "product evaluates to 0. Therefore, sinandcosare orthogonal functions.\n",
      "Remark. It also holds that the collection of functions\n",
      "f1;cos(x);cos(2x);cos(3x);:::g (3.38)\n",
      "is orthogonal if we integrate from \u0000\u0019to\u0019, i.e., any pair of functions are\n",
      "orthogonal to each other. The collection of functions in (3.38) spans a\n",
      "large subspace of the functions that are even and periodic on [\u0000\u0019;\u0019), and\n",
      "projecting functions onto this subspace is the fundamental idea behind\n",
      "Fourier series. }\n",
      "In Section 6.4.6, we will have a look at a second type of unconventional\n",
      "inner products: the inner product of random variables.\n",
      "3.8 Orthogonal Projections\n",
      "Projections are an important class of linear transformations (besides rota-\n",
      "tions and reﬂections) and play an important role in graphics, coding the-\n",
      "ory, statistics and machine learning. In machine learning, we often deal\n",
      "with data that is high-dimensional. High-dimensional data is often hard\n",
      "to analyze or visualize. However, high-dimensional data quite often pos-\n",
      "sesses the property that only a few dimensions contain most information,\n",
      "and most other dimensions are not essential to describe key properties\n",
      "of the data. When we compress or visualize high-dimensional data, we\n",
      "will lose information. To minimize this compression loss, we ideally ﬁnd\n",
      "the most informative dimensions in the data. As discussed in Chapter 1, “Feature” is a\n",
      "common expression\n",
      "for data\n",
      "representation.data can be represented as vectors, and in this chapter, we will discuss\n",
      "some of the fundamental tools for data compression. More speciﬁcally, we\n",
      "can project the original high-dimensional data onto a lower-dimensional\n",
      "feature space and work in this lower-dimensional space to learn more\n",
      "about the dataset and extract relevant patterns. For example, machine\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "82 Analytic Geometry\n",
      "Figure 3.9\n",
      "Orthogonal\n",
      "projection (orange\n",
      "dots) of a\n",
      "two-dimensional\n",
      "dataset (blue dots)\n",
      "onto a\n",
      "one-dimensional\n",
      "subspace (straight\n",
      "line).\n",
      "−4−2 0 2 4\n",
      "x1−2−1012x2\n",
      "learning algorithms, such as principal component analysis (PCA) by Pear-\n",
      "son (1901) and Hotelling (1933) and deep neural networks (e.g., deep\n",
      "auto-encoders (Deng et al., 2010)), heavily exploit the idea of dimension-\n",
      "ality reduction. In the following, we will focus on orthogonal projections,\n",
      "which we will use in Chapter 10 for linear dimensionality reduction and\n",
      "in Chapter 12 for classiﬁcation. Even linear regression, which we discuss\n",
      "in Chapter 9, can be interpreted using orthogonal projections. For a given\n",
      "lower-dimensional subspace, orthogonal projections of high-dimensional\n",
      "data retain as much information as possible and minimize the difference/\n",
      "error between the original data and the corresponding projection. An il-\n",
      "lustration of such an orthogonal projection is given in Figure 3.9. Before\n",
      "we detail how to obtain these projections, let us deﬁne what a projection\n",
      "actually is.\n",
      "Deﬁnition 3.10 (Projection) .LetVbe a vector space and U\u0012Va\n",
      "subspace of V. A linear mapping \u0019:V!Uis called a projection if projection\n",
      "\u00192=\u0019\u000e\u0019=\u0019.\n",
      "Since linear mappings can be expressed by transformation matrices (see\n",
      "Section 2.7), the preceding deﬁnition applies equally to a special kind\n",
      "of transformation matrices, the projection matrices P\u0019, which exhibit the projection matrix\n",
      "property that P2\n",
      "\u0019=P\u0019.\n",
      "In the following, we will derive orthogonal projections of vectors in the\n",
      "inner product space (Rn;h\u0001;\u0001i)onto subspaces. We will start with one-\n",
      "dimensional subspaces, which are also called lines. If not mentioned oth- line\n",
      "erwise, we assume the dot product hx;yi=x>yas the inner product.\n",
      "3.8.1 Projection onto One-Dimensional Subspaces (Lines)\n",
      "Assume we are given a line (one-dimensional subspace) through the ori-\n",
      "gin with basis vector b2Rn. The line is a one-dimensional subspace\n",
      "U\u0012Rnspanned byb. When we project x2RnontoU, we seek the\n",
      "vector\u0019U(x)2Uthat is closest to x. Using geometric arguments, let\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.8 Orthogonal Projections 83\n",
      "Figure 3.10\n",
      "Examples of\n",
      "projections onto\n",
      "one-dimensional\n",
      "subspaces.\n",
      "bx\n",
      "\u0019U(x)\n",
      "!\n",
      "(a) Projection of x2R2onto a subspace U\n",
      "with basis vector b.cos!!sin!\n",
      "bx\n",
      "(b) Projection of a two-dimensional vector\n",
      "xwithkxk= 1 onto a one-dimensional\n",
      "subspace spanned by b.\n",
      "us characterize some properties of the projection \u0019U(x)(Figure 3.10(a)\n",
      "serves as an illustration):\n",
      "The projection \u0019U(x)is closest tox, where “closest” implies that the\n",
      "distancekx\u0000\u0019U(x)kis minimal. It follows that the segment \u0019U(x)\u0000x\n",
      "from\u0019U(x)toxis orthogonal to U, and therefore the basis vector bof\n",
      "U. The orthogonality condition yields h\u0019U(x)\u0000x;bi= 0since angles\n",
      "between vectors are deﬁned via the inner product.\u0015is then the\n",
      "coordinate of \u0019U(x)\n",
      "with respect to b.The projection \u0019U(x)ofxontoUmust be an element of Uand, there-\n",
      "fore, a multiple of the basis vector bthat spansU. Hence,\u0019U(x) =\u0015b,\n",
      "for some\u00152R.\n",
      "In the following three steps, we determine the coordinate \u0015, the projection\n",
      "\u0019U(x)2U, and the projection matrix P\u0019that maps any x2RnontoU:\n",
      "1. Finding the coordinate \u0015. The orthogonality condition yields\n",
      "hx\u0000\u0019U(x);bi= 0\u0019U(x)=\u0015b() hx\u0000\u0015b;bi= 0: (3.39)\n",
      "We can now exploit the bilinearity of the inner product and arrive at With a general inner\n",
      "product, we get\n",
      "\u0015=hx;biif\n",
      "kbk= 1. hx;bi\u0000\u0015hb;bi= 0()\u0015=hx;bi\n",
      "hb;bi=hb;xi\n",
      "kbk2: (3.40)\n",
      "In the last step, we exploited the fact that inner products are symmet-\n",
      "ric. If we chooseh\u0001;\u0001ito be the dot product, we obtain\n",
      "\u0015=b>x\n",
      "b>b=b>x\n",
      "kbk2: (3.41)\n",
      "Ifkbk= 1, then the coordinate \u0015of the projection is given by b>x.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "84 Analytic Geometry\n",
      "2. Finding the projection point \u0019U(x)2U. Since\u0019U(x) =\u0015b, we imme-\n",
      "diately obtain with (3.40) that\n",
      "\u0019U(x) =\u0015b=hx;bi\n",
      "kbk2b=b>x\n",
      "kbk2b; (3.42)\n",
      "where the last equality holds for the dot product only. We can also\n",
      "compute the length of \u0019U(x)by means of Deﬁnition 3.1 as\n",
      "k\u0019U(x)k=k\u0015bk=j\u0015jkbk: (3.43)\n",
      "Hence, our projection is of length j\u0015jtimes the length of b. This also\n",
      "adds the intuition that \u0015is the coordinate of \u0019U(x)with respect to the\n",
      "basis vectorbthat spans our one-dimensional subspace U.\n",
      "If we use the dot product as an inner product, we get\n",
      "k\u0019U(x)k(3.42)=jb>xj\n",
      "kbk2kbk(3.25)=jcos!jkxkkbkkbk\n",
      "kbk2=jcos!jkxk:\n",
      "(3.44)\n",
      "Here,!is the angle between xandb. This equation should be familiar\n",
      "from trigonometry: If kxk= 1, thenxlies on the unit circle. It follows\n",
      "that the projection onto the horizontal axis spanned by bis exactly The horizontal axis\n",
      "is a one-dimensional\n",
      "subspace.cos!, and the length of the corresponding vector \u0019U(x) =jcos!j. An\n",
      "illustration is given in Figure 3.10(b).\n",
      "3. Finding the projection matrix P\u0019. We know that a projection is a lin-\n",
      "ear mapping (see Deﬁnition 3.10). Therefore, there exists a projection\n",
      "matrixP\u0019, such that \u0019U(x) =P\u0019x. With the dot product as inner\n",
      "product and\n",
      "\u0019U(x) =\u0015b=b\u0015=bb>x\n",
      "kbk2=bb>\n",
      "kbk2x; (3.45)\n",
      "we immediately see that\n",
      "P\u0019=bb>\n",
      "kbk2: (3.46)\n",
      "Note thatbb>(and, consequently, P\u0019) is a symmetric matrix (of rank Projection matrices\n",
      "are always\n",
      "symmetric.1), andkbk2=hb;biis a scalar.\n",
      "The projection matrix P\u0019projects any vector x2Rnonto the line through\n",
      "the origin with direction b(equivalently, the subspace Uspanned byb).\n",
      "Remark. The projection \u0019U(x)2Rnis still ann-dimensional vector and\n",
      "not a scalar. However, we no longer require ncoordinates to represent the\n",
      "projection, but only a single one if we want to express it with respect to\n",
      "the basis vector bthat spans the subspace U:\u0015. }\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.8 Orthogonal Projections 85\n",
      "Figure 3.11\n",
      "Projection onto a\n",
      "two-dimensional\n",
      "subspaceUwith\n",
      "basisb1;b2. The\n",
      "projection\u0019U(x)of\n",
      "x2R3ontoUcan\n",
      "be expressed as a\n",
      "linear combination\n",
      "ofb1;b2and the\n",
      "displacement vector\n",
      "x\u0000\u0019U(x)is\n",
      "orthogonal to both\n",
      "b1andb2.\n",
      "0x\n",
      "b1b2U\n",
      "\u0019U(x)x\u0000\u0019U(x)\n",
      "Example 3.10 (Projection onto a Line)\n",
      "Find the projection matrix P\u0019onto the line through the origin spanned\n",
      "byb=\u00021 2 2\u0003>.bis a direction and a basis of the one-dimensional\n",
      "subspace (line through origin).\n",
      "With (3.46), we obtain\n",
      "P\u0019=bb>\n",
      "b>b=1\n",
      "92\n",
      "41\n",
      "2\n",
      "23\n",
      "5\u00021 2 2\u0003=1\n",
      "92\n",
      "41 2 2\n",
      "2 4 4\n",
      "2 4 43\n",
      "5: (3.47)\n",
      "Let us now choose a particular xand see whether it lies in the subspace\n",
      "spanned byb. Forx=\u00021 1 1\u0003>, the projection is\n",
      "\u0019U(x) =P\u0019x=1\n",
      "92\n",
      "41 2 2\n",
      "2 4 4\n",
      "2 4 43\n",
      "52\n",
      "41\n",
      "1\n",
      "13\n",
      "5=1\n",
      "92\n",
      "45\n",
      "10\n",
      "103\n",
      "52span[2\n",
      "41\n",
      "2\n",
      "23\n",
      "5]:(3.48)\n",
      "Note that the application of P\u0019to\u0019U(x)does not change anything, i.e.,\n",
      "P\u0019\u0019U(x) =\u0019U(x). This is expected because according to Deﬁnition 3.10,\n",
      "we know that a projection matrix P\u0019satisﬁesP2\n",
      "\u0019x=P\u0019xfor allx.\n",
      "Remark. With the results from Chapter 4, we can show that \u0019U(x)is an\n",
      "eigenvector of P\u0019, and the corresponding eigenvalue is 1.}\n",
      "3.8.2 Projection onto General Subspaces\n",
      "IfUis given by a set\n",
      "of spanning vectors,\n",
      "which are not a\n",
      "basis, make sure\n",
      "you determine a\n",
      "basisb1;:::;bm\n",
      "before proceeding.In the following, we look at orthogonal projections of vectors x2Rn\n",
      "onto lower-dimensional subspaces U\u0012Rnwith dim(U) =m>1. An\n",
      "illustration is given in Figure 3.11.\n",
      "Assume that (b1;:::;bm)is an ordered basis of U. Any projection \u0019U(x)\n",
      "ontoUis necessarily an element of U. Therefore, they can be represented\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "86 Analytic Geometry\n",
      "as linear combinations of the basis vectors b1;:::;bmofU, such that\n",
      "\u0019U(x) =Pm\n",
      "i=1\u0015ibi. The basis vectors\n",
      "form the columns of\n",
      "B2Rn\u0002m, where\n",
      "B= [b1;:::;bm].As in the 1D case, we follow a three-step procedure to ﬁnd the projec-\n",
      "tion\u0019U(x)and the projection matrix P\u0019:\n",
      "1. Find the coordinates \u00151;:::;\u0015mof the projection (with respect to the\n",
      "basis ofU), such that the linear combination\n",
      "\u0019U(x) =mX\n",
      "i=1\u0015ibi=B\u0015; (3.49)\n",
      "B= [b1;:::;bm]2Rn\u0002m;\u0015= [\u00151;:::;\u0015m]>2Rm; (3.50)\n",
      "is closest tox2Rn. As in the 1D case, “closest” means “minimum\n",
      "distance”, which implies that the vector connecting \u0019U(x)2Uand\n",
      "x2Rnmust be orthogonal to all basis vectors of U. Therefore, we\n",
      "obtainmsimultaneous conditions (assuming the dot product as the\n",
      "inner product)\n",
      "hb1;x\u0000\u0019U(x)i=b>\n",
      "1(x\u0000\u0019U(x)) = 0 (3.51)\n",
      "...\n",
      "hbm;x\u0000\u0019U(x)i=b>\n",
      "m(x\u0000\u0019U(x)) = 0 (3.52)\n",
      "which, with \u0019U(x) =B\u0015, can be written as\n",
      "b>\n",
      "1(x\u0000B\u0015) = 0 (3.53)\n",
      "...\n",
      "b>\n",
      "m(x\u0000B\u0015) = 0 (3.54)\n",
      "such that we obtain a homogeneous linear equation system\n",
      "2\n",
      "64b>\n",
      "1...\n",
      "b>\n",
      "m3\n",
      "752\n",
      "4x\u0000B\u00153\n",
      "5=0()B>(x\u0000B\u0015) =0 (3.55)\n",
      "()B>B\u0015=B>x: (3.56)\n",
      "The last expression is called normal equation . Sinceb1;:::;bmare a normal equation\n",
      "basis ofUand, therefore, linearly independent, B>B2Rm\u0002mis reg-\n",
      "ular and can be inverted. This allows us to solve for the coefﬁcients/\n",
      "coordinates\n",
      "\u0015= (B>B)\u00001B>x: (3.57)\n",
      "The matrix (B>B)\u00001B>is also called the pseudo-inverse ofB, which pseudo-inverse\n",
      "can be computed for non-square matrices B. It only requires that B>B\n",
      "is positive deﬁnite, which is the case if Bis full rank. In practical ap-\n",
      "plications (e.g., linear regression), we often add a “jitter term” \u000fIto\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.8 Orthogonal Projections 87\n",
      "B>Bto guarantee increased numerical stability and positive deﬁnite-\n",
      "ness. This “ridge” can be rigorously derived using Bayesian inference.\n",
      "See Chapter 9 for details.\n",
      "2. Find the projection \u0019U(x)2U. We already established that \u0019U(x) =\n",
      "B\u0015. Therefore, with (3.57)\n",
      "\u0019U(x) =B(B>B)\u00001B>x: (3.58)\n",
      "3. Find the projection matrix P\u0019. From (3.58), we can immediately see\n",
      "that the projection matrix that solves P\u0019x=\u0019U(x)must be\n",
      "P\u0019=B(B>B)\u00001B>: (3.59)\n",
      "Remark. The solution for projecting onto general subspaces includes the\n",
      "1D case as a special case: If dim(U) = 1 , thenB>B2Ris a scalar and\n",
      "we can rewrite the projection matrix in (3.59) P\u0019=B(B>B)\u00001B>as\n",
      "P\u0019=BB>\n",
      "B>B, which is exactly the projection matrix in (3.46). }\n",
      "Example 3.11 (Projection onto a Two-dimensional Subspace)\n",
      "For a subspace U= span[2\n",
      "41\n",
      "1\n",
      "13\n",
      "5;2\n",
      "40\n",
      "1\n",
      "23\n",
      "5]\u0012R3andx=2\n",
      "46\n",
      "0\n",
      "03\n",
      "52R3ﬁnd the\n",
      "coordinates\u0015ofxin terms of the subspace U, the projection point \u0019U(x)\n",
      "and the projection matrix P\u0019.\n",
      "First, we see that the generating set of Uis a basis (linear indepen-\n",
      "dence) and write the basis vectors of Uinto a matrixB=2\n",
      "41 0\n",
      "1 1\n",
      "1 23\n",
      "5.\n",
      "Second, we compute the matrix B>Band the vector B>xas\n",
      "B>B=\u00141 1 1\n",
      "0 1 2\u00152\n",
      "41 0\n",
      "1 1\n",
      "1 23\n",
      "5=\u00143 3\n",
      "3 5\u0015\n",
      ";B>x=\u00141 1 1\n",
      "0 1 2\u00152\n",
      "46\n",
      "0\n",
      "03\n",
      "5=\u00146\n",
      "0\u0015\n",
      ":\n",
      "(3.60)\n",
      "Third, we solve the normal equation B>B\u0015=B>xto ﬁnd\u0015:\n",
      "\u00143 3\n",
      "3 5\u0015\u0014\u00151\n",
      "\u00152\u0015\n",
      "=\u00146\n",
      "0\u0015\n",
      "()\u0015=\u00145\n",
      "\u00003\u0015\n",
      ": (3.61)\n",
      "Fourth, the projection \u0019U(x)ofxontoU, i.e., into the column space of\n",
      "B, can be directly computed via\n",
      "\u0019U(x) =B\u0015=2\n",
      "45\n",
      "2\n",
      "\u000013\n",
      "5: (3.62)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "88 Analytic Geometry\n",
      "The corresponding projection error is the norm of the difference vector projection error\n",
      "between the original vector and its projection onto U, i.e., The projection error\n",
      "is also called the\n",
      "=p\u00002 1\u0003>uction error . kx\u0000\u0019U(x)k=\n",
      "6: (3.63)\n",
      "Fifth, the projection matrix (for any x2R3) is given by\n",
      "P\u0019=B(B>B)\u00001B>=1\n",
      "62\n",
      "45 2\u00001\n",
      "2 2 2\n",
      "\u00001 2 53\n",
      "5: (3.64)\n",
      "To verify the results, we can (a) check whether the displacement vector\n",
      "\u0019U(x)\u0000xis orthogonal to all basis vectors of U, and (b) verify that\n",
      "P\u0019=P2\n",
      "\u0019(see Deﬁnition 3.10).\n",
      "Remark. The projections \u0019U(x)are still vectors in Rnalthough they lie in\n",
      "anm-dimensional subspace U\u0012Rn. However, to represent a projected\n",
      "vector we only need the mcoordinates \u00151;:::;\u0015mwith respect to the\n",
      "basis vectorsb1;:::;bmofU. }\n",
      "Remark. In vector spaces with general inner products, we have to pay\n",
      "attention when computing angles and distances, which are deﬁned by\n",
      "means of the inner product. }We can ﬁnd\n",
      "approximate\n",
      "solutions to\n",
      "unsolvable linear\n",
      "equation systems\n",
      "using projections.Projections allow us to look at situations where we have a linear system\n",
      "Ax=bwithout a solution. Recall that this means that bdoes not lie in\n",
      "the span ofA, i.e., the vector bdoes not lie in the subspace spanned by\n",
      "the columns of A. Given that the linear equation cannot be solved exactly,\n",
      "we can ﬁnd an approximate solution . The idea is to ﬁnd the vector in the\n",
      "subspace spanned by the columns of Athat is closest to b, i.e., we compute\n",
      "the orthogonal projection of bonto the subspace spanned by the columns\n",
      "ofA. This problem arises often in practice, and the solution is called the\n",
      "least-squares solution (assuming the dot product as the inner product) of least-squares\n",
      "solution an overdetermined system. This is discussed further in Section 9.4. Using\n",
      "reconstruction errors (3.63) is one possible approach to derive principal\n",
      "component analysis (Section 10.3).\n",
      "Remark. We just looked at projections of vectors xonto a subspace Uwith\n",
      "basis vectorsfb1;:::;bkg. If this basis is an ONB, i.e., (3.33) and (3.34)\n",
      "are satisﬁed, the projection equation (3.58) simpliﬁes greatly to\n",
      "\u0019U(x) =BB>x (3.65)\n",
      "sinceB>B=Iwith coordinates\n",
      "\u0015=B>x: (3.66)\n",
      "This means that we no longer have to compute the inverse from (3.58),\n",
      "which saves computation time. }\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.8 Orthogonal Projections 89\n",
      "3.8.3 Gram-Schmidt Orthogonalization\n",
      "Projections are at the core of the Gram-Schmidt method that allows us to\n",
      "constructively transform any basis (b1;:::;bn)of ann-dimensional vector\n",
      "spaceVinto an orthogonal/orthonormal basis (u1;:::;un)ofV. This\n",
      "basis always exists (Liesen and Mehrmann, 2015) and span[b1;:::;bn] =\n",
      "span[u1;:::;un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt\n",
      "orthogonalization constructs an orthogonal basis (u1;:::;un)from any basis (b1;:::;bn)of\n",
      "Vas follows:\n",
      "u1:=b1 (3.67)\n",
      "uk:=bk\u0000\u0019span[u1;:::;uk\u00001](bk); k = 2;:::;n: (3.68)\n",
      "In (3.68), the kth basis vector bkis projected onto the subspace spanned\n",
      "by the ﬁrst k\u00001constructed orthogonal vectors u1;:::;uk\u00001; see Sec-\n",
      "tion 3.8.2. This projection is then subtracted from bkand yields a vector\n",
      "ukthat is orthogonal to the (k\u00001)-dimensional subspace spanned by\n",
      "u1;:::;uk\u00001. Repeating this procedure for all nbasis vectors b1;:::;bn\n",
      "yields an orthogonal basis (u1;:::;un)ofV. If we normalize the uk, we\n",
      "obtain an ONB where kukk= 1fork= 1;:::;n .\n",
      "Example 3.12 (Gram-Schmidt Orthogonalization)\n",
      "Figure 3.12\n",
      "Gram-Schmidt\n",
      "orthogonalization.\n",
      "(a) non-orthogonal\n",
      "basis (b1;b2)ofR2;\n",
      "(b) ﬁrst constructed\n",
      "basis vectoru1and\n",
      "orthogonal\n",
      "projection ofb2\n",
      "onto span[u1];\n",
      "(c) orthogonal basis\n",
      "(u1;u2)ofR2.b1b2\n",
      "0\n",
      "(a) Original non-orthogonal\n",
      "basis vectorsb1;b2.u1b2\n",
      "0\u0019span[u1](b2)\n",
      "(b) First new basis vector\n",
      "u1=b1and projection of b2\n",
      "onto the subspace spanned by\n",
      "u1.u1b2\n",
      "0\u0019span[u1](b2)u2\n",
      "(c) Orthogonal basis vectors u1\n",
      "andu2=b2\u0000\u0019span[u1](b2).\n",
      "Consider a basis (b1;b2)ofR2, where\n",
      "b1=\u00142\n",
      "0\u0015\n",
      ";b2=\u00141\n",
      "1\u0015\n",
      "; (3.69)\n",
      "see also Figure 3.12(a). Using the Gram-Schmidt method, we construct an\n",
      "orthogonal basis (u1;u2)ofR2as follows (assuming the dot product as\n",
      "the inner product):\n",
      "u1:=b1=\u00142\n",
      "0\u0015\n",
      "; (3.70)\n",
      "u2:=b2\u0000\u0019span[u1](b2)(3.45)=b2\u0000u1u>\n",
      "1\n",
      "ku1k2b2=\u00141\n",
      "1\u0015\n",
      "\u0000\u00141 0\n",
      "0 0\u0015\u00141\n",
      "1\u0015\n",
      "=\u00140\n",
      "1\u0015\n",
      ":\n",
      "(3.71)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "90 Analytic Geometry\n",
      "Figure 3.13\n",
      "Projection onto an\n",
      "afﬁne space.\n",
      "(a) original setting;\n",
      "(b) setting shifted\n",
      "by\u0000x0so that\n",
      "x\u0000x0can be\n",
      "projected onto the\n",
      "direction space U;\n",
      "(c) projection is\n",
      "translated back to\n",
      "x0+\u0019U(x\u0000x0),\n",
      "which gives the ﬁnal\n",
      "orthogonal\n",
      "projection\u0019L(x).L\n",
      "x0x\n",
      "b2\n",
      "b1 0\n",
      "(a) Setting.b1 0x\u0000x0\n",
      "U=L\u0000x0\n",
      "\u0019U(x\u0000x0)b2\n",
      "(b) Reduce problem to pro-\n",
      "jection\u0019Uonto vector sub-\n",
      "space.L\n",
      "x0x\n",
      "b2\n",
      "b1 0\u0019L(x)\n",
      "(c) Add support point back in\n",
      "to get afﬁne projection \u0019L.\n",
      "These steps are illustrated in Figures 3.12(b) and (c). We immediately see\n",
      "thatu1andu2are orthogonal, i.e., u>\n",
      "1u2= 0.\n",
      "3.8.4 Projection onto Afﬁne Subspaces\n",
      "Thus far, we discussed how to project a vector onto a lower-dimensional\n",
      "subspaceU. In the following, we provide a solution to projecting a vector\n",
      "onto an afﬁne subspace.\n",
      "Consider the setting in Figure 3.13(a). We are given an afﬁne space L=\n",
      "x0+U, whereb1;b2are basis vectors of U. To determine the orthogonal\n",
      "projection\u0019L(x)ofxontoL, we transform the problem into a problem\n",
      "that we know how to solve: the projection onto a vector subspace. In\n",
      "order to get there, we subtract the support point x0fromxand fromL,\n",
      "so thatL\u0000x0=Uis exactly the vector subspace U. We can now use the\n",
      "orthogonal projections onto a subspace we discussed in Section 3.8.2 and\n",
      "obtain the projection \u0019U(x\u0000x0), which is illustrated in Figure 3.13(b).\n",
      "This projection can now be translated back into Lby addingx0, such that\n",
      "we obtain the orthogonal projection onto an afﬁne space Las\n",
      "\u0019L(x) =x0+\u0019U(x\u0000x0); (3.72)\n",
      "where\u0019U(\u0001)is the orthogonal projection onto the subspace U, i.e., the\n",
      "direction space of L; see Figure 3.13(c).\n",
      "From Figure 3.13, it is also evident that the distance of xfrom the afﬁne\n",
      "spaceLis identical to the distance of x\u0000x0fromU, i.e.,\n",
      "d(x;L) =kx\u0000\u0019L(x)k=kx\u0000(x0+\u0019U(x\u0000x0))k (3.73a)\n",
      "=d(x\u0000x0;\u0019U(x\u0000x0)) =d(x\u0000x0;U): (3.73b)\n",
      "We will use projections onto an afﬁne subspace to derive the concept of\n",
      "a separating hyperplane in Section 12.1.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.9 Rotations 91\n",
      "Figure 3.14 A\n",
      "rotation rotates\n",
      "objects in a plane\n",
      "about the origin. If\n",
      "the rotation angle is\n",
      "positive, we rotate\n",
      "counterclockwise.\n",
      "Original\n",
      "Rotated by 112.5◦\n",
      "Figure 3.15 The\n",
      "robotic arm needs to\n",
      "rotate its joints in\n",
      "order to pick up\n",
      "objects or to place\n",
      "them correctly.\n",
      "Figure taken\n",
      "from (Deisenroth\n",
      "et al., 2015).\n",
      "3.9 Rotations\n",
      "Length and angle preservation, as discussed in Section 3.4, are the two\n",
      "characteristics of linear mappings with orthogonal transformation matri-\n",
      "ces. In the following, we will have a closer look at speciﬁc orthogonal\n",
      "transformation matrices, which describe rotations.\n",
      "Arotation is a linear mapping (more speciﬁcally, an automorphism of rotation\n",
      "a Euclidean vector space) that rotates a plane by an angle \u0012about the\n",
      "origin, i.e., the origin is a ﬁxed point. For a positive angle \u0012 >0, by com-\n",
      "mon convention, we rotate in a counterclockwise direction. An example is\n",
      "shown in Figure 3.14, where the transformation matrix is\n",
      "R=\u0014\u00000:38\u00000:92\n",
      "0:92\u00000:38\u0015\n",
      ": (3.74)\n",
      "Important application areas of rotations include computer graphics and\n",
      "robotics. For example, in robotics, it is often important to know how to\n",
      "rotate the joints of a robotic arm in order to pick up or place an object,\n",
      "see Figure 3.15.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "92 Analytic Geometry\n",
      "Figure 3.16\n",
      "Rotation of the\n",
      "standard basis in R2\n",
      "by an angle \u0012.\n",
      "e1e2\n",
      "\u0012(e2) = [\u0000sin\u0012;cos\u0012]>\n",
      "\b(e1) = [cos\u0012;sin\u0012]>\n",
      "cos\u0012sin\u0012\n",
      "\u0000sin\u0012cos\u0012\n",
      "3.9.1 Rotations in R2\n",
      "Consider the standard basis\u001a\n",
      "e1=\u00141\n",
      "0\u0015\n",
      ";e2=\u00140\n",
      "1\u0015\u001b\n",
      "ofR2, which deﬁnes\n",
      "the standard coordinate system in R2. We aim to rotate this coordinate\n",
      "system by an angle \u0012as illustrated in Figure 3.16. Note that the rotated\n",
      "vectors are still linearly independent and, therefore, are a basis of R2. This\n",
      "means that the rotation performs a basis change.\n",
      "Rotationsare linear mappings so that we can express them by a\n",
      "rotation matrix R(\u0012). Trigonometry (see Figure 3.16) allows us to de- rotation matrix\n",
      "termine the coordinates of the rotated axes (the image of) with respect\n",
      "to the standard basis in R2. We obtain\n",
      "\b(e1) =\u0014cos\u0012\n",
      "sin\u0012\u0015\n",
      "(e2) =\u0014\u0000sin\u0012\n",
      "cos\u0012\u0015\n",
      ": (3.75)\n",
      "Therefore, the rotation matrix that performs the basis change into the\n",
      "rotated coordinates R(\u0012)is given as\n",
      "R(\u0012) =(e1)(e2)\u0003=\u0014cos\u0012\u0000sin\u0012\n",
      "sin\u0012cos\u0012\u0015\n",
      ": (3.76)\n",
      "3.9.2 Rotations in R3\n",
      "In contrast to the R2case, in R3we can rotate any two-dimensional plane\n",
      "about a one-dimensional axis. The easiest way to specify the general rota-\n",
      "tion matrix is to specify how the images of the standard basis e1;e2;e3are\n",
      "supposed to be rotated, and making sure these images Re1;Re2;Re3are\n",
      "orthonormal to each other. We can then obtain a general rotation matrix\n",
      "Rby combining the images of the standard basis.\n",
      "To have a meaningful rotation angle, we have to deﬁne what “coun-\n",
      "terclockwise” means when we operate in more than two dimensions. We\n",
      "use the convention that a “counterclockwise” (planar) rotation about an\n",
      "axis refers to a rotation about an axis when we look at the axis “head on,\n",
      "from the end toward the origin”. In R3, there are therefore three (planar)\n",
      "rotations about the three standard basis vectors (see Figure 3.17):\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.9 Rotations 93\n",
      "Figure 3.17\n",
      "Rotation of a vector\n",
      "(gray) in R3by an\n",
      "angle\u0012about the\n",
      "e3-axis. The rotated\n",
      "vector is shown in\n",
      "blue.\n",
      "e1e2e3\n",
      "\u0012\n",
      "Rotation about the e1-axis\n",
      "R1(\u0012) =(e1)(e2)(e3)\u0003=2\n",
      "41 0 0\n",
      "0 cos\u0012\u0000sin\u0012\n",
      "0 sin\u0012cos\u00123\n",
      "5:(3.77)\n",
      "Here, thee1coordinate is ﬁxed, and the counterclockwise rotation is\n",
      "performed in the e2e3plane.\n",
      "Rotation about the e2-axis\n",
      "R2(\u0012) =2\n",
      "4cos\u00120 sin\u0012\n",
      "0 1 0\n",
      "\u0000sin\u00120 cos\u00123\n",
      "5: (3.78)\n",
      "If we rotate the e1e3plane about the e2axis, we need to look at the e2\n",
      "axis from its “tip” toward the origin.\n",
      "Rotation about the e3-axis\n",
      "R3(\u0012) =2\n",
      "4cos\u0012\u0000sin\u00120\n",
      "sin\u0012cos\u00120\n",
      "0 0 13\n",
      "5: (3.79)\n",
      "Figure 3.17 illustrates this.\n",
      "3.9.3 Rotations in nDimensions\n",
      "The generalization of rotations from 2D and 3D to n-dimensional Eu-\n",
      "clidean vector spaces can be intuitively described as ﬁxing n\u00002dimen-\n",
      "sions and restrict the rotation to a two-dimensional plane in the n-dimen-\n",
      "sional space. As in the three-dimensional case, we can rotate any plane\n",
      "(two-dimensional subspace of Rn).\n",
      "Deﬁnition 3.11 (Givens Rotation) .LetVbe ann-dimensional Euclidean\n",
      "vector space and :V!Van automorphism with transformation ma-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "94 Analytic Geometry\n",
      "trix\n",
      "Rij(\u0012) :=2\n",
      "66664Ii\u000010\u0001\u0001\u0001 \u0001\u0001\u0001 0\n",
      "0 cos\u0012 0\u0000sin\u00120\n",
      "0 0Ij\u0000i\u00001 0 0\n",
      "0 sin\u0012 0 cos\u0012 0\n",
      "0\u0001\u0001\u0001 \u0001\u0001\u0001 0In\u0000j3\n",
      "777752Rn\u0002n;(3.80)\n",
      "for16i < j6nand\u00122R. ThenRij(\u0012)is called a Givens rotation . Givens rotation\n",
      "Essentially,Rij(\u0012)is the identity matrix Inwith\n",
      "rii= cos\u0012; rij=\u0000sin\u0012; rji= sin\u0012; rjj= cos\u0012: (3.81)\n",
      "In two dimensions (i.e., n= 2), we obtain (3.76) as a special case.\n",
      "3.9.4 Properties of Rotations\n",
      "Rotations exhibit a number of useful properties, which can be derived by\n",
      "considering them as orthogonal matrices (Deﬁnition 3.8):\n",
      "Rotations preserve distances, i.e., kx\u0000yk=kR\u0012(x)\u0000R\u0012(y)k. In other\n",
      "words, rotations leave the distance between any two points unchanged\n",
      "after the transformation.\n",
      "Rotations preserve angles, i.e., the angle between R\u0012xandR\u0012yequals\n",
      "the angle between xandy.\n",
      "Rotations in three (or more) dimensions are generally not commuta-\n",
      "tive. Therefore, the order in which rotations are applied is important,\n",
      "even if they rotate about the same point. Only in two dimensions vector\n",
      "rotations are commutative, such that R(\u001e)R(\u0012) =R(\u0012)R(\u001e)for all\n",
      "\u001e;\u00122[0;2\u0019). They form an Abelian group (with multiplication) only if\n",
      "they rotate about the same point (e.g., the origin).\n",
      "3.10 Further Reading\n",
      "In this chapter, we gave a brief overview of some of the important concepts\n",
      "of analytic geometry, which we will use in later chapters of the book.\n",
      "For a broader and more in-depth overview of some of the concepts we\n",
      "presented, we refer to the following excellent books: Axler (2015) and\n",
      "Boyd and Vandenberghe (2018).\n",
      "Inner products allow us to determine speciﬁc bases of vector (sub)spaces,\n",
      "where each vector is orthogonal to all others (orthogonal bases) using the\n",
      "Gram-Schmidt method. These bases are important in optimization and\n",
      "numerical algorithms for solving linear equation systems. For instance,\n",
      "Krylov subspace methods, such as conjugate gradients or the generalized\n",
      "minimal residual method (GMRES), minimize residual errors that are or-\n",
      "thogonal to each other (Stoer and Burlirsch, 2002).\n",
      "In machine learning, inner products are important in the context of\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "3.10 Further Reading 95\n",
      "kernel methods (Sch ¨olkopf and Smola, 2002). Kernel methods exploit the\n",
      "fact that many linear algorithms can be expressed purely by inner prod-\n",
      "uct computations. Then, the “kernel trick” allows us to compute these\n",
      "inner products implicitly in a (potentially inﬁnite-dimensional) feature\n",
      "space, without even knowing this feature space explicitly. This allowed the\n",
      "“non-linearization” of many algorithms used in machine learning, such as\n",
      "kernel-PCA (Sch ¨olkopf et al., 1997) for dimensionality reduction. Gaus-\n",
      "sian processes (Rasmussen and Williams, 2006) also fall into the category\n",
      "of kernel methods and are the current state of the art in probabilistic re-\n",
      "gression (ﬁtting curves to data points). The idea of kernels is explored\n",
      "further in Chapter 12.\n",
      "Projections are often used in computer graphics, e.g., to generate shad-\n",
      "ows. In optimization, orthogonal projections are often used to (iteratively)\n",
      "minimize residual errors. This also has applications in machine learning,\n",
      "e.g., in linear regression where we want to ﬁnd a (linear) function that\n",
      "minimizes the residual errors, i.e., the lengths of the orthogonal projec-\n",
      "tions of the data onto the linear function (Bishop, 2006). We will investi-\n",
      "gate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also\n",
      "uses projections to reduce the dimensionality of high-dimensional data.\n",
      "We will discuss this in more detail in Chapter 10.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "96 Analytic Geometry\n",
      "Exercises\n",
      "3.1 Show thath\u0001;\u0001ideﬁned for all x= [x1;x2]>2R2andy= [y1;y2]>2R2by\n",
      "hx;yi:=x1y1\u0000(x1y2+x2y1) + 2(x2y2)\n",
      "is an inner product.\n",
      "3.2 Consider R2withh\u0001;\u0001ideﬁned for all xandyinR2as\n",
      "hx;yi:=x>\u0014\n",
      "2 0\n",
      "1 2\u0015\n",
      "|{z}\n",
      "=:Ay:\n",
      "Ish\u0001;\u0001ian inner product?\n",
      "3.3 Compute the distance between\n",
      "x=2\n",
      "41\n",
      "2\n",
      "33\n",
      "5;y=2\n",
      "4\u00001\n",
      "\u00001\n",
      "03\n",
      "5\n",
      "using\n",
      "a.hx;yi:=x>y\n",
      "b.hx;yi:=x>Ay;A:=2\n",
      "42 1 0\n",
      "1 3\u00001\n",
      "0\u00001 23\n",
      "5\n",
      "3.4 Compute the angle between\n",
      "x=\u0014\n",
      "1\n",
      "2\u0015\n",
      ";y=\u0014\n",
      "\u00001\n",
      "\u00001\u0015\n",
      "using\n",
      "a.hx;yi:=x>y\n",
      "b.hx;yi:=x>By;B:=\u0014\n",
      "2 1\n",
      "1 3\u0015\n",
      "3.5 Consider the Euclidean vector space R5with the dot product. A subspace\n",
      "U\u0012R5andx2R5are given by\n",
      "U= span[2\n",
      "666640\n",
      "\u00001\n",
      "2\n",
      "0\n",
      "23\n",
      "77775;2\n",
      "666641\n",
      "\u00003\n",
      "1\n",
      "\u00001\n",
      "23\n",
      "77775;2\n",
      "66664\u00003\n",
      "4\n",
      "1\n",
      "2\n",
      "13\n",
      "77775;2\n",
      "66664\u00001\n",
      "\u00003\n",
      "5\n",
      "0\n",
      "73\n",
      "77775];x=2\n",
      "66664\u00001\n",
      "\u00009\n",
      "\u00001\n",
      "4\n",
      "13\n",
      "77775:\n",
      "a. Determine the orthogonal projection \u0019U(x)ofxontoU\n",
      "b. Determine the distance d(x;U)\n",
      "3.6 Consider R3with the inner product\n",
      "hx;yi:=x>2\n",
      "42 1 0\n",
      "1 2\u00001\n",
      "0\u00001 23\n",
      "5y:\n",
      "Furthermore, we deﬁne e1;e2;e3as the standard/canonical basis in R3.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 97\n",
      "a. Determine the orthogonal projection \u0019U(e2)ofe2onto\n",
      "U= span[e1;e3]:\n",
      "Hint: Orthogonality is deﬁned through the inner product.\n",
      "b. Compute the distance d(e2;U).\n",
      "c. Draw the scenario: standard basis vectors and \u0019U(e2)\n",
      "3.7 LetVbe a vector space and \u0019an endomorphism of V.\n",
      "a. Prove that \u0019is a projection if and only if idV\u0000\u0019is a projection, where\n",
      "idVis the identity endomorphism on V.\n",
      "b. Assume now that \u0019is a projection. Calculate Im(idV\u0000\u0019)andker(idV\u0000\u0019)\n",
      "as a function of Im(\u0019)andker(\u0019).\n",
      "3.8 Using the Gram-Schmidt method, turn the basis B= (b1;b2)of a two-\n",
      "dimensional subspace U\u0012R3into an ONB C= (c1;c2)ofU, where\n",
      "b1:=2\n",
      "41\n",
      "1\n",
      "13\n",
      "5;b2:=2\n",
      "4\u00001\n",
      "2\n",
      "03\n",
      "5:\n",
      "3.9 Letn2Nand letx1;:::;xn>0benpositive real numbers so that x1+\n",
      ":::+xn= 1. Use the Cauchy-Schwarz inequality and show that\n",
      "a.Pn\n",
      "i=1x2\n",
      "i>1\n",
      "n\n",
      "b.Pn\n",
      "i=11\n",
      "xi>n2\n",
      "Hint: Think about the dot product on Rn. Then, choose speciﬁc vectors\n",
      "x;y2Rnand apply the Cauchy-Schwarz inequality.\n",
      "3.10 Rotate the vectors\n",
      "x1:=\u0014\n",
      "2\n",
      "3\u0015\n",
      ";x2:=\u0014\n",
      "0\n",
      "\u00001\u0015\n",
      "by30\u000e.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "4\n",
      "Matrix Decompositions\n",
      "In Chapters 2 and 3, we studied ways to manipulate and measure vectors,\n",
      "projections of vectors, and linear mappings. Mappings and transforma-\n",
      "tions of vectors can be conveniently described as operations performed by\n",
      "matrices. Moreover, data is often represented in matrix form as well, e.g.,\n",
      "where the rows of the matrix represent different people and the columns\n",
      "describe different features of the people, such as weight, height, and socio-\n",
      "economic status. In this chapter, we present three aspects of matrices: how\n",
      "to summarize matrices, how matrices can be decomposed, and how these\n",
      "decompositions can be used for matrix approximations.\n",
      "We ﬁrst consider methods that allow us to describe matrices with just\n",
      "a few numbers that characterize the overall properties of matrices. We\n",
      "will do this in the sections on determinants (Section 4.1) and eigenval-\n",
      "ues (Section 4.2) for the important special case of square matrices. These\n",
      "characteristic numbers have important mathematical consequences and\n",
      "allow us to quickly grasp what useful properties a matrix has. From here\n",
      "we will proceed to matrix decomposition methods: An analogy for ma-\n",
      "trix decomposition is the factoring of numbers, such as the factoring of\n",
      "21into prime numbers 7\u00013. For this reason matrix decomposition is also\n",
      "often referred to as matrix factorization . Matrix decompositions are used matrix factorization\n",
      "to describe a matrix by means of a different representation using factors\n",
      "of interpretable matrices.\n",
      "We will ﬁrst cover a square-root-like operation for symmetric, positive\n",
      "deﬁnite matrices, the Cholesky decomposition (Section 4.3). From here\n",
      "we will look at two related methods for factorizing matrices into canoni-\n",
      "cal forms. The ﬁrst one is known as matrix diagonalization (Section 4.4),\n",
      "which allows us to represent the linear mapping using a diagonal trans-\n",
      "formation matrix if we choose an appropriate basis. The second method,\n",
      "singular value decomposition (Section 4.5), extends this factorization to\n",
      "non-square matrices, and it is considered one of the fundamental concepts\n",
      "in linear algebra. These decompositions are helpful, as matrices represent-\n",
      "ing numerical data are often very large and hard to analyze. We conclude\n",
      "the chapter with a systematic overview of the types of matrices and the\n",
      "characteristic properties that distinguish them in the form of a matrix tax-\n",
      "onomy (Section 4.7).\n",
      "The methods that we cover in this chapter will become important in\n",
      "98\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "4.1 Determinant and Trace 99\n",
      "Figure 4.1 A mind\n",
      "map of the concepts\n",
      "introduced in this\n",
      "chapter, along with\n",
      "where they are used\n",
      "in other parts of the\n",
      "book.Determinant Invertibility Cholesky\n",
      "Eigenvalues\n",
      "Eigenvectors Orthogonal matrix Diagonalization\n",
      "SVDChapter 6\n",
      "Probability\n",
      "& distributions\n",
      "Chapter 10\n",
      "Dimensionality\n",
      "reductiontests used inused in\n",
      "used in determines\n",
      "used in\n",
      "used in\n",
      "used inconstructs used in\n",
      "used inused in\n",
      "both subsequent mathematical chapters, such as Chapter 6, but also in\n",
      "applied chapters, such as dimensionality reduction in Chapters 10 or den-\n",
      "sity estimation in Chapter 11. This chapter’s overall structure is depicted\n",
      "in the mind map of Figure 4.1.\n",
      "4.1 Determinant and TraceThe determinant\n",
      "notationjAjmust\n",
      "not be confused\n",
      "with the absolute\n",
      "value.Determinants are important concepts in linear algebra. A determinant is\n",
      "a mathematical object in the analysis and solution of systems of linear\n",
      "equations. Determinants are only deﬁned for square matrices A2Rn\u0002n,\n",
      "i.e., matrices with the same number of rows and columns. In this book,\n",
      "we write the determinant as det(A)or sometimes asjAjso that\n",
      "det(A) =\f\f\f\f\f\f\f\f\fa11a12::: a 1n\n",
      "a21a22::: a 2n\n",
      ".........\n",
      "an1an2::: ann\f\f\f\f\f\f\f\f\f: (4.1)\n",
      "Thedeterminant of a square matrix A2Rn\u0002nis a function that maps A determinant\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "100 Matrix Decompositions\n",
      "onto a real number. Before providing a deﬁnition of the determinant for\n",
      "generaln\u0002nmatrices, let us have a look at some motivating examples,\n",
      "and deﬁne determinants for some special matrices.\n",
      "Example 4.1 (Testing for Matrix Invertibility)\n",
      "Let us begin with exploring if a square matrix Ais invertible (see Sec-\n",
      "tion 2.2.2). For the smallest cases, we already know when a matrix\n",
      "is invertible. If Ais a 1\u00021matrix, i.e., it is a scalar number, then\n",
      "A=a=)A\u00001=1\n",
      "a. Thusa1\n",
      "a= 1holds, if and only if a6= 0.\n",
      "For2\u00022matrices, by the deﬁnition of the inverse (Deﬁnition 2.3), we\n",
      "know thatAA\u00001=I. Then, with (2.24), the inverse of Ais\n",
      "A\u00001=1\n",
      "a11a22\u0000a12a21\u0014a22\u0000a12\n",
      "\u0000a21a11\u0015\n",
      ": (4.2)\n",
      "Hence,Ais invertible if and only if\n",
      "a11a22\u0000a12a216= 0: (4.3)\n",
      "This quantity is the determinant of A2R2\u00022, i.e.,\n",
      "det(A) =\f\f\f\f\fa11a12\n",
      "a21a22\f\f\f\f\f=a11a22\u0000a12a21: (4.4)\n",
      "Example 4.1 points already at the relationship between determinants\n",
      "and the existence of inverse matrices. The next theorem states the same\n",
      "result forn\u0002nmatrices.\n",
      "Theorem 4.1. For any square matrix A2Rn\u0002nit holds thatAis invertible\n",
      "if and only if det(A)6= 0.\n",
      "We have explicit (closed-form) expressions for determinants of small\n",
      "matrices in terms of the elements of the matrix. For n= 1,\n",
      "det(A) = det(a11) =a11: (4.5)\n",
      "Forn= 2,\n",
      "det(A) =\f\f\f\fa11a12\n",
      "a21a22\f\f\f\f=a11a22\u0000a12a21; (4.6)\n",
      "which we have observed in the preceding example.\n",
      "Forn= 3(known as Sarrus’ rule),\n",
      "\f\f\f\f\f\fa11a12a13\n",
      "a21a22a23\n",
      "a31a32a33\f\f\f\f\f\f=a11a22a33+a21a32a13+a31a12a23 (4.7)\n",
      "\u0000a31a22a13\u0000a11a32a23\u0000a21a12a33:\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.1 Determinant and Trace 101\n",
      "For a memory aid of the product terms in Sarrus’ rule, try tracing the\n",
      "elements of the triple products in the matrix.\n",
      "We call a square matrix Tanupper-triangular matrix ifTij= 0 for upper-triangular\n",
      "matrix i>j , i.e., the matrix is zero below its diagonal. Analogously, we deﬁne a\n",
      "lower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular\n",
      "matrix angular matrix T2Rn\u0002n, the determinant is the product of the diagonal\n",
      "elements, i.e.,\n",
      "det(T) =nY\n",
      "i=1Tii: (4.8)\n",
      "The determinant is\n",
      "the signed volume\n",
      "of the parallelepiped\n",
      "formed by the\n",
      "columns of the\n",
      "matrix.\n",
      "Figure 4.2 The area\n",
      "of the parallelogram\n",
      "(shaded region)\n",
      "spanned by the\n",
      "vectorsbandgis\n",
      "jdet([b;g])j.\n",
      "b\n",
      "g\n",
      "Figure 4.3 The\n",
      "volume of the\n",
      "parallelepiped\n",
      "(shaded volume)\n",
      "spanned by vectors\n",
      "r;b;gis\n",
      "jdet([r;b;g])j.\n",
      "b\n",
      "grExample 4.2 (Determinants as Measures of Volume)\n",
      "The notion of a determinant is natural when we consider it as a mapping\n",
      "from a set of nvectors spanning an object in Rn. It turns out that the de-\n",
      "terminant det(A)is the signed volume of an n-dimensional parallelepiped\n",
      "formed by columns of the matrix A.\n",
      "Forn= 2, the columns of the matrix form a parallelogram; see Fig-\n",
      "ure 4.2. As the angle between vectors gets smaller, the area of a parallel-\n",
      "ogram shrinks, too. Consider two vectors b;gthat form the columns of a\n",
      "matrixA= [b;g]. Then, the absolute value of the determinant of Ais the\n",
      "area of the parallelogram with vertices 0;b;g;b+g. In particular, if b;g\n",
      "are linearly dependent so that b=\u0015gfor some\u00152R, they no longer\n",
      "form a two-dimensional parallelogram. Therefore, the corresponding area\n",
      "is0. On the contrary, if b;gare linearly independent and are multiples of\n",
      "the canonical basis vectors e1;e2then they can be written as b=\u0014b\n",
      "0\u0015\n",
      "and\n",
      "g=\u00140\n",
      "g\u0015\n",
      ", and the determinant is\f\f\f\fb0\n",
      "0g\f\f\f\f=bg\u00000 =bg.\n",
      "The sign of the determinant indicates the orientation of the spanning\n",
      "vectorsb;gwith respect to the standard basis (e1;e2). In our ﬁgure, ﬂip-\n",
      "ping the order to g;bswaps the columns of Aand reverses the orientation\n",
      "of the shaded area. This becomes the familiar formula: area =height\u0002\n",
      "length. This intuition extends to higher dimensions. In R3, we consider\n",
      "three vectors r;b;g2R3spanning the edges of a parallelepiped, i.e., a\n",
      "solid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the\n",
      "determinant\n",
      "indicates the\n",
      "orientation of the\n",
      "spanning vectors.solute value of the determinant of the 3\u00023matrix [r;b;g]is the volume\n",
      "of the solid. Thus, the determinant acts as a function that measures the\n",
      "signed volume formed by column vectors composed in a matrix.\n",
      "Consider the three linearly independent vectors r;g;b2R3given as\n",
      "r=2\n",
      "42\n",
      "0\n",
      "\u000083\n",
      "5;g=2\n",
      "46\n",
      "1\n",
      "03\n",
      "5;b=2\n",
      "41\n",
      "4\n",
      "\u000013\n",
      "5: (4.9)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "102 Matrix Decompositions\n",
      "Writing these vectors as the columns of a matrix\n",
      "A= [r;g;b] =2\n",
      "42 6 1\n",
      "0 1 4\n",
      "\u00008 0\u000013\n",
      "5 (4.10)\n",
      "allows us to compute the desired volume as\n",
      "V=jdet(A)j= 186: (4.11)\n",
      "Computing the determinant of an n\u0002nmatrix requires a general algo-\n",
      "rithm to solve the cases for n>3, which we are going to explore in the fol-\n",
      "lowing. Theorem 4.2 below reduces the problem of computing the deter-\n",
      "minant of an n\u0002nmatrix to computing the determinant of (n\u00001)\u0002(n\u00001)\n",
      "matrices. By recursively applying the Laplace expansion (Theorem 4.2),\n",
      "we can therefore compute determinants of n\u0002nmatrices by ultimately\n",
      "computing determinants of 2\u00022matrices.\n",
      "Laplace expansion\n",
      "Theorem 4.2 (Laplace Expansion) .Consider a matrix A2Rn\u0002n. Then,\n",
      "for allj= 1;:::;n :\n",
      "1. Expansion along column j det(Ak;j)is called\n",
      "aminor and\n",
      "(\u00001)k+jdet(Ak;j)\n",
      "acofactor .det(A) =nX\n",
      "k=1(\u00001)k+jakjdet(Ak;j): (4.12)\n",
      "2. Expansion along row j\n",
      "det(A) =nX\n",
      "k=1(\u00001)k+jajkdet(Aj;k): (4.13)\n",
      "HereAk;j2R(n\u00001)\u0002(n\u00001)is the submatrix of Athat we obtain when delet-\n",
      "ing rowkand column j.\n",
      "Example 4.3 (Laplace Expansion)\n",
      "Let us compute the determinant of\n",
      "A=2\n",
      "41 2 3\n",
      "3 1 2\n",
      "0 0 13\n",
      "5 (4.14)\n",
      "using the Laplace expansion along the ﬁrst row. Applying (4.13) yields\n",
      "\f\f\f\f\f\f1 2 3\n",
      "3 1 2\n",
      "0 0 1\f\f\f\f\f\f= (\u00001)1+1\u00011\f\f\f\f1 2\n",
      "0 1\f\f\f\f\n",
      "+ (\u00001)1+2\u00012\f\f\f\f3 2\n",
      "0 1\f\f\f\f+ (\u00001)1+3\u00013\f\f\f\f3 1\n",
      "0 0\f\f\f\f:(4.15)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.1 Determinant and Trace 103\n",
      "We use (4.6) to compute the determinants of all 2\u00022matrices and obtain\n",
      "det(A) = 1(1\u00000)\u00002(3\u00000) + 3(0\u00000) =\u00005: (4.16)\n",
      "For completeness we can compare this result to computing the determi-\n",
      "nant using Sarrus’ rule (4.7):\n",
      "det(A) = 1\u00011\u00011+3\u00010\u00013+0\u00012\u00012\u00000\u00011\u00013\u00001\u00010\u00012\u00003\u00012\u00011 = 1\u00006 =\u00005:(4.17)\n",
      "ForA2Rn\u0002nthe determinant exhibits the following properties:\n",
      "The determinant of a matrix product is the product of the corresponding\n",
      "determinants, det(AB) = det(A)det(B).\n",
      "Determinants are invariant to transposition, i.e., det(A) = det(A>).\n",
      "IfAis regular (invertible), then det(A\u00001) =1\n",
      "det(A).\n",
      "Similar matrices (Deﬁnition 2.22) possess the same determinant. There-\n",
      "fore, for a linear mapping :V!Vall transformation matrices \n",
      "ohave the same determinant. Thus, the determinant is invariant to\n",
      "the choice of basis of a linear mapping.\n",
      "Adding a multiple of a column/row to another one does not change\n",
      "det(A).\n",
      "Multiplication of a column/row with \u00152Rscales det(A)by\u0015. In\n",
      "particular, det(\u0015A) =\u0015ndet(A).\n",
      "Swapping two rows/columns changes the sign of det(A).\n",
      "Because of the last three properties, we can use Gaussian elimination (see\n",
      "Section 2.1) to compute det(A)by bringingAinto row-echelon form.\n",
      "We can stop Gaussian elimination when we have Ain a triangular form\n",
      "where the elements below the diagonal are all 0. Recall from (4.8) that the\n",
      "determinant of a triangular matrix is the product of the diagonal elements.\n",
      "Theorem 4.3. A square matrix A2Rn\u0002nhasdet(A)6= 0if and only if\n",
      "rk(A) =n. In other words, Ais invertible if and only if it is full rank.\n",
      "When mathematics was mainly performed by hand, the determinant\n",
      "calculation was considered an essential way to analyze matrix invertibil-\n",
      "ity. However, contemporary approaches in machine learning use direct\n",
      "numerical methods that superseded the explicit calculation of the deter-\n",
      "minant. For example, in Chapter 2, we learned that inverse matrices can\n",
      "be computed by Gaussian elimination. Gaussian elimination can thus be\n",
      "used to compute the determinant of a matrix.\n",
      "Determinants will play an important theoretical role for the following\n",
      "sections, especially when we learn about eigenvalues and eigenvectors\n",
      "(Section 4.2) through the characteristic polynomial.\n",
      "Deﬁnition 4.4. Thetrace of a square matrix A2Rn\u0002nis deﬁned as trace\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "104 Matrix Decompositions\n",
      "tr(A) :=nX\n",
      "i=1aii; (4.18)\n",
      "i.e. , the trace is the sum of the diagonal elements of A.\n",
      "The trace satisﬁes the following properties:\n",
      "tr(A+B) =tr(A) +tr(B)forA;B2Rn\u0002n\n",
      "tr(\u000bA) =\u000btr(A);\u000b2RforA2Rn\u0002n\n",
      "tr(In) =n\n",
      "tr(AB) =tr(BA)forA2Rn\u0002k;B2Rk\u0002n\n",
      "It can be shown that only one function satisﬁes these four properties to-\n",
      "gether – the trace (Gohberg et al., 2012).\n",
      "The properties of the trace of matrix products are more general. Specif-\n",
      "ically, the trace is invariant under cyclic permutations, i.e., The trace is\n",
      "invariant under\n",
      "cyclic permutations. tr(AKL ) =tr(KLA ) (4.19)\n",
      "for matricesA2Ra\u0002k;K2Rk\u0002l;L2Rl\u0002a. This property generalizes to\n",
      "products of an arbitrary number of matrices. As a special case of (4.19), it\n",
      "follows that for two vectors x;y2Rn\n",
      "tr(xy>) =tr(y>x) =y>x2R: (4.20)\n",
      "Given a linear mapping :V!V, whereVis a vector space, we\n",
      "deﬁne the trace of this map by using the trace of matrix representation\n",
      "o. For a given basis of V, we can describeby means of the transfor-\n",
      "mation matrix A. Then the trace ofis the trace of A. For a different\n",
      "basis ofV, it holds that the corresponding transformation matrix Bo\n",
      "can be obtained by a basis change of the form S\u00001ASfor suitableS(see\n",
      "Section 2.7.2). For the corresponding trace of, this means\n",
      "tr(B) =tr(S\u00001AS)(4.19)=tr(ASS\u00001) =tr(A): (4.21)\n",
      "Hence, while matrix representations of linear mappings are basis depen-\n",
      "dent the trace of a linear mappingis independent of the basis.\n",
      "In this section, we covered determinants and traces as functions char-\n",
      "acterizing a square matrix. Taking together our understanding of determi-\n",
      "nants and traces we can now deﬁne an important equation describing a\n",
      "matrixAin terms of a polynomial, which we will use extensively in the\n",
      "following sections.\n",
      "Deﬁnition 4.5 (Characteristic Polynomial) .For\u00152Rand a square ma-\n",
      "trixA2Rn\u0002n\n",
      "pA(\u0015) := det(A\u0000\u0015I) (4.22a)\n",
      "=c0+c1\u0015+c2\u00152+\u0001\u0001\u0001+cn\u00001\u0015n\u00001+ (\u00001)n\u0015n; (4.22b)\n",
      "c0;:::;cn\u000012R, is the characteristic polynomial ofA. In particular, characteristic\n",
      "polynomial\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.2 Eigenvalues and Eigenvectors 105\n",
      "c0= det(A); (4.23)\n",
      "cn\u00001= (\u00001)n\u00001tr(A): (4.24)\n",
      "The characteristic polynomial (4.22a) will allow us to compute eigen-\n",
      "values and eigenvectors, covered in the next section.\n",
      "4.2 Eigenvalues and Eigenvectors\n",
      "We will now get to know a new way to characterize a matrix and its associ-\n",
      "ated linear mapping. Recall from Section 2.7.1 that every linear mapping\n",
      "has a unique transformation matrix given an ordered basis. We can in-\n",
      "terpret linear mappings and their associated transformation matrices by\n",
      "performing an “eigen” analysis. As we will see, the eigenvalues of a lin- Eigen is a German\n",
      "word meaning\n",
      "“characteristic”,\n",
      "“self”, or “own”.ear mapping will tell us how a special set of vectors, the eigenvectors, is\n",
      "transformed by the linear mapping.\n",
      "Deﬁnition 4.6. LetA2Rn\u0002nbe a square matrix. Then \u00152Ris an\n",
      "eigenvalue ofAandx2Rnnf0gis the corresponding eigenvector ofAif eigenvalue\n",
      "eigenvectorAx=\u0015x: (4.25)\n",
      "We call (4.25) the eigenvalue equation . eigenvalue equation\n",
      "Remark. In the linear algebra literature and software, it is often a conven-\n",
      "tion that eigenvalues are sorted in descending order, so that the largest\n",
      "eigenvalue and associated eigenvector are called the ﬁrst eigenvalue and\n",
      "its associated eigenvector, and the second largest called the second eigen-\n",
      "value and its associated eigenvector, and so on. However, textbooks and\n",
      "publications may have different or no notion of orderings. We do not want\n",
      "to presume an ordering in this book if not stated explicitly. }\n",
      "The following statements are equivalent:\n",
      "\u0015is an eigenvalue of A2Rn\u0002n.\n",
      "There exists an x2Rnnf0gwithAx=\u0015x, or equivalently, (A\u0000\n",
      "\u0015In)x=0can be solved non-trivially, i.e., x6=0.\n",
      "rk(A\u0000\u0015In)<n.\n",
      "det(A\u0000\u0015In) = 0 .\n",
      "Deﬁnition 4.7 (Collinearity and Codirection) .Two vectors that point in\n",
      "the same direction are called codirected . Two vectors are collinear if they codirected\n",
      "collinear point in the same or the opposite direction.\n",
      "Remark (Non-uniqueness of eigenvectors) .Ifxis an eigenvector of A\n",
      "associated with eigenvalue \u0015, then for any c2Rnf0git holds that cxis\n",
      "an eigenvector of Awith the same eigenvalue since\n",
      "A(cx) =cAx=c\u0015x=\u0015(cx): (4.26)\n",
      "Thus, all vectors that are collinear to xare also eigenvectors of A.\n",
      "}\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "106 Matrix Decompositions\n",
      "Theorem 4.8. \u00152Ris an eigenvalue of A2Rn\u0002nif and only if \u0015is a\n",
      "root of the characteristic polynomial pA(\u0015)ofA.\n",
      "Deﬁnition 4.9. Let a square matrix Ahave an eigenvalue \u0015i. The algebraic algebraic\n",
      "multiplicity multiplicity of\u0015iis the number of times the root appears in the character-\n",
      "istic polynomial.\n",
      "Deﬁnition 4.10 (Eigenspace and Eigenspectrum) .ForA2Rn\u0002n, the set\n",
      "of all eigenvectors of Aassociated with an eigenvalue \u0015spans a subspace\n",
      "ofRn, which is called the eigenspace ofAwith respect to \u0015and is denoted eigenspace\n",
      "byE\u0015. The set of all eigenvalues of Ais called the eigenspectrum , or just eigenspectrum\n",
      "spectrum , ofA. spectrum\n",
      "If\u0015is an eigenvalue of A2Rn\u0002n, then the corresponding eigenspace\n",
      "E\u0015is the solution space of the homogeneous system of linear equations\n",
      "(A\u0000\u0015I)x=0. Geometrically, the eigenvector corresponding to a nonzero\n",
      "eigenvalue points in a direction that is stretched by the linear mapping.\n",
      "The eigenvalue is the factor by which it is stretched. If the eigenvalue is\n",
      "negative, the direction of the stretching is ﬂipped.\n",
      "Example 4.4 (The Case of the Identity Matrix)\n",
      "The identity matrix I2Rn\u0002nhas characteristic polynomial pI(\u0015) =\n",
      "det(I\u0000\u0015I) = (1\u0000\u0015)n= 0, which has only one eigenvalue \u0015= 1that oc-\n",
      "cursntimes. Moreover, Ix=\u0015x= 1xholds for all vectors x2Rnnf0g.\n",
      "Because of this, the sole eigenspace E1of the identity matrix spans ndi-\n",
      "mensions, and all nstandard basis vectors of Rnare eigenvectors of I.\n",
      "Useful properties regarding eigenvalues and eigenvectors include the\n",
      "following:\n",
      "A matrixAand its transpose A>possess the same eigenvalues, but not\n",
      "necessarily the same eigenvectors.\n",
      "The eigenspace E\u0015is the null space of A\u0000\u0015Isince\n",
      "Ax=\u0015x()Ax\u0000\u0015x=0 (4.27a)\n",
      "() (A\u0000\u0015I)x=0()x2ker(A\u0000\u0015I):(4.27b)\n",
      "Similar matrices (see Deﬁnition 2.22) possess the same eigenvalues.\n",
      "Therefore, a linear mappinghas eigenvalues that are independent of\n",
      "the choice of basis of its transformation matrix. This makes eigenvalues,\n",
      "together with the determinant and the trace, key characteristic param-\n",
      "eters of a linear mapping as they are all invariant under basis change.\n",
      "Symmetric, positive deﬁnite matrices always have positive, real eigen-\n",
      "values.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.2 Eigenvalues and Eigenvectors 107\n",
      "Example 4.5 (Computing Eigenvalues, Eigenvectors, and\n",
      "Eigenspaces)\n",
      "Let us ﬁnd the eigenvalues and eigenvectors of the 2\u00022matrix\n",
      "A=\u00144 2\n",
      "1 3\u0015\n",
      ": (4.28)\n",
      "Step 1: Characteristic Polynomial. From our deﬁnition of the eigen-\n",
      "vectorx6=0and eigenvalue \u0015ofA, there will be a vector such that\n",
      "Ax=\u0015x, i.e., (A\u0000\u0015I)x=0. Sincex6=0, this requires that the kernel\n",
      "(null space) of A\u0000\u0015Icontains more elements than just 0. This means\n",
      "thatA\u0000\u0015Iis not invertible and therefore det(A\u0000\u0015I) = 0 . Hence, we\n",
      "need to compute the roots of the characteristic polynomial (4.22a) to ﬁnd\n",
      "the eigenvalues.\n",
      "Step 2: Eigenvalues. The characteristic polynomial is\n",
      "pA(\u0015) = det(A\u0000\u0015I) (4.29a)\n",
      "= det\u0012\u00144 2\n",
      "1 3\u0015\n",
      "\u0000\u0014\u00150\n",
      "0\u0015\u0015\u0013\n",
      "=\f\f\f\f4\u0000\u0015 2\n",
      "1 3\u0000\u0015\f\f\f\f(4.29b)\n",
      "= (4\u0000\u0015)(3\u0000\u0015)\u00002\u00011: (4.29c)\n",
      "We factorize the characteristic polynomial and obtain\n",
      "p(\u0015) = (4\u0000\u0015)(3\u0000\u0015)\u00002\u00011 = 10\u00007\u0015+\u00152= (2\u0000\u0015)(5\u0000\u0015)(4.30)\n",
      "giving the roots \u00151= 2and\u00152= 5.\n",
      "Step 3: Eigenvectors and Eigenspaces. We ﬁnd the eigenvectors that\n",
      "correspond to these eigenvalues by looking at vectors xsuch that\n",
      "\u00144\u0000\u0015 2\n",
      "1 3\u0000\u0015\u0015\n",
      "x=0: (4.31)\n",
      "For\u0015= 5we obtain\n",
      "\u00144\u00005 2\n",
      "1 3\u00005\u0015\u0014x1\n",
      "x2\u0015\n",
      "=\u0014\u00001 2\n",
      "1\u00002\u0015\u0014x1\n",
      "x2\u0015\n",
      "=0: (4.32)\n",
      "We solve this homogeneous system and obtain a solution space\n",
      "E5= span[\u00142\n",
      "1\u0015\n",
      "]: (4.33)\n",
      "This eigenspace is one-dimensional as it possesses a single basis vector.\n",
      "Analogously, we ﬁnd the eigenvector for \u0015= 2by solving the homoge-\n",
      "neous system of equations\n",
      "\u00144\u00002 2\n",
      "1 3\u00002\u0015\n",
      "x=\u00142 2\n",
      "1 1\u0015\n",
      "x=0: (4.34)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "108 Matrix Decompositions\n",
      "This means any vector x=\u0014x1\n",
      "x2\u0015\n",
      ", wherex2=\u0000x1, such as\u00141\n",
      "\u00001\u0015\n",
      ", is an\n",
      "eigenvector with eigenvalue 2. The corresponding eigenspace is given as\n",
      "E2= span[\u00141\n",
      "\u00001\u0015\n",
      "]: (4.35)\n",
      "The two eigenspaces E5andE2in Example 4.5 are one-dimensional\n",
      "as they are each spanned by a single vector. However, in other cases\n",
      "we may have multiple identical eigenvalues (see Deﬁnition 4.9) and the\n",
      "eigenspace may have more than one dimension.\n",
      "Deﬁnition 4.11. Let\u0015ibe an eigenvalue of a square matrix A. Then the\n",
      "geometric multiplicity of\u0015iis the number of linearly independent eigen- geometric\n",
      "multiplicity vectors associated with \u0015i. In other words, it is the dimensionality of the\n",
      "eigenspace spanned by the eigenvectors associated with \u0015i.\n",
      "Remark. A speciﬁc eigenvalue’s geometric multiplicity must be at least\n",
      "one because every eigenvalue has at least one associated eigenvector. An\n",
      "eigenvalue’s geometric multiplicity cannot exceed its algebraic multiplic-\n",
      "ity, but it may be lower. }\n",
      "Example 4.6\n",
      "The matrixA=\u00142 1\n",
      "0 2\u0015\n",
      "has two repeated eigenvalues \u00151=\u00152= 2and an\n",
      "algebraic multiplicity of 2. The eigenvalue has, however, only one distinct\n",
      "unit eigenvector x1=\u00141\n",
      "0\u0015\n",
      "and, thus, geometric multiplicity 1.\n",
      "Graphical Intuition in Two Dimensions\n",
      "Let us gain some intuition for determinants, eigenvectors, and eigenval-\n",
      "ues using different linear mappings. Figure 4.4 depicts ﬁve transformation\n",
      "matricesA1;:::;A5and their impact on a square grid of points, centered\n",
      "at the origin: In geometry, the\n",
      "area-preserving\n",
      "properties of this\n",
      "type of shearing\n",
      "parallel to an axis is\n",
      "also known as\n",
      "Cavalieri’s principle\n",
      "of equal areas for\n",
      "parallelograms\n",
      "(Katz, 2004).A1=\u00141\n",
      "20\n",
      "0 2\u0015\n",
      ". The direction of the two eigenvectors correspond to the\n",
      "canonical basis vectors in R2, i.e., to two cardinal axes. The vertical axis\n",
      "is extended by a factor of 2(eigenvalue \u00151= 2), and the horizontal axis\n",
      "is compressed by factor1\n",
      "2(eigenvalue \u00152=1\n",
      "2). The mapping is area\n",
      "preserving ( det(A1) = 1 = 2\u00011\n",
      "2).\n",
      "A2=\u001411\n",
      "2\n",
      "0 1\u0015\n",
      "corresponds to a shearing mapping , i.e., it shears the\n",
      "points along the horizontal axis to the right if they are on the positive\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.2 Eigenvalues and Eigenvectors 109\n",
      "Figure 4.4\n",
      "Determinants and\n",
      "eigenspaces.\n",
      "Overview of ﬁve\n",
      "linear mappings and\n",
      "their associated\n",
      "transformation\n",
      "matrices\n",
      "Ai2R2\u00022\n",
      "projecting 400\n",
      "color-coded points\n",
      "x2R2(left\n",
      "column) onto target\n",
      "pointsAix(right\n",
      "column). The\n",
      "central column\n",
      "depicts the ﬁrst\n",
      "eigenvector,\n",
      "stretched by its\n",
      "associated\n",
      "eigenvalue\u00151, and\n",
      "the second\n",
      "eigenvector\n",
      "stretched by its\n",
      "eigenvalue\u00152. Each\n",
      "row depicts the\n",
      "effect of one of ﬁve\n",
      "transformation\n",
      "matricesAiwith\n",
      "respect to the\n",
      "standard basis.\n",
      "det(A) = 1.0\u00151= 2.0\n",
      "\u00152= 0.5\n",
      "det(A) = 1.0\u00151= 1.0\n",
      "\u00152= 1.0\n",
      "det(A) = 1.0\u00151= (0.87-0.5j)\n",
      "\u00152= (0.87+0.5j)\n",
      "det(A) = 0.0\u00151= 0.0\n",
      "\u00152= 2.0\n",
      "det(A) = 0.75\u00151= 0.5\n",
      "\u00152= 1.5\n",
      "half of the vertical axis, and to the left vice versa. This mapping is area\n",
      "preserving ( det(A2) = 1 ). The eigenvalue \u00151= 1 =\u00152is repeated\n",
      "and the eigenvectors are collinear (drawn here for emphasis in two\n",
      "opposite directions). This indicates that the mapping acts only along\n",
      "one direction (the horizontal axis).\n",
      "A3=\u0014cos(\u0019\n",
      "6)\u0000sin(\u0019\n",
      "6)\n",
      "sin(\u0019\n",
      "6) cos(\u0019\n",
      "6)\u0015\n",
      "=1\n",
      "2\u0014p\n",
      "3\u00001\n",
      "1p\n",
      "3\u0015\n",
      "The matrixA3rotates the\n",
      "points by\u0019\n",
      "6rad = 30\u000ecounter-clockwise and has only complex eigen-\n",
      "values, reﬂecting that the mapping is a rotation (hence, no eigenvectors\n",
      "are drawn). A rotation has to be volume preserving, and so the deter-\n",
      "minant is 1. For more details on rotations, we refer to Section 3.9.\n",
      "A4=\u00141\u00001\n",
      "\u00001 1\u0015\n",
      "represents a mapping in the standard basis that col-\n",
      "lapses a two-dimensional domain onto one dimension. Since one eigen-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "110 Matrix Decompositions\n",
      "value is 0, the space in direction of the (blue) eigenvector corresponding\n",
      "to\u00151= 0 collapses, while the orthogonal (red) eigenvector stretches\n",
      "space by a factor \u00152= 2. Therefore, the area of the image is 0.\n",
      "A5=\u001411\n",
      "21\n",
      "21\u0015\n",
      "is a shear-and-stretch mapping that scales space by 75%\n",
      "sincejdet(A5)j=3\n",
      "4. It stretches space along the (red) eigenvector\n",
      "of\u00152by a factor 1:5and compresses it along the orthogonal (blue)\n",
      "eigenvector by a factor 0:5.\n",
      "Example 4.7 (Eigenspectrum of a Biological Neural Network)\n",
      "Figure 4.5\n",
      "Caenorhabditis\n",
      "elegans neural\n",
      "network (Kaiser and\n",
      "Hilgetag,\n",
      "2006).(a) Sym-\n",
      "metrized\n",
      "connectivity matrix;\n",
      "(b) Eigenspectrum.\n",
      "0 50 100 150 200 250\n",
      "neuron index0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250neuron index\n",
      "(a) Connectivity matrix.\n",
      "0 100 200\n",
      "index of sorted eigenvalue\u000010\u000050510152025eigenvalue\n",
      " (b) Eigenspectrum.\n",
      "Methods to analyze and learn from network data are an essential com-\n",
      "ponent of machine learning methods. The key to understanding networks\n",
      "is the connectivity between network nodes, especially if two nodes are\n",
      "connected to each other or not. In data science applications, it is often\n",
      "useful to study the matrix that captures this connectivity data.\n",
      "We build a connectivity/adjacency matrix A2R277\u0002277of the complete\n",
      "neural network of the worm C.Elegans . Each row/column represents one\n",
      "of the 277neurons of this worm’s brain. The connectivity matrix Ahas\n",
      "a value ofaij= 1 if neuronitalks to neuron jthrough a synapse, and\n",
      "aij= 0 otherwise. The connectivity matrix is not symmetric, which im-\n",
      "plies that eigenvalues may not be real valued. Therefore, we compute a\n",
      "symmetrized version of the connectivity matrix as Asym:=A+A>. This\n",
      "new matrixAsymis shown in Figure 4.5(a) and has a nonzero value aijif\n",
      "and only if two neurons are connected (white pixels), irrespective of the\n",
      "direction of the connection. In Figure 4.5(b), we show the correspond-\n",
      "ing eigenspectrum of Asym. The horizontal axis shows the index of the\n",
      "eigenvalues, sorted in descending order. The vertical axis shows the corre-\n",
      "sponding eigenvalue. The S-like shape of this eigenspectrum is typical for\n",
      "many biological neural networks. The underlying mechanism responsible\n",
      "for this is an area of active neuroscience research.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.2 Eigenvalues and Eigenvectors 111\n",
      "Theorem 4.12. The eigenvectors x1;:::;xnof a matrixA2Rn\u0002nwithn\n",
      "distinct eigenvalues \u00151;:::;\u0015nare linearly independent.\n",
      "This theorem states that eigenvectors of a matrix with ndistinct eigen-\n",
      "values form a basis of Rn.\n",
      "Deﬁnition 4.13. A square matrix A2Rn\u0002nisdefective if it possesses defective\n",
      "fewer thannlinearly independent eigenvectors.\n",
      "A non-defective matrix A2Rn\u0002ndoes not necessarily require ndis-\n",
      "tinct eigenvalues, but it does require that the eigenvectors form a basis of\n",
      "Rn. Looking at the eigenspaces of a defective matrix, it follows that the\n",
      "sum of the dimensions of the eigenspaces is less than n. Speciﬁcally, a de-\n",
      "fective matrix has at least one eigenvalue \u0015iwith an algebraic multiplicity\n",
      "m> 1and a geometric multiplicity of less than m.\n",
      "Remark. A defective matrix cannot have ndistinct eigenvalues, as distinct\n",
      "eigenvalues have linearly independent eigenvectors (Theorem 4.12). }\n",
      "Theorem 4.14. Given a matrix A2Rm\u0002n, we can always obtain a sym-\n",
      "metric, positive semideﬁnite matrix S2Rn\u0002nby deﬁning\n",
      "S:=A>A: (4.36)\n",
      "Remark. Ifrk(A) =n, thenS:=A>Ais symmetric, positive deﬁnite.\n",
      "}\n",
      "Understanding why Theorem 4.14 holds is insightful for how we can\n",
      "use symmetrized matrices: Symmetry requires S=S>, and by insert-\n",
      "ing (4.36) we obtain S=A>A=A>(A>)>= (A>A)>=S>. More-\n",
      "over, positive semideﬁniteness (Section 3.2.3) requires that x>Sx>0\n",
      "and inserting (4.36) we obtain x>Sx=x>A>Ax= (x>A>)(Ax) =\n",
      "(Ax)>(Ax)>0, because the dot product computes a sum of squares\n",
      "(which are themselves non-negative).\n",
      "spectral theorem\n",
      "Theorem 4.15 (Spectral Theorem) .IfA2Rn\u0002nis symmetric, there ex-\n",
      "ists an orthonormal basis of the corresponding vector space Vconsisting of\n",
      "eigenvectors of A, and each eigenvalue is real.\n",
      "A direct implication of the spectral theorem is that the eigendecompo-\n",
      "sition of a symmetric matrix Aexists (with real eigenvalues), and that\n",
      "we can ﬁnd an ONB of eigenvectors so that A=PDP>, whereDis\n",
      "diagonal and the columns of Pcontain the eigenvectors.\n",
      "Example 4.8\n",
      "Consider the matrix\n",
      "A=2\n",
      "43 2 2\n",
      "2 3 2\n",
      "2 2 33\n",
      "5: (4.37)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "112 Matrix Decompositions\n",
      "The characteristic polynomial of Ais\n",
      "pA(\u0015) =\u0000(\u0015\u00001)2(\u0015\u00007); (4.38)\n",
      "so that we obtain the eigenvalues \u00151= 1 and\u00152= 7, where\u00151is a\n",
      "repeated eigenvalue. Following our standard procedure for computing\n",
      "eigenvectors, we obtain the eigenspaces\n",
      "E1= span[2\n",
      "4\u00001\n",
      "1\n",
      "03\n",
      "5\n",
      "|{z}\n",
      "=:x1;2\n",
      "4\u00001\n",
      "0\n",
      "13\n",
      "5\n",
      "|{z}\n",
      "=:x2]; E 7= span[2\n",
      "41\n",
      "1\n",
      "13\n",
      "5\n",
      "|{z}\n",
      "=:x3]: (4.39)\n",
      "We see thatx3is orthogonal to both x1andx2. However, since x>\n",
      "1x2=\n",
      "16= 0, they are not orthogonal. The spectral theorem (Theorem 4.15)\n",
      "states that there exists an orthogonal basis, but the one we have is not\n",
      "orthogonal. However, we can construct one.\n",
      "To construct such a basis, we exploit the fact that x1;x2are eigenvec-\n",
      "tors associated with the same eigenvalue \u0015. Therefore, for any \u000b;\f2Rit\n",
      "holds that\n",
      "A(\u000bx1+\fx2) =Ax1\u000b+Ax2\f=\u0015(\u000bx1+\fx2); (4.40)\n",
      "i.e., any linear combination of x1andx2is also an eigenvector of Aas-\n",
      "sociated with \u0015. The Gram-Schmidt algorithm (Section 3.8.3) is a method\n",
      "for iteratively constructing an orthogonal/orthonormal basis from a set of\n",
      "basis vectors using such linear combinations. Therefore, even if x1andx2\n",
      "are not orthogonal, we can apply the Gram-Schmidt algorithm and ﬁnd\n",
      "eigenvectors associated with \u00151= 1 that are orthogonal to each other\n",
      "(and tox3). In our example, we will obtain\n",
      "x0\n",
      "1=2\n",
      "4\u00001\n",
      "1\n",
      "03\n",
      "5;x0\n",
      "2=1\n",
      "22\n",
      "4\u00001\n",
      "\u00001\n",
      "23\n",
      "5; (4.41)\n",
      "which are orthogonal to each other, orthogonal to x3, and eigenvectors of\n",
      "Aassociated with \u00151= 1.\n",
      "Before we conclude our considerations of eigenvalues and eigenvectors\n",
      "it is useful to tie these matrix characteristics together with the concepts of\n",
      "the determinant and the trace.\n",
      "Theorem 4.16. The determinant of a matrix A2Rn\u0002nis the product of\n",
      "its eigenvalues, i.e.,\n",
      "det(A) =nY\n",
      "i=1\u0015i; (4.42)\n",
      "where\u0015i2Care (possibly repeated) eigenvalues of A.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.2 Eigenvalues and Eigenvectors 113\n",
      "Figure 4.6\n",
      "Geometric\n",
      "interpretation of\n",
      "eigenvalues. The\n",
      "eigenvectors of A\n",
      "get stretched by the\n",
      "corresponding\n",
      "eigenvalues. The\n",
      "area of the unit\n",
      "square changes by\n",
      "j\u00151\u00152j, the\n",
      "perimeter changes\n",
      "by a factor of\n",
      "1\n",
      "2(j\u00151j+j\u00152j).x1x2\n",
      "v1v2A\n",
      "Theorem 4.17. The trace of a matrix A2Rn\u0002nis the sum of its eigenval-\n",
      "ues, i.e.,\n",
      "tr(A) =nX\n",
      "i=1\u0015i; (4.43)\n",
      "where\u0015i2Care (possibly repeated) eigenvalues of A.\n",
      "Let us provide a geometric intuition of these two theorems. Consider\n",
      "a matrixA2R2\u00022that possesses two linearly independent eigenvectors\n",
      "x1;x2. For this example, we assume (x1;x2)are an ONB of R2so that they\n",
      "are orthogonal and the area of the square they span is 1; see Figure 4.6.\n",
      "From Section 4.1, we know that the determinant computes the change of\n",
      "area of unit square under the transformation A. In this example, we can\n",
      "compute the change of area explicitly: Mapping the eigenvectors using\n",
      "Agives us vectors v1=Ax1=\u00151x1andv2=Ax2=\u00152x2, i.e., the\n",
      "new vectorsviare scaled versions of the eigenvectors xi, and the scaling\n",
      "factors are the corresponding eigenvalues \u0015i.v1;v2are still orthogonal,\n",
      "and the area of the rectangle they span is j\u00151\u00152j.\n",
      "Given thatx1;x2(in our example) are orthonormal, we can directly\n",
      "compute the perimeter of the unit square as 2(1 + 1) . Mapping the eigen-\n",
      "vectors using Acreates a rectangle whose perimeter is 2(j\u00151j+j\u00152j).\n",
      "Therefore, the sum of the absolute values of the eigenvalues tells us how\n",
      "the perimeter of the unit square changes under the transformation matrix\n",
      "A.\n",
      "Example 4.9 (Google’s PageRank – Webpages as Eigenvectors)\n",
      "Google uses the eigenvector corresponding to the maximal eigenvalue of\n",
      "a matrixAto determine the rank of a page for search. The idea for the\n",
      "PageRank algorithm, developed at Stanford University by Larry Page and\n",
      "Sergey Brin in 1996, was that the importance of any web page can be ap-\n",
      "proximated by the importance of pages that link to it. For this, they write\n",
      "down all web sites as a huge directed graph that shows which page links\n",
      "to which. PageRank computes the weight (importance) xi>0of a web\n",
      "siteaiby counting the number of pages pointing to ai. Moreover, PageR-\n",
      "ank takes into account the importance of the web sites that link to ai. The\n",
      "navigation behavior of a user is then modeled by a transition matrix Aof\n",
      "this graph that tells us with what (click) probability somebody will end up\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "114 Matrix Decompositions\n",
      "on a different web site. The matrix Ahas the property that for any ini-\n",
      "tial rank/importance vector xof a web site the sequence x;Ax;A2x;:::\n",
      "converges to a vector x\u0003. This vector is called the PageRank and satisﬁes PageRank\n",
      "Ax\u0003=x\u0003, i.e., it is an eigenvector (with corresponding eigenvalue 1) of\n",
      "A. After normalizing x\u0003, such thatkx\u0003k= 1, we can interpret the entries\n",
      "as probabilities. More details and different perspectives on PageRank can\n",
      "be found in the original technical report (Page et al., 1999).\n",
      "4.3 Cholesky Decomposition\n",
      "There are many ways to factorize special types of matrices that we en-\n",
      "counter often in machine learning. In the positive real numbers, we have\n",
      "the square-root operation that gives us a decomposition of the number\n",
      "into identical components, e.g., 9 = 3\u00013. For matrices, we need to be\n",
      "careful that we compute a square-root-like operation on positive quanti-\n",
      "ties. For symmetric, positive deﬁnite matrices (see Section 3.2.3), we can\n",
      "choose from a number of square-root equivalent operations. The Cholesky Cholesky\n",
      "decomposition decomposition /Cholesky factorization provides a square-root equivalent op-\n",
      "Cholesky\n",
      "factorizationeration on symmetric, positive deﬁnite matrices that is useful in practice.\n",
      "Theorem 4.18 (Cholesky Decomposition) .A symmetric, positive deﬁnite\n",
      "matrixAcan be factorized into a product A=LL>, whereLis a lower-\n",
      "triangular matrix with positive diagonal elements:\n",
      "2\n",
      "64a11\u0001\u0001\u0001a1n\n",
      ".........\n",
      "an1\u0001\u0001\u0001ann3\n",
      "75=2\n",
      "64l11\u0001\u0001\u0001 0\n",
      ".........\n",
      "ln1\u0001\u0001\u0001lnn3\n",
      "752\n",
      "64l11\u0001\u0001\u0001ln1\n",
      ".........\n",
      "0\u0001\u0001\u0001lnn3\n",
      "75: (4.44)\n",
      "Lis called the Cholesky factor of A, andLis unique. Cholesky factor\n",
      "Example 4.10 (Cholesky Factorization)\n",
      "Consider a symmetric, positive deﬁnite matrix A2R3\u00023. We are inter-\n",
      "ested in ﬁnding its Cholesky factorization A=LL>, i.e.,\n",
      "A=2\n",
      "4a11a21a31\n",
      "a21a22a32\n",
      "a31a32a333\n",
      "5=LL>=2\n",
      "4l110 0\n",
      "l21l220\n",
      "l31l32l333\n",
      "52\n",
      "4l11l21l31\n",
      "0l22l32\n",
      "0 0l333\n",
      "5:(4.45)\n",
      "Multiplying out the right-hand side yields\n",
      "A=2\n",
      "4l2\n",
      "11l21l11 l31l11\n",
      "l21l11l2\n",
      "21+l2\n",
      "22l31l21+l32l22\n",
      "l31l11l31l21+l32l22l2\n",
      "31+l2\n",
      "32+l2\n",
      "333\n",
      "5: (4.46)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.4 Eigendecomposition and Diagonalization 115\n",
      "Comparing the left-hand side of (4.45) and the right-hand side of (4.46)\n",
      "shows that there is a simple pattern in the diagonal elements lii:\n",
      "l11=pa11; l 22=q\n",
      "a22\u0000l2\n",
      "21; l 33=q\n",
      "a33\u0000(l2\n",
      "31+l2\n",
      "32):(4.47)\n",
      "Similarly for the elements below the diagonal ( lij, wherei > j ), there is\n",
      "also a repeating pattern:\n",
      "l21=1\n",
      "l11a21; l 31=1\n",
      "l11a31; l 32=1\n",
      "l22(a32\u0000l31l21): (4.48)\n",
      "Thus, we constructed the Cholesky decomposition for any symmetric, pos-\n",
      "itive deﬁnite 3\u00023matrix. The key realization is that we can backward\n",
      "calculate what the components lijfor theLshould be, given the values\n",
      "aijforAand previously computed values of lij.\n",
      "The Cholesky decomposition is an important tool for the numerical\n",
      "computations underlying machine learning. Here, symmetric positive def-\n",
      "inite matrices require frequent manipulation, e.g., the covariance matrix\n",
      "of a multivariate Gaussian variable (see Section 6.5) is symmetric, positive\n",
      "deﬁnite. The Cholesky factorization of this covariance matrix allows us to\n",
      "generate samples from a Gaussian distribution. It also allows us to perform\n",
      "a linear transformation of random variables, which is heavily exploited\n",
      "when computing gradients in deep stochastic models, such as the varia-\n",
      "tional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\n",
      "2014). The Cholesky decomposition also allows us to compute determi-\n",
      "nants very efﬁciently. Given the Cholesky decomposition A=LL>, we\n",
      "know that det(A) = det(L) det(L>) = det(L)2. SinceLis a triangular\n",
      "matrix, the determinant is simply the product of its diagonal entries so\n",
      "thatdet(A) =Q\n",
      "il2\n",
      "ii. Thus, many numerical software packages use the\n",
      "Cholesky decomposition to make computations more efﬁcient.\n",
      "4.4 Eigendecomposition and Diagonalization\n",
      "Adiagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\n",
      "ments, i.e., they are of the form\n",
      "D=2\n",
      "64c1\u0001\u0001\u0001 0\n",
      ".........\n",
      "0\u0001\u0001\u0001cn3\n",
      "75: (4.49)\n",
      "They allow fast computation of determinants, powers, and inverses. The\n",
      "determinant is the product of its diagonal entries, a matrix power Dkis\n",
      "given by each diagonal element raised to the power k, and the inverse\n",
      "D\u00001is the reciprocal of its diagonal elements if all of them are nonzero.\n",
      "In this section, we will discuss how to transform matrices into diagonal\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "116 Matrix Decompositions\n",
      "form. This is an important application of the basis change we discussed in\n",
      "Section 2.7.2 and eigenvalues from Section 4.2.\n",
      "Recall that two matrices A;Dare similar (Deﬁnition 2.22) if there ex-\n",
      "ists an invertible matrix P, such thatD=P\u00001AP. More speciﬁcally, we\n",
      "will look at matrices Athat are similar to diagonal matrices Dthat con-\n",
      "tain the eigenvalues of Aon the diagonal.\n",
      "Deﬁnition 4.19 (Diagonalizable) .A matrixA2Rn\u0002nisdiagonalizable diagonalizable\n",
      "if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix\n",
      "P2Rn\u0002nsuch thatD=P\u00001AP.\n",
      "In the following, we will see that diagonalizing a matrix A2Rn\u0002nis\n",
      "a way of expressing the same linear mapping but in another basis (see\n",
      "Section 2.6.1), which will turn out to be a basis that consists of the eigen-\n",
      "vectors ofA.\n",
      "LetA2Rn\u0002n, let\u00151;:::;\u0015nbe a set of scalars, and let p1;:::;pnbe a\n",
      "set of vectors in Rn. We deﬁneP:= [p1;:::;pn]and letD2Rn\u0002nbe a\n",
      "diagonal matrix with diagonal entries \u00151;:::;\u0015n. Then we can show that\n",
      "AP=PD (4.50)\n",
      "if and only if \u00151;:::;\u0015nare the eigenvalues of Aandp1;:::;pnare cor-\n",
      "responding eigenvectors of A.\n",
      "We can see that this statement holds because\n",
      "AP=A[p1;:::;pn] = [Ap1;:::;Apn]; (4.51)\n",
      "PD = [p1;:::;pn]2\n",
      "64\u00151 0\n",
      "...\n",
      "0\u0015n3\n",
      "75= [\u00151p1;:::;\u0015npn]: (4.52)\n",
      "Thus, (4.50) implies that\n",
      "Ap1=\u00151p1 (4.53)\n",
      "...\n",
      "Apn=\u0015npn: (4.54)\n",
      "Therefore, the columns of Pmust be eigenvectors of A.\n",
      "Our deﬁnition of diagonalization requires that P2Rn\u0002nis invertible,\n",
      "i.e.,Phas full rank (Theorem 4.3). This requires us to have nlinearly\n",
      "independent eigenvectors p1;:::;pn, i.e., thepiform a basis of Rn.\n",
      "Theorem 4.20 (Eigendecomposition) .A square matrix A2Rn\u0002ncan be\n",
      "factored into\n",
      "A=PDP\u00001; (4.55)\n",
      "whereP2Rn\u0002nandDis a diagonal matrix whose diagonal entries are\n",
      "the eigenvalues of A, if and only if the eigenvectors of Aform a basis of Rn.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.4 Eigendecomposition and Diagonalization 117\n",
      "Figure 4.7 Intuition\n",
      "behind the\n",
      "eigendecomposition\n",
      "as sequential\n",
      "transformations.\n",
      "Top-left to\n",
      "bottom-left:P\u00001\n",
      "performs a basis\n",
      "change (here drawn\n",
      "inR2and depicted\n",
      "as a rotation-like\n",
      "operation) from the\n",
      "standard basis into\n",
      "the eigenbasis.\n",
      "Bottom-left to\n",
      "bottom-right: D\n",
      "performs a scaling\n",
      "along the remapped\n",
      "orthogonal\n",
      "eigenvectors,\n",
      "depicted here by a\n",
      "circle being\n",
      "stretched to an\n",
      "ellipse. Bottom-right\n",
      "to top-right:P\n",
      "undoes the basis\n",
      "change (depicted as\n",
      "a reverse rotation)\n",
      "and restores the\n",
      "original coordinate\n",
      "frame.\n",
      "e1e2\n",
      "p1p2\n",
      "p1p2e1e2\n",
      "p1p2\n",
      "\u00151p1\u00152p2\n",
      "e1e2\n",
      "Ae 1Ae 2P\u00001\n",
      "DPA\n",
      "Theorem 4.20 implies that only non-defective matrices can be diagonal-\n",
      "ized and that the columns of Pare theneigenvectors of A. For symmetric\n",
      "matrices we can obtain even stronger outcomes for the eigenvalue decom-\n",
      "position.\n",
      "Theorem 4.21. A symmetric matrix S2Rn\u0002ncan always be diagonalized.\n",
      "Theorem 4.21 follows directly from the spectral theorem 4.15. More-\n",
      "over, the spectral theorem states that we can ﬁnd an ONB of eigenvectors\n",
      "ofRn. This makesPan orthogonal matrix so that D=P>AP.\n",
      "Remark. The Jordan normal form of a matrix offers a decomposition that\n",
      "works for defective matrices (Lang, 1987) but is beyond the scope of this\n",
      "book. }\n",
      "Geometric Intuition for the Eigendecomposition\n",
      "We can interpret the eigendecomposition of a matrix as follows (see also\n",
      "Figure 4.7): Let Abe the transformation matrix of a linear mapping with\n",
      "respect to the standard basis ei(blue arrows). P\u00001performs a basis\n",
      "change from the standard basis into the eigenbasis. Then, the diagonal\n",
      "Dscales the vectors along these axes by the eigenvalues \u0015i. Finally,P\n",
      "transforms these scaled vectors back into the standard/canonical coordi-\n",
      "nates yielding \u0015ipi.\n",
      "Example 4.11 (Eigendecomposition)\n",
      "Let us compute the eigendecomposition of A=1\n",
      "2\u00145\u00002\n",
      "\u00002 5\u0015\n",
      ".\n",
      "Step 1: Compute eigenvalues and eigenvectors. The characteristic\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "118 Matrix Decompositions\n",
      "polynomial of Ais\n",
      "det(A\u0000\u0015I) = det\u0012\u00145\n",
      "2\u0000\u0015\u00001\n",
      "\u000015\n",
      "2\u0000\u0015\u0015\u0013\n",
      "(4.56a)\n",
      "= (5\n",
      "2\u0000\u0015)2\u00001 =\u00152\u00005\u0015+21\n",
      "4= (\u0015\u00007\n",
      "2)(\u0015\u00003\n",
      "2): (4.56b)\n",
      "Therefore, the eigenvalues of Aare\u00151=7\n",
      "2and\u00152=3\n",
      "2(the roots of the\n",
      "characteristic polynomial), and the associated (normalized) eigenvectors\n",
      "are obtained via\n",
      "Ap1=7\n",
      "2p1;Ap2=3\n",
      "2p2: (4.57)\n",
      "This yields\n",
      "p1=1p\n",
      "2\u00141\n",
      "\u00001\u0015\n",
      ";p2=1p\n",
      "2\u00141\n",
      "1\u0015\n",
      ": (4.58)\n",
      "Step 2: Check for existence. The eigenvectors p1;p2form a basis of R2.\n",
      "Therefore,Acan be diagonalized.\n",
      "Step 3: Construct the matrix Pto diagonalize A.We collect the eigen-\n",
      "vectors ofAinPso that\n",
      "P= [p1;p2] =1p\n",
      "2\u00141 1\n",
      "\u00001 1\u0015\n",
      ": (4.59)\n",
      "We then obtain\n",
      "P\u00001AP=\u00147\n",
      "20\n",
      "03\n",
      "2\u0015\n",
      "=D: (4.60)\n",
      "Equivalently, we get (exploiting that P\u00001=P>since the eigenvectors Figure 4.7 visualizes\n",
      "the\n",
      "eigendecomposition\n",
      "ofA=\u00145\u00002\n",
      "\u00002 5\u0015\n",
      "as a sequence of\n",
      "linear\n",
      "transformations.p1andp2in this example form an ONB)\n",
      "1\n",
      "2\u00145\u00002\n",
      "\u00002 5\u0015\n",
      "|{z}\n",
      "A=1p\n",
      "2\u00141 1\n",
      "\u00001 1\u0015\n",
      "|{z}\n",
      "P\u00147\n",
      "20\n",
      "03\n",
      "2\u0015\n",
      "|{z}\n",
      "D1p\n",
      "2\u00141\u00001\n",
      "1 1\u0015\n",
      "|{z}\n",
      "P\u00001: (4.61)\n",
      "Diagonal matrices Dcan efﬁciently be raised to a power. Therefore,\n",
      "we can ﬁnd a matrix power for a matrix A2Rn\u0002nvia the eigenvalue\n",
      "decomposition (if it exists) so that\n",
      "Ak= (PDP\u00001)k=PDkP\u00001: (4.62)\n",
      "ComputingDkis efﬁcient because we apply this operation individually\n",
      "to any diagonal element.\n",
      "Assume that the eigendecomposition A=PDP\u00001exists. Then,\n",
      "det(A) = det(PDP\u00001) = det(P) det(D) det(P\u00001) (4.63a)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.5 Singular Value Decomposition 119\n",
      "= det(D) =Y\n",
      "idii (4.63b)\n",
      "allows for an efﬁcient computation of the determinant of A.\n",
      "The eigenvalue decomposition requires square matrices. It would be\n",
      "useful to perform a decomposition on general matrices. In the next sec-\n",
      "tion, we introduce a more general matrix decomposition technique, the\n",
      "singular value decomposition.\n",
      "4.5 Singular Value Decomposition\n",
      "The singular value decomposition (SVD) of a matrix is a central matrix\n",
      "decomposition method in linear algebra. It has been referred to as the\n",
      "“fundamental theorem of linear algebra” (Strang, 1993) because it can be\n",
      "applied to all matrices, not only to square matrices, and it always exists.\n",
      "Moreover, as we will explore in the following, the SVD of a matrix A,\n",
      "which represents a linear mapping :V!W, quantiﬁes the change\n",
      "between the underlying geometry of these two vector spaces. We recom-\n",
      "mend the work by Kalman (1996) and Roy and Banerjee (2014) for a\n",
      "deeper overview of the mathematics of the SVD.\n",
      "SVD theorem\n",
      "Theorem 4.22 (SVD Theorem) .LetA2Rm\u0002nbe a rectangular matrix of\n",
      "rankr2[0;min(m;n)]. The SVD ofAis a decomposition of the form SVD\n",
      "singular value\n",
      "decomposition\n",
      "=UA V>\u0006mn\n",
      "mm\n",
      "mn\n",
      "nn\n",
      "(4.64)\n",
      "with an orthogonal matrix U2Rm\u0002mwith column vectors ui,i= 1;:::;m ,\n",
      "and an orthogonal matrix V2Rn\u0002nwith column vectors vj,j= 1;:::;n .\n",
      "Moreover, \u0006is anm\u0002nmatrix with \u0006ii=\u001bi>0and\u0006ij= 0; i6=j.\n",
      "The diagonal entries \u001bi,i= 1;:::;r , of\u0006are called the singular values ,singular values\n",
      "uiare called the left-singular vectors , andvjare called the right-singular left-singular vectors\n",
      "right-singular\n",
      "vectorsvectors . By convention, the singular values are ordered, i.e., \u001b1>\u001b2>\n",
      "\u001br>0.\n",
      "Thesingular value matrix \u0006is unique, but it requires some attention. singular value\n",
      "matrix Observe that the \u00062Rm\u0002nis rectangular. In particular, \u0006is of the same\n",
      "size asA. This means that \u0006has a diagonal submatrix that contains the\n",
      "singular values and needs additional zero padding. Speciﬁcally, if m>n ,\n",
      "then the matrix \u0006has diagonal structure up to row nand then consists of\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "120 Matrix Decompositions\n",
      "Figure 4.8 Intuition\n",
      "behind the SVD of a\n",
      "matrixA2R3\u00022\n",
      "as sequential\n",
      "transformations.\n",
      "Top-left to\n",
      "bottom-left:V>\n",
      "performs a basis\n",
      "change in R2.\n",
      "Bottom-left to\n",
      "bottom-right: \u0006\n",
      "scales and maps\n",
      "fromR2toR3. The\n",
      "ellipse in the\n",
      "bottom-right lives in\n",
      "R3. The third\n",
      "dimension is\n",
      "orthogonal to the\n",
      "surface of the\n",
      "elliptical disk.\n",
      "Bottom-right to\n",
      "top-right:U\n",
      "performs a basis\n",
      "change within R3.v2\n",
      "v1\n",
      " \u001b2u2\n",
      "\u001b1u1\n",
      "e2\n",
      "e1\u001b2e2\n",
      "\u001b1e1A\n",
      "V>\n",
      "\u0006U\n",
      "0>row vectors from n+ 1tombelow so that\n",
      "\u0006=2\n",
      "666666664\u001b10 0\n",
      "0...0\n",
      "0 0\u001bn\n",
      "0::: 0\n",
      "......\n",
      "0::: 03\n",
      "777777775: (4.65)\n",
      "Ifm < n , the matrix \u0006has a diagonal structure up to column mand\n",
      "columns that consist of 0fromm+ 1ton:\n",
      "\u0006=2\n",
      "64\u001b10 0 0 :::0\n",
      "0...0......\n",
      "0 0\u001bm0:::03\n",
      "75: (4.66)\n",
      "Remark. The SVD exists for any matrix A2Rm\u0002n. }\n",
      "4.5.1 Geometric Intuitions for the SVD\n",
      "The SVD offers geometric intuitions to describe a transformation matrix\n",
      "A. In the following, we will discuss the SVD as sequential linear trans-\n",
      "formations performed on the bases. In Example 4.12, we will then apply\n",
      "transformation matrices of the SVD to a set of vectors in R2, which allows\n",
      "us to visualize the effect of each transformation more clearly.\n",
      "The SVD of a matrix can be interpreted as a decomposition of a corre-\n",
      "sponding linear mapping (recall Section 2.7.1) :Rn!Rminto three\n",
      "operations; see Figure 4.8. The SVD intuition follows superﬁcially a simi-\n",
      "lar structure to our eigendecomposition intuition, see Figure 4.7: Broadly\n",
      "speaking, the SVD performs a basis change via V>followed by a scal-\n",
      "ing and augmentation (or reduction) in dimensionality via the singular\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.5 Singular Value Decomposition 121\n",
      "value matrix \u0006. Finally, it performs a second basis change via U. The SVD\n",
      "entails a number of important details and caveats, which is why we will\n",
      "review our intuition in more detail. It is useful to review\n",
      "basis changes\n",
      "(Section 2.7.2),\n",
      "orthogonal matrices\n",
      "(Deﬁnition 3.8) and\n",
      "orthonormal bases\n",
      "(Section 3.5).Assume we are given a transformation matrix of a linear mapping :\n",
      "Rn!Rmwith respect to the standard bases BandCofRnandRm,\n",
      "respectively. Moreover, assume a second basis ~BofRnand~CofRm. Then\n",
      "1. The matrix Vperforms a basis change in the domain Rnfrom ~B(rep-\n",
      "resented by the red and orange vectors v1andv2in the top-left of Fig-\n",
      "ure 4.8) to the standard basis B.V>=V\u00001performs a basis change\n",
      "fromBto~B. The red and orange vectors are now aligned with the\n",
      "canonical basis in the bottom-left of Figure 4.8.\n",
      "2. Having changed the coordinate system to ~B,\u0006scales the new coordi-\n",
      "nates by the singular values \u001bi(and adds or deletes dimensions), i.e.,\n",
      "\u0006is the transformation matrix ofwith respect to ~Band ~C, rep-\n",
      "resented by the red and orange vectors being stretched and lying in\n",
      "thee1-e2plane, which is now embedded in a third dimension in the\n",
      "bottom-right of Figure 4.8.\n",
      "3.Uperforms a basis change in the codomain Rmfrom ~Cinto the canoni-\n",
      "cal basis of Rm, represented by a rotation of the red and orange vectors\n",
      "out of thee1-e2plane. This is shown in the top-right of Figure 4.8.\n",
      "The SVD expresses a change of basis in both the domain and codomain.\n",
      "This is in contrast with the eigendecomposition that operates within the\n",
      "same vector space, where the same basis change is applied and then un-\n",
      "done. What makes the SVD special is that these two different bases are\n",
      "simultaneously linked by the singular value matrix \u0006.\n",
      "Example 4.12 (Vectors and the SVD)\n",
      "Consider a mapping of a square grid of vectors X2R2that ﬁt in a box of\n",
      "size2\u00022centered at the origin. Using the standard basis, we map these\n",
      "vectors using\n",
      "A=2\n",
      "41\u00000:8\n",
      "0 1\n",
      "1 03\n",
      "5=U\u0006V>(4.67a)\n",
      "=2\n",
      "4\u00000:79 0\u00000:62\n",
      "0:38\u00000:78\u00000:49\n",
      "\u00000:48\u00000:62 0:623\n",
      "52\n",
      "41:62 0\n",
      "0 1:0\n",
      "0 03\n",
      "5\u0014\u00000:78 0:62\n",
      "\u00000:62\u00000:78\u0015\n",
      ":(4.67b)\n",
      "We start with a set of vectors X(colored dots; see top-left panel of Fig-\n",
      "ure 4.9) arranged in a grid. We then apply V>2R2\u00022, which rotatesX.\n",
      "The rotated vectors are shown in the bottom-left panel of Figure 4.9. We\n",
      "now map these vectors using the singular value matrix \u0006to the codomain\n",
      "R3(see the bottom-right panel in Figure 4.9). Note that all vectors lie in\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "122 Matrix Decompositions\n",
      "thex1-x2plane. The third coordinate is always 0. The vectors in the x1-x2\n",
      "plane have been stretched by the singular values.\n",
      "The direct mapping of the vectors XbyAto the codomain R3equals\n",
      "the transformation of XbyU\u0006V>, whereUperforms a rotation within\n",
      "the codomain R3so that the mapped vectors are no longer restricted to\n",
      "thex1-x2plane; they still are on a plane as shown in the top-right panel\n",
      "of Figure 4.9.\n",
      "Figure 4.9 SVD and\n",
      "mapping of vectors\n",
      "(represented by\n",
      "discs). The panels\n",
      "follow the same\n",
      "anti-clockwise\n",
      "structure of\n",
      "Figure 4.8.\n",
      "−1.5−1.0−0.5 0.0 0.5 1.0 1.5\n",
      "x1−1.5−1.0−0.50.00.51.01.5x2\n",
      "x1-1.5-0.5\n",
      "0.5\n",
      "1.5x2\n",
      "-1.5-0.50.51.5x3\n",
      "-1.0-0.50.00.51.0\n",
      "−1.5−1.0−0.5 0.0 0.5 1.0 1.5\n",
      "x1−1.5−1.0−0.50.00.51.01.5x2\n",
      "x1-1.5-0.50.51.5x2\n",
      "-1.5-0.50.51.5x3\n",
      "0\n",
      "4.5.2 Construction of the SVD\n",
      "We will next discuss why the SVD exists and show how to compute it\n",
      "in detail. The SVD of a general matrix shares some similarities with the\n",
      "eigendecomposition of a square matrix.\n",
      "Remark. Compare the eigendecomposition of an SPD matrix\n",
      "S=S>=PDP>(4.68)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.5 Singular Value Decomposition 123\n",
      "with the corresponding SVD\n",
      "S=U\u0006V>: (4.69)\n",
      "If we set\n",
      "U=P=V;D=\u0006; (4.70)\n",
      "we see that the SVD of SPD matrices is their eigendecomposition. }\n",
      "In the following, we will explore why Theorem 4.22 holds and how\n",
      "the SVD is constructed. Computing the SVD of A2Rm\u0002nis equivalent\n",
      "to ﬁnding two sets of orthonormal bases U= (u1;:::;um)andV=\n",
      "(v1;:::;vn)of the codomain Rmand the domain Rn, respectively. From\n",
      "these ordered bases, we will construct the matrices UandV.\n",
      "Our plan is to start with constructing the orthonormal set of right-\n",
      "singular vectors v1;:::;vn2Rn. We then construct the orthonormal set\n",
      "of left-singular vectors u1;:::;um2Rm. Thereafter, we will link the two\n",
      "and require that the orthogonality of the viis preserved under the trans-\n",
      "formation ofA. This is important because we know that the images Avi\n",
      "form a set of orthogonal vectors. We will then normalize these images by\n",
      "scalar factors, which will turn out to be the singular values.\n",
      "Let us begin with constructing the right-singular vectors. The spectral\n",
      "theorem (Theorem 4.15) tells us that the eigenvectors of a symmetric\n",
      "matrix form an ONB, which also means it can be diagonalized. More-\n",
      "over, from Theorem 4.14 we can always construct a symmetric, positive\n",
      "semideﬁnite matrix A>A2Rn\u0002nfrom any rectangular matrix A2\n",
      "Rm\u0002n. Thus, we can always diagonalize A>Aand obtain\n",
      "A>A=PDP>=P2\n",
      "64\u00151\u0001\u0001\u0001 0\n",
      ".........\n",
      "0\u0001\u0001\u0001\u0015n3\n",
      "75P>; (4.71)\n",
      "wherePis an orthogonal matrix, which is composed of the orthonormal\n",
      "eigenbasis. The \u0015i>0are the eigenvalues of A>A. Let us assume the\n",
      "SVD ofAexists and inject (4.64) into (4.71). This yields\n",
      "A>A= (U\u0006V>)>(U\u0006V>) =V\u0006>U>U\u0006V>; (4.72)\n",
      "whereU;Vare orthogonal matrices. Therefore, with U>U=Iwe ob-\n",
      "tain\n",
      "A>A=V\u0006>\u0006V>=V2\n",
      "64\u001b2\n",
      "10 0\n",
      "0...0\n",
      "0 0\u001b2\n",
      "n3\n",
      "75V>: (4.73)\n",
      "Comparing now (4.71) and (4.73), we identify\n",
      "V>=P>; (4.74)\n",
      "\u001b2\n",
      "i=\u0015i: (4.75)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "124 Matrix Decompositions\n",
      "Therefore, the eigenvectors of A>Athat compose Pare the right-singular\n",
      "vectorsVofA(see (4.74)). The eigenvalues of A>Aare the squared\n",
      "singular values of \u0006(see (4.75)).\n",
      "To obtain the left-singular vectors U, we follow a similar procedure.\n",
      "We start by computing the SVD of the symmetric matrix AA>2Rm\u0002m\n",
      "(instead of the previous A>A2Rn\u0002n). The SVD of Ayields\n",
      "AA>= (U\u0006V>)(U\u0006V>)>=U\u0006V>V\u0006>U>(4.76a)\n",
      "=U2\n",
      "64\u001b2\n",
      "10 0\n",
      "0...0\n",
      "0 0\u001b2\n",
      "m3\n",
      "75U>: (4.76b)\n",
      "The spectral theorem tells us that AA>=SDS>can be diagonalized\n",
      "and we can ﬁnd an ONB of eigenvectors of AA>, which are collected in\n",
      "S. The orthonormal eigenvectors of AA>are the left-singular vectors U\n",
      "and form an orthonormal basis in the codomain of the SVD.\n",
      "This leaves the question of the structure of the matrix \u0006. SinceAA>\n",
      "andA>Ahave the same nonzero eigenvalues (see page 106), the nonzero\n",
      "entries of the \u0006matrices in the SVD for both cases have to be the same.\n",
      "The last step is to link up all the parts we touched upon so far. We have\n",
      "an orthonormal set of right-singular vectors in V. To ﬁnish the construc-\n",
      "tion of the SVD, we connect them with the orthonormal vectors U. To\n",
      "reach this goal, we use the fact the images of the viunderAhave to be\n",
      "orthogonal, too. We can show this by using the results from Section 3.4.\n",
      "We require that the inner product between AviandAvjmust be 0for\n",
      "i6=j. For any two orthogonal eigenvectors vi;vj,i6=j, it holds that\n",
      "(Avi)>(Avj) =v>\n",
      "i(A>A)vj=v>\n",
      "i(\u0015jvj) =\u0015jv>\n",
      "ivj= 0: (4.77)\n",
      "For the case m>r, it holds thatfAv1;:::;Avrgis a basis of an r-\n",
      "dimensional subspace of Rm.\n",
      "To complete the SVD construction, we need left-singular vectors that\n",
      "are ortho normal : We normalize the images of the right-singular vectors\n",
      "Aviand obtain\n",
      "ui:=Avi\n",
      "kAvik=1p\u0015iAvi=1\n",
      "\u001biAvi; (4.78)\n",
      "where the last equality was obtained from (4.75) and (4.76b), showing\n",
      "us that the eigenvalues of AA>are such that \u001b2\n",
      "i=\u0015i.\n",
      "Therefore, the eigenvectors of A>A, which we know are the right-\n",
      "singular vectors vi, and their normalized images under A, the left-singular\n",
      "vectorsui, form two self-consistent ONBs that are connected through the\n",
      "singular value matrix \u0006.\n",
      "Let us rearrange (4.78) to obtain the singular value equation singular value\n",
      "equation\n",
      "Avi=\u001biui; i= 1;:::;r: (4.79)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.5 Singular Value Decomposition 125\n",
      "This equation closely resembles the eigenvalue equation (4.25), but the\n",
      "vectors on the left- and the right-hand sides are not the same.\n",
      "Forn < m , (4.79) holds only for i6n, but (4.79) says nothing about\n",
      "theuifori > n . However, we know by construction that they are or-\n",
      "thonormal. Conversely, for m<n , (4.79) holds only for i6m. Fori>m ,\n",
      "we haveAvi=0and we still know that the viform an orthonormal set.\n",
      "This means that the SVD also supplies an orthonormal basis of the kernel\n",
      "(null space) of A, the set of vectors xwithAx=0(see Section 2.7.3).\n",
      "Concatenating the vias the columns of Vand theuias the columns of\n",
      "Uyields\n",
      "AV=U\u0006; (4.80)\n",
      "where \u0006has the same dimensions as Aand a diagonal structure for rows\n",
      "1;:::;r . Hence, right-multiplying with V>yieldsA=U\u0006V>, which is\n",
      "the SVD ofA.\n",
      "Example 4.13 (Computing the SVD)\n",
      "Let us ﬁnd the singular value decomposition of\n",
      "A=\u00141 0 1\n",
      "\u00002 1 0\u0015\n",
      ": (4.81)\n",
      "The SVD requires us to compute the right-singular vectors vj, the singular\n",
      "values\u001bk, and the left-singular vectors ui.\n",
      "Step 1: Right-singular vectors as the eigenbasis of A>A.\n",
      "We start by computing\n",
      "A>A=2\n",
      "41\u00002\n",
      "0 1\n",
      "1 03\n",
      "5\u00141 0 1\n",
      "\u00002 1 0\u0015\n",
      "=2\n",
      "45\u00002 1\n",
      "\u00002 1 0\n",
      "1 0 13\n",
      "5: (4.82)\n",
      "We compute the singular values and right-singular vectors vjthrough\n",
      "the eigenvalue decomposition of A>A, which is given as\n",
      "A>A=2\n",
      "645p\n",
      "300\u00001p\n",
      "6\n",
      "\u00002p\n",
      "301p\n",
      "5\u00002p\n",
      "6\n",
      "1p\n",
      "302p\n",
      "51p\n",
      "63\n",
      "752\n",
      "46 0 0\n",
      "0 1 0\n",
      "0 0 03\n",
      "52\n",
      "645p\n",
      "30\u00002p\n",
      "301p\n",
      "30\n",
      "01p\n",
      "52p\n",
      "5\n",
      "\u00001p\n",
      "6\u00002p\n",
      "61p\n",
      "63\n",
      "75=PDP>;\n",
      "(4.83)\n",
      "and we obtain the right-singular vectors as the columns of Pso that\n",
      "V=P=2\n",
      "645p\n",
      "300\u00001p\n",
      "6\n",
      "\u00002p\n",
      "301p\n",
      "5\u00002p\n",
      "6\n",
      "1p\n",
      "302p\n",
      "51p\n",
      "63\n",
      "75: (4.84)\n",
      "Step 2: Singular-value matrix.\n",
      "As the singular values \u001biare the square roots of the eigenvalues of\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "126 Matrix Decompositions\n",
      "A>Awe obtain them straight from D. Since rk(A) = 2 , there are only\n",
      "two nonzero singular values: \u001b1=p\n",
      "6and\u001b2= 1. The singular value\n",
      "matrix must be the same size as A, and we obtain\n",
      "\u0006=\u0014p\n",
      "6 0 0\n",
      "0 1 0\u0015\n",
      ": (4.85)\n",
      "Step 3: Left-singular vectors as the normalized image of the right-\n",
      "singular vectors.\n",
      "We ﬁnd the left-singular vectors by computing the image of the right-\n",
      "singular vectors under Aand normalizing them by dividing them by their\n",
      "corresponding singular value. We obtain\n",
      "u1=1\n",
      "\u001b1Av1=1p\n",
      "6\u00141 0 1\n",
      "\u00002 1 0\u00152\n",
      "645p\n",
      "30\n",
      "\u00002p\n",
      "30\n",
      "1p\n",
      "303\n",
      "75=\"\n",
      "1p\n",
      "5\n",
      "\u00002p\n",
      "5#\n",
      "; (4.86)\n",
      "u2=1\n",
      "\u001b2Av2=1\n",
      "1\u00141 0 1\n",
      "\u00002 1 0\u00152\n",
      "40\n",
      "1p\n",
      "5\n",
      "2p\n",
      "53\n",
      "5=\"\n",
      "2p\n",
      "5\n",
      "1p\n",
      "5#\n",
      "; (4.87)\n",
      "U= [u1;u2] =1p\n",
      "5\u00141 2\n",
      "\u00002 1\u0015\n",
      ": (4.88)\n",
      "Note that on a computer the approach illustrated here has poor numerical\n",
      "behavior, and the SVD of Ais normally computed without resorting to the\n",
      "eigenvalue decomposition of A>A.\n",
      "4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition\n",
      "Let us consider the eigendecomposition A=PDP\u00001and the SVDA=\n",
      "U\u0006V>and review the core elements of the past sections.\n",
      "The SVD always exists for any matrix Rm\u0002n. The eigendecomposition is\n",
      "only deﬁned for square matrices Rn\u0002nand only exists if we can ﬁnd a\n",
      "basis of eigenvectors of Rn.\n",
      "The vectors in the eigendecomposition matrix Pare not necessarily\n",
      "orthogonal, i.e., the change of basis is not a simple rotation and scaling.\n",
      "On the other hand, the vectors in the matrices UandVin the SVD are\n",
      "orthonormal, so they do represent rotations.\n",
      "Both the eigendecomposition and the SVD are compositions of three\n",
      "linear mappings:\n",
      "1. Change of basis in the domain\n",
      "2. Independent scaling of each new basis vector and mapping from do-\n",
      "main to codomain\n",
      "3. Change of basis in the codomain\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.5 Singular Value Decomposition 127\n",
      "Figure 4.10 Movie\n",
      "ratings of three\n",
      "people for four\n",
      "movies and its SVD\n",
      "decomposition. 5 4 1\n",
      "5 5 0\n",
      "0 0 5\n",
      "1 0 42\n",
      "6643\n",
      "775Ali\n",
      "Beatrix\n",
      "Chandra\n",
      "Star Wars\n",
      "Blade Runner\n",
      "Amelie\n",
      "Delicatessen=\u00000:6710 0:0236 0:4647\u00000:5774\n",
      "\u00000:7197 0:2054\u00000:4759 0:4619\n",
      "\u00000:0939\u00000:7705\u00000:5268\u00000:3464\n",
      "\u00000:1515\u00000:6030 0:5293\u00000:57742\n",
      "66643\n",
      "7775\n",
      "9:6438 0 0\n",
      "06:3639 0\n",
      "0 00:7056\n",
      "0 0 02\n",
      "6643\n",
      "775\n",
      "\u00000:7367\u00000:6515\u00000:1811\n",
      "0:0852 0:1762\u00000:9807\n",
      "0:6708\u00000:7379\u00000:07432\n",
      "643\n",
      "75\n",
      "A key difference between the eigendecomposition and the SVD is that\n",
      "in the SVD, domain and codomain can be vector spaces of different\n",
      "dimensions.\n",
      "In the SVD, the left- and right-singular vector matrices UandVare\n",
      "generally not inverse of each other (they perform basis changes in dif-\n",
      "ferent vector spaces). In the eigendecomposition, the basis change ma-\n",
      "tricesPandP\u00001are inverses of each other.\n",
      "In the SVD, the entries in the diagonal matrix \u0006are all real and non-\n",
      "negative, which is not generally true for the diagonal matrix in the\n",
      "eigendecomposition.\n",
      "The SVD and the eigendecomposition are closely related through their\n",
      "projections\n",
      "–The left-singular vectors of Aare eigenvectors of AA>\n",
      "–The right-singular vectors of Aare eigenvectors of A>A.\n",
      "–The nonzero singular values of Aare the square roots of the nonzero\n",
      "eigenvalues of both AA>andA>A.\n",
      "For symmetric matrices A2Rn\u0002n, the eigenvalue decomposition and\n",
      "the SVD are one and the same, which follows from the spectral theo-\n",
      "rem 4.15.\n",
      "Example 4.14 (Finding Structure in Movie Ratings and Consumers)\n",
      "Let us add a practical interpretation of the SVD by analyzing data on\n",
      "people and their preferred movies. Consider three viewers (Ali, Beatrix,\n",
      "Chandra) rating four different movies ( Star Wars ,Blade Runner ,Amelie ,\n",
      "Delicatessen ). Their ratings are values between 0(worst) and 5(best) and\n",
      "encoded in a data matrix A2R4\u00023as shown in Figure 4.10. Each row\n",
      "represents a movie and each column a user. Thus, the column vectors of\n",
      "movie ratings, one for each viewer, are xAli,xBeatrix ,xChandra .\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "128 Matrix Decompositions\n",
      "FactoringAusing the SVD offers us a way to capture the relationships\n",
      "of how people rate movies, and especially if there is a structure linking\n",
      "which people like which movies. Applying the SVD to our data matrix A\n",
      "makes a number of assumptions:\n",
      "1. All viewers rate movies consistently using the same linear mapping.\n",
      "2. There are no errors or noise in the ratings.\n",
      "3. We interpret the left-singular vectors uias stereotypical movies and\n",
      "the right-singular vectors vjas stereotypical viewers.\n",
      "We then make the assumption that any viewer’s speciﬁc movie preferences\n",
      "can be expressed as a linear combination of the vj. Similarly, any movie’s\n",
      "like-ability can be expressed as a linear combination of the ui. Therefore,\n",
      "a vector in the domain of the SVD can be interpreted as a viewer in the\n",
      "“space” of stereotypical viewers, and a vector in the codomain of the SVD\n",
      "correspondingly as a movie in the “space” of stereotypical movies. Let us These two “spaces”\n",
      "are only\n",
      "meaningfully\n",
      "spanned by the\n",
      "respective viewer\n",
      "and movie data if\n",
      "the data itself covers\n",
      "a sufﬁcient diversity\n",
      "of viewers and\n",
      "movies.inspect the SVD of our movie-user matrix. The ﬁrst left-singular vector u1\n",
      "has large absolute values for the two science ﬁction movies and a large\n",
      "ﬁrst singular value (red shading in Figure 4.10). Thus, this groups a type\n",
      "of users with a speciﬁc set of movies (science ﬁction theme). Similarly, the\n",
      "ﬁrst right-singular v1shows large absolute values for Ali and Beatrix, who\n",
      "give high ratings to science ﬁction movies (green shading in Figure 4.10).\n",
      "This suggests that v1reﬂects the notion of a science ﬁction lover.\n",
      "Similarly,u2, seems to capture a French art house ﬁlm theme, and v2in-\n",
      "dicates that Chandra is close to an idealized lover of such movies. An ide-\n",
      "alized science ﬁction lover is a purist and only loves science ﬁction movies,\n",
      "so a science ﬁction lover v1gives a rating of zero to everything but science\n",
      "ﬁction themed—this logic is implied by the diagonal substructure for the\n",
      "singular value matrix \u0006. A speciﬁc movie is therefore represented by how\n",
      "it decomposes (linearly) into its stereotypical movies. Likewise, a person\n",
      "would be represented by how they decompose (via linear combination)\n",
      "into movie themes.\n",
      "It is worth to brieﬂy discuss SVD terminology and conventions, as there\n",
      "are different versions used in the literature. While these differences can\n",
      "be confusing, the mathematics remains invariant to them.\n",
      "For convenience in notation and abstraction, we use an SVD notation\n",
      "where the SVD is described as having two square left- and right-singular\n",
      "vector matrices, but a non-square singular value matrix. Our deﬁni-\n",
      "tion (4.64) for the SVD is sometimes called the full SVD . full SVD\n",
      "Some authors deﬁne the SVD a bit differently and focus on square sin-\n",
      "gular matrices. Then, for A2Rm\u0002nandm>n,\n",
      "A\n",
      "m\u0002n=U\n",
      "m\u0002n\u0006\n",
      "n\u0002nV>\n",
      "n\u0002n: (4.89)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.6 Matrix Approximation 129\n",
      "Sometimes this formulation is called the reduced SVD (e.g., Datta (2010)) reduced SVD\n",
      "ortheSVD (e.g., Press et al. (2007)). This alternative format changes\n",
      "merely how the matrices are constructed but leaves the mathematical\n",
      "structure of the SVD unchanged. The convenience of this alternative\n",
      "formulation is that \u0006is diagonal, as in the eigenvalue decomposition.\n",
      "In Section 4.6, we will learn about matrix approximation techniques\n",
      "using the SVD, which is also called the truncated SVD . truncated SVD\n",
      "It is possible to deﬁne the SVD of a rank- rmatrixAso thatUis an\n",
      "m\u0002rmatrix, \u0006a diagonal matrix of size r\u0002r, andVann\u0002rmatrix.\n",
      "This construction is very similar to our deﬁnition and ensures that the\n",
      "diagonal matrix \u0006has only nonzero entries along the diagonal. The\n",
      "main convenience of this alternative notation is that \u0006is diagonal, as\n",
      "in the eigenvalue decomposition.\n",
      "A restriction that the SVD for Aonly applies to m\u0002nmatrices with\n",
      "m>n is practically unnecessary. When m<n , the SVD decomposition\n",
      "will yield \u0006with more zero columns than rows and, consequently, the\n",
      "singular values \u001bm+1;:::;\u001bnare0.\n",
      "The SVD is used in a variety of applications in machine learning from\n",
      "least-squares problems in curve ﬁtting to solving systems of linear equa-\n",
      "tions. These applications harness various important properties of the SVD,\n",
      "its relation to the rank of a matrix, and its ability to approximate matrices\n",
      "of a given rank with lower-rank matrices. Substituting a matrix with its\n",
      "SVD has often the advantage of making calculation more robust to nu-\n",
      "merical rounding errors. As we will explore in the next section, the SVD’s\n",
      "ability to approximate matrices with “simpler” matrices in a principled\n",
      "manner opens up machine learning applications ranging from dimension-\n",
      "ality reduction and topic modeling to data compression and clustering.\n",
      "4.6 Matrix Approximation\n",
      "We considered the SVD as a way to factorize A=U\u0006V>2Rm\u0002ninto\n",
      "the product of three matrices, where U2Rm\u0002mandV2Rn\u0002nare or-\n",
      "thogonal and \u0006contains the singular values on its main diagonal. Instead\n",
      "of doing the full SVD factorization, we will now investigate how the SVD\n",
      "allows us to represent a matrix Aas a sum of simpler (low-rank) matrices\n",
      "Ai, which lends itself to a matrix approximation scheme that is cheaper\n",
      "to compute than the full SVD.\n",
      "We construct a rank- 1matrixAi2Rm\u0002nas\n",
      "Ai:=uiv>\n",
      "i; (4.90)\n",
      "which is formed by the outer product of the ith orthogonal column vector\n",
      "ofUandV. Figure 4.11 shows an image of Stonehenge, which can be\n",
      "represented by a matrix A2R1432\u00021910, and some outer products Ai, as\n",
      "deﬁned in (4.90).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "130 Matrix Decompositions\n",
      "Figure 4.11 Image\n",
      "processing with the\n",
      "SVD. (a) The\n",
      "original grayscale\n",
      "image is a\n",
      "1;432\u00021;910\n",
      "matrix of values\n",
      "between 0(black)\n",
      "and1(white).\n",
      "(b)–(f) Rank- 1\n",
      "matrices\n",
      "A1;:::;A5and\n",
      "their corresponding\n",
      "singular values\n",
      "\u001b1;:::;\u001b 5. The\n",
      "grid-like structure of\n",
      "each rank- 1matrix\n",
      "is imposed by the\n",
      "outer-product of the\n",
      "left and\n",
      "right-singular\n",
      "vectors.\n",
      "(a) Original image A.\n",
      " (b)A1; \u001b1\u0019228;052.\n",
      " (c)A2; \u001b2\u001940;647.\n",
      "(d)A3; \u001b3\u001926;125.\n",
      " (e)A4; \u001b4\u001920;232.\n",
      " (f)A5; \u001b5\u001915;436.\n",
      "A matrixA2Rm\u0002nof rankrcan be written as a sum of rank-1 matrices\n",
      "Aiso that\n",
      "A=rX\n",
      "i=1\u001biuiv>\n",
      "i=rX\n",
      "i=1\u001biAi; (4.91)\n",
      "where the outer-product matrices Aiare weighted by the ith singular\n",
      "value\u001bi. We can see why (4.91) holds: The diagonal structure of the\n",
      "singular value matrix \u0006multiplies only matching left- and right-singular\n",
      "vectorsuiv>\n",
      "iand scales them by the corresponding singular value \u001bi. All\n",
      "terms \u0006ijuiv>\n",
      "jvanish fori6=jbecause \u0006is a diagonal matrix. Any terms\n",
      "i>r vanish because the corresponding singular values are 0.\n",
      "In (4.90), we introduced rank- 1matricesAi. We summed up the rin-\n",
      "dividual rank- 1matrices to obtain a rank- rmatrixA; see (4.91). If the\n",
      "sum does not run over all matrices Ai,i= 1;:::;r , but only up to an\n",
      "intermediate value k<r , we obtain a rank-kapproximation rank-k\n",
      "approximation\n",
      "bA(k) :=kX\n",
      "i=1\u001biuiv>\n",
      "i=kX\n",
      "i=1\u001biAi (4.92)\n",
      "ofAwith rk(bA(k)) =k. Figure 4.12 shows low-rank approximations\n",
      "bA(k)of an original image Aof Stonehenge. The shape of the rocks be-\n",
      "comes increasingly visible and clearly recognizable in the rank- 5approx-\n",
      "imation. While the original image requires 1;432\u00011;910 = 2;735;120\n",
      "numbers, the rank- 5approximation requires us only to store the ﬁve sin-\n",
      "gular values and the ﬁve left- and right-singular vectors ( 1;432and1;910-\n",
      "dimensional each) for a total of 5\u0001(1;432+1;910+1) = 16 ;715numbers\n",
      "– just above 0:6%of the original.\n",
      "To measure the difference (error) between Aand its rank- kapproxima-\n",
      "tionbA(k), we need the notion of a norm. In Section 3.1, we already used\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.6 Matrix Approximation 131\n",
      "Figure 4.12 Image\n",
      "reconstruction with\n",
      "the SVD. (a)\n",
      "Original image.\n",
      "(b)–(f) Image\n",
      "reconstruction using\n",
      "the low-rank\n",
      "approximation of\n",
      "the SVD, where the\n",
      "rank-k\n",
      "approximation is\n",
      "given bybA(k) =Pk\n",
      "i=1\u001biAi.\n",
      "(a) Original image A.\n",
      " (b) Rank-1 approximation bA(1).\n",
      "(c) Rank-2 approximation bA(2).\n",
      "(d) Rank-3 approximation bA(3).\n",
      "(e) Rank-4 approximation bA(4).\n",
      "(f) Rank-5 approximation bA(5).\n",
      "norms on vectors that measure the length of a vector. By analogy we can\n",
      "also deﬁne norms on matrices.\n",
      "Deﬁnition 4.23 (Spectral Norm of a Matrix) .Forx2Rnnf0g, the spectral spectral norm\n",
      "norm of a matrixA2Rm\u0002nis deﬁned as\n",
      "kAk2:= max\n",
      "xkAxk2\n",
      "kxk2: (4.93)\n",
      "We introduce the notation of a subscript in the matrix norm (left-hand\n",
      "side), similar to the Euclidean norm for vectors (right-hand side), which\n",
      "has subscript 2. The spectral norm (4.93) determines how long any vector\n",
      "xcan at most become when multiplied by A.\n",
      "Theorem 4.24. The spectral norm of Ais its largest singular value \u001b1.\n",
      "We leave the proof of this theorem as an exercise.\n",
      "Eckart-Young\n",
      "theorem Theorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)) .Con-\n",
      "sider a matrix A2Rm\u0002nof rankrand letB2Rm\u0002nbe a matrix of rank\n",
      "k. For anyk6rwithbA(k) =Pk\n",
      "i=1\u001biuiv>\n",
      "iit holds that\n",
      "A\u0000bA(k) argminrk(B)=kkA\u0000Bk2; (4.94)\n",
      "2=\u001bk+1: (4.95)\n",
      "The Eckart-Young theorem states explicitly how much error we intro-\n",
      "duce by approximating Ausing a rank- kapproximation. We can inter-\n",
      "pret the rank- kapproximation obtained with the SVD as a projection of\n",
      "the full-rank matrix Aonto a lower-dimensional space of rank-at-most- k\n",
      "matrices. Of all possible projections, the SVD minimizes the error (with\n",
      "respect to the spectral norm) between Aand any rank- kapproximation.\n",
      "We can retrace some of the steps to understand why (4.95) should hold.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "132 Matrix Decompositions\n",
      "We observe that the difference between A\u0000bA(k)is a matrix containing\n",
      "the sum of the remaining rank- 1matrices\n",
      "A\u0000bA(k) =rX\n",
      "i=k+1\u001biuiv>\n",
      "i: (4.96)\n",
      "By Theorem 4.24, we immediately obtain \u001bk+1as the spectral norm of the\n",
      "difference matrix. Let us have a closer look at (4.94). If we assume that\n",
      "there is another matrix Bwith rk(B)6k, such that\n",
      "A\u0000bA(k)\n",
      "2; (4.97)\n",
      "then there exists an at least ( n\u0000k)-dimensional null space Z\u0012Rn, such\n",
      "thatx2Zimplies thatBx=0. Then it follows that\n",
      "kAxk2=k(A\u0000B)xk2; (4.98)\n",
      "and by using a version of the Cauchy-Schwartz inequality (3.17) that en-\n",
      "compasses norms of matrices, we obtain\n",
      "kAxk26kA\u0000Bk2kxk2<\u001bk+1kxk2: (4.99)\n",
      "However, there exists a (k+ 1)-dimensional subspace where kAxk2>\n",
      "\u001bk+1kxk2, which is spanned by the right-singular vectors vj;j6k+ 1of\n",
      "A. Adding up dimensions of these two spaces yields a number greater than\n",
      "n, as there must be a nonzero vector in both spaces. This is a contradiction\n",
      "of the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\n",
      "The Eckart-Young theorem implies that we can use SVD to reduce a\n",
      "rank-rmatrixAto a rank-kmatrixbAin a principled, optimal (in the\n",
      "spectral norm sense) manner. We can interpret the approximation of Aby\n",
      "a rank-kmatrix as a form of lossy compression. Therefore, the low-rank\n",
      "approximation of a matrix appears in many machine learning applications,\n",
      "e.g., image processing, noise ﬁltering, and regularization of ill-posed prob-\n",
      "lems. Furthermore, it plays a key role in dimensionality reduction and\n",
      "principal component analysis, as we will see in Chapter 10.\n",
      "Example 4.15 (Finding Structure in Movie Ratings and Consumers\n",
      "(continued))\n",
      "Coming back to our movie-rating example, we can now apply the con-\n",
      "cept of low-rank approximations to approximate the original data matrix.\n",
      "Recall that our ﬁrst singular value captures the notion of science ﬁction\n",
      "theme in movies and science ﬁction lovers. Thus, by using only the ﬁrst\n",
      "singular value term in a rank- 1decomposition of the movie-rating matrix,\n",
      "we obtain the predicted ratings\n",
      "A1=u1v>\n",
      "1=2\n",
      "664\u00000:6710\n",
      "\u00000:7197\n",
      "\u00000:0939\n",
      "\u00000:15153\n",
      "775\u0002\u00000:7367\u00000:6515\u00000:1811\u0003\n",
      "(4.100a)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.7 Matrix Phylogeny 133\n",
      "=2\n",
      "6640:4943 0:4372 0:1215\n",
      "0:5302 0:4689 0:1303\n",
      "0:0692 0:0612 0:0170\n",
      "0:1116 0:0987 0:02743\n",
      "775: (4.100b)\n",
      "This ﬁrst rank- 1approximation A1is insightful: it tells us that Ali and\n",
      "Beatrix like science ﬁction movies, such as Star Wars and Bladerunner\n",
      "(entries have values >0:4), but fails to capture the ratings of the other\n",
      "movies by Chandra. This is not surprising, as Chandra’s type of movies is\n",
      "not captured by the ﬁrst singular value. The second singular value gives\n",
      "us a better rank- 1approximation for those movie-theme lovers:\n",
      "A2=u2v>\n",
      "2=2\n",
      "6640:0236\n",
      "0:2054\n",
      "\u00000:7705\n",
      "\u00000:60303\n",
      "775\u00020:0852 0:1762\u00000:9807\u0003\n",
      "(4.101a)\n",
      "=2\n",
      "6640:0020 0:0042\u00000:0231\n",
      "0:0175 0:0362\u00000:2014\n",
      "\u00000:0656\u00000:1358 0:7556\n",
      "\u00000:0514\u00000:1063 0:59143\n",
      "775: (4.101b)\n",
      "In this second rank- 1approximation A2, we capture Chandra’s ratings\n",
      "and movie types well, but not the science ﬁction movies. This leads us to\n",
      "consider the rank- 2approximation bA(2), where we combine the ﬁrst two\n",
      "rank- 1approximations\n",
      "bA(2) =\u001b1A1+\u001b2A2=2\n",
      "6644:7801 4:2419 1:0244\n",
      "5:2252 4:7522\u00000:0250\n",
      "0:2493\u00000:2743 4:9724\n",
      "0:7495 0:2756 4:02783\n",
      "775:(4.102)\n",
      "bA(2)is similar to the original movie ratings table\n",
      "A=2\n",
      "6645 4 1\n",
      "5 5 0\n",
      "0 0 5\n",
      "1 0 43\n",
      "775; (4.103)\n",
      "and this suggests that we can ignore the contribution of A3. We can in-\n",
      "terpret this so that in the data table there is no evidence of a third movie-\n",
      "theme/movie-lovers category. This also means that the entire space of\n",
      "movie-themes/movie-lovers in our example is a two-dimensional space\n",
      "spanned by science ﬁction and French art house movies and lovers.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "134 Matrix Decompositions\n",
      "Figure 4.13 A\n",
      "functional\n",
      "phylogeny of\n",
      "matrices\n",
      "encountered in\n",
      "machine learning.Real matrices\n",
      "9Pseudo-inverse\n",
      "9SVD\n",
      "Square\n",
      "9Determinant\n",
      "9TraceNonsquare\n",
      "DefectiveSingular\n",
      "Non-defective\n",
      "(diagonalizable)Singular\n",
      "Normal Non-normal\n",
      "Symmetric\n",
      "eigenvalues2R\n",
      "Positive deﬁnite\n",
      "Cholesky\n",
      "eigenvalues >0Diagonal\n",
      "Identity\n",
      "matrix9Inverse Matrix\n",
      "Regular\n",
      "(invertible)\n",
      "Orthogonal RotationRn\u0002nRn\u0002m\n",
      "No basis of\n",
      "eigenvectors\n",
      "Basis of\n",
      "eigenvectors\n",
      "A>A=AA>A>A6=AA>\n",
      "Columns are\n",
      "orthogonal\n",
      "eigenvectorsA>A=AA\n",
      ">\n",
      "=Idet\n",
      "6= 0det\n",
      "6= 0det = 0\n",
      "4.7 Matrix Phylogeny\n",
      "The word\n",
      "“phylogenetic”\n",
      "describes how we\n",
      "capture the\n",
      "relationships among\n",
      "individuals or\n",
      "groups and derived\n",
      "from the Greek\n",
      "words for “tribe”\n",
      "and “source”.In Chapters 2 and 3, we covered the basics of linear algebra and analytic\n",
      "geometry. In this chapter, we looked at fundamental characteristics of ma-\n",
      "trices and linear mappings. Figure 4.13 depicts the phylogenetic tree of\n",
      "relationships between different types of matrices (black arrows indicating\n",
      "“is a subset of”) and the covered operations we can perform on them (in\n",
      "blue). We consider all real matricesA2Rn\u0002m. For non-square matrices\n",
      "(wheren6=m), the SVD always exists, as we saw in this chapter. Focus-\n",
      "ing on square matrices A2Rn\u0002n, the determinant informs us whether a\n",
      "square matrix possesses an inverse matrix , i.e., whether it belongs to the\n",
      "class of regular, invertible matrices. If the square n\u0002nmatrix possesses n\n",
      "linearly independent eigenvectors, then the matrix is non-defective and an\n",
      "eigendecomposition exists (Theorem 4.12). We know that repeated eigen-\n",
      "values may result in defective matrices, which cannot be diagonalized.\n",
      "Non-singular and non-defective matrices are not the same. For exam-\n",
      "ple, a rotation matrix will be invertible (determinant is nonzero) but not\n",
      "diagonalizable in the real numbers (eigenvalues are not guaranteed to be\n",
      "real numbers).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "4.8 Further Reading 135\n",
      "We dive further into the branch of non-defective square n\u0002nmatrices.\n",
      "Aisnormal if the condition A>A=AA>holds. Moreover, if the more\n",
      "restrictive condition holds that A>A=AA>=I, thenAis called or-\n",
      "thogona l (see Deﬁnition 3.8). The set of orthogonal matrices is a subset of\n",
      "the regular (invertible) matrices and satisﬁes A>=A\u00001.\n",
      "Normal matrices have a frequently encountered subset, the symmetric\n",
      "matricesS2Rn\u0002n, which satisfy S=S>. Symmetric matrices have only\n",
      "real eigenvalues. A subset of the symmetric matrices consists of the pos-\n",
      "itive deﬁnite matrices Pthat satisfy the condition of x>Px>0for all\n",
      "x2Rnnf0g. In this case, a unique Cholesky decomposition exists (Theo-\n",
      "rem 4.18). Positive deﬁnite matrices have only positive eigenvalues and\n",
      "are always invertible (i.e., have a nonzero determinant).\n",
      "Another subset of symmetric matrices consists of the diagonal matrices\n",
      "D. Diagonal matrices are closed under multiplication and addition, but do\n",
      "not necessarily form a group (this is only the case if all diagonal entries\n",
      "are nonzero so that the matrix is invertible). A special diagonal matrix is\n",
      "the identity matrix I.\n",
      "4.8 Further Reading\n",
      "Most of the content in this chapter establishes underlying mathematics\n",
      "and connects them to methods for studying mappings, many of which are\n",
      "at the heart of machine learning at the level of underpinning software so-\n",
      "lutions and building blocks for almost all machine learning theory. Matrix\n",
      "characterization using determinants, eigenspectra, and eigenspaces pro-\n",
      "vides fundamental features and conditions for categorizing and analyzing\n",
      "matrices. This extends to all forms of representations of data and map-\n",
      "pings involving data, as well as judging the numerical stability of compu-\n",
      "tational operations on such matrices (Press et al., 2007).\n",
      "Determinants are fundamental tools in order to invert matrices and\n",
      "compute eigenvalues “by hand”. However, for almost all but the smallest\n",
      "instances, numerical computation by Gaussian elimination outperforms\n",
      "determinants (Press et al., 2007). Determinants remain nevertheless a\n",
      "powerful theoretical concept, e.g., to gain intuition about the orientation\n",
      "of a basis based on the sign of the determinant. Eigenvectors can be used\n",
      "to perform basis changes to transform data into the coordinates of mean-\n",
      "ingful orthogonal, feature vectors. Similarly, matrix decomposition meth-\n",
      "ods, such as the Cholesky decomposition, reappear often when we com-\n",
      "pute or simulate random events (Rubinstein and Kroese, 2016). Therefore,\n",
      "the Cholesky decomposition enables us to compute the reparametrization\n",
      "trick where we want to perform continuous differentiation over random\n",
      "variables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014;\n",
      "Kingma and Welling, 2014).\n",
      "Eigendecomposition is fundamental in enabling us to extract mean-\n",
      "ingful and interpretable information that characterizes linear mappings.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "136 Matrix Decompositions\n",
      "Therefore, the eigendecomposition underlies a general class of machine\n",
      "learning algorithms called spectral methods that perform eigendecomposi-\n",
      "tion of a positive-deﬁnite kernel. These spectral decomposition methods\n",
      "encompass classical approaches to statistical data analysis, such as the\n",
      "following:\n",
      "principal component\n",
      "analysis Principal component analysis (PCA (Pearson, 1901), see also Chapter 10),\n",
      "in which a low-dimensional subspace, which explains most of the vari-\n",
      "ability in the data, is sought. Fisher discriminant\n",
      "analysis Fisher discriminant analysis , which aims to determine a separating hy-\n",
      "perplane for data classiﬁcation (Mika et al., 1999). multidimensional\n",
      "scaling Multidimensional scaling (MDS) (Carroll and Chang, 1970).\n",
      "The computational efﬁciency of these methods typically comes from ﬁnd-\n",
      "ing the best rank- kapproximation to a symmetric, positive semideﬁnite\n",
      "matrix. More contemporary examples of spectral methods have different\n",
      "origins, but each of them requires the computation of the eigenvectors\n",
      "and eigenvalues of a positive-deﬁnite kernel, such as Isomap (Tenenbaum Isomap\n",
      "et al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), Hessian Laplacian\n",
      "eigenmaps\n",
      "Hessian eigenmapseigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and\n",
      "spectral clusteringMalik, 2000). The core computations of these are generally underpinned\n",
      "by low-rank matrix approximation techniques (Belabbas and Wolfe, 2009)\n",
      "as we encountered here via the SVD.\n",
      "The SVD allows us to discover some of the same kind of information as\n",
      "the eigendecomposition. However, the SVD is more generally applicable\n",
      "to non-square matrices and data tables. These matrix factorization meth-\n",
      "ods become relevant whenever we want to identify heterogeneity in data\n",
      "when we want to perform data compression by approximation, e.g., in-\n",
      "stead of storing n\u0002mvalues just storing (n+m)kvalues, or when we want\n",
      "to perform data pre-processing, e.g., to decorrelate predictor variables of\n",
      "a design matrix (Ormoneit et al., 2001). The SVD operates on matrices,\n",
      "which we can interpret as rectangular arrays with two indices (rows and\n",
      "columns). The extension of matrix-like structure to higher-dimensional\n",
      "arrays are called tensors. It turns out that the SVD is the special case of\n",
      "a more general family of decompositions that operate on such tensors\n",
      "(Kolda and Bader, 2009). SVD-like operations and low-rank approxima-\n",
      "tions on tensors are, for example, the Tucker decomposition (Tucker, 1966) Tucker\n",
      "decomposition or the CP decomposition (Carroll and Chang, 1970).\n",
      "CP decomposition The SVD low-rank approximation is frequently used in machine learn-\n",
      "ing for computational efﬁciency reasons. This is because it reduces the\n",
      "amount of memory and operations with nonzero multiplications we need\n",
      "to perform on potentially very large matrices of data (Trefethen and Bau III,\n",
      "1997). Moreover, low-rank approximations are used to operate on ma-\n",
      "trices that may contain missing values as well as for purposes of lossy\n",
      "compression and dimensionality reduction (Moonen and De Moor, 1995;\n",
      "Markovsky, 2011).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 137\n",
      "Exercises\n",
      "4.1 Compute the determinant using the Laplace expansion (using the ﬁrst row)\n",
      "and the Sarrus rule for\n",
      "A=2\n",
      "41 3 5\n",
      "2 4 6\n",
      "0 2 43\n",
      "5:\n",
      "4.2 Compute the following determinant efﬁciently:\n",
      "2\n",
      "666642 0 1 2 0\n",
      "2\u00001 0 1 1\n",
      "0 1 2 1 2\n",
      "\u00002 0 2\u00001 2\n",
      "2 0 0 1 13\n",
      "77775:\n",
      "4.3 Compute the eigenspaces of\n",
      "a.\n",
      "A:=\u0014\n",
      "1 0\n",
      "1 1\u0015\n",
      "b.\n",
      "B:=\u0014\n",
      "\u00002 2\n",
      "2 1\u0015\n",
      "4.4 Compute all eigenspaces of\n",
      "A=2\n",
      "6640\u00001 1 1\n",
      "\u00001 1\u00002 3\n",
      "2\u00001 0 0\n",
      "1\u00001 1 03\n",
      "775:\n",
      "4.5 Diagonalizability of a matrix is unrelated to its invertibility. Determine for\n",
      "the following four matrices whether they are diagonalizable and/or invert-\n",
      "ible\n",
      "\u0014\n",
      "1 0\n",
      "0 1\u0015\n",
      ";\u0014\n",
      "1 0\n",
      "0 0\u0015\n",
      ";\u0014\n",
      "1 1\n",
      "0 1\u0015\n",
      ";\u0014\n",
      "0 1\n",
      "0 0\u0015\n",
      ":\n",
      "4.6 Compute the eigenspaces of the following transformation matrices. Are they\n",
      "diagonalizable?\n",
      "a. For\n",
      "A=2\n",
      "42 3 0\n",
      "1 4 3\n",
      "0 0 13\n",
      "5\n",
      "b. For\n",
      "A=2\n",
      "6641 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 03\n",
      "775\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "138 Matrix Decompositions\n",
      "4.7 Are the following matrices diagonalizable? If yes, determine their diagonal\n",
      "form and a basis with respect to which the transformation matrices are di-\n",
      "agonal. If no, give reasons why they are not diagonalizable.\n",
      "a.\n",
      "A=\u0014\n",
      "0 1\n",
      "\u00008 4\u0015\n",
      "b.\n",
      "A=2\n",
      "41 1 1\n",
      "1 1 1\n",
      "1 1 13\n",
      "5\n",
      "c.\n",
      "A=2\n",
      "6645 4 2 1\n",
      "0 1\u00001\u00001\n",
      "\u00001\u00001 3 0\n",
      "1 1\u00001 23\n",
      "775\n",
      "d.\n",
      "A=2\n",
      "45\u00006\u00006\n",
      "\u00001 4 2\n",
      "3\u00006\u000043\n",
      "5\n",
      "4.8 Find the SVD of the matrix\n",
      "A=\u0014\n",
      "3 2 2\n",
      "2 3\u00002\u0015\n",
      ":\n",
      "4.9 Find the singular value decomposition of\n",
      "A=\u0014\n",
      "2 2\n",
      "\u00001 1\u0015\n",
      ":\n",
      "4.10 Find the rank-1 approximation of\n",
      "A=\u0014\n",
      "3 2 2\n",
      "2 3\u00002\u0015\n",
      "4.11 Show that for any A2Rm\u0002nthe matrices A>AandAA>possess the\n",
      "same nonzero eigenvalues.\n",
      "4.12 Show that for x6=0Theorem 4.24 holds, i.e., show that\n",
      "max\n",
      "xkAxk2\n",
      "kxk2=\u001b1;\n",
      "where\u001b1is the largest singular value of A2Rm\u0002n.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5\n",
      "Vector Calculus\n",
      "Many algorithms in machine learning optimize an objective function with\n",
      "respect to a set of desired model parameters that control how well a model\n",
      "explains the data: Finding good parameters can be phrased as an opti-\n",
      "mization problem (see Sections 8.2 and 8.3). Examples include: (i) lin-\n",
      "ear regression (see Chapter 9), where we look at curve-ﬁtting problems\n",
      "and optimize linear weight parameters to maximize the likelihood; (ii)\n",
      "neural-network auto-encoders for dimensionality reduction and data com-\n",
      "pression, where the parameters are the weights and biases of each layer,\n",
      "and where we minimize a reconstruction error by repeated application of\n",
      "the chain rule; and (iii) Gaussian mixture models (see Chapter 11) for\n",
      "modeling data distributions, where we optimize the location and shape\n",
      "parameters of each mixture component to maximize the likelihood of the\n",
      "model. Figure 5.1 illustrates some of these problems, which we typically\n",
      "solve by using optimization algorithms that exploit gradient information\n",
      "(Section 7.1). Figure 5.2 gives an overview of how concepts in this chap-\n",
      "ter are related and how they are connected to other chapters of the book.\n",
      "Central to this chapter is the concept of a function. A function fis\n",
      "a quantity that relates two quantities to each other. In this book, these\n",
      "quantities are typically inputs x2RDand targets (function values) f(x),\n",
      "which we assume are real-valued if not stated otherwise. Here RDis the\n",
      "domain off, and the function values f(x)are the image/codomain off.domain\n",
      "image/codomain\n",
      "Figure 5.1 Vector\n",
      "calculus plays a\n",
      "central role in (a)\n",
      "regression (curve\n",
      "ﬁtting) and (b)\n",
      "density estimation,\n",
      "i.e., modeling data\n",
      "distributions.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "(a) Regression problem: Find parameters,\n",
      "such that the curve explains the observations\n",
      "(crosses) well.\n",
      "−10−5 0 5 10\n",
      "x1−10−50510x2\n",
      "(b) Density estimation with a Gaussian mixture\n",
      "model: Find means and covariances, such that\n",
      "the data (dots) can be explained well.\n",
      "139\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "140 Vector Calculus\n",
      "Figure 5.2 A mind\n",
      "map of the concepts\n",
      "introduced in this\n",
      "chapter, along with\n",
      "when they are used\n",
      "in other parts of the\n",
      "book.Difference quotient\n",
      "Partial derivatives\n",
      "Jacobian\n",
      "Hessian\n",
      "Taylor seriesChapter 7\n",
      "Optimization\n",
      "Chapter 6\n",
      "ProbabilityChapter 9\n",
      "Regression\n",
      "Chapter 10\n",
      "Dimensionality\n",
      "reduction\n",
      "Chapter 11\n",
      "Density estimation\n",
      "Chapter 12\n",
      "Classiﬁcation\n",
      "deﬁnes collected in used inused inused in\n",
      "used inused in\n",
      "used inused in\n",
      "Section 2.7.3 provides much more detailed discussion in the context of\n",
      "linear functions. We often write\n",
      "f:RD!R (5.1a)\n",
      "x7!f(x) (5.1b)\n",
      "to specify a function, where (5.1a) speciﬁes that fis a mapping from\n",
      "RDtoRand (5.1b) speciﬁes the explicit assignment of an input xto\n",
      "a function value f(x). A function fassigns every input xexactly one\n",
      "function value f(x).\n",
      "Example 5.1\n",
      "Recall the dot product as a special case of an inner product (Section 3.2).\n",
      "In the previous notation, the function f(x) =x>x;x2R2, would be\n",
      "speciﬁed as\n",
      "f:R2!R (5.2a)\n",
      "x7!x2\n",
      "1+x2\n",
      "2: (5.2b)\n",
      "In this chapter, we will discuss how to compute gradients of functions,\n",
      "which is often essential to facilitate learning in machine learning models\n",
      "since the gradient points in the direction of steepest ascent. Therefore,\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.1 Differentiation of Univariate Functions 141\n",
      "Figure 5.3 The\n",
      "average incline of a\n",
      "functionfbetween\n",
      "x0andx0+\u000exis\n",
      "the incline of the\n",
      "secant (blue)\n",
      "throughf(x0)and\n",
      "f(x0+\u000ex)and\n",
      "given by\u000ey=\u000ex .\n",
      "\u000ey\n",
      "\u000exf(x)\n",
      "xy\n",
      "f(x0)f(x0+\u000ex)\n",
      "vector calculus is one of the fundamental mathematical tools we need in\n",
      "machine learning. Throughout this book, we assume that functions are\n",
      "differentiable. With some additional technical deﬁnitions, which we do\n",
      "not cover here, many of the approaches presented can be extended to\n",
      "sub-differentials (functions that are continuous but not differentiable at\n",
      "certain points). We will look at an extension to the case of functions with\n",
      "constraints in Chapter 7.\n",
      "5.1 Differentiation of Univariate Functions\n",
      "In the following, we brieﬂy revisit differentiation of a univariate function,\n",
      "which may be familiar from high school mathematics. We start with the\n",
      "difference quotient of a univariate function y=f(x); x;y2R, which we\n",
      "will subsequently use to deﬁne derivatives.\n",
      "Deﬁnition 5.1 (Difference Quotient) .Thedifference quotient difference quotient\n",
      "\u000ey\n",
      "\u000ex:=f(x+\u000ex)\u0000f(x)\n",
      "\u000ex(5.3)\n",
      "computes the slope of the secant line through two points on the graph of\n",
      "f. In Figure 5.3, these are the points with x-coordinates x0andx0+\u000ex.\n",
      "The difference quotient can also be considered the average slope of f\n",
      "betweenxandx+\u000exif we assume fto be a linear function. In the limit\n",
      "for\u000ex!0, we obtain the tangent of fatx, iffis differentiable. The\n",
      "tangent is then the derivative of fatx.\n",
      "Deﬁnition 5.2 (Derivative) .More formally, for h >0thederivative off derivative\n",
      "atxis deﬁned as the limit\n",
      "df\n",
      "dx:= lim\n",
      "h!0f(x+h)\u0000f(x)\n",
      "h; (5.4)\n",
      "and the secant in Figure 5.3 becomes a tangent.\n",
      "The derivative of fpoints in the direction of steepest ascent of f.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "142 Vector Calculus\n",
      "Example 5.2 (Derivative of a Polynomial)\n",
      "We want to compute the derivative of f(x) =xn;n2N. We may already\n",
      "know that the answer will be nxn\u00001, but we want to derive this result\n",
      "using the deﬁnition of the derivative as the limit of the difference quotient.\n",
      "Using the deﬁnition of the derivative in (5.4), we obtain\n",
      "df\n",
      "dx= lim\n",
      "h!0f(x+h)\u0000f(x)\n",
      "h(5.5a)\n",
      "= lim\n",
      "h!0(x+h)n\u0000xn\n",
      "h(5.5b)\n",
      "= lim\n",
      "h!0Pn\n",
      "i=0\u0000n\n",
      "i\u0001xn\u0000ihi\u0000xn\n",
      "h: (5.5c)\n",
      "We see that xn=\u0000n\n",
      "0\u0001xn\u00000h0. By starting the sum at 1, thexn-term cancels,\n",
      "and we obtain\n",
      "df\n",
      "dx= lim\n",
      "h!0Pn\n",
      "i=1\u0000n\n",
      "i\u0001xn\u0000ihi\n",
      "h(5.6a)\n",
      "= lim\n",
      "h!0nX\n",
      "i=1 \n",
      "n\n",
      "i!\n",
      "xn\u0000ihi\u00001(5.6b)\n",
      "= lim\n",
      "h!0  \n",
      "n\n",
      "1!\n",
      "xn\u00001+nX\n",
      "i=2 \n",
      "n\n",
      "i!\n",
      "xn\u0000ihi\u00001\n",
      "|{z}\n",
      "!0ash!0!\n",
      "(5.6c)\n",
      "=n!\n",
      "1!(n\u00001)!xn\u00001=nxn\u00001: (5.6d)\n",
      "5.1.1 Taylor Series\n",
      "The Taylor series is a representation of a function fas an inﬁnite sum of\n",
      "terms. These terms are determined using derivatives of fevaluated at x0.\n",
      "Deﬁnition 5.3 (Taylor Polynomial) .TheTaylor polynomial of degreenof Taylor polynomial\n",
      "f:R!Ratx0is deﬁned as We deﬁnet0:= 1\n",
      "for allt2R.\n",
      "Tn(x) :=nX\n",
      "k=0f(k)(x0)\n",
      "k!(x\u0000x0)k; (5.7)\n",
      "wheref(k)(x0)is thekth derivative of fatx0(which we assume exists)\n",
      "andf(k)(x0)\n",
      "k!are the coefﬁcients of the polynomial.\n",
      "Deﬁnition 5.4 (Taylor Series) .For a smooth function f2C1,f:R!R,\n",
      "theTaylor series offatx0is deﬁned as Taylor series\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.1 Differentiation of Univariate Functions 143\n",
      "T1(x) =1X\n",
      "k=0f(k)(x0)\n",
      "k!(x\u0000x0)k: (5.8)\n",
      "Forx0= 0, we obtain the Maclaurin series as a special instance of the f2C1means that\n",
      "fis continuously\n",
      "differentiable\n",
      "inﬁnitely many\n",
      "times.\n",
      "Maclaurin seriesTaylor series. If f(x) =T1(x), thenfis called analytic .\n",
      "analyticRemark. In general, a Taylor polynomial of degree nis an approximation\n",
      "of a function, which does not need to be a polynomial. The Taylor poly-\n",
      "nomial is similar to fin a neighborhood around x0. However, a Taylor\n",
      "polynomial of degree nis an exact representation of a polynomial fof\n",
      "degreek6nsince all derivatives f(i),i>k vanish. }\n",
      "Example 5.3 (Taylor Polynomial)\n",
      "We consider the polynomial\n",
      "f(x) =x4(5.9)\n",
      "and seek the Taylor polynomial T6, evaluated at x0= 1. We start by com-\n",
      "puting the coefﬁcients f(k)(1)fork= 0;:::; 6:\n",
      "f(1) = 1 (5.10)\n",
      "f0(1) = 4 (5.11)\n",
      "f00(1) = 12 (5.12)\n",
      "f(3)(1) = 24 (5.13)\n",
      "f(4)(1) = 24 (5.14)\n",
      "f(5)(1) = 0 (5.15)\n",
      "f(6)(1) = 0 (5.16)\n",
      "Therefore, the desired Taylor polynomial is\n",
      "T6(x) =6X\n",
      "k=0f(k)(x0)\n",
      "k!(x\u0000x0)k(5.17a)\n",
      "= 1 + 4(x\u00001) + 6(x\u00001)2+ 4(x\u00001)3+ (x\u00001)4+ 0:(5.17b)\n",
      "Multiplying out and re-arranging yields\n",
      "T6(x) = (1\u00004 + 6\u00004 + 1) +x(4\u000012 + 12\u00004)\n",
      "+x2(6\u000012 + 6) +x3(4\u00004) +x4(5.18a)\n",
      "=x4=f(x); (5.18b)\n",
      "i.e., we obtain an exact representation of the original function.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "144 Vector Calculus\n",
      "Figure 5.4 Taylor\n",
      "polynomials. The\n",
      "original function\n",
      "f(x) =\n",
      "sin(x) + cos(x)\n",
      "(black, solid) is\n",
      "approximated by\n",
      "Taylor polynomials\n",
      "(dashed) around\n",
      "x0= 0.\n",
      "Higher-order Taylor\n",
      "polynomials\n",
      "approximate the\n",
      "functionfbetter\n",
      "and more globally.\n",
      "T10is already\n",
      "similar tofin\n",
      "[\u00004;4].\n",
      "−4−2 0 2 4\n",
      "x−2024yf\n",
      "T0\n",
      "T1\n",
      "T5\n",
      "T10\n",
      "Example 5.4 (Taylor Series)\n",
      "Consider the function in Figure 5.4 given by\n",
      "f(x) = sin(x) + cos(x)2C1: (5.19)\n",
      "We seek a Taylor series expansion of fatx0= 0, which is the Maclaurin\n",
      "series expansion of f. We obtain the following derivatives:\n",
      "f(0) = sin(0) + cos(0) = 1 (5.20)\n",
      "f0(0) = cos(0)\u0000sin(0) = 1 (5.21)\n",
      "f00(0) =\u0000sin(0)\u0000cos(0) =\u00001 (5.22)\n",
      "f(3)(0) =\u0000cos(0) + sin(0) = \u00001 (5.23)\n",
      "f(4)(0) = sin(0) + cos(0) = f(0) = 1 (5.24)\n",
      "...\n",
      "We can see a pattern here: The coefﬁcients in our Taylor series are only\n",
      "\u00061(since sin(0) = 0 ), each of which occurs twice before switching to the\n",
      "other one. Furthermore, f(k+4)(0) =f(k)(0).\n",
      "Therefore, the full Taylor series expansion of fatx0= 0is given by\n",
      "T1(x) =1X\n",
      "k=0f(k)(x0)\n",
      "k!(x\u0000x0)k(5.25a)\n",
      "= 1 +x\u00001\n",
      "2!x2\u00001\n",
      "3!x3+1\n",
      "4!x4+1\n",
      "5!x5\u0000\u0001\u0001\u0001 (5.25b)\n",
      "= 1\u00001\n",
      "2!x2+1\n",
      "4!x4\u0007\u0001\u0001\u0001 +x\u00001\n",
      "3!x3+1\n",
      "5!x5\u0007\u0001\u0001\u0001 (5.25c)\n",
      "=1X\n",
      "k=0(\u00001)k1\n",
      "(2k)!x2k+1X\n",
      "k=0(\u00001)k 1\n",
      "(2k+ 1)!x2k+1(5.25d)\n",
      "= cos(x) + sin(x); (5.25e)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.1 Differentiation of Univariate Functions 145\n",
      "where we used the power series representations power series\n",
      "representation\n",
      "cos(x) =1X\n",
      "k=0(\u00001)k1\n",
      "(2k)!x2k; (5.26)\n",
      "sin(x) =1X\n",
      "k=0(\u00001)k 1\n",
      "(2k+ 1)!x2k+1: (5.27)\n",
      "Figure 5.4 shows the corresponding ﬁrst Taylor polynomials Tnforn=\n",
      "0;1;5;10.\n",
      "Remark. A Taylor series is a special case of a power series\n",
      "f(x) =1X\n",
      "k=0ak(x\u0000c)k(5.28)\n",
      "whereakare coefﬁcients and cis a constant, which has the special form\n",
      "in Deﬁnition 5.4. }\n",
      "5.1.2 Differentiation Rules\n",
      "In the following, we brieﬂy state basic differentiation rules, where we\n",
      "denote the derivative of fbyf0.\n",
      "Product rule: (f(x)g(x))0=f0(x)g(x) +f(x)g0(x) (5.29)\n",
      "Quotient rule:\u0012f(x)\n",
      "g(x)\u00130\n",
      "=f0(x)g(x)\u0000f(x)g0(x)\n",
      "(g(x))2(5.30)\n",
      "Sum rule: (f(x) +g(x))0=f0(x) +g0(x) (5.31)\n",
      "Chain rule:\u0000g(f(x))\u00010= (g\u000ef)0(x) =g0(f(x))f0(x) (5.32)\n",
      "Here,g\u000efdenotes function composition x7!f(x)7!g(f(x)).\n",
      "Example 5.5 (Chain Rule)\n",
      "Let us compute the derivative of the function h(x) = (2x+ 1)4using the\n",
      "chain rule. With\n",
      "h(x) = (2x+ 1)4=g(f(x)); (5.33)\n",
      "f(x) = 2x+ 1; (5.34)\n",
      "g(f) =f4; (5.35)\n",
      "we obtain the derivatives of fandgas\n",
      "f0(x) = 2; (5.36)\n",
      "g0(f) = 4f3; (5.37)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "146 Vector Calculus\n",
      "such that the derivative of his given as\n",
      "h0(x) =g0(f)f0(x) = (4f3)\u00012(5.34)= 4(2x+ 1)3\u00012 = 8(2x+ 1)3;(5.38)\n",
      "where we used the chain rule (5.32) and substituted the deﬁnition of f\n",
      "in (5.34) in g0(f).\n",
      "5.2 Partial Differentiation and Gradients\n",
      "Differentiation as discussed in Section 5.1 applies to functions fof a\n",
      "scalar variable x2R. In the following, we consider the general case\n",
      "where the function fdepends on one or more variables x2Rn, e.g.,\n",
      "f(x) =f(x1;x2). The generalization of the derivative to functions of sev-\n",
      "eral variables is the gradient .\n",
      "We ﬁnd the gradient of the function fwith respect to xbyvarying one\n",
      "variable at a time and keeping the others constant. The gradient is then\n",
      "the collection of these partial derivatives .\n",
      "Deﬁnition 5.5 (Partial Derivative) .For a function f:Rn!R;x7!\n",
      "f(x);x2Rnofnvariablesx1;:::;xnwe deﬁne the partial derivatives as partial derivative\n",
      "@f\n",
      "@x1= lim\n",
      "h!0f(x1+h;x 2;:::;xn)\u0000f(x)\n",
      "h\n",
      "...\n",
      "@f\n",
      "@xn= lim\n",
      "h!0f(x1;:::;xn\u00001;xn+h)\u0000f(x)\n",
      "h(5.39)\n",
      "and collect them in the row vector\n",
      "rxf= gradf=df\n",
      "dx=\u0014@f(x)\n",
      "@x1@f(x)\n",
      "@x2\u0001\u0001\u0001@f(x)\n",
      "@xn\u0015\n",
      "2R1\u0002n;(5.40)\n",
      "wherenis the number of variables and 1is the dimension of the image/\n",
      "range/codomain of f. Here, we deﬁned the column vector x= [x1;:::;xn]>\n",
      "2Rn. The row vector in (5.40) is called the gradient offor the Jacobian gradient\n",
      "Jacobian and is the generalization of the derivative from Section 5.1.\n",
      "Remark. This deﬁnition of the Jacobian is a special case of the general\n",
      "deﬁnition of the Jacobian for vector-valued functions as the collection of\n",
      "partial derivatives. We will get back to this in Section 5.3. }We can use results\n",
      "from scalar\n",
      "differentiation: Each\n",
      "partial derivative is\n",
      "a derivative with\n",
      "respect to a scalar.Example 5.6 (Partial Derivatives Using the Chain Rule)\n",
      "Forf(x;y) = (x+ 2y3)2, we obtain the partial derivatives\n",
      "@f(x;y)\n",
      "@x= 2(x+ 2y3)@\n",
      "@x(x+ 2y3) = 2(x+ 2y3); (5.41)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.2 Partial Differentiation and Gradients 147\n",
      "@f(x;y)\n",
      "@y= 2(x+ 2y3)@\n",
      "@y(x+ 2y3) = 12(x+ 2y3)y2: (5.42)\n",
      "where we used the chain rule (5.32) to compute the partial derivatives.\n",
      "Remark (Gradient as a Row Vector) .It is not uncommon in the literature\n",
      "to deﬁne the gradient vector as a column vector, following the conven-\n",
      "tion that vectors are generally column vectors. The reason why we deﬁne\n",
      "the gradient vector as a row vector is twofold: First, we can consistently\n",
      "generalize the gradient to vector-valued functions f:Rn!Rm(then\n",
      "the gradient becomes a matrix). Second, we can immediately apply the\n",
      "multi-variate chain rule without paying attention to the dimension of the\n",
      "gradient. We will discuss both points in Section 5.3. }\n",
      "Example 5.7 (Gradient)\n",
      "Forf(x1;x2) =x2\n",
      "1x2+x1x3\n",
      "22R, the partial derivatives (i.e., the deriva-\n",
      "tives offwith respect to x1andx2) are\n",
      "@f(x1;x2)\n",
      "@x1= 2x1x2+x3\n",
      "2 (5.43)\n",
      "@f(x1;x2)\n",
      "@x2=x2\n",
      "1+ 3x1x2\n",
      "2 (5.44)\n",
      "and the gradient is then\n",
      "df\n",
      "dx=\u0014@f(x1;x2)\n",
      "@x1@f(x1;x2)\n",
      "@x2\u0015\n",
      "=\u00022x1x2+x3\n",
      "2x2\n",
      "1+ 3x1x2\n",
      "2\u00032R1\u00022:\n",
      "(5.45)\n",
      "5.2.1 Basic Rules of Partial Differentiation\n",
      "Product rule:\n",
      "(fg)0=f0g+fg0,\n",
      "Sum rule:\n",
      "(f+g)0=f0+g0,\n",
      "Chain rule:\n",
      "(g(f))0=g0(f)f0In the multivariate case, where x2Rn, the basic differentiation rules that\n",
      "we know from school (e.g., sum rule, product rule, chain rule; see also\n",
      "Section 5.1.2) still apply. However, when we compute derivatives with re-\n",
      "spect to vectors x2Rnwe need to pay attention: Our gradients now\n",
      "involve vectors and matrices, and matrix multiplication is not commuta-\n",
      "tive (Section 2.2.1), i.e., the order matters.\n",
      "Here are the general product rule, sum rule, and chain rule:\n",
      "Product rule:@\n",
      "@x\u0000f(x)g(x)\u0001=@f\n",
      "@xg(x) +f(x)@g\n",
      "@x(5.46)\n",
      "Sum rule:@\n",
      "@x\u0000f(x) +g(x)\u0001=@f\n",
      "@x+@g\n",
      "@x(5.47)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "148 Vector Calculus\n",
      "Chain rule:@\n",
      "@x(g\u000ef)(x) =@\n",
      "@x\u0000g(f(x))\u0001=@g\n",
      "@f@f\n",
      "@x(5.48)\n",
      "Let us have a closer look at the chain rule. The chain rule (5.48) resem- This is only an\n",
      "intuition, but not\n",
      "mathematically\n",
      "correct since the\n",
      "partial derivative is\n",
      "not a fraction.bles to some degree the rules for matrix multiplication where we said that\n",
      "neighboring dimensions have to match for matrix multiplication to be de-\n",
      "ﬁned; see Section 2.2.1. If we go from left to right, the chain rule exhibits\n",
      "similar properties: @fshows up in the “denominator” of the ﬁrst factor\n",
      "and in the “numerator” of the second factor. If we multiply the factors to-\n",
      "gether, multiplication is deﬁned, i.e., the dimensions of @fmatch, and @f\n",
      "“cancels”, such that @g=@xremains.\n",
      "5.2.2 Chain Rule\n",
      "Consider a function f:R2!Rof two variables x1;x2. Furthermore,\n",
      "x1(t)andx2(t)are themselves functions of t. To compute the gradient of\n",
      "fwith respect to t, we need to apply the chain rule (5.48) for multivariate\n",
      "functions as\n",
      "df\n",
      "dt=h\n",
      "@f\n",
      "@x1@f\n",
      "@x2i\"\n",
      "@x1(t)\n",
      "@t@x2(t)\n",
      "@t#\n",
      "=@f\n",
      "@x1@x1\n",
      "@t+@f\n",
      "@x2@x2\n",
      "@t; (5.49)\n",
      "where ddenotes the gradient and @partial derivatives.\n",
      "Example 5.8\n",
      "Considerf(x1;x2) =x2\n",
      "1+ 2x2, wherex1= sintandx2= cost, then\n",
      "df\n",
      "dt=@f\n",
      "@x1@x1\n",
      "@t+@f\n",
      "@x2@x2\n",
      "@t(5.50a)\n",
      "= 2 sint@sint\n",
      "@t+ 2@cost\n",
      "@t(5.50b)\n",
      "= 2 sintcost\u00002 sint= 2 sint(cost\u00001) (5.50c)\n",
      "is the corresponding derivative of fwith respect to t.\n",
      "Iff(x1;x2)is a function of x1andx2, wherex1(s;t)andx2(s;t)are\n",
      "themselves functions of two variables sandt, the chain rule yields the\n",
      "partial derivatives\n",
      "@f\n",
      "@s=@f\n",
      "@x1@x1\n",
      "@s+@f\n",
      "@x2@x2\n",
      "@s; (5.51)\n",
      "@f\n",
      "@t=@f\n",
      "@x1@x1\n",
      "@t+@f\n",
      "@x2@x2\n",
      "@t; (5.52)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.3 Gradients of Vector-Valued Functions 149\n",
      "and the gradient is obtained by the matrix multiplication\n",
      "df\n",
      "d(s;t)=@f\n",
      "@x@x\n",
      "@(s;t)=h@f\n",
      "@x1@f\n",
      "@x2i\n",
      "|{z}\n",
      "=@f\n",
      "@x2\n",
      "64@x1\n",
      "@s@x1\n",
      "@t\n",
      "@x2\n",
      "@s@x2\n",
      "@t3\n",
      "75\n",
      "|{z}\n",
      "=@x\n",
      "@(s;t): (5.53)\n",
      "This compact way of writing the chain rule as a matrix multiplication only The chain rule can\n",
      "be written as a\n",
      "matrix\n",
      "multiplication.makes sense if the gradient is deﬁned as a row vector. Otherwise, we will\n",
      "need to start transposing gradients for the matrix dimensions to match.\n",
      "This may still be straightforward as long as the gradient is a vector or a\n",
      "matrix; however, when the gradient becomes a tensor (we will discuss this\n",
      "in the following), the transpose is no longer a triviality.\n",
      "Remark (Verifying the Correctness of a Gradient Implementation) .The\n",
      "deﬁnition of the partial derivatives as the limit of the corresponding dif-\n",
      "ference quotient (see (5.39)) can be exploited when numerically checking\n",
      "the correctness of gradients in computer programs: When we compute Gradient checking\n",
      "gradients and implement them, we can use ﬁnite differences to numer-\n",
      "ically test our computation and implementation: We choose the value h\n",
      "to be small (e.g., h= 10\u00004) and compare the ﬁnite-difference approxima-\n",
      "tion from (5.39) with our (analytic) implementation of the gradient. If the\n",
      "error is small, our gradient implementation is probably correct. “Small”\n",
      "could mean thatqP\n",
      "i(dhi\u0000dfi)2\n",
      "P\n",
      "i(dhi+dfi)2<10\u00006, wheredhiis the ﬁnite-difference\n",
      "approximation and dfiis the analytic gradient of fwith respect to the ith\n",
      "variablexi. }\n",
      "5.3 Gradients of Vector-Valued Functions\n",
      "Thus far, we discussed partial derivatives and gradients of functions f:\n",
      "Rn!Rmapping to the real numbers. In the following, we will generalize\n",
      "the concept of the gradient to vector-valued functions (vector ﬁelds) f:\n",
      "Rn!Rm, wheren>1andm> 1.\n",
      "For a function f:Rn!Rmand a vectorx= [x1;:::;xn]>2Rn, the\n",
      "corresponding vector of function values is given as\n",
      "f(x) =2\n",
      "64f1(x)\n",
      "...\n",
      "fm(x)3\n",
      "752Rm: (5.54)\n",
      "Writing the vector-valued function in this way allows us to view a vector-\n",
      "valued function f:Rn!Rmas a vector of functions [f1;:::;fm]>,\n",
      "fi:Rn!Rthat map onto R. The differentiation rules for every fiare\n",
      "exactly the ones we discussed in Section 5.2.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "150 Vector Calculus\n",
      "Therefore, the partial derivative of a vector-valued function f:Rn!\n",
      "Rmwith respect to xi2R,i= 1;:::n , is given as the vector\n",
      "@f\n",
      "@xi=2\n",
      "64@f1\n",
      "@xi...\n",
      "@fm\n",
      "@xi3\n",
      "75=2\n",
      "64limh!0f1(x1;:::;x i\u00001;xi+h;xi+1;:::xn)\u0000f1(x)\n",
      "h...\n",
      "limh!0fm(x1;:::;x i\u00001;xi+h;xi+1;:::xn)\u0000fm(x)\n",
      "h3\n",
      "752Rm:\n",
      "(5.55)\n",
      "From (5.40), we know that the gradient of fwith respect to a vector is\n",
      "the row vector of the partial derivatives. In (5.55), every partial derivative\n",
      "@f=@xiis itself a column vector. Therefore, we obtain the gradient of f:\n",
      "Rn!Rmwith respect to x2Rnby collecting these partial derivatives:\n",
      "df(x)\n",
      "dx=@f(x)\n",
      "@x1\u0001\u0001\u0001@f(x)\n",
      "@xn\u0014 \u0015\n",
      "(5.56a)\n",
      "=@f1(x)\n",
      "@x1\u0001\u0001\u0001@f1(x)\n",
      "@xn\n",
      "......\n",
      "@fm(x)\n",
      "@x1\u0001\u0001\u0001@fm(x)\n",
      "@xn2\n",
      "666643\n",
      "777752Rm\u0002n:(5.56b)\n",
      "Deﬁnition 5.6 (Jacobian) .The collection of all ﬁrst-order partial deriva-\n",
      "tives of a vector-valued function f:Rn!Rmis called the Jacobian . The Jacobian\n",
      "JacobianJis anm\u0002nmatrix, which we deﬁne and arrange as follows: The gradient of a\n",
      "function\n",
      "f:Rn!Rmis a\n",
      "matrix of size\n",
      "m\u0002n.J=rxf=df(x)\n",
      "dx=\u0014@f(x)\n",
      "@x1\u0001\u0001\u0001@f(x)\n",
      "@xn\u0015\n",
      "(5.57)\n",
      "=2\n",
      "666664@f1(x)\n",
      "@x1\u0001\u0001\u0001@f1(x)\n",
      "@xn......\n",
      "@fm(x)\n",
      "@x1\u0001\u0001\u0001@fm(x)\n",
      "@xn3\n",
      "777775; (5.58)\n",
      "x=2\n",
      "64x1\n",
      "...\n",
      "xn3\n",
      "75; J(i;j) =@fi\n",
      "@xj: (5.59)\n",
      "As a special case of (5.58), a function f:Rn!R1, which maps a\n",
      "vectorx2Rnonto a scalar (e.g., f(x) =Pn\n",
      "i=1xi), possesses a Jacobian\n",
      "that is a row vector (matrix of dimension 1\u0002n); see (5.40).\n",
      "Remark. In this book, we use the numerator layout of the derivative, i.e., numerator layout\n",
      "the derivative df=dxoff2Rmwith respect to x2Rnis anm\u0002\n",
      "nmatrix, where the elements of fdeﬁne the rows and the elements of\n",
      "xdeﬁne the columns of the corresponding Jacobian; see (5.58). There\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.3 Gradients of Vector-Valued Functions 151\n",
      "Figure 5.5 The\n",
      "determinant of the\n",
      "Jacobian offcan\n",
      "be used to compute\n",
      "the magniﬁer\n",
      "between the blue\n",
      "and orange area.b1b2 c1 c2f(\u0001)\n",
      "exists also the denominator layout , which is the transpose of the numerator denominator layout\n",
      "layout. In this book, we will use the numerator layout. }\n",
      "We will see how the Jacobian is used in the change-of-variable method\n",
      "for probability distributions in Section 6.7. The amount of scaling due to\n",
      "the transformation of a variable is provided by the determinant.\n",
      "In Section 4.1, we saw that the determinant can be used to compute\n",
      "the area of a parallelogram. If we are given two vectors b1= [1;0]>,\n",
      "b2= [0;1]>as the sides of the unit square (blue; see Figure 5.5), the area\n",
      "of this square is\n",
      "\f\f\f\fdet\u0012\u00141 0\n",
      "0 1\u0015\u0013\f\f\f\f= 1: (5.60)\n",
      "If we take a parallelogram with the sides c1= [\u00002;1]>,c2= [1;1]>\n",
      "(orange in Figure 5.5), its area is given as the absolute value of the deter-\n",
      "minant (see Section 4.1)\n",
      "\f\f\f\fdet\u0012\u0014\u00002 1\n",
      "1 1\u0015\u0013\f\f\f\f=j\u00003j= 3; (5.61)\n",
      "i.e., the area of this is exactly three times the area of the unit square.\n",
      "We can ﬁnd this scaling factor by ﬁnding a mapping that transforms the\n",
      "unit square into the other square. In linear algebra terms, we effectively\n",
      "perform a variable transformation from (b1;b2)to(c1;c2). In our case,\n",
      "the mapping is linear and the absolute value of the determinant of this\n",
      "mapping gives us exactly the scaling factor we are looking for.\n",
      "We will describe two approaches to identify this mapping. First, we ex-\n",
      "ploit that the mapping is linear so that we can use the tools from Chapter 2\n",
      "to identify this mapping. Second, we will ﬁnd the mapping using partial\n",
      "derivatives using the tools we have been discussing in this chapter.\n",
      "Approach 1 To get started with the linear algebra approach, we\n",
      "identify bothfb1;b2gandfc1;c2gas bases of R2(see Section 2.6.1 for a\n",
      "recap). What we effectively perform is a change of basis from (b1;b2)to\n",
      "(c1;c2), and we are looking for the transformation matrix that implements\n",
      "the basis change. Using results from Section 2.7.2, we identify the desired\n",
      "basis change matrix as\n",
      "J=\u0014\u00002 1\n",
      "1 1\u0015\n",
      "; (5.62)\n",
      "such thatJb1=c1andJb2=c2. The absolute value of the determi-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "152 Vector Calculus\n",
      "nant ofJ, which yields the scaling factor we are looking for, is given as\n",
      "jdet(J)j= 3, i.e., the area of the square spanned by (c1;c2)is three times\n",
      "greater than the area spanned by (b1;b2).\n",
      "Approach 2 The linear algebra approach works for linear trans-\n",
      "formations; for nonlinear transformations (which become relevant in Sec-\n",
      "tion 6.7), we follow a more general approach using partial derivatives.\n",
      "For this approach, we consider a function f:R2!R2that performs a\n",
      "variable transformation. In our example, fmaps the coordinate represen-\n",
      "tation of any vector x2R2with respect to (b1;b2)onto the coordinate\n",
      "representation y2R2with respect to (c1;c2). We want to identify the\n",
      "mapping so that we can compute how an area (or volume) changes when\n",
      "it is being transformed by f. For this, we need to ﬁnd out how f(x)\n",
      "changes if we modify xa bit. This question is exactly answered by the\n",
      "Jacobian matrixdf\n",
      "dx2R2\u00022. Since we can write\n",
      "y1=\u00002x1+x2 (5.63)\n",
      "y2=x1+x2 (5.64)\n",
      "we obtain the functional relationship between xandy, which allows us\n",
      "to get the partial derivatives\n",
      "@y1\n",
      "@x1=\u00002;@y1\n",
      "@x2= 1;@y2\n",
      "@x1= 1;@y2\n",
      "@x2= 1 (5.65)\n",
      "and compose the Jacobian as\n",
      "J=2\n",
      "64@y1\n",
      "@x1@y1\n",
      "@x2@y2\n",
      "@x1@y2\n",
      "@x23\n",
      "75=\u0014\u00002 1\n",
      "1 1\u0015\n",
      ": (5.66)\n",
      "The Jacobian represents the coordinate transformation we are looking Geometrically, the\n",
      "Jacobian\n",
      "determinant gives\n",
      "the magniﬁcation/\n",
      "scaling factor when\n",
      "we transform an\n",
      "area or volume.for. It is exact if the coordinate transformation is linear (as in our case),\n",
      "and (5.66) recovers exactly the basis change matrix in (5.62). If the co-\n",
      "ordinate transformation is nonlinear, the Jacobian approximates this non-\n",
      "linear transformation locally with a linear one. The absolute value of the\n",
      "Jacobian determinant jdet(J)jis the factor by which areas or volumes are\n",
      "Jacobian\n",
      "determinantscaled when coordinates are transformed. Our case yields jdet(J)j= 3.\n",
      "The Jacobian determinant and variable transformations will become\n",
      "relevant in Section 6.7 when we transform random variables and prob-\n",
      "ability distributions. These transformations are extremely relevant in ma- Figure 5.6\n",
      "Dimensionality of\n",
      "(partial) derivatives.\n",
      "f(x)\n",
      "x\n",
      "@f\n",
      "@xchine learning in the context of training deep neural networks using the\n",
      "reparametrization trick , also called inﬁnite perturbation analysis .\n",
      "In this chapter, we encountered derivatives of functions. Figure 5.6 sum-\n",
      "marizes the dimensions of those derivatives. If f:R!Rthe gradient is\n",
      "simply a scalar (top-left entry). For f:RD!Rthe gradient is a 1\u0002D\n",
      "row vector (top-right entry). For f:R!RE, the gradient is an E\u00021\n",
      "column vector, and for f:RD!REthe gradient is an E\u0002Dmatrix.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.3 Gradients of Vector-Valued Functions 153\n",
      "Example 5.9 (Gradient of a Vector-Valued Function)\n",
      "We are given\n",
      "f(x) =Ax;f(x)2RM;A2RM\u0002N;x2RN:\n",
      "To compute the gradient df=dxwe ﬁrst determine the dimension of\n",
      "df=dx: Sincef:RN!RM, it follows that df=dx2RM\u0002N. Second,\n",
      "to compute the gradient we determine the partial derivatives of fwith\n",
      "respect to every xj:\n",
      "fi(x) =NX\n",
      "j=1Aijxj=)@fi\n",
      "@xj=Aij (5.67)\n",
      "We collect the partial derivatives in the Jacobian and obtain the gradient\n",
      "df\n",
      "dx=2\n",
      "64@f1\n",
      "@x1\u0001\u0001\u0001@f1\n",
      "@xN......\n",
      "@fM\n",
      "@x1\u0001\u0001\u0001@fM\n",
      "@xN3\n",
      "75=2\n",
      "64A11\u0001\u0001\u0001A1N\n",
      "......\n",
      "AM1\u0001\u0001\u0001AMN3\n",
      "75=A2RM\u0002N:(5.68)\n",
      "Example 5.10 (Chain Rule)\n",
      "Consider the function h:R!R,h(t) = (f\u000eg)(t)with\n",
      "f:R2!R (5.69)\n",
      "g:R!R2(5.70)\n",
      "f(x) = exp(x1x2\n",
      "2); (5.71)\n",
      "x=\u0014x1\n",
      "x2\u0015\n",
      "=g(t) =\u0014tcost\n",
      "tsint\u0015\n",
      "(5.72)\n",
      "and compute the gradient of hwith respect to t. Sincef:R2!Rand\n",
      "g:R!R2we note that\n",
      "@f\n",
      "@x2R1\u00022;@g\n",
      "@t2R2\u00021: (5.73)\n",
      "The desired gradient is computed by applying the chain rule:\n",
      "dh\n",
      "dt=@f\n",
      "@x@x\n",
      "@t=\u0014@f\n",
      "@x1@f\n",
      "@x2\u00152\n",
      "64@x1\n",
      "@t@x2\n",
      "@t3\n",
      "75 (5.74a)\n",
      "=\u0002exp(x1x2\n",
      "2)x2\n",
      "22 exp(x1x2\n",
      "2)x1x2\u0003\u0014cost\u0000tsint\n",
      "sint+tcost\u0015\n",
      "(5.74b)\n",
      "= exp(x1x2\n",
      "2)\u0000x2\n",
      "2(cost\u0000tsint) + 2x1x2(sint+tcost)\u0001;(5.74c)\n",
      "wherex1=tcostandx2=tsint; see (5.72).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "154 Vector Calculus\n",
      "Example 5.11 (Gradient of a Least-Squares Loss in a Linear Model)\n",
      "Let us consider the linear model We will discuss this\n",
      "model in much\n",
      "more detail in\n",
      "Chapter 9 in the\n",
      "context of linear\n",
      "regression, where\n",
      "we need derivatives\n",
      "of the least-squares\n",
      "lossLwith respect\n",
      "to the parameters \u0012.y\u0012; (5.75)\n",
      "where\u00122RDis a parameter vector,2RN\u0002Dare input features and\n",
      "y2RNare the corresponding observations. We deﬁne the functions\n",
      "L(e) :=kek2; (5.76)\n",
      "e(\u0012) :=y\u0012: (5.77)\n",
      "We seek@L\n",
      "@\u0012, and we will use the chain rule for this purpose. Lis called a\n",
      "least-squares loss function. least-squares loss\n",
      "Before we start our calculation, we determine the dimensionality of the\n",
      "gradient as\n",
      "@L\n",
      "@\u00122R1\u0002D: (5.78)\n",
      "The chain rule allows us to compute the gradient as\n",
      "@L\n",
      "@\u0012=@L\n",
      "@e@e\n",
      "@\u0012; (5.79)\n",
      "where thedth element is given by dLdtheta =\n",
      "np.einsum(\n",
      "'n,nd',\n",
      "dLde,dedtheta)@L\n",
      "@\u0012[1;d] =NX\n",
      "n=1@L\n",
      "@e[n]@e\n",
      "@\u0012[n;d]: (5.80)\n",
      "We know thatkek2=e>e(see Section 3.2) and determine\n",
      "@L\n",
      "@e= 2e>2R1\u0002N: (5.81)\n",
      "Furthermore, we obtain\n",
      "@e\n",
      "@\u0012=2RN\u0002D; (5.82)\n",
      "such that our desired derivative is\n",
      "@L\n",
      "@\u0012=\u00002e(5.77)=\u00002(y>\u0000\u0012>)|{z}\n",
      "1\u0002|{z}\n",
      "N\u0002D2R1\u0002D: (5.83)\n",
      "Remark. We would have obtained the same result without using the chain\n",
      "rule by immediately looking at the function\n",
      "L2(\u0012) :=ky\u0012k2= (y\u0012)>(y\u0012): (5.84)\n",
      "This approach is still practical for simple functions like L2but becomes\n",
      "impractical for deep function compositions. }\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.4 Gradients of Matrices 155\n",
      "Figure 5.7\n",
      "Visualization of\n",
      "gradient\n",
      "computation of a\n",
      "matrix with respect\n",
      "to a vector. We are\n",
      "interested in\n",
      "computing the\n",
      "gradient of\n",
      "A2R4\u00022with\n",
      "respect to a vector\n",
      "x2R3. We know\n",
      "that gradient\n",
      "dA\n",
      "dx2R4\u00022\u00023. We\n",
      "follow two\n",
      "equivalent\n",
      "approaches to arrive\n",
      "there: (a) collating\n",
      "partial derivatives\n",
      "into a Jacobian\n",
      "tensor;\n",
      "(b) ﬂattening of the\n",
      "matrix into a vector,\n",
      "computing the\n",
      "Jacobian matrix,\n",
      "re-shaping into a\n",
      "Jacobian tensor.A2R4\u00022x2R3\n",
      "@A\n",
      "@x12R4\u00022@A\n",
      "@x22R4\u00022@A\n",
      "@x32R4\u00022x1\n",
      "x2\n",
      "x3\n",
      "dA\n",
      "dx2R4\u00022\u00023\n",
      "4\n",
      "23Partial derivatives:\n",
      "collate\n",
      "(a) Approach 1: We compute the partial derivative\n",
      "@A\n",
      "@x1;@A\n",
      "@x2;@A\n",
      "@x3, each of which is a 4\u00022matrix, and col-\n",
      "late them in a 4\u00022\u00023tensor.\n",
      "A2R4\u00022x2R3\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "dA\n",
      "dx2R4\u00022\u00023\n",
      "re-shape re-shape gradientA2R4\u00022 ~A2R8d~A\n",
      "dx2R8\u00023\n",
      "(b) Approach 2: We re-shape (ﬂatten) A2R4\u00022into a vec-\n",
      "tor~A2R8. Then, we compute the gradientd~A\n",
      "dx2R8\u00023.\n",
      "We obtain the gradient tensor by re-shaping this gradient as\n",
      "illustrated above.\n",
      "5.4 Gradients of MatricesWe can think of a\n",
      "tensor as a\n",
      "multidimensional\n",
      "array.We will encounter situations where we need to take gradients of matrices\n",
      "with respect to vectors (or other matrices), which results in a multidimen-\n",
      "sional tensor. We can think of this tensor as a multidimensional array that\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "156 Vector Calculus\n",
      "collects partial derivatives. For example, if we compute the gradient of an\n",
      "m\u0002nmatrixAwith respect to a p\u0002qmatrixB, the resulting Jacobian\n",
      "would be (m\u0002n)\u0002(p\u0002q), i.e., a four-dimensional tensor J, whose entries\n",
      "are given as Jijkl=@Aij=@Bkl.\n",
      "Since matrices represent linear mappings, we can exploit the fact that\n",
      "there is a vector-space isomorphism (linear, invertible mapping) between\n",
      "the space Rm\u0002nofm\u0002nmatrices and the space Rmnofmnvectors.\n",
      "Therefore, we can re-shape our matrices into vectors of lengths mnand\n",
      "pq, respectively. The gradient using these mnvectors results in a Jacobian\n",
      "of sizemn\u0002pq. Figure 5.7 visualizes both approaches. In practical ap- Matrices can be\n",
      "transformed into\n",
      "vectors by stacking\n",
      "the columns of the\n",
      "matrix\n",
      "(“ﬂattening”).plications, it is often desirable to re-shape the matrix into a vector and\n",
      "continue working with this Jacobian matrix: The chain rule (5.48) boils\n",
      "down to simple matrix multiplication, whereas in the case of a Jacobian\n",
      "tensor, we will need to pay more attention to what dimensions we need\n",
      "to sum out.\n",
      "Example 5.12 (Gradient of Vectors with Respect to Matrices)\n",
      "Let us consider the following example, where\n",
      "f=Ax;f2RM;A2RM\u0002N;x2RN(5.85)\n",
      "and where we seek the gradient df=dA. Let us start again by determining\n",
      "the dimension of the gradient as\n",
      "df\n",
      "dA2RM\u0002(M\u0002N): (5.86)\n",
      "By deﬁnition, the gradient is the collection of the partial derivatives:\n",
      "df\n",
      "dA=2\n",
      "64@f1\n",
      "@A...\n",
      "@fM\n",
      "@A3\n",
      "75;@fi\n",
      "@A2R1\u0002(M\u0002N): (5.87)\n",
      "To compute the partial derivatives, it will be helpful to explicitly write out\n",
      "the matrix vector multiplication:\n",
      "fi=NX\n",
      "j=1Aijxj; i= 1;:::;M; (5.88)\n",
      "and the partial derivatives are then given as\n",
      "@fi\n",
      "@Aiq=xq: (5.89)\n",
      "This allows us to compute the partial derivatives of fiwith respect to a\n",
      "row ofA, which is given as\n",
      "@fi\n",
      "@Ai;:=x>2R1\u00021\u0002N; (5.90)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.4 Gradients of Matrices 157\n",
      "@fi\n",
      "@Ak6=i;:=0>2R1\u00021\u0002N(5.91)\n",
      "where we have to pay attention to the correct dimensionality. Since fi\n",
      "maps onto Rand each row of Ais of size 1\u0002N, we obtain a 1\u00021\u0002N-\n",
      "sized tensor as the partial derivative of fiwith respect to a row of A.\n",
      "We stack the partial derivatives (5.91) and get the desired gradient\n",
      "in (5.87) via\n",
      "@fi\n",
      "@A=2\n",
      "666666666640>\n",
      "...\n",
      "0>\n",
      "x>\n",
      "0>\n",
      "...\n",
      "0>3\n",
      "777777777752R1\u0002(M\u0002N): (5.92)\n",
      "Example 5.13 (Gradient of Matrices with Respect to Matrices)\n",
      "Consider a matrix R2RM\u0002Nandf:RM\u0002N!RN\u0002Nwith\n",
      "f(R) =R>R=:K2RN\u0002N; (5.93)\n",
      "where we seek the gradient dK=dR.\n",
      "To solve this hard problem, let us ﬁrst write down what we already\n",
      "know: The gradient has the dimensions\n",
      "dK\n",
      "dR2R(N\u0002N)\u0002(M\u0002N); (5.94)\n",
      "which is a tensor. Moreover,\n",
      "dKpq\n",
      "dR2R1\u0002M\u0002N(5.95)\n",
      "forp;q= 1;:::;N , whereKpqis the (p;q)th entry ofK=f(R). De-\n",
      "noting theith column of Rbyri, every entry of Kis given by the dot\n",
      "product of two columns of R, i.e.,\n",
      "Kpq=r>\n",
      "prq=MX\n",
      "m=1RmpRmq: (5.96)\n",
      "When we now compute the partial derivative@Kpq\n",
      "@Rijwe obtain\n",
      "@Kpq\n",
      "@Rij=MX\n",
      "m=1@\n",
      "@RijRmpRmq=@pqij; (5.97)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "158 Vector Calculus\n",
      "@pqij=8\n",
      ">><\n",
      ">>:Riq ifj=p; p6=q\n",
      "Rip ifj=q; p6=q\n",
      "2Riqifj=p; p=q\n",
      "0 otherwise: (5.98)\n",
      "From (5.94), we know that the desired gradient has the dimension (N\u0002\n",
      "N)\u0002(M\u0002N), and every single entry of this tensor is given by @pqij\n",
      "in (5.98), where p;q;j = 1;:::;N andi= 1;:::;M .\n",
      "5.5 Useful Identities for Computing Gradients\n",
      "In the following, we list some useful gradients that are frequently required\n",
      "in a machine learning context (Petersen and Pedersen, 2012). Here, we\n",
      "use tr (\u0001)as the trace (see Deﬁnition 4.4), det(\u0001)as the determinant (see\n",
      "Section 4.1) and f(X)\u00001as the inverse of f(X), assuming it exists.\n",
      "@\n",
      "@Xf(X)>=\u0012@f(X)\n",
      "@X\u0013>\n",
      "(5.99)\n",
      "@\n",
      "@Xtr(f(X)) = tr\u0012@f(X)\n",
      "@X\u0013\n",
      "(5.100)\n",
      "@\n",
      "@Xdet(f(X)) = det(f(X))tr\u0012\n",
      "f(X)\u00001@f(X)\n",
      "@X\u0013\n",
      "(5.101)\n",
      "@\n",
      "@Xf(X)\u00001=\u0000f(X)\u00001@f(X)\n",
      "@Xf(X)\u00001(5.102)\n",
      "@a>X\u00001b\n",
      "@X=\u0000(X\u00001)>ab>(X\u00001)>(5.103)\n",
      "@x>a\n",
      "@x=a>(5.104)\n",
      "@a>x\n",
      "@x=a>(5.105)\n",
      "@a>Xb\n",
      "@X=ab>(5.106)\n",
      "@x>Bx\n",
      "@x=x>(B+B>) (5.107)\n",
      "@\n",
      "@s(x\u0000As)>W(x\u0000As) =\u00002(x\u0000As)>WA for symmetric W\n",
      "(5.108)\n",
      "Remark. In this book, we only cover traces and transposes of matrices.\n",
      "However, we have seen that derivatives can be higher-dimensional ten-\n",
      "sors, in which case the usual trace and transpose are not deﬁned. In these\n",
      "cases, the trace of a D\u0002D\u0002E\u0002Ftensor would be an E\u0002F-dimensional\n",
      "matrix. This is a special case of a tensor contraction. Similarly, when we\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.6 Backpropagation and Automatic Differentiation 159\n",
      "“transpose” a tensor, we mean swapping the ﬁrst two dimensions. Specif-\n",
      "ically, in (5.99) through (5.102), we require tensor-related computations\n",
      "when we work with multivariate functions f(\u0001)and compute derivatives\n",
      "with respect to matrices (and choose not to vectorize them as discussed in\n",
      "Section 5.4). }\n",
      "5.6 Backpropagation and Automatic Differentiation\n",
      "A good discussion\n",
      "about\n",
      "backpropagation\n",
      "and the chain rule is\n",
      "available at a blog\n",
      "by Tim Vieira at\n",
      "https://tinyurl.\n",
      "com/ycfm2yrw .In many machine learning applications, we ﬁnd good model parameters\n",
      "by performing gradient descent (Section 7.1), which relies on the fact\n",
      "that we can compute the gradient of a learning objective with respect\n",
      "to the parameters of the model. For a given objective function, we can\n",
      "obtain the gradient with respect to the model parameters using calculus\n",
      "and applying the chain rule; see Section 5.2.2. We already had a taste in\n",
      "Section 5.3 when we looked at the gradient of a squared loss with respect\n",
      "to the parameters of a linear regression model.\n",
      "Consider the function\n",
      "f(x) =q\n",
      "x2+ exp(x2) + cos\u0000x2+ exp(x2)\u0001: (5.109)\n",
      "By application of the chain rule, and noting that differentiation is linear,\n",
      "we compute the gradient\n",
      "df\n",
      "dx=2x+ 2xexp(x2)\n",
      "2p\n",
      "x2+ exp(x2)\u0000sin\u0000x2+ exp(x2)\u0001\u00002x+ 2xexp(x2)\u0001\n",
      "= 2x \n",
      "1\n",
      "2p\n",
      "x2+ exp(x2)\u0000sin\u0000x2+ exp(x2)\u0001!\n",
      "\u00001 + exp(x2)\u0001:\n",
      "(5.110)\n",
      "Writing out the gradient in this explicit way is often impractical since it\n",
      "often results in a very lengthy expression for a derivative. In practice,\n",
      "it means that, if we are not careful, the implementation of the gradient\n",
      "could be signiﬁcantly more expensive than computing the function, which\n",
      "imposes unnecessary overhead. For training deep neural network mod-\n",
      "els, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation\n",
      "1962; Rumelhart et al., 1986) is an efﬁcient way to compute the gradient\n",
      "of an error function with respect to the parameters of the model.\n",
      "5.6.1 Gradients in a Deep Network\n",
      "An area where the chain rule is used to an extreme is deep learning, where\n",
      "the function value yis computed as a many-level function composition\n",
      "y= (fK\u000efK\u00001\u000e\u0001\u0001\u0001\u000ef1)(x) =fK(fK\u00001(\u0001\u0001\u0001(f1(x))\u0001\u0001\u0001));(5.111)\n",
      "wherexare the inputs (e.g., images), yare the observations (e.g., class\n",
      "labels), and every function fi,i= 1;:::;K , possesses its own parameters.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "160 Vector Calculus\n",
      "Figure 5.8 Forward\n",
      "pass in a multi-layer\n",
      "neural network to\n",
      "compute the loss L\n",
      "as a function of the\n",
      "inputsxand the\n",
      "parametersAi;bi.x fK\n",
      "A0;b0 AK\u00001;bK\u00001L fK\u00001\n",
      "AK\u00002;bK\u00002f1\n",
      "A1;b1\n",
      "In neural networks with multiple layers, we have functions fi(xi\u00001) = We discuss the case,\n",
      "where the activation\n",
      "functions are\n",
      "identical in each\n",
      "layer to unclutter\n",
      "notation.\u001b(Ai\u00001xi\u00001+bi\u00001)in theith layer. Here xi\u00001is the output of layer i\u00001\n",
      "and\u001ban activation function, such as the logistic sigmoid1\n",
      "1+e\u0000x,tanh or a\n",
      "rectiﬁed linear unit (ReLU). In order to train these models, we require the\n",
      "gradient of a loss function Lwith respect to all model parameters Aj;bj\n",
      "forj= 1;:::;K . This also requires us to compute the gradient of Lwith\n",
      "respect to the inputs of each layer. For example, if we have inputs xand\n",
      "observationsyand a network structure deﬁned by\n",
      "f0:=x (5.112)\n",
      "fi:=\u001bi(Ai\u00001fi\u00001+bi\u00001); i= 1;:::;K; (5.113)\n",
      "see also Figure 5.8 for a visualization, we may be interested in ﬁnding\n",
      "Aj;bjforj= 0;:::;K\u00001, such that the squared loss\n",
      "L(\u0012) =ky\u0000fK(\u0012;x)k2(5.114)\n",
      "is minimized, where \u0012=fA0;b0;:::;AK\u00001;bK\u00001g.\n",
      "To obtain the gradients with respect to the parameter set \u0012, we require\n",
      "the partial derivatives of Lwith respect to the parameters \u0012j=fAj;bjg\n",
      "of each layer j= 0;:::;K\u00001. The chain rule allows us to determine the\n",
      "partial derivatives as A more in-depth\n",
      "discussion about\n",
      "gradients of neural\n",
      "networks can be\n",
      "found in Justin\n",
      "Domke’s lecture\n",
      "notes\n",
      "https://tinyurl.\n",
      "com/yalcxgtv .@L\n",
      "@\u0012K\u00001=@L\n",
      "@fK@fK\n",
      "@\u0012K\u00001(5.115)\n",
      "@L\n",
      "@\u0012K\u00002=@L\n",
      "@fK@fK\n",
      "@fK\u00001@fK\u00001\n",
      "@\u0012K\u00002(5.116)\n",
      "@L\n",
      "@\u0012K\u00003=@L\n",
      "@fK@fK\n",
      "@fK\u00001@fK\u00001\n",
      "@fK\u00002@fK\u00002\n",
      "@\u0012K\u00003(5.117)\n",
      "@L\n",
      "@\u0012i=@L\n",
      "@fK@fK\n",
      "@fK\u00001\u0001\u0001\u0001@fi+2\n",
      "@fi+1@fi+1\n",
      "@\u0012i(5.118)\n",
      "Theorange terms are partial derivatives of the output of a layer with\n",
      "respect to its inputs, whereas the blue terms are partial derivatives of\n",
      "the output of a layer with respect to its parameters. Assuming, we have\n",
      "already computed the partial derivatives @L=@\u0012i+1, then most of the com-\n",
      "putation can be reused to compute @L=@\u0012i. The additional terms that we\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.6 Backpropagation and Automatic Differentiation 161\n",
      "Figure 5.9\n",
      "Backward pass in a\n",
      "multi-layer neural\n",
      "network to compute\n",
      "the gradients of the\n",
      "loss function.x fK\n",
      "A0;b0 AK\u00001;bK\u00001L fK\u00001\n",
      "AK\u00002;bK\u00002f1\n",
      "A1;b1\n",
      "Figure 5.10 Simple\n",
      "graph illustrating\n",
      "the ﬂow of data\n",
      "fromxtoyvia\n",
      "some intermediate\n",
      "variablesa;b.xaby\n",
      "need to compute are indicated by the boxes. Figure 5.9 visualizes that the\n",
      "gradients are passed backward through the network.\n",
      "5.6.2 Automatic Differentiation\n",
      "It turns out that backpropagation is a special case of a general technique\n",
      "in numerical analysis called automatic differentiation . We can think of au- automatic\n",
      "differentiation tomatic differentation as a set of techniques to numerically (in contrast to\n",
      "symbolically) evaluate the exact (up to machine precision) gradient of a\n",
      "function by working with intermediate variables and applying the chain\n",
      "rule. Automatic differentiation applies a series of elementary arithmetic Automatic\n",
      "differentiation is\n",
      "different from\n",
      "symbolic\n",
      "differentiation and\n",
      "numerical\n",
      "approximations of\n",
      "the gradient, e.g., by\n",
      "using ﬁnite\n",
      "differences.operations, e.g., addition and multiplication and elementary functions,\n",
      "e.g.,sin;cos;exp;log. By applying the chain rule to these operations, the\n",
      "gradient of quite complicated functions can be computed automatically.\n",
      "Automatic differentiation applies to general computer programs and has\n",
      "forward and reverse modes. Baydin et al. (2018) give a great overview of\n",
      "automatic differentiation in machine learning.\n",
      "Figure 5.10 shows a simple graph representing the data ﬂow from in-\n",
      "putsxto outputsyvia some intermediate variables a;b. If we were to\n",
      "compute the derivative dy=dx, we would apply the chain rule and obtain\n",
      "dy\n",
      "dx=dy\n",
      "dbdb\n",
      "dada\n",
      "dx: (5.119)\n",
      "Intuitively, the forward and reverse mode differ in the order of multipli- In the general case,\n",
      "we work with\n",
      "Jacobians, which\n",
      "can be vectors,\n",
      "matrices, or tensors.cation. Due to the associativity of matrix multiplication, we can choose\n",
      "between\n",
      "dy\n",
      "dx=\u0012dy\n",
      "dbdb\n",
      "da\u0013da\n",
      "dx; (5.120)\n",
      "dy\n",
      "dx=dy\n",
      "db\u0012db\n",
      "dada\n",
      "dx\u0013\n",
      ": (5.121)\n",
      "Equation (5.120) would be the reverse mode because gradients are prop- reverse mode\n",
      "agated backward through the graph, i.e., reverse to the data ﬂow. Equa-\n",
      "tion (5.121) would be the forward mode , where the gradients ﬂow with forward mode\n",
      "the data from left to right through the graph.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "162 Vector Calculus\n",
      "In the following, we will focus on reverse mode automatic differentia-\n",
      "tion, which is backpropagation. In the context of neural networks, where\n",
      "the input dimensionality is often much higher than the dimensionality of\n",
      "the labels, the reverse mode is computationally signiﬁcantly cheaper than\n",
      "the forward mode. Let us start with an instructive example.\n",
      "Example 5.14\n",
      "Consider the function\n",
      "f(x) =q\n",
      "x2+ exp(x2) + cos\u0000x2+ exp(x2)\u0001\n",
      "(5.122)\n",
      "from (5.109). If we were to implement a function fon a computer, we\n",
      "would be able to save some computation by using intermediate variables : intermediate\n",
      "variables\n",
      "a=x2; (5.123)\n",
      "b= exp(a); (5.124)\n",
      "c=a+b; (5.125)\n",
      "d=pc; (5.126)\n",
      "e= cos(c); (5.127)\n",
      "f=d+e: (5.128)\n",
      "Figure 5.11\n",
      "Computation graph\n",
      "with inputsx,\n",
      "function values f,\n",
      "and intermediate\n",
      "variablesa;b;c;d;e .x (\u0001)2aexp(\u0001)b\n",
      "+cp\u0001\n",
      "cos(\u0001)d\n",
      "e+f\n",
      "This is the same kind of thinking process that occurs when applying\n",
      "the chain rule. Note that the preceding set of equations requires fewer\n",
      "operations than a direct implementation of the function f(x)as deﬁned\n",
      "in (5.109). The corresponding computation graph in Figure 5.11 shows\n",
      "the ﬂow of data and computations required to obtain the function value\n",
      "f.\n",
      "The set of equations that include intermediate variables can be thought\n",
      "of as a computation graph, a representation that is widely used in imple-\n",
      "mentations of neural network software libraries. We can directly compute\n",
      "the derivatives of the intermediate variables with respect to their corre-\n",
      "sponding inputs by recalling the deﬁnition of the derivative of elementary\n",
      "functions. We obtain the following:\n",
      "@a\n",
      "@x= 2x (5.129)\n",
      "@b\n",
      "@a= exp(a) (5.130)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.6 Backpropagation and Automatic Differentiation 163\n",
      "@c\n",
      "@a= 1 =@c\n",
      "@b(5.131)\n",
      "@d\n",
      "@c=1\n",
      "2pc(5.132)\n",
      "@e\n",
      "@c=\u0000sin(c) (5.133)\n",
      "@f\n",
      "@d= 1 =@f\n",
      "@e: (5.134)\n",
      "By looking at the computation graph in Figure 5.11, we can compute\n",
      "@f=@x by working backward from the output and obtain\n",
      "@f\n",
      "@c=@f\n",
      "@d@d\n",
      "@c+@f\n",
      "@e@e\n",
      "@c(5.135)\n",
      "@f\n",
      "@b=@f\n",
      "@c@c\n",
      "@b(5.136)\n",
      "@f\n",
      "@a=@f\n",
      "@b@b\n",
      "@a+@f\n",
      "@c@c\n",
      "@a(5.137)\n",
      "@f\n",
      "@x=@f\n",
      "@a@a\n",
      "@x: (5.138)\n",
      "Note that we implicitly applied the chain rule to obtain @f=@x . By substi-\n",
      "tuting the results of the derivatives of the elementary functions, we get\n",
      "@f\n",
      "@c= 1\u00011\n",
      "2pc+ 1\u0001(\u0000sin(c)) (5.139)\n",
      "@f\n",
      "@b=@f\n",
      "@c\u00011 (5.140)\n",
      "@f\n",
      "@a=@f\n",
      "@bexp(a) +@f\n",
      "@c\u00011 (5.141)\n",
      "@f\n",
      "@x=@f\n",
      "@a\u00012x: (5.142)\n",
      "By thinking of each of the derivatives above as a variable, we observe\n",
      "that the computation required for calculating the derivative is of similar\n",
      "complexity as the computation of the function itself. This is quite counter-\n",
      "intuitive since the mathematical expression for the derivative@f\n",
      "@x(5.110)\n",
      "is signiﬁcantly more complicated than the mathematical expression of the\n",
      "functionf(x)in (5.109).\n",
      "Automatic differentiation is a formalization of Example 5.14. Let x1;:::;xd\n",
      "be the input variables to the function, xd+1;:::;xD\u00001be the intermediate\n",
      "variables, and xDthe output variable. Then the computation graph can be\n",
      "expressed as follows:\n",
      "Fori=d+ 1;:::;D :xi=gi(xPa(xi)); (5.143)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "164 Vector Calculus\n",
      "where thegi(\u0001)are elementary functions and xPa(xi)are the parent nodes\n",
      "of the variable xiin the graph. Given a function deﬁned in this way, we\n",
      "can use the chain rule to compute the derivative of the function in a step-\n",
      "by-step fashion. Recall that by deﬁnition f=xDand hence\n",
      "@f\n",
      "@xD= 1: (5.144)\n",
      "For other variables xi, we apply the chain rule\n",
      "@f\n",
      "@xi=X\n",
      "xj:xi2Pa(xj)@f\n",
      "@xj@xj\n",
      "@xi=X\n",
      "xj:xi2Pa(xj)@f\n",
      "@xj@gj\n",
      "@xi; (5.145)\n",
      "where Pa(xj)is the set of parent nodes of xjin the computation graph.\n",
      "Equation (5.143) is the forward propagation of a function, whereas (5.145) Auto-differentiation\n",
      "in reverse mode\n",
      "requires a parse\n",
      "tree.is the backpropagation of the gradient through the computation graph.\n",
      "For neural network training, we backpropagate the error of the prediction\n",
      "with respect to the label.\n",
      "The automatic differentiation approach above works whenever we have\n",
      "a function that can be expressed as a computation graph, where the ele-\n",
      "mentary functions are differentiable. In fact, the function may not even be\n",
      "a mathematical function but a computer program. However, not all com-\n",
      "puter programs can be automatically differentiated, e.g., if we cannot ﬁnd\n",
      "differential elementary functions. Programming structures, such as for\n",
      "loops and ifstatements, require more care as well.\n",
      "5.7 Higher-Order Derivatives\n",
      "So far, we have discussed gradients, i.e., ﬁrst-order derivatives. Some-\n",
      "times, we are interested in derivatives of higher order, e.g., when we want\n",
      "to use Newton’s Method for optimization, which requires second-order\n",
      "derivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed\n",
      "the Taylor series to approximate functions using polynomials. In the mul-\n",
      "tivariate case, we can do exactly the same. In the following, we will do\n",
      "exactly this. But let us start with some notation.\n",
      "Consider a function f:R2!Rof two variables x;y. We use the\n",
      "following notation for higher-order partial derivatives (and for gradients):\n",
      "@2f\n",
      "@x2is the second partial derivative of fwith respect to x.\n",
      "@nf\n",
      "@xnis thenth partial derivative of fwith respect to x.\n",
      "@2f\n",
      "@y@x=@\n",
      "@y\u0000@f\n",
      "@x\u0001\n",
      "is the partial derivative obtained by ﬁrst partial differ-\n",
      "entiating with respect to xand then with respect to y.\n",
      "@2f\n",
      "@x@yis the partial derivative obtained by ﬁrst partial differentiating by\n",
      "yand thenx.\n",
      "TheHessian is the collection of all second-order partial derivatives. Hessian\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.8 Linearization and Multivariate Taylor Series 165\n",
      "Figure 5.12 Linear\n",
      "approximation of a\n",
      "function. The\n",
      "original function f\n",
      "is linearized at\n",
      "x0=\u00002using a\n",
      "ﬁrst-order Taylor\n",
      "series expansion.\n",
      "−4−2 0 2 4\n",
      "x−2−101f(x)f(x)\n",
      "f(x0)f(x0) +f/prime(x0)(x−x0)\n",
      "Iff(x;y)is a twice (continuously) differentiable function, then\n",
      "@2f\n",
      "@x@y=@2f\n",
      "@y@x; (5.146)\n",
      "i.e., the order of differentiation does not matter, and the corresponding\n",
      "Hessian matrix Hessian matrix\n",
      "H=2\n",
      "664@2f\n",
      "@x2@2f\n",
      "@x@y\n",
      "@2f\n",
      "@x@y@2f\n",
      "@y23\n",
      "775(5.147)\n",
      "is symmetric. The Hessian is denoted as r2\n",
      "x;yf(x;y). Generally, for x2Rn\n",
      "andf:Rn!R, the Hessian is an n\u0002nmatrix. The Hessian measures\n",
      "the curvature of the function locally around (x;y).\n",
      "Remark (Hessian of a Vector Field) .Iff:Rn!Rmis a vector ﬁeld, the\n",
      "Hessian is an (m\u0002n\u0002n)-tensor. }\n",
      "5.8 Linearization and Multivariate Taylor Series\n",
      "The gradientrfof a function fis often used for a locally linear approxi-\n",
      "mation offaroundx0:\n",
      "f(x)\u0019f(x0) + (rxf)(x0)(x\u0000x0): (5.148)\n",
      "Here (rxf)(x0)is the gradient of fwith respect to x, evaluated at x0.\n",
      "Figure 5.12 illustrates the linear approximation of a function fat an input\n",
      "x0. The original function is approximated by a straight line. This approx-\n",
      "imation is locally accurate, but the farther we move away from x0the\n",
      "worse the approximation gets. Equation (5.148) is a special case of a mul-\n",
      "tivariate Taylor series expansion of fatx0, where we consider only the\n",
      "ﬁrst two terms. We discuss the more general case in the following, which\n",
      "will allow for better approximations.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "166 Vector Calculus\n",
      "Figure 5.13\n",
      "Visualizing outer\n",
      "products. Outer\n",
      "products of vectors\n",
      "increase the\n",
      "dimensionality of\n",
      "the array by 1 per\n",
      "term. (a) The outer\n",
      "product of two\n",
      "vectors results in a\n",
      "matrix; (b) the\n",
      "outer product of\n",
      "three vectors yields\n",
      "a third-order tensor.\n",
      "(a) Given a vector \u000e2R4, we obtain the outer product \u000e2:=\u000e\n",
      "\u000e=\u000e\u000e>2\n",
      "R4\u00024as a matrix.\n",
      "(b) An outer product \u000e3:=\u000e\n",
      "\u000e\n",
      "\u000e2R4\u00024\u00024results in a third-order tensor (“three-\n",
      "dimensional matrix”), i.e., an array with three indexes.\n",
      "Deﬁnition 5.7 (Multivariate Taylor Series) .We consider a function\n",
      "f:RD!R (5.149)\n",
      "x7!f(x);x2RD; (5.150)\n",
      "that is smooth at x0. When we deﬁne the difference vector \u000e:=x\u0000x0,\n",
      "themultivariate Taylor series offat(x0)is deﬁned as multivariate Taylor\n",
      "series\n",
      "f(x) =1X\n",
      "k=0Dk\n",
      "xf(x0)\n",
      "k!\u000ek; (5.151)\n",
      "whereDk\n",
      "xf(x0)is thek-th (total) derivative of fwith respect to x, eval-\n",
      "uated atx0.\n",
      "Deﬁnition 5.8 (Taylor Polynomial) .TheTaylor polynomial of degreenof Taylor polynomial\n",
      "fatx0contains the ﬁrst n+ 1components of the series in (5.151) and is\n",
      "deﬁned as\n",
      "Tn(x) =nX\n",
      "k=0Dk\n",
      "xf(x0)\n",
      "k!\u000ek: (5.152)\n",
      "In (5.151) and (5.152), we used the slightly sloppy notation of \u000ek,\n",
      "which is not deﬁned for vectors x2RD; D > 1;andk > 1. Note that\n",
      "bothDk\n",
      "xfand\u000ekarek-th order tensors, i.e., k-dimensional arrays. The A vector can be\n",
      "implemented as a\n",
      "one-dimensional\n",
      "array, a matrix as a\n",
      "two-dimensional\n",
      "array.kth-order tensor \u000ek2Rktimesz}|{\n",
      "D\u0002D\u0002:::\u0002Dis obtained as a k-fold outer product,\n",
      "denoted by\n",
      ", of the vector \u000e2RD. For example,\n",
      "\u000e2:=\u000e\n",
      "\u000e=\u000e\u000e>;\u000e2[i;j] =\u000e[i]\u000e[j] (5.153)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.8 Linearization and Multivariate Taylor Series 167\n",
      "\u000e3:=\u000e\n",
      "\u000e\n",
      "\u000e;\u000e3[i;j;k ] =\u000e[i]\u000e[j]\u000e[k]: (5.154)\n",
      "Figure 5.13 visualizes two such outer products. In general, we obtain the\n",
      "terms\n",
      "Dk\n",
      "xf(x0)\u000ek=DX\n",
      "i1=1\u0001\u0001\u0001DX\n",
      "ik=1Dk\n",
      "xf(x0)[i1;:::;ik]\u000e[i1]\u0001\u0001\u0001\u000e[ik](5.155)\n",
      "in the Taylor series, where Dk\n",
      "xf(x0)\u000ekcontainsk-th order polynomials.\n",
      "Now that we deﬁned the Taylor series for vector ﬁelds, let us explicitly\n",
      "write down the ﬁrst terms Dk\n",
      "xf(x0)\u000ekof the Taylor series expansion for\n",
      "k= 0;:::; 3and\u000e:=x\u0000x0:np.einsum(\n",
      "'i,i',Df1,d)\n",
      "np.einsum(\n",
      "'ij,i,j',\n",
      "Df2,d,d)\n",
      "np.einsum(\n",
      "'ijk,i,j,k',\n",
      "Df3,d,d,d)k= 0 :D0\n",
      "xf(x0)\u000e0=f(x0)2R (5.156)\n",
      "k= 1 :D1\n",
      "xf(x0)\u000e1=rxf(x0)|{z}\n",
      "1\u0002D\u000e|{z}\n",
      "D\u00021=DX\n",
      "i=1rxf(x0)[i]\u000e[i]2R(5.157)\n",
      "k= 2 :D2\n",
      "xf(x0)\u000e2=tr\u0000H(x0)|{z}\n",
      "D\u0002D\u000e|{z}\n",
      "D\u00021\u000e>\n",
      "|{z}\n",
      "1\u0002D\u0001=\u000e>H(x0)\u000e (5.158)\n",
      "=DX\n",
      "i=1DX\n",
      "j=1H[i;j]\u000e[i]\u000e[j]2R (5.159)\n",
      "k= 3 :D3\n",
      "xf(x0)\u000e3=DX\n",
      "i=1DX\n",
      "j=1DX\n",
      "k=1D3\n",
      "xf(x0)[i;j;k ]\u000e[i]\u000e[j]\u000e[k]2R\n",
      "(5.160)\n",
      "Here,H(x0)is the Hessian of fevaluated atx0.\n",
      "Example 5.15 (Taylor Series Expansion of a Function with Two Vari-\n",
      "ables)\n",
      "Consider the function\n",
      "f(x;y) =x2+ 2xy+y3: (5.161)\n",
      "We want to compute the Taylor series expansion of fat(x0;y0) = (1;2).\n",
      "Before we start, let us discuss what to expect: The function in (5.161) is\n",
      "a polynomial of degree 3. We are looking for a Taylor series expansion,\n",
      "which itself is a linear combination of polynomials. Therefore, we do not\n",
      "expect the Taylor series expansion to contain terms of fourth or higher\n",
      "order to express a third-order polynomial. This means that it should be\n",
      "sufﬁcient to determine the ﬁrst four terms of (5.151) for an exact alterna-\n",
      "tive representation of (5.161).\n",
      "To determine the Taylor series expansion, we start with the constant\n",
      "term and the ﬁrst-order derivatives, which are given by\n",
      "f(1;2) = 13 (5.162)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "168 Vector Calculus\n",
      "@f\n",
      "@x= 2x+ 2y=)@f\n",
      "@x(1;2) = 6 (5.163)\n",
      "@f\n",
      "@y= 2x+ 3y2=)@f\n",
      "@y(1;2) = 14: (5.164)\n",
      "Therefore, we obtain\n",
      "D1\n",
      "x;yf(1;2) =rx;yf(1;2) =h\n",
      "@f\n",
      "@x(1;2)@f\n",
      "@y(1;2)i\n",
      "=\u00026 14\u00032R1\u00022\n",
      "(5.165)\n",
      "such that\n",
      "D1\n",
      "x;yf(1;2)\n",
      "1!\u000e=\u00026 14\u0003\u0014x\u00001\n",
      "y\u00002\u0015\n",
      "= 6(x\u00001) + 14(y\u00002):(5.166)\n",
      "Note thatD1\n",
      "x;yf(1;2)\u000econtains only linear terms, i.e., ﬁrst-order polyno-\n",
      "mials.\n",
      "The second-order partial derivatives are given by\n",
      "@2f\n",
      "@x2= 2 =)@2f\n",
      "@x2(1;2) = 2 (5.167)\n",
      "@2f\n",
      "@y2= 6y=)@2f\n",
      "@y2(1;2) = 12 (5.168)\n",
      "@2f\n",
      "@y@x= 2 =)@2f\n",
      "@y@x(1;2) = 2 (5.169)\n",
      "@2f\n",
      "@x@y= 2 =)@2f\n",
      "@x@y(1;2) = 2: (5.170)\n",
      "When we collect the second-order partial derivatives, we obtain the Hes-\n",
      "sian\n",
      "H=\"@2f\n",
      "@x2@2f\n",
      "@x@y\n",
      "@2f\n",
      "@y@x@2f\n",
      "@y2#\n",
      "=\u00142 2\n",
      "2 6y\u0015\n",
      "; (5.171)\n",
      "such that\n",
      "H(1;2) =\u00142 2\n",
      "2 12\u0015\n",
      "2R2\u00022: (5.172)\n",
      "Therefore, the next term of the Taylor-series expansion is given by\n",
      "D2\n",
      "x;yf(1;2)\n",
      "2!\u000e2=1\n",
      "2\u000e>H(1;2)\u000e (5.173a)\n",
      "=1\n",
      "2\u0002x\u00001y\u00002\u0003\u00142 2\n",
      "2 12\u0015\u0014x\u00001\n",
      "y\u00002\u0015\n",
      "(5.173b)\n",
      "= (x\u00001)2+ 2(x\u00001)(y\u00002) + 6(y\u00002)2:(5.173c)\n",
      "Here,D2\n",
      "x;yf(1;2)\u000e2contains only quadratic terms, i.e., second-order poly-\n",
      "nomials.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "5.8 Linearization and Multivariate Taylor Series 169\n",
      "The third-order derivatives are obtained as\n",
      "D3\n",
      "x;yf=h\n",
      "@H\n",
      "@x@H\n",
      "@yi\n",
      "2R2\u00022\u00022; (5.174)\n",
      "D3\n",
      "x;yf[:;:;1] =@H\n",
      "@x=\"@3f\n",
      "@x3@3f\n",
      "@x2@y\n",
      "@3f\n",
      "@x@y@x@3f\n",
      "@x@y2#\n",
      "; (5.175)\n",
      "D3\n",
      "x;yf[:;:;2] =@H\n",
      "@y=\"@3f\n",
      "@y@x2@3f\n",
      "@y@x@y\n",
      "@3f\n",
      "@y2@x@3f\n",
      "@y3#\n",
      ": (5.176)\n",
      "Since most second-order partial derivatives in the Hessian in (5.171) are\n",
      "constant, the only nonzero third-order partial derivative is\n",
      "@3f\n",
      "@y3= 6 =)@3f\n",
      "@y3(1;2) = 6: (5.177)\n",
      "Higher-order derivatives and the mixed derivatives of degree 3 (e.g.,\n",
      "@f3\n",
      "@x2@y) vanish, such that\n",
      "D3\n",
      "x;yf[:;:;1] =\u00140 0\n",
      "0 0\u0015\n",
      "; D3\n",
      "x;yf[:;:;2] =\u00140 0\n",
      "0 6\u0015\n",
      "(5.178)\n",
      "and\n",
      "D3\n",
      "x;yf(1;2)\n",
      "3!\u000e3= (y\u00002)3; (5.179)\n",
      "which collects all cubic terms of the Taylor series. Overall, the (exact)\n",
      "Taylor series expansion of fat(x0;y0) = (1;2)is\n",
      "f(x) =f(1;2) +D1\n",
      "x;yf(1;2)\u000e+D2\n",
      "x;yf(1;2)\n",
      "2!\u000e2+D3\n",
      "x;yf(1;2)\n",
      "3!\u000e3\n",
      "(5.180a)\n",
      "=f(1;2) +@f(1;2)\n",
      "@x(x\u00001) +@f(1;2)\n",
      "@y(y\u00002)\n",
      "+1\n",
      "2!\u0012@2f(1;2)\n",
      "@x2(x\u00001)2+@2f(1;2)\n",
      "@y2(y\u00002)2\n",
      "+ 2@2f(1;2)\n",
      "@x@y(x\u00001)(y\u00002)\u0013\n",
      "+1\n",
      "6@3f(1;2)\n",
      "@y3(y\u00002)3(5.180b)\n",
      "= 13 + 6(x\u00001) + 14(y\u00002)\n",
      "+ (x\u00001)2+ 6(y\u00002)2+ 2(x\u00001)(y\u00002) + (y\u00002)3:(5.180c)\n",
      "In this case, we obtained an exact Taylor series expansion of the polyno-\n",
      "mial in (5.161), i.e., the polynomial in (5.180c) is identical to the original\n",
      "polynomial in (5.161). In this particular example, this result is not sur-\n",
      "prising since the original function was a third-order polynomial, which\n",
      "we expressed through a linear combination of constant terms, ﬁrst-order,\n",
      "second-order, and third-order polynomials in (5.180c).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "170 Vector Calculus\n",
      "5.9 Further Reading\n",
      "Further details of matrix differentials, along with a short review of the\n",
      "required linear algebra, can be found in Magnus and Neudecker (2007).\n",
      "Automatic differentiation has had a long history, and we refer to Griewank\n",
      "and Walther (2003), Griewank and Walther (2008), and Elliott (2009)\n",
      "and the references therein.\n",
      "In machine learning (and other disciplines), we often need to compute\n",
      "expectations, i.e., we need to solve integrals of the form\n",
      "Ex[f(x)] =Z\n",
      "f(x)p(x)dx: (5.181)\n",
      "Even ifp(x)is in a convenient form (e.g., Gaussian), this integral gen-\n",
      "erally cannot be solved analytically. The Taylor series expansion of fis\n",
      "one way of ﬁnding an approximate solution: Assuming p(x) =N\u0000\u0016;\u0006\u0001\n",
      "is Gaussian, then the ﬁrst-order Taylor series expansion around \u0016locally\n",
      "linearizes the nonlinear function f. For linear functions, we can compute\n",
      "the mean (and the covariance) exactly if p(x)is Gaussian distributed (see\n",
      "Section 6.5). This property is heavily exploited by the extended Kalman extended Kalman\n",
      "ﬁlter ﬁlter (Maybeck, 1979) for online state estimation in nonlinear dynami-\n",
      "cal systems (also called “state-space models”). Other deterministic ways\n",
      "to approximate the integral in (5.181) are the unscented transform (Julier unscented transform\n",
      "and Uhlmann, 1997), which does not require any gradients, or the Laplace Laplace\n",
      "approximation approximation (MacKay, 2003; Bishop, 2006; Murphy, 2012), which uses\n",
      "a second-order Taylor series expansion (requiring the Hessian) for a local\n",
      "Gaussian approximation of p(x)around its mode.\n",
      "Exercises\n",
      "5.1 Compute the derivative f0(x)for\n",
      "f(x) = log(x4) sin(x3):\n",
      "5.2 Compute the derivative f0(x)of the logistic sigmoid\n",
      "f(x) =1\n",
      "1 + exp(\u0000x):\n",
      "5.3 Compute the derivative f0(x)of the function\n",
      "f(x) = exp(\u00001\n",
      "2\u001b2(x\u0000\u0016)2);\n",
      "where\u0016; \u001b2Rare constants.\n",
      "5.4 Compute the Taylor polynomials Tn,n= 0;:::; 5off(x) = sin(x) + cos(x)\n",
      "atx0= 0.\n",
      "5.5 Consider the following functions:\n",
      "f1(x) = sin(x1) cos(x2);x2R2\n",
      "f2(x;y) =x>y;x;y2Rn\n",
      "f3(x) =xx>;x2Rn\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 171\n",
      "a. What are the dimensions of@fi\n",
      "@x?\n",
      "b. Compute the Jacobians.\n",
      "5.6 Differentiate fwith respect to tandgwith respect to X, where\n",
      "f(t) = sin(log(t>t)); t2RD\n",
      "g(X) =tr(AXB );A2RD\u0002E;X2RE\u0002F;B2RF\u0002D;\n",
      "where tr (\u0001)denotes the trace.\n",
      "5.7 Compute the derivatives df=dxof the following functions by using the chain\n",
      "rule. Provide the dimensions of every single partial derivative. Describe your\n",
      "steps in detail.\n",
      "a.\n",
      "f(z) = log(1 + z); z =x>x;x2RD\n",
      "b.\n",
      "f(z) = sin(z);z=Ax+b;A2RE\u0002D;x2RD;b2RE\n",
      "where sin(\u0001)is applied to every element of z.\n",
      "5.8 Compute the derivatives df=dxof the following functions. Describe your\n",
      "steps in detail.\n",
      "a. Use the chain rule. Provide the dimensions of every single partial deriva-\n",
      "tive.\n",
      "f(z) = exp(\u00001\n",
      "2z)\n",
      "z=g(y) =y>S\u00001y\n",
      "y=h(x) =x\u0000\u0016\n",
      "wherex;\u00162RD,S2RD\u0002D.\n",
      "b.\n",
      "f(x) =tr(xx>+\u001b2I);x2RD\n",
      "Here tr (A)is the trace of A, i.e., the sum of the diagonal elements Aii.\n",
      "Hint: Explicitly write out the outer product.\n",
      "c. Use the chain rule. Provide the dimensions of every single partial deriva-\n",
      "tive. You do not need to compute the product of the partial derivatives\n",
      "explicitly.\n",
      "f= tanh(z)2RM\n",
      "z=Ax+b;x2RN;A2RM\u0002N;b2RM:\n",
      "Here, tanh is applied to every component of z.\n",
      "5.9 We deﬁne\n",
      "g(z;\u0017) := logp(x;z)\u0000logq(z;\u0017)\n",
      "z:=t(\u000f;\u0017)\n",
      "for differentiable functions p;q;t , andx2RD;z2RE;\u00172RF;\u000f2RG. By\n",
      "using the chain rule, compute the gradient\n",
      "d\n",
      "d\u0017g(z;\u0017):\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "6\n",
      "Probability and Distributions\n",
      "Probability, loosely speaking, concerns the study of uncertainty. Probabil-\n",
      "ity can be thought of as the fraction of times an event occurs, or as a degree\n",
      "of belief about an event. We then would like to use this probability to mea-\n",
      "sure the chance of something occurring in an experiment. As mentioned\n",
      "in Chapter 1, we often quantify uncertainty in the data, uncertainty in the\n",
      "machine learning model, and uncertainty in the predictions produced by\n",
      "the model. Quantifying uncertainty requires the idea of a random variable , random variable\n",
      "which is a function that maps outcomes of random experiments to a set of\n",
      "properties that we are interested in. Associated with the random variable\n",
      "is a function that measures the probability that a particular outcome (or\n",
      "set of outcomes) will occur; this is called the probability distribution . probability\n",
      "distribution Probability distributions are used as a building block for other con-\n",
      "cepts, such as probabilistic modeling (Section 8.4), graphical models (Sec-\n",
      "tion 8.5), and model selection (Section 8.6). In the next section, we present\n",
      "the three concepts that deﬁne a probability space (the sample space, the\n",
      "events, and the probability of an event) and how they are related to a\n",
      "fourth concept called the random variable. The presentation is deliber-\n",
      "ately slightly hand wavy since a rigorous presentation may occlude the\n",
      "intuition behind the concepts. An outline of the concepts presented in this\n",
      "chapter are shown in Figure 6.1.\n",
      "6.1 Construction of a Probability Space\n",
      "The theory of probability aims at deﬁning a mathematical structure to\n",
      "describe random outcomes of experiments. For example, when tossing a\n",
      "single coin, we cannot determine the outcome, but by doing a large num-\n",
      "ber of coin tosses, we can observe a regularity in the average outcome.\n",
      "Using this mathematical structure of probability, the goal is to perform\n",
      "automated reasoning, and in this sense, probability generalizes logical\n",
      "reasoning (Jaynes, 2003).\n",
      "6.1.1 Philosophical Issues\n",
      "When constructing automated reasoning systems, classical Boolean logic\n",
      "does not allow us to express certain forms of plausible reasoning. Consider\n",
      "172\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "6.1 Construction of a Probability Space 173\n",
      "Figure 6.1 A mind\n",
      "map of the concepts\n",
      "related to random\n",
      "variables and\n",
      "probability\n",
      "distributions, as\n",
      "described in this\n",
      "chapter.\n",
      "Random variable\n",
      "& distributionSum rule Product ruleBayes’ Theorem\n",
      "Summary statisticsMean Variance\n",
      "Transformations\n",
      "Independence\n",
      "Inner productGaussian\n",
      "Bernoulli\n",
      "BetaSufﬁcient statistics\n",
      "Exponential familyChapter 9\n",
      "Regression\n",
      "Chapter 10\n",
      "Dimensionality\n",
      "reduction\n",
      "Chapter 11\n",
      "Density estimationProperty\n",
      "SimilarityExample\n",
      "ExampleConjugate\n",
      "Property Finite\n",
      "the following scenario: We observe that Ais false. We ﬁnd Bbecomes\n",
      "less plausible, although no conclusion can be drawn from classical logic.\n",
      "We observe that Bis true. It seems Abecomes more plausible. We use\n",
      "this form of reasoning daily. We are waiting for a friend, and consider\n",
      "three possibilities: H1, she is on time; H2, she has been delayed by trafﬁc;\n",
      "and H3, she has been abducted by aliens. When we observe our friend\n",
      "is late, we must logically rule out H1. We also tend to consider H2 to be\n",
      "more likely, though we are not logically required to do so. Finally, we may\n",
      "consider H3 to be possible, but we continue to consider it quite unlikely.\n",
      "How do we conclude H2 is the most plausible answer? Seen in this way, “For plausible\n",
      "reasoning it is\n",
      "necessary to extend\n",
      "the discrete true and\n",
      "false values of truth\n",
      "to continuous\n",
      "plausibilities”\n",
      "(Jaynes, 2003).probability theory can be considered a generalization of Boolean logic. In\n",
      "the context of machine learning, it is often applied in this way to formalize\n",
      "the design of automated reasoning systems. Further arguments about how\n",
      "probability theory is the foundation of reasoning systems can be found\n",
      "in Pearl (1988).\n",
      "The philosophical basis of probability and how it should be somehow\n",
      "related to what we think should be true (in the logical sense) was studied\n",
      "by Cox (Jaynes, 2003). Another way to think about it is that if we are\n",
      "precise about our common sense we end up constructing probabilities.\n",
      "E. T. Jaynes (1922–1998) identiﬁed three mathematical criteria, which\n",
      "must apply to all plausibilities:\n",
      "1. The degrees of plausibility are represented by real numbers.\n",
      "2. These numbers must be based on the rules of common sense.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "174 Probability and Distributions\n",
      "3. The resulting reasoning must be consistent, with the three following\n",
      "meanings of the word “consistent”:\n",
      "(a) Consistency or non-contradiction: When the same result can be\n",
      "reached through different means, the same plausibility value must\n",
      "be found in all cases.\n",
      "(b) Honesty: All available data must be taken into account.\n",
      "(c) Reproducibility: If our state of knowledge about two problems are\n",
      "the same, then we must assign the same degree of plausibility to\n",
      "both of them.\n",
      "The Cox–Jaynes theorem proves these plausibilities to be sufﬁcient to\n",
      "deﬁne the universal mathematical rules that apply to plausibility p, up to\n",
      "transformation by an arbitrary monotonic function. Crucially, these rules\n",
      "arethe rules of probability.\n",
      "Remark. In machine learning and statistics, there are two major interpre-\n",
      "tations of probability: the Bayesian and frequentist interpretations (Bishop,\n",
      "2006; Efron and Hastie, 2016). The Bayesian interpretation uses probabil-\n",
      "ity to specify the degree of uncertainty that the user has about an event. It\n",
      "is sometimes referred to as “subjective probability” or “degree of belief”.\n",
      "The frequentist interpretation considers the relative frequencies of events\n",
      "of interest to the total number of events that occurred. The probability of\n",
      "an event is deﬁned as the relative frequency of the event in the limit when\n",
      "one has inﬁnite data. }\n",
      "Some machine learning texts on probabilistic models use lazy notation\n",
      "and jargon, which is confusing. This text is no exception. Multiple distinct\n",
      "concepts are all referred to as “probability distribution”, and the reader\n",
      "has to often disentangle the meaning from the context. One trick to help\n",
      "make sense of probability distributions is to check whether we are trying\n",
      "to model something categorical (a discrete random variable) or some-\n",
      "thing continuous (a continuous random variable). The kinds of questions\n",
      "we tackle in machine learning are closely related to whether we are con-\n",
      "sidering categorical or continuous models.\n",
      "6.1.2 Probability and Random Variables\n",
      "There are three distinct ideas that are often confused when discussing\n",
      "probabilities. First is the idea of a probability space, which allows us to\n",
      "quantify the idea of a probability. However, we mostly do not work directly\n",
      "with this basic probability space. Instead, we work with random variables\n",
      "(the second idea), which transfers the probability to a more convenient\n",
      "(often numerical) space. The third idea is the idea of a distribution or law\n",
      "associated with a random variable. We will introduce the ﬁrst two ideas\n",
      "in this section and expand on the third idea in Section 6.2.\n",
      "Modern probability is based on a set of axioms proposed by Kolmogorov\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.1 Construction of a Probability Space 175\n",
      "(Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con-\n",
      "cepts of sample space, event space, and probability measure. The prob-\n",
      "ability space models a real-world process (referred to as an experiment)\n",
      "with random outcomes.\n",
      "The sample space \n",
      "Thesample space is the set of all possible outcomes of the experiment, sample space\n",
      "usually denoted by \n",
      ". For example, two successive coin tosses have\n",
      "a sample space of fhh, tt, ht, thg, where “h” denotes “heads” and “t”\n",
      "denotes “tails”.\n",
      "The event spaceA\n",
      "Theevent space is the space of potential results of the experiment. A event space\n",
      "subsetAof the sample space \n",
      "is in the event space Aif at the end\n",
      "of the experiment we can observe whether a particular outcome !2\n",
      "is inA. The event space Ais obtained by considering the collection of\n",
      "subsets of \n",
      ", and for discrete probability distributions (Section 6.2.1)\n",
      "Ais often the power set of \n",
      ".\n",
      "The probability P\n",
      "With each event A2A, we associate a number P(A)that measures the\n",
      "probability or degree of belief that the event will occur. P(A)is called\n",
      "theprobability ofA. probability\n",
      "The probability of a single event must lie in the interval [0;1], and the\n",
      "total probability over all outcomes in the sample space \n",
      "must be 1, i.e.,\n",
      "P(\n",
      ") = 1 . Given a probability space (\n",
      ";A;P), we want to use it to model\n",
      "some real-world phenomenon. In machine learning, we often avoid explic-\n",
      "itly referring to the probability space, but instead refer to probabilities on\n",
      "quantities of interest, which we denote by T. In this book, we refer to T\n",
      "as the target space and refer to elements of Tas states. We introduce a target space\n",
      "functionX: \n",
      "!T that takes an element of \n",
      "(an outcome) and returns\n",
      "a particular quantity of interest x, a value inT. This association/mapping\n",
      "from \n",
      "toTis called a random variable . For example, in the case of tossing random variable\n",
      "two coins and counting the number of heads, a random variable Xmaps\n",
      "to the three possible outcomes: X(hh) = 2 ,X(ht) = 1 ,X(th) = 1 , and\n",
      "X(tt) = 0 . In this particular case, T=f0;1;2g, and it is the probabilities\n",
      "on elements ofTthat we are interested in. For a ﬁnite sample space \n",
      "and The name “random\n",
      "variable” is a great\n",
      "source of\n",
      "misunderstanding\n",
      "as it is neither\n",
      "random nor is it a\n",
      "variable. It is a\n",
      "function.ﬁniteT, the function corresponding to a random variable is essentially a\n",
      "lookup table. For any subset S\u0012T , we associate PX(S)2[0;1](the\n",
      "probability) to a particular event occurring corresponding to the random\n",
      "variableX. Example 6.1 provides a concrete illustration of the terminol-\n",
      "ogy.\n",
      "Remark. The aforementioned sample space \n",
      "unfortunately is referred\n",
      "to by different names in different books. Another common name for \n",
      "is “state space” (Jacod and Protter, 2004), but state space is sometimes\n",
      "reserved for referring to states in a dynamical system (Hasselblatt and\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "176 Probability and Distributions\n",
      "Katok, 2003). Other names sometimes used to describe \n",
      "are: “sample\n",
      "description space”, “possibility space,” and “event space”. }\n",
      "Example 6.1\n",
      "We assume that the reader is already familiar with computing probabilities This toy example is\n",
      "essentially a biased\n",
      "coin ﬂip example.of intersections and unions of sets of events. A gentler introduction to\n",
      "probability with many examples can be found in chapter 2 of Walpole\n",
      "et al. (2011).\n",
      "Consider a statistical experiment where we model a funfair game con-\n",
      "sisting of drawing two coins from a bag (with replacement). There are\n",
      "coins from USA (denoted as $) and UK (denoted as £) in the bag, and\n",
      "since we draw two coins from the bag, there are four outcomes in total.\n",
      "The state space or sample space \n",
      "of this experiment is then ($, $), ($,\n",
      "£), (£, $), (£, £). Let us assume that the composition of the bag of coins is\n",
      "such that a draw returns at random a $ with probability 0:3.\n",
      "The event we are interested in is the total number of times the repeated\n",
      "draw returns $. Let us deﬁne a random variable Xthat maps the sample\n",
      "space \n",
      "toT, which denotes the number of times we draw $ out of the\n",
      "bag. We can see from the preceding sample space we can get zero $, one $,\n",
      "or two $s, and therefore T=f0;1;2g. The random variable X(a function\n",
      "or lookup table) can be represented as a table like the following:\n",
      "X(($;$)) = 2 (6.1)\n",
      "X(($;$)) = 1 (6.2)\n",
      "X(($;$)) = 1 (6.3)\n",
      "X(($;$)) = 0: (6.4)\n",
      "Since we return the ﬁrst coin we draw before drawing the second, this\n",
      "implies that the two draws are independent of each other, which we will\n",
      "discuss in Section 6.4.5. Note that there are two experimental outcomes,\n",
      "which map to the same event, where only one of the draws returns $.\n",
      "Therefore, the probability mass function (Section 6.2.1) of Xis given by\n",
      "P(X= 2) =P(($;$))\n",
      "=P($)\u0001P($)\n",
      "= 0:3\u00010:3 = 0:09 (6.5)\n",
      "P(X= 1) =P(($;$)[($;$))\n",
      "=P(($;$)) +P(($;$))\n",
      "= 0:3\u0001(1\u00000:3) + (1\u00000:3)\u00010:3 = 0:42 (6.6)\n",
      "P(X= 0) =P(($;$))\n",
      "=P($)\u0001P($)\n",
      "= (1\u00000:3)\u0001(1\u00000:3) = 0:49: (6.7)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.1 Construction of a Probability Space 177\n",
      "In the calculation, we equated two different concepts, the probability\n",
      "of the output of Xand the probability of the samples in \n",
      ". For example,\n",
      "in (6.7) we say P(X= 0) =P(($;$)). Consider the random variable\n",
      "X: \n",
      "!T and a subset S\u0012T (for example, a single element of T,\n",
      "such as the outcome that one head is obtained when tossing two coins).\n",
      "LetX\u00001(S)be the pre-image of SbyX, i.e., the set of elements of \n",
      "that\n",
      "map toSunderX;f!2\n",
      " :X(!)2Sg. One way to understand the\n",
      "transformation of probability from events in \n",
      "via the random variable\n",
      "Xis to associate it with the probability of the pre-image of S(Jacod and\n",
      "Protter, 2004). For S\u0012T, we have the notation\n",
      "PX(S) =P(X2S) =P(X\u00001(S)) =P(f!2\n",
      " :X(!)2Sg):(6.8)\n",
      "The left-hand side of (6.8) is the probability of the set of possible outcomes\n",
      "(e.g., number of $= 1) that we are interested in. Via the random variable\n",
      "X, which maps states to outcomes, we see in the right-hand side of (6.8)\n",
      "that this is the probability of the set of states (in \n",
      ") that have the property\n",
      "(e.g., $$,$$). We say that a random variable Xis distributed according\n",
      "to a particular probability distribution PX, which deﬁnes the probability\n",
      "mapping between the event and the probability of the outcome of the\n",
      "random variable. In other words, the function PXor equivalently P\u000eX\u00001\n",
      "is the lawordistribution of random variable X. law\n",
      "distributionRemark. The target space, that is, the range Tof the random variable X,\n",
      "is used to indicate the kind of probability space, i.e., a Trandom variable.\n",
      "WhenTis ﬁnite or countably inﬁnite, this is called a discrete random\n",
      "variable (Section 6.2.1). For continuous random variables (Section 6.2.2),\n",
      "we only considerT=RorT=RD. }\n",
      "6.1.3 Statistics\n",
      "Probability theory and statistics are often presented together, but they con-\n",
      "cern different aspects of uncertainty. One way of contrasting them is by the\n",
      "kinds of problems that are considered. Using probability, we can consider\n",
      "a model of some process, where the underlying uncertainty is captured\n",
      "by random variables, and we use the rules of probability to derive what\n",
      "happens. In statistics, we observe that something has happened and try\n",
      "to ﬁgure out the underlying process that explains the observations. In this\n",
      "sense, machine learning is close to statistics in its goals to construct a\n",
      "model that adequately represents the process that generated the data. We\n",
      "can use the rules of probability to obtain a “best-ﬁtting” model for some\n",
      "data.\n",
      "Another aspect of machine learning systems is that we are interested\n",
      "in generalization error (see Chapter 8). This means that we are actually\n",
      "interested in the performance of our system on instances that we will\n",
      "observe in future, which are not identical to the instances that we have\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "178 Probability and Distributions\n",
      "seen so far. This analysis of future performance relies on probability and\n",
      "statistics, most of which is beyond what will be presented in this chapter.\n",
      "The interested reader is encouraged to look at the books by Boucheron\n",
      "et al. (2013) and Shalev-Shwartz and Ben-David (2014). We will see more\n",
      "about statistics in Chapter 8.\n",
      "6.2 Discrete and Continuous Probabilities\n",
      "Let us focus our attention on ways to describe the probability of an event\n",
      "as introduced in Section 6.1. Depending on whether the target space is dis-\n",
      "crete or continuous, the natural way to refer to distributions is different.\n",
      "When the target space Tis discrete, we can specify the probability that a\n",
      "random variable Xtakes a particular value x2T, denoted as P(X=x).\n",
      "The expression P(X=x)for a discrete random variable Xis known as\n",
      "theprobability mass function . When the target space Tis continuous, e.g., probability mass\n",
      "function the real line R, it is more natural to specify the probability that a random\n",
      "variableXis in an interval, denoted by P(a6X6b)fora<b . By con-\n",
      "vention, we specify the probability that a random variable Xis less than\n",
      "a particular value x, denoted by P(X6x). The expression P(X6x)for\n",
      "a continuous random variable Xis known as the cumulative distribution cumulative\n",
      "distribution function function . We will discuss continuous random variables in Section 6.2.2.\n",
      "We will revisit the nomenclature and contrast discrete and continuous\n",
      "random variables in Section 6.2.3.\n",
      "Remark. We will use the phrase univariate distribution to refer to distribu- univariate\n",
      "tions of a single random variable (whose states are denoted by non-bold\n",
      "x). We will refer to distributions of more than one random variable as\n",
      "multivariate distributions, and will usually consider a vector of random multivariate\n",
      "variables (whose states are denoted by bold x). }\n",
      "6.2.1 Discrete Probabilities\n",
      "When the target space is discrete, we can imagine the probability distri-\n",
      "bution of multiple random variables as ﬁlling out a (multidimensional)\n",
      "array of numbers. Figure 6.2 shows an example. The target space of the\n",
      "joint probability is the Cartesian product of the target spaces of each of\n",
      "the random variables. We deﬁne the joint probability as the entry of both joint probability\n",
      "values jointly\n",
      "P(X=xi;Y=yj) =nij\n",
      "N; (6.9)\n",
      "wherenijis the number of events with state xiandyjandNthe total\n",
      "number of events. The joint probability is the probability of the intersec-\n",
      "tion of both events, that is, P(X=xi;Y=yj) =P(X=xi\\Y=yj).\n",
      "Figure 6.2 illustrates the probability mass function (pmf) of a discrete prob- probability mass\n",
      "function ability distribution. For two random variables XandY, the probability\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.2 Discrete and Continuous Probabilities 179\n",
      "Figure 6.2\n",
      "Visualization of a\n",
      "discrete bivariate\n",
      "probability mass\n",
      "function, with\n",
      "random variables X\n",
      "andY. This\n",
      "diagram is adapted\n",
      "from Bishop (2006).\n",
      "Xx1x2x3x4x5Y\n",
      "y3y2y1\n",
      "nijo\n",
      "rjciz}|{\n",
      "thatX=xandY=yis (lazily) written as p(x;y)and is called the joint\n",
      "probability. One can think of a probability as a function that takes state\n",
      "xandyand returns a real number, which is the reason we write p(x;y).\n",
      "Themarginal probability thatXtakes the value xirrespective of the value marginal probability\n",
      "of random variable Yis (lazily) written as p(x). We writeX\u0018p(x)to\n",
      "denote that the random variable Xis distributed according to p(x). If we\n",
      "consider only the instances where X=x, then the fraction of instances\n",
      "(theconditional probability ) for whichY=yis written (lazily) as p(yjx).conditional\n",
      "probability\n",
      "Example 6.2\n",
      "Consider two random variables XandY, whereXhas ﬁve possible states\n",
      "andYhas three possible states, as shown in Figure 6.2. We denote by nij\n",
      "the number of events with state X=xiandY=yj, and denote by\n",
      "Nthe total number of events. The value ciis the sum of the individual\n",
      "frequencies for the ith column, that is, ci=P3\n",
      "j=1nij. Similarly, the value\n",
      "rjis the row sum, that is, rj=P5\n",
      "i=1nij. Using these deﬁnitions, we can\n",
      "compactly express the distribution of XandY.\n",
      "The probability distribution of each random variable, the marginal\n",
      "probability, can be seen as the sum over a row or column\n",
      "P(X=xi) =ci\n",
      "N=P3\n",
      "j=1nij\n",
      "N(6.10)\n",
      "and\n",
      "P(Y=yj) =rj\n",
      "N=P5\n",
      "i=1nij\n",
      "N; (6.11)\n",
      "whereciandrjare theith column and jth row of the probability table,\n",
      "respectively. By convention, for discrete random variables with a ﬁnite\n",
      "number of events, we assume that probabilties sum up to one, that is,\n",
      "5X\n",
      "i=1P(X=xi) = 1 and3X\n",
      "j=1P(Y=yj) = 1: (6.12)\n",
      "The conditional probability is the fraction of a row or column in a par-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "180 Probability and Distributions\n",
      "ticular cell. For example, the conditional probability of YgivenXis\n",
      "P(Y=yjjX=xi) =nij\n",
      "ci; (6.13)\n",
      "and the conditional probability of XgivenYis\n",
      "P(X=xijY=yj) =nij\n",
      "rj: (6.14)\n",
      "In machine learning, we use discrete probability distributions to model\n",
      "categorical variables , i.e., variables that take a ﬁnite set of unordered val- categorical variable\n",
      "ues. They could be categorical features, such as the degree taken at uni-\n",
      "versity when used for predicting the salary of a person, or categorical la-\n",
      "bels, such as letters of the alphabet when doing handwriting recognition.\n",
      "Discrete distributions are also often used to construct probabilistic models\n",
      "that combine a ﬁnite number of continuous distributions (Chapter 11).\n",
      "6.2.2 Continuous Probabilities\n",
      "We consider real-valued random variables in this section, i.e., we consider\n",
      "target spaces that are intervals of the real line R. In this book, we pretend\n",
      "that we can perform operations on real random variables as if we have dis-\n",
      "crete probability spaces with ﬁnite states. However, this simpliﬁcation is\n",
      "not precise for two situations: when we repeat something inﬁnitely often,\n",
      "and when we want to draw a point from an interval. The ﬁrst situation\n",
      "arises when we discuss generalization errors in machine learning (Chap-\n",
      "ter 8). The second situation arises when we want to discuss continuous\n",
      "distributions, such as the Gaussian (Section 6.5). For our purposes, the\n",
      "lack of precision allows for a briefer introduction to probability.\n",
      "Remark. In continuous spaces, there are two additional technicalities,\n",
      "which are counterintuitive. First, the set of all subsets (used to deﬁne\n",
      "the event spaceAin Section 6.1) is not well behaved enough. Aneeds\n",
      "to be restricted to behave well under set complements, set intersections,\n",
      "and set unions. Second, the size of a set (which in discrete spaces can be\n",
      "obtained by counting the elements) turns out to be tricky. The size of a\n",
      "set is called its measure . For example, the cardinality of discrete sets, the measure\n",
      "length of an interval in R, and the volume of a region in Rdare all mea-\n",
      "sures. Sets that behave well under set operations and additionally have\n",
      "a topology are called a Borel\u001b-algebra . Betancourt details a careful con- Borel\u001b-algebra\n",
      "struction of probability spaces from set theory without being bogged down\n",
      "in technicalities; see https://tinyurl.com/yb3t6mfd . For a more pre-\n",
      "cise construction, we refer to Billingsley (1995) and Jacod and Protter\n",
      "(2004).\n",
      "In this book, we consider real-valued random variables with their cor-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.2 Discrete and Continuous Probabilities 181\n",
      "responding Borel \u001b-algebra. We consider random variables with values in\n",
      "RDto be a vector of real-valued random variables. }\n",
      "Deﬁnition 6.1 (Probability Density Function) .A functionf:RD!Ris\n",
      "called a probability density function (pdf) if probability density\n",
      "function\n",
      "pdf 1.8x2RD:f(x)>0\n",
      "2. Its integral exists and\n",
      "Z\n",
      "RDf(x)dx= 1: (6.15)\n",
      "For probability mass functions (pmf) of discrete random variables, the\n",
      "integral in (6.15) is replaced with a sum (6.12).\n",
      "Observe that the probability density function is any function fthat is\n",
      "non-negative and integrates to one. We associate a random variable X\n",
      "with this function fby\n",
      "P(a6X6b) =Zb\n",
      "af(x)dx; (6.16)\n",
      "wherea;b2Randx2Rare outcomes of the continuous random vari-\n",
      "ableX. Statesx2RDare deﬁned analogously by considering a vector\n",
      "ofx2R. This association (6.16) is called the lawordistribution of the law\n",
      "random variable X.P(X=x)is a set of\n",
      "measure zero. Remark. In contrast to discrete random variables, the probability of a con-\n",
      "tinuous random variable Xtaking a particular value P(X=x)is zero.\n",
      "This is like trying to specify an interval in (6.16) where a=b.}\n",
      "Deﬁnition 6.2 (Cumulative Distribution Function) .Acumulative distribu- cumulative\n",
      "distribution function tion function (cdf) of a multivariate real-valued random variable Xwith\n",
      "statesx2RDis given by\n",
      "FX(x) =P(X16x1;:::;XD6xD); (6.17)\n",
      "whereX= [X1;:::;XD]>,x= [x1;:::;xD]>, and the right-hand side\n",
      "represents the probability that random variable Xitakes the value smaller\n",
      "than or equal to xi.\n",
      "There are cdfs,\n",
      "which do not have\n",
      "corresponding pdfs.The cdf can be expressed also as the integral of the probability density\n",
      "functionf(x)so that\n",
      "FX(x) =Zx1\n",
      "\u00001\u0001\u0001\u0001ZxD\n",
      "\u00001f(z1;:::;zD)dz1\u0001\u0001\u0001dzD: (6.18)\n",
      "Remark. We reiterate that there are in fact two distinct concepts when\n",
      "talking about distributions. First is the idea of a pdf (denoted by f(x)),\n",
      "which is a nonnegative function that sums to one. Second is the law of a\n",
      "random variable X, that is, the association of a random variable Xwith\n",
      "the pdff(x). }\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "182 Probability and Distributions\n",
      "Figure 6.3\n",
      "Examples of\n",
      "(a) discrete and\n",
      "(b) continuous\n",
      "uniform\n",
      "distributions. See\n",
      "Example 6.3 for\n",
      "details of the\n",
      "distributions.\n",
      "−1 0 1 2\n",
      "z0.00.51.01.52.0P(Z=z)\n",
      "(a) Discrete distribution\n",
      "−1 0 1 2\n",
      "x0.00.51.01.52.0p(x) (b) Continuous distribution\n",
      "For most of this book, we will not use the notation f(x)andFX(x)as\n",
      "we mostly do not need to distinguish between the pdf and cdf. However,\n",
      "we will need to be careful about pdfs and cdfs in Section 6.7.\n",
      "6.2.3 Contrasting Discrete and Continuous Distributions\n",
      "Recall from Section 6.1.2 that probabilities are positive and the total prob-\n",
      "ability sums up to one. For discrete random variables (see (6.12)), this\n",
      "implies that the probability of each state must lie in the interval [0;1].\n",
      "However, for continuous random variables the normalization (see (6.15))\n",
      "does not imply that the value of the density is less than or equal to 1for\n",
      "all values. We illustrate this in Figure 6.3 using the uniform distribution uniform distribution\n",
      "for both discrete and continuous random variables.\n",
      "Example 6.3\n",
      "We consider two examples of the uniform distribution, where each state is\n",
      "equally likely to occur. This example illustrates some differences between\n",
      "discrete and continuous probability distributions.\n",
      "LetZbe a discrete uniform random variable with three states fz=\n",
      "\u00001:1;z= 0:3;z= 1:5g. The probability mass function can be represented The actual values of\n",
      "these states are not\n",
      "meaningful here,\n",
      "and we deliberately\n",
      "chose numbers to\n",
      "drive home the\n",
      "point that we do not\n",
      "want to use (and\n",
      "should ignore) the\n",
      "ordering of the\n",
      "states.as a table of probability values:\n",
      "z\n",
      "P(Z=z)\u00001:1\n",
      "1\n",
      "30:3\n",
      "1\n",
      "31:5\n",
      "1\n",
      "3\n",
      "Alternatively, we can think of this as a graph (Figure 6.3(a)), where we\n",
      "use the fact that the states can be located on the x-axis, and the y-axis\n",
      "represents the probability of a particular state. The y-axis in Figure 6.3(a)\n",
      "is deliberately extended so that is it the same as in Figure 6.3(b).\n",
      "LetXbe a continuous random variable taking values in the range 0:96\n",
      "X61:6, as represented by Figure 6.3(b). Observe that the height of the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\n",
      "Table 6.1\n",
      "Nomenclature for\n",
      "probability\n",
      "distributions.Type “Point probability” “Interval probability”\n",
      "Discrete P(X=x) Not applicable\n",
      "Probability mass function\n",
      "Continuous p(x) P(X6x)\n",
      "Probability density function Cumulative distribution function\n",
      "density can be greater than 1. However, it needs to hold that\n",
      "Z1:6\n",
      "0:9p(x)dx= 1: (6.19)\n",
      "Remark. There is an additional subtlety with regards to discrete prob-\n",
      "ability distributions. The states z1;:::;zddo not in principle have any\n",
      "structure, i.e., there is usually no way to compare them, for example\n",
      "z1= red;z2= green;z3= blue . However, in many machine learning\n",
      "applications discrete states take numerical values, e.g., z1=\u00001:1;z2=\n",
      "0:3;z3= 1:5, where we could say z1< z 2< z 3. Discrete states that as-\n",
      "sume numerical values are particularly useful because we often consider\n",
      "expected values (Section 6.4.1) of random variables. }\n",
      "Unfortunately, machine learning literature uses notation and nomen-\n",
      "clature that hides the distinction between the sample space \n",
      ", the target\n",
      "spaceT, and the random variable X. For a value xof the set of possible\n",
      "outcomes of the random variable X, i.e.,x2T,p(x)denotes the prob- We think of the\n",
      "outcomexas the\n",
      "argument that\n",
      "results in the\n",
      "probabilityp(x).ability that random variable Xhas the outcome x. For discrete random\n",
      "variables, this is written as P(X=x), which is known as the probabil-\n",
      "ity mass function. The pmf is often referred to as the “distribution”. For\n",
      "continuous variables, p(x)is called the probability density function (often\n",
      "referred to as a density). To muddy things even further, the cumulative\n",
      "distribution function P(X6x)is often also referred to as the “distribu-\n",
      "tion”. In this chapter, we will use the notation Xto refer to both univariate\n",
      "and multivariate random variables, and denote the states by xandxre-\n",
      "spectively. We summarize the nomenclature in Table 6.1.\n",
      "Remark. We will be using the expression “probability distribution” not\n",
      "only for discrete probability mass functions but also for continuous proba-\n",
      "bility density functions, although this is technically incorrect. In line with\n",
      "most machine learning literature, we also rely on context to distinguish\n",
      "the different uses of the phrase probability distribution. }\n",
      "6.3 Sum Rule, Product Rule, and Bayes’ Theorem\n",
      "We think of probability theory as an extension to logical reasoning. As we\n",
      "discussed in Section 6.1.1, the rules of probability presented here follow\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "184 Probability and Distributions\n",
      "naturally from fulﬁlling the desiderata (Jaynes, 2003, chapter 2). Prob-\n",
      "abilistic modeling (Section 8.4) provides a principled foundation for de-\n",
      "signing machine learning methods. Once we have deﬁned probability dis-\n",
      "tributions (Section 6.2) corresponding to the uncertainties of the data and\n",
      "our problem, it turns out that there are only two fundamental rules, the\n",
      "sum rule and the product rule.\n",
      "Recall from (6.9) that p(x;y)is the joint distribution of the two ran-\n",
      "dom variables x;y. The distributions p(x)andp(y)are the correspond-\n",
      "ing marginal distributions, and p(yjx)is the conditional distribution of y\n",
      "givenx. Given the deﬁnitions of the marginal and conditional probability\n",
      "for discrete and continuous random variables in Section 6.2, we can now\n",
      "present the two fundamental rules in probability theory. These two rules\n",
      "arise\n",
      "naturally (Jaynes,\n",
      "2003) from the\n",
      "requirements we\n",
      "discussed in\n",
      "Section 6.1.1.The ﬁrst rule, the sum rule , states that\n",
      "sum rulep(x) =8\n",
      ">><\n",
      ">>:X\n",
      "y2Yp(x;y) ifyis discrete\n",
      "Z\n",
      "Yp(x;y)dy ifyis continuous; (6.20)\n",
      "whereYare the states of the target space of random variable Y. This\n",
      "means that we sum out (or integrate out) the set of states yof the random\n",
      "variableY. The sum rule is also known as the marginalization property . marginalization\n",
      "property The sum rule relates the joint distribution to a marginal distribution. In\n",
      "general, when the joint distribution contains more than two random vari-\n",
      "ables, the sum rule can be applied to any subset of the random variables,\n",
      "resulting in a marginal distribution of potentially more than one random\n",
      "variable. More concretely, if x= [x1;:::;xD]>, we obtain the marginal\n",
      "p(xi) =Z\n",
      "p(x1;:::;xD)dxni (6.21)\n",
      "by repeated application of the sum rule where we integrate/sum out all\n",
      "random variables except xi, which is indicated by ni, which reads “all\n",
      "excepti.”\n",
      "Remark. Many of the computational challenges of probabilistic modeling\n",
      "are due to the application of the sum rule. When there are many variables\n",
      "or discrete variables with many states, the sum rule boils down to per-\n",
      "forming a high-dimensional sum or integral. Performing high-dimensional\n",
      "sums or integrals is generally computationally hard, in the sense that there\n",
      "is no known polynomial-time algorithm to calculate them exactly. }\n",
      "The second rule, known as the product rule , relates the joint distribution product rule\n",
      "to the conditional distribution via\n",
      "p(x;y) =p(yjx)p(x): (6.22)\n",
      "The product rule can be interpreted as the fact that every joint distribu-\n",
      "tion of two random variables can be factorized (written as a product)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.3 Sum Rule, Product Rule, and Bayes’ Theorem 185\n",
      "of two other distributions. The two factors are the marginal distribu-\n",
      "tion of the ﬁrst random variable p(x), and the conditional distribution\n",
      "of the second random variable given the ﬁrst p(yjx). Since the ordering\n",
      "of random variables is arbitrary in p(x;y), the product rule also implies\n",
      "p(x;y) =p(xjy)p(y). To be precise, (6.22) is expressed in terms of the\n",
      "probability mass functions for discrete random variables. For continuous\n",
      "random variables, the product rule is expressed in terms of the probability\n",
      "density functions (Section 6.2.3).\n",
      "In machine learning and Bayesian statistics, we are often interested in\n",
      "making inferences of unobserved (latent) random variables given that we\n",
      "have observed other random variables. Let us assume we have some prior\n",
      "knowledgep(x)about an unobserved random variable xand some rela-\n",
      "tionshipp(yjx)betweenxand a second random variable y, which we\n",
      "can observe. If we observe y, we can use Bayes’ theorem to draw some\n",
      "conclusions about xgiven the observed values of y.Bayes’ theorem (also Bayes’ theorem\n",
      "Bayes’ rule orBayes’ law ) Bayes’ rule\n",
      "Bayes’ law\n",
      "p(xjy)|{z}\n",
      "posterior=likelihoodz}|{\n",
      "p(yjx)priorz}|{\n",
      "p(x)\n",
      "p(y)|{z}\n",
      "evidence(6.23)\n",
      "is a direct consequence of the product rule in (6.22) since\n",
      "p(x;y) =p(xjy)p(y) (6.24)\n",
      "and\n",
      "p(x;y) =p(yjx)p(x) (6.25)\n",
      "so that\n",
      "p(xjy)p(y) =p(yjx)p(x)()p(xjy) =p(yjx)p(x)\n",
      "p(y):(6.26)\n",
      "In (6.23),p(x)is the prior , which encapsulates our subjective prior prior\n",
      "knowledge of the unobserved (latent) variable xbefore observing any\n",
      "data. We can choose any prior that makes sense to us, but it is critical to\n",
      "ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even\n",
      "if they are very rare.\n",
      "Thelikelihoodp(yjx)describes how xandyare related, and in the likelihood\n",
      "The likelihood is\n",
      "sometimes also\n",
      "called the\n",
      "“measurement\n",
      "model”.case of discrete probability distributions, it is the probability of the data y\n",
      "if we were to know the latent variable x. Note that the likelihood is not a\n",
      "distribution in x, but only iny. We callp(yjx)either the “likelihood of\n",
      "x(giveny)” or the “probability of ygivenx” but never the likelihood of\n",
      "y(MacKay, 2003).\n",
      "Theposteriorp(xjy)is the quantity of interest in Bayesian statistics posterior\n",
      "because it expresses exactly what we are interested in, i.e., what we know\n",
      "aboutxafter having observed y.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "186 Probability and Distributions\n",
      "The quantity\n",
      "p(y) :=Z\n",
      "p(yjx)p(x)dx=EX[p(yjx)] (6.27)\n",
      "is the marginal likelihood /evidence . The right-hand side of (6.27) uses the marginal likelihood\n",
      "evidence expectation operator which we deﬁne in Section 6.4.1. By deﬁnition, the\n",
      "marginal likelihood integrates the numerator of (6.23) with respect to the\n",
      "latent variable x. Therefore, the marginal likelihood is independent of\n",
      "x, and it ensures that the posterior p(xjy)is normalized. The marginal\n",
      "likelihood can also be interpreted as the expected likelihood where we\n",
      "take the expectation with respect to the prior p(x). Beyond normalization\n",
      "of the posterior, the marginal likelihood also plays an important role in\n",
      "Bayesian model selection, as we will discuss in Section 8.6. Due to the\n",
      "integration in (8.44), the evidence is often hard to compute. Bayes’ theorem is\n",
      "also called the\n",
      "“probabilistic\n",
      "inverse.”Bayes’ theorem (6.23) allows us to invert the relationship between x\n",
      "andygiven by the likelihood. Therefore, Bayes’ theorem is sometimes\n",
      "called the probabilistic inverse . We will discuss Bayes’ theorem further inprobabilistic inverse\n",
      "Section 8.4.\n",
      "Remark. In Bayesian statistics, the posterior distribution is the quantity\n",
      "of interest as it encapsulates all available information from the prior and\n",
      "the data. Instead of carrying the posterior around, it is possible to focus\n",
      "on some statistic of the posterior, such as the maximum of the posterior,\n",
      "which we will discuss in Section 8.3. However, focusing on some statistic\n",
      "of the posterior leads to loss of information. If we think in a bigger con-\n",
      "text, then the posterior can be used within a decision-making system, and\n",
      "having the full posterior can be extremely useful and lead to decisions that\n",
      "are robust to disturbances. For example, in the context of model-based re-\n",
      "inforcement learning, Deisenroth et al. (2015) show that using the full\n",
      "posterior distribution of plausible transition functions leads to very fast\n",
      "(data/sample efﬁcient) learning, whereas focusing on the maximum of\n",
      "the posterior leads to consistent failures. Therefore, having the full pos-\n",
      "terior can be very useful for a downstream task. In Chapter 9, we will\n",
      "continue this discussion in the context of linear regression. }\n",
      "6.4 Summary Statistics and Independence\n",
      "We are often interested in summarizing sets of random variables and com-\n",
      "paring pairs of random variables. A statistic of a random variable is a de-\n",
      "terministic function of that random variable. The summary statistics of a\n",
      "distribution provide one useful view of how a random variable behaves,\n",
      "and as the name suggests, provide numbers that summarize and charac-\n",
      "terize the distribution. We describe the mean and the variance, two well-\n",
      "known summary statistics. Then we discuss two ways to compare a pair\n",
      "of random variables: ﬁrst, how to say that two random variables are inde-\n",
      "pendent; and second, how to compute an inner product between them.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.4 Summary Statistics and Independence 187\n",
      "6.4.1 Means and Covariances\n",
      "Mean and (co)variance are often useful to describe properties of probabil-\n",
      "ity distributions (expected values and spread). We will see in Section 6.6\n",
      "that there is a useful family of distributions (called the exponential fam-\n",
      "ily), where the statistics of the random variable capture all possible infor-\n",
      "mation.\n",
      "The concept of the expected value is central to machine learning, and\n",
      "the foundational concepts of probability itself can be derived from the\n",
      "expected value (Whittle, 2000).\n",
      "Deﬁnition 6.3 (Expected Value) .Theexpected value of a function g:R! expected value\n",
      "Rof a univariate continuous random variable X\u0018p(x)is given by\n",
      "EX[g(x)] =Z\n",
      "Xg(x)p(x)dx: (6.28)\n",
      "Correspondingly, the expected value of a function gof a discrete random\n",
      "variableX\u0018p(x)is given by\n",
      "EX[g(x)] =X\n",
      "x2Xg(x)p(x); (6.29)\n",
      "whereXis the set of possible outcomes (the target space) of the random\n",
      "variableX.\n",
      "In this section, we consider discrete random variables to have numerical\n",
      "outcomes. This can be seen by observing that the function gtakes real\n",
      "numbers as inputs. The expected value\n",
      "of a function of a\n",
      "random variable is\n",
      "sometimes referred\n",
      "to as the law of the\n",
      "unconscious\n",
      "statistician (Casella\n",
      "and Berger, 2002,\n",
      "Section 2.2).Remark. We consider multivariate random variables Xas a ﬁnite vector\n",
      "of univariate random variables [X1;:::;XD]>. For multivariate random\n",
      "variables, we deﬁne the expected value element wise\n",
      "EX[g(x)] =2\n",
      "64EX1[g(x1)]\n",
      "...\n",
      "EXD[g(xD)]3\n",
      "752RD; (6.30)\n",
      "where the subscript EXdindicates that we are taking the expected value\n",
      "with respect to the dth element of the vector x. }\n",
      "Deﬁnition 6.3 deﬁnes the meaning of the notation EXas the operator\n",
      "indicating that we should take the integral with respect to the probabil-\n",
      "ity density (for continuous distributions) or the sum over all states (for\n",
      "discrete distributions). The deﬁnition of the mean (Deﬁnition 6.4), is a\n",
      "special case of the expected value, obtained by choosing gto be the iden-\n",
      "tity function.\n",
      "Deﬁnition 6.4 (Mean) .Themean of a random variable Xwith states mean\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "188 Probability and Distributions\n",
      "x2RDis an average and is deﬁned as\n",
      "EX[x] =2\n",
      "64EX1[x1]\n",
      "...\n",
      "EXD[xD]3\n",
      "752RD; (6.31)\n",
      "where\n",
      "EXd[xd] :=8\n",
      ">><\n",
      ">>:Z\n",
      "Xxdp(xd)dxd ifXis a continuous random variable\n",
      "X\n",
      "xi2Xxip(xd=xi)ifXis a discrete random variable\n",
      "(6.32)\n",
      "ford= 1;:::;D , where the subscript dindicates the corresponding di-\n",
      "mension ofx. The integral and sum are over the states Xof the target\n",
      "space of the random variable X.\n",
      "In one dimension, there are two other intuitive notions of “average”,\n",
      "which are the median and the mode . The median is the “middle” value if median\n",
      "we sort the values, i.e., 50% of the values are greater than the median and\n",
      "50% are smaller than the median. This idea can be generalized to contin-\n",
      "uous values by considering the value where the cdf (Deﬁnition 6.2) is 0:5.\n",
      "For distributions, which are asymmetric or have long tails, the median\n",
      "provides an estimate of a typical value that is closer to human intuition\n",
      "than the mean value. Furthermore, the median is more robust to outliers\n",
      "than the mean. The generalization of the median to higher dimensions is\n",
      "non-trivial as there is no obvious way to “sort” in more than one dimen-\n",
      "sion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the most mode\n",
      "frequently occurring value. For a discrete random variable, the mode is\n",
      "deﬁned as the value of xhaving the highest frequency of occurrence. For\n",
      "a continuous random variable, the mode is deﬁned as a peak in the density\n",
      "p(x). A particular density p(x)may have more than one mode, and fur-\n",
      "thermore there may be a very large number of modes in high-dimensional\n",
      "distributions. Therefore, ﬁnding all the modes of a distribution can be\n",
      "computationally challenging.\n",
      "Example 6.4\n",
      "Consider the two-dimensional distribution illustrated in Figure 6.4:\n",
      "p(x) = 0:4N\u0012\n",
      "x\f\f\f\f\u001410\n",
      "2\u0015\n",
      ";\u00141 0\n",
      "0 1\u0015\u0013\n",
      "+ 0:6N\u0012\n",
      "x\f\f\f\f\u00140\n",
      "0\u0015\n",
      ";\u00148:4 2:0\n",
      "2:0 1:7\u0015\u0013\n",
      ":\n",
      "(6.33)\n",
      "We will deﬁne the Gaussian distribution N\u0000\u0016; \u001b2\u0001\n",
      "in Section 6.5. Also\n",
      "shown is its corresponding marginal distribution in each dimension. Ob-\n",
      "serve that the distribution is bimodal (has two modes), but one of the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.4 Summary Statistics and Independence 189\n",
      "marginal distributions is unimodal (has one mode). The horizontal bi-\n",
      "modal univariate distribution illustrates that the mean and median can\n",
      "be different from each other. While it is tempting to deﬁne the two-\n",
      "dimensional median to be the concatenation of the medians in each di-\n",
      "mension, the fact that we cannot deﬁne an ordering of two-dimensional\n",
      "points makes it difﬁcult. When we say “cannot deﬁne an ordering”, we\n",
      "mean that there is more than one way to deﬁne the relation <so that\u00143\n",
      "0\u0015\n",
      "<\u00142\n",
      "3\u0015\n",
      ".\n",
      "Figure 6.4\n",
      "Illustration of the\n",
      "mean, mode, and\n",
      "median for a\n",
      "two-dimensional\n",
      "dataset, as well as\n",
      "its marginal\n",
      "densities.\n",
      "Mean\n",
      "Modes\n",
      "Median\n",
      "Remark. The expected value (Deﬁnition 6.3) is a linear operator. For ex-\n",
      "ample, given a real-valued function f(x) =ag(x)+bh(x)wherea;b2R\n",
      "andx2RD, we obtain\n",
      "EX[f(x)] =Z\n",
      "f(x)p(x)dx (6.34a)\n",
      "=Z\n",
      "[ag(x) +bh(x)]p(x)dx (6.34b)\n",
      "=aZ\n",
      "g(x)p(x)dx+bZ\n",
      "h(x)p(x)dx (6.34c)\n",
      "=aEX[g(x)] +bEX[h(x)]: (6.34d)\n",
      "}\n",
      "For two random variables, we may wish to characterize their correspon-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "190 Probability and Distributions\n",
      "dence to each other. The covariance intuitively represents the notion of\n",
      "how dependent random variables are to one another.\n",
      "Deﬁnition 6.5 (Covariance (Univariate)) .Thecovariance between two covariance\n",
      "univariate random variables X;Y2Ris given by the expected product\n",
      "of their deviations from their respective means, i.e.,\n",
      "CovX;Y[x;y] :=EX;Y\u0002(x\u0000EX[x])(y\u0000EY[y])\u0003: (6.35)\n",
      "Terminology: The\n",
      "covariance of\n",
      "multivariate random\n",
      "variables Cov[x;y]\n",
      "is sometimes\n",
      "referred to as\n",
      "cross-covariance,\n",
      "with covariance\n",
      "referring to\n",
      "Cov[x;x].Remark. When the random variable associated with the expectation or\n",
      "covariance is clear by its arguments, the subscript is often suppressed (for\n",
      "example, EX[x]is often written as E[x]). }\n",
      "By using the linearity of expectations, the expression in Deﬁnition 6.5\n",
      "can be rewritten as the expected value of the product minus the product\n",
      "of the expected values, i.e.,\n",
      "Cov[x;y] =E[xy]\u0000E[x]E[y]: (6.36)\n",
      "The covariance of a variable with itself Cov[x;x]is called the variance and variance\n",
      "is denoted by VX[x]. The square root of the variance is called the standard standard deviation\n",
      "deviation and is often denoted by \u001b(x). The notion of covariance can be\n",
      "generalized to multivariate random variables.\n",
      "Deﬁnition 6.6 (Covariance (Multivariate)) .If we consider two multivari-\n",
      "ate random variables XandYwith statesx2RDandy2RErespec-\n",
      "tively, the covariance betweenXandYis deﬁned as covariance\n",
      "Cov[x;y] =E[xy>]\u0000E[x]E[y]>= Cov[y;x]>2RD\u0002E: (6.37)\n",
      "Deﬁnition 6.6 can be applied with the same multivariate random vari-\n",
      "able in both arguments, which results in a useful concept that intuitively\n",
      "captures the “spread” of a random variable. For a multivariate random\n",
      "variable, the variance describes the relation between individual dimen-\n",
      "sions of the random variable.\n",
      "Deﬁnition 6.7 (Variance) .The variance of a random variable Xwith variance\n",
      "statesx2RDand a mean vector \u00162RDis deﬁned as\n",
      "VX[x] = CovX[x;x] (6.38a)\n",
      "=EX[(x\u0000\u0016)(x\u0000\u0016)>] =EX[xx>]\u0000EX[x]EX[x]>(6.38b)\n",
      "=2\n",
      "6664Cov[x1;x1] Cov[x1;x2]::: Cov[x1;xD]\n",
      "Cov[x2;x1] Cov[x2;x2]::: Cov[x2;xD]\n",
      "............\n",
      "Cov[xD;x1]::: ::: Cov[xD;xD]3\n",
      "7775: (6.38c)\n",
      "TheD\u0002Dmatrix in (6.38c) is called the covariance matrix of the mul- covariance matrix\n",
      "tivariate random variable X. The covariance matrix is symmetric and pos-\n",
      "itive semideﬁnite and tells us something about the spread of the data. On\n",
      "its diagonal, the covariance matrix contains the variances of the marginals marginal\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.4 Summary Statistics and Independence 191\n",
      "Figure 6.5\n",
      "Two-dimensional\n",
      "datasets with\n",
      "identical means and\n",
      "variances along\n",
      "each axis (colored\n",
      "lines) but with\n",
      "different\n",
      "covariances.\n",
      "−5 0 5\n",
      "x−20246y\n",
      "(a)xandyare negatively correlated.\n",
      "−5 0 5\n",
      "x−20246y\n",
      " (b)xandyare positively correlated.\n",
      "p(xi) =Z\n",
      "p(x1;:::;xD)dxni; (6.39)\n",
      "where “ni” denotes “all variables but i”. The off-diagonal entries are the\n",
      "cross-covariance terms Cov[xi;xj]fori;j= 1;:::;D; i6=j. cross-covariance\n",
      "Remark. In this book, we generally assume that covariance matrices are\n",
      "positive deﬁnite to enable better intuition. We therefore do not discuss\n",
      "corner cases that result in positive semideﬁnite (low-rank) covariance ma-\n",
      "trices. }\n",
      "When we want to compare the covariances between different pairs of\n",
      "random variables, it turns out that the variance of each random variable\n",
      "affects the value of the covariance. The normalized version of covariance\n",
      "is called the correlation.\n",
      "Deﬁnition 6.8 (Correlation) .Thecorrelation between two random vari- correlation\n",
      "ablesX;Y is given by\n",
      "corr[x;y] =Cov[x;y]p\n",
      "V[x]V[y]2[\u00001;1]: (6.40)\n",
      "The correlation matrix is the covariance matrix of standardized random\n",
      "variables,x=\u001b(x). In other words, each random variable is divided by its\n",
      "standard deviation (the square root of the variance) in the correlation\n",
      "matrix.\n",
      "The covariance (and correlation) indicate how two random variables\n",
      "are related; see Figure 6.5. Positive correlation corr [x;y]means that when\n",
      "xgrows, then yis also expected to grow. Negative correlation means that\n",
      "asxincreases, then ydecreases.\n",
      "6.4.2 Empirical Means and Covariances\n",
      "The deﬁnitions in Section 6.4.1 are often also called the population mean population mean\n",
      "and covariance and covariance , as it refers to the true statistics for the population. In ma-\n",
      "chine learning, we need to learn from empirical observations of data. Con-\n",
      "sider a random variable X. There are two conceptual steps to go from\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "192 Probability and Distributions\n",
      "population statistics to the realization of empirical statistics. First, we use\n",
      "the fact that we have a ﬁnite dataset (of size N) to construct an empirical\n",
      "statistic that is a function of a ﬁnite number of identical random variables,\n",
      "X1;:::;XN. Second, we observe the data, that is, we look at the realiza-\n",
      "tionx1;:::;xNof each of the random variables and apply the empirical\n",
      "statistic.\n",
      "Speciﬁcally, for the mean (Deﬁnition 6.4), given a particular dataset we\n",
      "can obtain an estimate of the mean, which is called the empirical mean or empirical mean\n",
      "sample mean . The same holds for the empirical covariance. sample mean\n",
      "Deﬁnition 6.9 (Empirical Mean and Covariance) .Theempirical mean vec- empirical mean\n",
      "tor is the arithmetic average of the observations for each variable, and it\n",
      "is deﬁned as\n",
      "\u0016x:=1\n",
      "NNX\n",
      "n=1xn; (6.41)\n",
      "wherexn2RD.\n",
      "Similar to the empirical mean, the empirical covariance matrix is aD\u0002D empirical covariance\n",
      "matrix\n",
      "\u0006:=1\n",
      "NNX\n",
      "n=1(xn\u0000\u0016x)(xn\u0000\u0016x)>: (6.42)\n",
      "Throughout the\n",
      "book, we use the\n",
      "empirical\n",
      "covariance, which is\n",
      "a biased estimate.\n",
      "The unbiased\n",
      "(sometimes called\n",
      "corrected)\n",
      "covariance has the\n",
      "factorN\u00001in the\n",
      "denominator\n",
      "instead ofN.To compute the statistics for a particular dataset, we would use the\n",
      "realizations (observations) x1;:::;xNand use (6.41) and (6.42). Em-\n",
      "pirical covariance matrices are symmetric, positive semideﬁnite (see Sec-\n",
      "tion 3.2.3).\n",
      "6.4.3 Three Expressions for the Variance\n",
      "We now focus on a single random variable Xand use the preceding em-\n",
      "pirical formulas to derive three possible expressions for the variance. The\n",
      "The derivations are\n",
      "exercises at the end\n",
      "of this chapter.following derivation is the same for the population variance, except that\n",
      "we need to take care of integrals. The standard deﬁnition of variance, cor-\n",
      "responding to the deﬁnition of covariance (Deﬁnition 6.5), is the expec-\n",
      "tation of the squared deviation of a random variable Xfrom its expected\n",
      "value\u0016, i.e.,\n",
      "VX[x] :=EX[(x\u0000\u0016)2]: (6.43)\n",
      "The expectation in (6.43) and the mean \u0016=EX(x)are computed us-\n",
      "ing (6.32), depending on whether Xis a discrete or continuous random\n",
      "variable. The variance as expressed in (6.43) is the mean of a new random\n",
      "variableZ:= (X\u0000\u0016)2.\n",
      "When estimating the variance in (6.43) empirically, we need to resort\n",
      "to a two-pass algorithm: one pass through the data to calculate the mean\n",
      "\u0016using (6.41), and then a second pass using this estimate ^\u0016calculate the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.4 Summary Statistics and Independence 193\n",
      "variance. It turns out that we can avoid two passes by rearranging the\n",
      "terms. The formula in (6.43) can be converted to the so-called raw-score raw-score formula\n",
      "for variance formula for variance :\n",
      "VX[x] =EX[x2]\u0000(EX[x])2: (6.44)\n",
      "The expression in (6.44) can be remembered as “the mean of the square\n",
      "minus the square of the mean”. It can be calculated empirically in one pass\n",
      "through data since we can accumulate xi(to calculate the mean) and x2\n",
      "i\n",
      "simultaneously, where xiis theith observation. Unfortunately, if imple- If the two terms\n",
      "in (6.44) are huge\n",
      "and approximately\n",
      "equal, we may\n",
      "suffer from an\n",
      "unnecessary loss of\n",
      "numerical precision\n",
      "in ﬂoating-point\n",
      "arithmetic.mented in this way, it can be numerically unstable. The raw-score version\n",
      "of the variance can be useful in machine learning, e.g., when deriving the\n",
      "bias–variance decomposition (Bishop, 2006).\n",
      "A third way to understand the variance is that it is a sum of pairwise dif-\n",
      "ferences between all pairs of observations. Consider a sample x1;:::;xN\n",
      "of realizations of random variable X, and we compute the squared differ-\n",
      "ence between pairs of xiandxj. By expanding the square, we can show\n",
      "that the sum of N2pairwise differences is the empirical variance of the\n",
      "observations:\n",
      "1\n",
      "N2NX\n",
      "i;j=1(xi\u0000xj)2= 22\n",
      "41\n",
      "NNX\n",
      "i=1x2\n",
      "i\u0000 \n",
      "1\n",
      "NNX\n",
      "i=1xi!23\n",
      "5: (6.45)\n",
      "We see that (6.45) is twice the raw-score expression (6.44). This means\n",
      "that we can express the sum of pairwise distances (of which there are N2\n",
      "of them) as a sum of deviations from the mean (of which there are N). Ge-\n",
      "ometrically, this means that there is an equivalence between the pairwise\n",
      "distances and the distances from the center of the set of points. From a\n",
      "computational perspective, this means that by computing the mean ( N\n",
      "terms in the summation), and then computing the variance (again N\n",
      "terms in the summation), we can obtain an expression (left-hand side\n",
      "of (6.45)) that has N2terms.\n",
      "6.4.4 Sums and Transformations of Random Variables\n",
      "We may want to model a phenomenon that cannot be well explained by\n",
      "textbook distributions (we introduce some in Sections 6.5 and 6.6), and\n",
      "hence may perform simple manipulations of random variables (such as\n",
      "adding two random variables).\n",
      "Consider two random variables X;Y with statesx;y2RD. Then:\n",
      "E[x+y] =E[x] +E[y] (6.46)\n",
      "E[x\u0000y] =E[x]\u0000E[y] (6.47)\n",
      "V[x+y] =V[x] +V[y] + Cov[x;y] + Cov[y;x] (6.48)\n",
      "V[x\u0000y] =V[x] +V[y]\u0000Cov[x;y]\u0000Cov[y;x]: (6.49)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "194 Probability and Distributions\n",
      "Mean and (co)variance exhibit some useful properties when it comes\n",
      "to afﬁne transformation of random variables. Consider a random variable\n",
      "Xwith mean\u0016and covariance matrix \u0006and a (deterministic) afﬁne\n",
      "transformation y=Ax+bofx. Thenyis itself a random variable\n",
      "whose mean vector and covariance matrix are given by\n",
      "EY[y] =EX[Ax+b] =AEX[x] +b=A\u0016+b; (6.50)\n",
      "VY[y] =VX[Ax+b] =VX[Ax] =AVX[x]A>=A\u0006A>;(6.51)\n",
      "respectively. Furthermore, This can be shown\n",
      "directly by using the\n",
      "deﬁnition of the\n",
      "mean and\n",
      "covariance.Cov[x;y] =E[x(Ax+b)>]\u0000E[x]E[Ax+b]>(6.52a)\n",
      "=E[x]b>+E[xx>]A>\u0000\u0016b>\u0000\u0016\u0016>A>(6.52b)\n",
      "=\u0016b>\u0000\u0016b>+\u0000\n",
      "E[xx>]\u0000\u0016\u0016>\u0001A>(6.52c)\n",
      "(6.38b)=\u0006A>; (6.52d)\n",
      "where \u0006=E[xx>]\u0000\u0016\u0016>is the covariance of X.\n",
      "6.4.5 Statistical Independence\n",
      "Deﬁnition 6.10 (Independence) .Two random variables X;Y arestatis- statistical\n",
      "independence tically independent if and only if\n",
      "p(x;y) =p(x)p(y): (6.53)\n",
      "Intuitively, two random variables XandYare independent if the value\n",
      "ofy(once known) does not add any additional information about x(and\n",
      "vice versa). If X;Y are (statistically) independent, then\n",
      "p(yjx) =p(y)\n",
      "p(xjy) =p(x)\n",
      "VX;Y[x+y] =VX[x] +VY[y]\n",
      "CovX;Y[x;y] =0\n",
      "The last point may not hold in converse, i.e., two random variables can\n",
      "have covariance zero but are not statistically independent. To understand\n",
      "why, recall that covariance measures only linear dependence. Therefore,\n",
      "random variables that are nonlinearly dependent could have covariance\n",
      "zero.\n",
      "Example 6.5\n",
      "Consider a random variable Xwith zero mean ( EX[x] = 0 ) and also\n",
      "EX[x3] = 0 . Lety=x2(hence,Yis dependent on X) and consider the\n",
      "covariance (6.36) between XandY. But this gives\n",
      "Cov[x;y] =E[xy]\u0000E[x]E[y] =E[x3] = 0: (6.54)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.4 Summary Statistics and Independence 195\n",
      "In machine learning, we often consider problems that can be mod-\n",
      "eled as independent and identically distributed (i.i.d.) random variables, independent and\n",
      "identically\n",
      "distributed\n",
      "i.i.d.X1;:::;XN. For more than two random variables, the word “indepen-\n",
      "dent” (Deﬁnition 6.10) usually refers to mutually independent random\n",
      "variables, where all subsets are independent (see Pollard (2002, chap-\n",
      "ter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically\n",
      "distributed” means that all the random variables are from the same distri-\n",
      "bution.\n",
      "Another concept that is important in machine learning is conditional\n",
      "independence.\n",
      "Deﬁnition 6.11 (Conditional Independence) .Two random variables X\n",
      "andYareconditionally independent givenZif and only if conditionally\n",
      "independent\n",
      "p(x;yjz) =p(xjz)p(yjz) for allz2Z; (6.55)\n",
      "whereZis the set of states of random variable Z. We writeX? ?YjZto\n",
      "denote that Xis conditionally independent of YgivenZ.\n",
      "Deﬁnition 6.11 requires that the relation in (6.55) must hold true for\n",
      "every value of z. The interpretation of (6.55) can be understood as “given\n",
      "knowledge about z, the distribution of xandyfactorizes”. Independence\n",
      "can be cast as a special case of conditional independence if we write X? ?\n",
      "Yj;. By using the product rule of probability (6.22), we can expand the\n",
      "left-hand side of (6.55) to obtain\n",
      "p(x;yjz) =p(xjy;z)p(yjz): (6.56)\n",
      "By comparing the right-hand side of (6.55) with (6.56), we see that p(yjz)\n",
      "appears in both of them so that\n",
      "p(xjy;z) =p(xjz): (6.57)\n",
      "Equation (6.57) provides an alternative deﬁnition of conditional indepen-\n",
      "dence, i.e., X? ?YjZ. This alternative presentation provides the inter-\n",
      "pretation “given that we know z, knowledge about ydoes not change our\n",
      "knowledge of x”.\n",
      "6.4.6 Inner Products of Random Variables\n",
      "Recall the deﬁnition of inner products from Section 3.2. We can deﬁne an Inner products\n",
      "between\n",
      "multivariate random\n",
      "variables can be\n",
      "treated in a similar\n",
      "fashioninner product between random variables, which we brieﬂy describe in this\n",
      "section. If we have two uncorrelated random variables X;Y , then\n",
      "V[x+y] =V[x] +V[y]: (6.58)\n",
      "Since variances are measured in squared units, this looks very much like\n",
      "the Pythagorean theorem for right triangles c2=a2+b2.\n",
      "In the following, we see whether we can ﬁnd a geometric interpreta-\n",
      "tion of the variance relation of uncorrelated random variables in (6.58).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "196 Probability and Distributions\n",
      "Figure 6.6\n",
      "Geometry of\n",
      "random variables. If\n",
      "random variables X\n",
      "andYare\n",
      "uncorrelated, they\n",
      "are orthogonal\n",
      "vectors in a\n",
      "corresponding\n",
      "vector space, and\n",
      "the Pythagorean\n",
      "theorem applies.\n",
      "p\n",
      "var[y]\n",
      "p\n",
      "var[x]\n",
      "p\n",
      "var[x+y] =p\n",
      "var[x] + var[ y]\n",
      "a\n",
      "c\n",
      "b\n",
      "Random variables can be considered vectors in a vector space, and we\n",
      "can deﬁne inner products to obtain geometric properties of random vari-\n",
      "ables (Eaton, 2007). If we deﬁne\n",
      "hX;Yi:= Cov[x;y] (6.59)\n",
      "for zero mean random variables XandY, we obtain an inner product. We\n",
      "see that the covariance is symmetric, positive deﬁnite, and linear in either Cov[x;x] = 0()\n",
      "x= 0 argument. The length of a random variable is\n",
      "Cov[\u000bx+z;y] =\n",
      "\u000bCov[x;y] +\n",
      "Cov[z;y]for\u000b2R.kXk=q\n",
      "Cov[x;x] =q\n",
      "V[x] =\u001b[x]; (6.60)\n",
      "i.e., its standard deviation. The “longer” the random variable, the more\n",
      "uncertain it is; and a random variable with length 0is deterministic.\n",
      "If we look at the angle \u0012between two random variables X;Y , we get\n",
      "cos\u0012=hX;Yi\n",
      "kXkkYk=Cov[x;y]p\n",
      "V[x]V[y]; (6.61)\n",
      "which is the correlation (Deﬁnition 6.8) between the two random vari-\n",
      "ables. This means that we can think of correlation as the cosine of the\n",
      "angle between two random variables when we consider them geometri-\n",
      "cally. We know from Deﬁnition 3.7 that X?Y() hX;Yi= 0. In our\n",
      "case, this means that XandYare orthogonal if and only if Cov[x;y] = 0 ,\n",
      "i.e., they are uncorrelated. Figure 6.6 illustrates this relationship.\n",
      "Remark. While it is tempting to use the Euclidean distance (constructed\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.5 Gaussian Distribution 197\n",
      "Figure 6.7\n",
      "Gaussian\n",
      "distribution of two\n",
      "random variables x1\n",
      "andx2.\n",
      "x1−101x2\n",
      "−5.0−2.50.02.55.07.5p(x1,x2)\n",
      "0.000.050.100.150.20\n",
      "from the preceding deﬁnition of inner products) to compare probability\n",
      "distributions, it is unfortunately not the best way to obtain distances be-\n",
      "tween distributions. Recall that the probability mass (or density) is posi-\n",
      "tive and needs to add up to 1. These constraints mean that distributions\n",
      "live on something called a statistical manifold. The study of this space of\n",
      "probability distributions is called information geometry. Computing dis-\n",
      "tances between distributions are often done using Kullback-Leibler diver-\n",
      "gence, which is a generalization of distances that account for properties of\n",
      "the statistical manifold. Just like the Euclidean distance is a special case of\n",
      "a metric (Section 3.3), the Kullback-Leibler divergence is a special case of\n",
      "two more general classes of divergences called Bregman divergences and\n",
      "f-divergences. The study of divergences is beyond the scope of this book,\n",
      "and we refer for more details to the recent book by Amari (2016), one of\n",
      "the founders of the ﬁeld of information geometry. }\n",
      "6.5 Gaussian Distribution\n",
      "The Gaussian distribution is the most well-studied probability distribution\n",
      "for continuous-valued random variables. It is also referred to as the normal normal distribution\n",
      "distribution . Its importance originates from the fact that it has many com- The Gaussian\n",
      "distribution arises\n",
      "naturally when we\n",
      "consider sums of\n",
      "independent and\n",
      "identically\n",
      "distributed random\n",
      "variables. This is\n",
      "known as the\n",
      "central limit\n",
      "theorem (Grinstead\n",
      "and Snell, 1997).putationally convenient properties, which we will be discussing in the fol-\n",
      "lowing. In particular, we will use it to deﬁne the likelihood and prior for\n",
      "linear regression (Chapter 9), and consider a mixture of Gaussians for\n",
      "density estimation (Chapter 11).\n",
      "There are many other areas of machine learning that also beneﬁt from\n",
      "using a Gaussian distribution, for example Gaussian processes, variational\n",
      "inference, and reinforcement learning. It is also widely used in other ap-\n",
      "plication areas such as signal processing (e.g., Kalman ﬁlter), control (e.g.,\n",
      "linear quadratic regulator), and statistics (e.g., hypothesis testing).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "198 Probability and Distributions\n",
      "Figure 6.8\n",
      "Gaussian\n",
      "distributions\n",
      "overlaid with 100\n",
      "samples. (a) One-\n",
      "dimensional case;\n",
      "(b) two-dimensional\n",
      "case.\n",
      "−5.0−2.5 0.0 2.5 5.0 7.5\n",
      "x0.000.050.100.150.20\n",
      "p(x)\n",
      "Mean\n",
      "Sample\n",
      "2σ\n",
      "(a) Univariate (one-dimensional) Gaussian;\n",
      "The red cross shows the mean and the red\n",
      "line shows the extent of the variance.\n",
      "−1 0 1\n",
      "x1−4−202468x2Mean\n",
      "Sample(b) Multivariate (two-dimensional) Gaus-\n",
      "sian, viewed from top. The red cross shows\n",
      "the mean and the colored lines show the con-\n",
      "tour lines of the density.\n",
      "For a univariate random variable, the Gaussian distribution has a den-\n",
      "sity that is given by\n",
      "p(xj\u0016;\u001b2) =1p\n",
      "2\u0019\u001b2exp\u0012\n",
      "\u0000(x\u0000\u0016)2\n",
      "2\u001b2\u0013\n",
      ": (6.62)\n",
      "The multivariate Gaussian distribution is fully characterized by a mean multivariate\n",
      "Gaussian\n",
      "distribution\n",
      "mean vectorvector\u0016and a covariance matrix \u0006and deﬁned as\n",
      "covariance matrixp(xj\u0016;\u0006) = (2\u0019)\u0000D\n",
      "2j\u0006j\u00001\n",
      "2exp\u0000\u00001\n",
      "2(x\u0000\u0016)>\u0006\u00001(x\u0000\u0016)\u0001;(6.63)\n",
      "wherex2RD. We writep(x) =N\u0000xj\u0016;\u0006\u0001\n",
      "orX\u0018N\u0000\u0016;\u0006\u0001\n",
      ". Fig- Also known as a\n",
      "multivariate normal\n",
      "distribution.ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-\n",
      "tour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian\n",
      "with corresponding samples. The special case of the Gaussian with zero\n",
      "mean and identity covariance, that is, \u0016=0and\u0006=I, is referred to as\n",
      "thestandard normal distribution . standard normal\n",
      "distribution Gaussians are widely used in statistical estimation and machine learn-\n",
      "ing as they have closed-form expressions for marginal and conditional dis-\n",
      "tributions. In Chapter 9, we use these closed-form expressions extensively\n",
      "for linear regression. A major advantage of modeling with Gaussian ran-\n",
      "dom variables is that variable transformations (Section 6.7) are often not\n",
      "needed. Since the Gaussian distribution is fully speciﬁed by its mean and\n",
      "covariance, we often can obtain the transformed distribution by applying\n",
      "the transformation to the mean and covariance of the random variable.\n",
      "6.5.1 Marginals and Conditionals of Gaussians are Gaussians\n",
      "In the following, we present marginalization and conditioning in the gen-\n",
      "eral case of multivariate random variables. If this is confusing at ﬁrst read-\n",
      "ing, the reader is advised to consider two univariate random variables in-\n",
      "stead. LetXandYbe two multivariate random variables, that may have\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.5 Gaussian Distribution 199\n",
      "different dimensions. To consider the effect of applying the sum rule of\n",
      "probability and the effect of conditioning, we explicitly write the Gaus-\n",
      "sian distribution in terms of the concatenated states [x>;y>],\n",
      "p(x;y) =N\u0012\u0014\u0016x\n",
      "\u0016y\u0015\n",
      ";\u0014\u0006xx\u0006xy\n",
      "\u0006yx\u0006yy\u0015\u0013\n",
      ": (6.64)\n",
      "where \u0006xx= Cov[x;x]and\u0006yy= Cov[y;y]are the marginal covari-\n",
      "ance matrices of xandy, respectively, and \u0006xy= Cov[x;y]is the cross-\n",
      "covariance matrix between xandy.\n",
      "The conditional distribution p(xjy)is also Gaussian (illustrated in Fig-\n",
      "ure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006)\n",
      "p(xjy) =N\u0000\u0016xjy;\u0006xjy\u0001\n",
      "(6.65)\n",
      "\u0016xjy=\u0016x+\u0006xy\u0006\u00001\n",
      "yy(y\u0000\u0016y) (6.66)\n",
      "\u0006xjy=\u0006xx\u0000\u0006xy\u0006\u00001\n",
      "yy\u0006yx: (6.67)\n",
      "Note that in the computation of the mean in (6.66), the y-value is an\n",
      "observation and no longer random.\n",
      "Remark. The conditional Gaussian distribution shows up in many places,\n",
      "where we are interested in posterior distributions:\n",
      "The Kalman ﬁlter (Kalman, 1960), one of the most central algorithms\n",
      "for state estimation in signal processing, does nothing but computing\n",
      "Gaussian conditionals of joint distributions (Deisenroth and Ohlsson,\n",
      "2011; S ¨arkk¨a, 2013).\n",
      "Gaussian processes (Rasmussen and Williams, 2006), which are a prac-\n",
      "tical implementation of a distribution over functions. In a Gaussian pro-\n",
      "cess, we make assumptions of joint Gaussianity of random variables. By\n",
      "(Gaussian) conditioning on observed data, we can determine a poste-\n",
      "rior distribution over functions.\n",
      "Latent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-\n",
      "phy, 2012), which include probabilistic principal component analysis\n",
      "(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-\n",
      "tail in Section 10.7.\n",
      "}\n",
      "The marginal distribution p(x)of a joint Gaussian distribution p(x;y)\n",
      "(see (6.64)) is itself Gaussian and computed by applying the sum rule\n",
      "(6.20) and given by\n",
      "p(x) =Z\n",
      "p(x;y)dy=N\u0000xj\u0016x;\u0006xx\u0001: (6.68)\n",
      "The corresponding result holds for p(y), which is obtained by marginaliz-\n",
      "ing with respect to x. Intuitively, looking at the joint distribution in (6.64),\n",
      "we ignore (i.e., integrate out) everything we are not interested in. This is\n",
      "illustrated in Figure 6.9(b).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "200 Probability and Distributions\n",
      "Example 6.6\n",
      "Figure 6.9\n",
      "(a) Bivariate\n",
      "Gaussian;\n",
      "(b) marginal of a\n",
      "joint Gaussian\n",
      "distribution is\n",
      "Gaussian; (c) the\n",
      "conditional\n",
      "distribution of a\n",
      "Gaussian is also\n",
      "Gaussian.\n",
      "−1 0 1\n",
      "x1−4−202468x2\n",
      "x2=−1\n",
      "(a) Bivariate Gaussian.\n",
      "−1.5−1.0−0.5 0.0 0.5 1.0 1.5\n",
      "x10.00.20.40.6p(x1)\n",
      "Mean\n",
      "2σ\n",
      "(b) Marginal distribution.\n",
      "−1.5−1.0−0.5 0.0 0.5 1.0 1.5\n",
      "x10.00.20.40.60.81.01.2p(x1|x2=−1)\n",
      "Mean\n",
      "2σ (c) Conditional distribution.\n",
      "Consider the bivariate Gaussian distribution (illustrated in Figure 6.9):\n",
      "p(x1;x2) =N\u0012\u00140\n",
      "2\u0015\n",
      ";\u00140:3\u00001\n",
      "\u00001 5\u0015\u0013\n",
      ": (6.69)\n",
      "We can compute the parameters of the univariate Gaussian, conditioned\n",
      "onx2=\u00001, by applying (6.66) and (6.67) to obtain the mean and vari-\n",
      "ance respectively. Numerically, this is\n",
      "\u0016x1jx2=\u00001= 0 + (\u00001)\u00010:2\u0001(\u00001\u00002) = 0:6 (6.70)\n",
      "and\n",
      "\u001b2\n",
      "x1jx2=\u00001= 0:3\u0000(\u00001)\u00010:2\u0001(\u00001) = 0:1: (6.71)\n",
      "Therefore, the conditional Gaussian is given by\n",
      "p(x1jx2=\u00001) =N\u00000:6;0:1\u0001: (6.72)\n",
      "The marginal distribution p(x1), in contrast, can be obtained by apply-\n",
      "ing (6.68), which is essentially using the mean and variance of the random\n",
      "variablex1, giving us\n",
      "p(x1) =N\u00000;0:3\u0001: (6.73)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.5 Gaussian Distribution 201\n",
      "6.5.2 Product of Gaussian Densities\n",
      "For linear regression (Chapter 9), we need to compute a Gaussian likeli-\n",
      "hood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3).\n",
      "We apply Bayes’ Theorem to compute the posterior, which results in a mul-\n",
      "tiplication of the likelihood and the prior, that is, the multiplication of two\n",
      "Gaussian densities. The product of two GaussiansN\u0000xja;A\u0001N\u0000xjb;B\u0001\n",
      "The derivation is an\n",
      "exercise at the end\n",
      "of this chapter.is a Gaussian distribution scaled by a c2R, given bycN\u0000xjc;C\u0001\n",
      "with\n",
      "C= (A\u00001+B\u00001)\u00001(6.74)\n",
      "c=C(A\u00001a+B\u00001b) (6.75)\n",
      "c= (2\u0019)\u0000D\n",
      "2jA+Bj\u00001\n",
      "2exp\u0000\u00001\n",
      "2(a\u0000b)>(A+B)\u00001(a\u0000b)\u0001:(6.76)\n",
      "The scaling constant citself can be written in the form of a Gaussian\n",
      "density either in aor inbwith an “inﬂated” covariance matrix A+B,\n",
      "i.e.,c=N\u0000ajb;A+B\u0001=N\u0000bja;A+B\u0001\n",
      ".\n",
      "Remark. For notation convenience, we will sometimes use N\u0000xjm;S\u0001\n",
      "to describe the functional form of a Gaussian density even if xis not a\n",
      "random variable. We have just done this in the preceding demonstration\n",
      "when we wrote\n",
      "c=N\u0000ajb;A+B\u0001=N\u0000bja;A+B\u0001: (6.77)\n",
      "Here, neither anorbare random variables. However, writing cin this way\n",
      "is more compact than (6.76). }\n",
      "6.5.3 Sums and Linear Transformations\n",
      "IfX;Y are independent Gaussian random variables (i.e., the joint distri-\n",
      "bution is given as p(x;y) =p(x)p(y)) withp(x) =N\u0000xj\u0016x;\u0006x\u0001\n",
      "and\n",
      "p(y) =N\u0000yj\u0016y;\u0006y\u0001\n",
      ", thenx+yis also Gaussian distributed and given\n",
      "by\n",
      "p(x+y) =N\u0000\u0016x+\u0016y;\u0006x+\u0006y\u0001: (6.78)\n",
      "Knowing that p(x+y)is Gaussian, the mean and covariance matrix can\n",
      "be determined immediately using the results from (6.46) through (6.49).\n",
      "This property will be important when we consider i.i.d. Gaussian noise\n",
      "acting on random variables, as is the case for linear regression (Chap-\n",
      "ter 9).\n",
      "Example 6.7\n",
      "Since expectations are linear operations, we can obtain the weighted sum\n",
      "of independent Gaussian random variables\n",
      "p(ax+by) =N\u0000a\u0016x+b\u0016y; a2\u0006x+b2\u0006y\u0001: (6.79)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "202 Probability and Distributions\n",
      "Remark. A case that will be useful in Chapter 11 is the weighted sum of\n",
      "Gaussian densities. This is different from the weighted sum of Gaussian\n",
      "random variables. }\n",
      "In Theorem 6.12, the random variable xis from a density that is a\n",
      "mixture of two densities p1(x)andp2(x), weighted by \u000b. The theorem can\n",
      "be generalized to the multivariate random variable case, since linearity of\n",
      "expectations holds also for multivariate random variables. However, the\n",
      "idea of a squared random variable needs to be replaced by xx>.\n",
      "Theorem 6.12. Consider a mixture of two univariate Gaussian densities\n",
      "p(x) =\u000bp1(x) + (1\u0000\u000b)p2(x); (6.80)\n",
      "where the scalar 0<\u000b< 1is the mixture weight, and p1(x)andp2(x)are\n",
      "univariate Gaussian densities (Equation (6.62) ) with different parameters,\n",
      "i.e.,(\u00161;\u001b2\n",
      "1)6= (\u00162;\u001b2\n",
      "2).\n",
      "Then the mean of the mixture density p(x)is given by the weighted sum\n",
      "of the means of each random variable:\n",
      "E[x] =\u000b\u00161+ (1\u0000\u000b)\u00162: (6.81)\n",
      "The variance of the mixture density p(x)is given by\n",
      "V[x] =\u0002\u000b\u001b2\n",
      "1+ (1\u0000\u000b)\u001b2\n",
      "2\u0003+\u0010\u0002\u000b\u00162\n",
      "1+ (1\u0000\u000b)\u00162\n",
      "2\u0003\u0000[\u000b\u00161+ (1\u0000\u000b)\u00162]2\u0011\n",
      ":\n",
      "(6.82)\n",
      "Proof The mean of the mixture density p(x)is given by the weighted\n",
      "sum of the means of each random variable. We apply the deﬁnition of the\n",
      "mean (Deﬁnition 6.4), and plug in our mixture (6.80), which yields\n",
      "E[x] =Z1\n",
      "\u00001xp(x)dx (6.83a)\n",
      "=Z1\n",
      "\u00001(\u000bxp 1(x) + (1\u0000\u000b)xp2(x)) dx (6.83b)\n",
      "=\u000bZ1\n",
      "\u00001xp1(x)dx+ (1\u0000\u000b)Z1\n",
      "\u00001xp2(x)dx (6.83c)\n",
      "=\u000b\u00161+ (1\u0000\u000b)\u00162: (6.83d)\n",
      "To compute the variance, we can use the raw-score version of the vari-\n",
      "ance from (6.44), which requires an expression of the expectation of the\n",
      "squared random variable. Here we use the deﬁnition of an expectation of\n",
      "a function (the square) of a random variable (Deﬁnition 6.3),\n",
      "E[x2] =Z1\n",
      "\u00001x2p(x)dx (6.84a)\n",
      "=Z1\n",
      "\u00001\u0000\u000bx2p1(x) + (1\u0000\u000b)x2p2(x)\u0001dx (6.84b)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.5 Gaussian Distribution 203\n",
      "=\u000bZ1\n",
      "\u00001x2p1(x)dx+ (1\u0000\u000b)Z1\n",
      "\u00001x2p2(x)dx (6.84c)\n",
      "=\u000b(\u00162\n",
      "1+\u001b2\n",
      "1) + (1\u0000\u000b)(\u00162\n",
      "2+\u001b2\n",
      "2); (6.84d)\n",
      "where in the last equality, we again used the raw-score version of the\n",
      "variance (6.44) giving \u001b2=E[x2]\u0000\u00162. This is rearranged such that the\n",
      "expectation of a squared random variable is the sum of the squared mean\n",
      "and the variance.\n",
      "Therefore, the variance is given by subtracting (6.83d) from (6.84d),\n",
      "V[x] =E[x2]\u0000(E[x])2(6.85a)\n",
      "=\u000b(\u00162\n",
      "1+\u001b2\n",
      "1) + (1\u0000\u000b)(\u00162\n",
      "2+\u001b2\n",
      "2)\u0000(\u000b\u00161+ (1\u0000\u000b)\u00162)2(6.85b)\n",
      "=\u0002\u000b\u001b2\n",
      "1+ (1\u0000\u000b)\u001b2\n",
      "2\u0003\n",
      "+\u0010\u0002\u000b\u00162\n",
      "1+ (1\u0000\u000b)\u00162\n",
      "2\u0003\u0000[\u000b\u00161+ (1\u0000\u000b)\u00162]2\u0011\n",
      ": (6.85c)\n",
      "Remark. The preceding derivation holds for any density, but since the\n",
      "Gaussian is fully determined by the mean and variance, the mixture den-\n",
      "sity can be determined in closed form. }\n",
      "For a mixture density, the individual components can be considered\n",
      "to be conditional distributions (conditioned on the component identity).\n",
      "Equation (6.85c) is an example of the conditional variance formula, also\n",
      "known as the law of total variance , which generally states that for two ran- law of total variance\n",
      "dom variables XandYit holds that VX[x] =EY[VX[xjy]]+VY[EX[xjy]],\n",
      "i.e., the (total) variance of Xis the expected conditional variance plus the\n",
      "variance of a conditional mean.\n",
      "We consider in Example 6.17 a bivariate standard Gaussian random\n",
      "variableXand performed a linear transformation Axon it. The outcome\n",
      "is a Gaussian random variable with mean zero and covariance AA>. Ob-\n",
      "serve that adding a constant vector will change the mean of the distribu-\n",
      "tion, without affecting its variance, that is, the random variable x+\u0016is\n",
      "Gaussian with mean \u0016and identity covariance. Hence, any linear/afﬁne\n",
      "transformation of a Gaussian random variable is Gaussian distributed. Any linear/afﬁne\n",
      "transformation of a\n",
      "Gaussian random\n",
      "variable is also\n",
      "Gaussian\n",
      "distributed.Consider a Gaussian distributed random variable X\u0018N\u0000\u0016;\u0006\u0001\n",
      ". For\n",
      "a given matrix Aof appropriate shape, let Ybe a random variable such\n",
      "thaty=Axis a transformed version of x. We can compute the mean of\n",
      "yby exploiting that the expectation is a linear operator (6.50) as follows:\n",
      "E[y] =E[Ax] =AE[x] =A\u0016: (6.86)\n",
      "Similarly the variance of ycan be found by using (6.51):\n",
      "V[y] =V[Ax] =AV[x]A>=A\u0006A>: (6.87)\n",
      "This means that the random variable yis distributed according to\n",
      "p(y) =N\u0000yjA\u0016;A\u0006A>\u0001: (6.88)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "204 Probability and Distributions\n",
      "Let us now consider the reverse transformation: when we know that a\n",
      "random variable has a mean that is a linear transformation of another\n",
      "random variable. For a given full rank matrix A2RM\u0002N, whereM>N,\n",
      "lety2RMbe a Gaussian random variable with mean Ax, i.e.,\n",
      "p(y) =N\u0000yjAx;\u0006\u0001: (6.89)\n",
      "What is the corresponding probability distribution p(x)? IfAis invert-\n",
      "ible, then we can write x=A\u00001yand apply the transformation in the\n",
      "previous paragraph. However, in general Ais not invertible, and we use\n",
      "an approach similar to that of the pseudo-inverse (3.57). That is, we pre-\n",
      "multiply both sides with A>and then invert A>A, which is symmetric\n",
      "and positive deﬁnite, giving us the relation\n",
      "y=Ax() (A>A)\u00001A>y=x: (6.90)\n",
      "Hence,xis a linear transformation of y, and we obtain\n",
      "p(x) =N\u0000xj(A>A)\u00001A>y;(A>A)\u00001A>\u0006A(A>A)\u00001\u0001:(6.91)\n",
      "6.5.4 Sampling from Multivariate Gaussian Distributions\n",
      "We will not explain the subtleties of random sampling on a computer, and\n",
      "the interested reader is referred to Gentle (2004). In the case of a mul-\n",
      "tivariate Gaussian, this process consists of three stages: ﬁrst, we need a\n",
      "source of pseudo-random numbers that provide a uniform sample in the\n",
      "interval [0,1]; second, we use a non-linear transformation such as the\n",
      "Box-M ¨uller transform (Devroye, 1986) to obtain a sample from a univari-\n",
      "ate Gaussian; and third, we collate a vector of these samples to obtain a\n",
      "sample from a multivariate standard normal N\u00000;I\u0001\n",
      ".\n",
      "For a general multivariate Gaussian, that is, where the mean is non\n",
      "zero and the covariance is not the identity matrix, we use the proper-\n",
      "ties of linear transformations of a Gaussian random variable. Assume we\n",
      "are interested in generating samples xi;i= 1;:::;n; from a multivariate\n",
      "Gaussian distribution with mean \u0016and covariance matrix \u0006. We would To compute the\n",
      "Cholesky\n",
      "factorization of a\n",
      "matrix, it is required\n",
      "that the matrix is\n",
      "symmetric and\n",
      "positive deﬁnite\n",
      "(Section 3.2.3).\n",
      "Covariance matrices\n",
      "possess this\n",
      "property.like to construct the sample from a sampler that provides samples from\n",
      "the multivariate standard normal N\u00000;I\u0001\n",
      ".\n",
      "To obtain samples from a multivariate normal N\u0000\u0016;\u0006\u0001\n",
      ", we can use\n",
      "the properties of a linear transformation of a Gaussian random variable:\n",
      "Ifx\u0018N\u00000;I\u0001\n",
      ", theny=Ax+\u0016, whereAA>=\u0006is Gaussian dis-\n",
      "tributed with mean \u0016and covariance matrix \u0006. One convenient choice of\n",
      "Ais to use the Cholesky decomposition (Section 4.3) of the covariance\n",
      "matrix \u0006=AA>. The Cholesky decomposition has the beneﬁt that Ais\n",
      "triangular, leading to efﬁcient computation.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.6 Conjugacy and the Exponential Family 205\n",
      "6.6 Conjugacy and the Exponential Family\n",
      "Many of the probability distributions “with names” that we ﬁnd in statis-\n",
      "tics textbooks were discovered to model particular types of phenomena.\n",
      "For example, we have seen the Gaussian distribution in Section 6.5. The\n",
      "distributions are also related to each other in complex ways (Leemis and\n",
      "McQueston, 2008). For a beginner in the ﬁeld, it can be overwhelming to\n",
      "ﬁgure out which distribution to use. In addition, many of these distribu-\n",
      "tions were discovered at a time that statistics and computation were done “Computers” used to\n",
      "be a job description. by pencil and paper. It is natural to ask what are meaningful concepts\n",
      "in the computing age (Efron and Hastie, 2016). In the previous section,\n",
      "we saw that many of the operations required for inference can be conve-\n",
      "niently calculated when the distribution is Gaussian. It is worth recalling\n",
      "at this point the desiderata for manipulating probability distributions in\n",
      "the machine learning context:\n",
      "1. There is some “closure property” when applying the rules of probability,\n",
      "e.g., Bayes’ theorem. By closure, we mean that applying a particular\n",
      "operation returns an object of the same type.\n",
      "2. As we collect more data, we do not need more parameters to describe\n",
      "the distribution.\n",
      "3. Since we are interested in learning from data, we want parameter es-\n",
      "timation to behave nicely.\n",
      "It turns out that the class of distributions called the exponential family exponential family\n",
      "provides the right balance of generality while retaining favorable compu-\n",
      "tation and inference properties. Before we introduce the exponential fam-\n",
      "ily, let us see three more members of “named” probability distributions,\n",
      "the Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-\n",
      "ple 6.10) distributions.\n",
      "Example 6.8\n",
      "The Bernoulli distribution is a distribution for a single binary random Bernoulli\n",
      "distribution variableXwith statex2f0;1g. It is governed by a single continuous pa-\n",
      "rameter\u00162[0;1]that represents the probability of X= 1. The Bernoulli\n",
      "distribution Ber (\u0016)is deﬁned as\n",
      "p(xj\u0016) =\u0016x(1\u0000\u0016)1\u0000x; x2f0;1g; (6.92)\n",
      "E[x] =\u0016; (6.93)\n",
      "V[x] =\u0016(1\u0000\u0016); (6.94)\n",
      "where E[x]andV[x]are the mean and variance of the binary random\n",
      "variableX.\n",
      "An example where the Bernoulli distribution can be used is when we\n",
      "are interested in modeling the probability of “heads” when ﬂipping a coin.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "206 Probability and Distributions\n",
      "Figure 6.10\n",
      "Examples of the\n",
      "Binomial\n",
      "distribution for\n",
      "\u00162f0:1;0:4;0:75g\n",
      "andN= 15 .\n",
      "0.0 2.5 5.0 7.5 10.0 12.5 15.0\n",
      "Numbermof observations x= 1 inN= 15 experiments0.00.10.20.3p(m)µ= 0.1\n",
      "µ= 0.4\n",
      "µ= 0.75\n",
      "Remark. The rewriting above of the Bernoulli distribution, where we use\n",
      "Boolean variables as numerical 0or1and express them in the exponents,\n",
      "is a trick that is often used in machine learning textbooks. Another oc-\n",
      "curence of this is when expressing the Multinomial distribution. }\n",
      "Example 6.9 (Binomial Distribution)\n",
      "The Binomial distribution is a generalization of the Bernoulli distribution Binomial\n",
      "distribution to a distribution over integers (illustrated in Figure 6.10). In particular,\n",
      "the Binomial can be used to describe the probability of observing moc-\n",
      "currences of X= 1 in a set ofNsamples from a Bernoulli distribution\n",
      "wherep(X= 1) =\u00162[0;1]. The Binomial distribution Bin (N;\u0016)is\n",
      "deﬁned as\n",
      "p(mjN;\u0016) = \n",
      "N\n",
      "m!\n",
      "\u0016m(1\u0000\u0016)N\u0000m; (6.95)\n",
      "E[m] =N\u0016; (6.96)\n",
      "V[m] =N\u0016(1\u0000\u0016); (6.97)\n",
      "where E[m]andV[m]are the mean and variance of m, respectively.\n",
      "An example where the Binomial could be used is if we want to describe\n",
      "the probability of observing m“heads” inNcoin-ﬂip experiments if the\n",
      "probability for observing head in a single experiment is \u0016.\n",
      "Example 6.10 (Beta Distribution)\n",
      "We may wish to model a continuous random variable on a ﬁnite interval.\n",
      "The Beta distribution is a distribution over a continuous random variable Beta distribution\n",
      "\u00162[0;1], which is often used to represent the probability for some binary\n",
      "event (e.g., the parameter governing the Bernoulli distribution). The Beta\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.6 Conjugacy and the Exponential Family 207\n",
      "distribution Beta (\u000b;\f)(illustrated in Figure 6.11) itself is governed by\n",
      "two parameters \u000b>0; \f > 0and is deﬁned as\n",
      "p(\u0016j\u000b;\f) =\u0000(\u000b+\f)\n",
      "\u0000(\u000b)\u0000(\f)\u0016\u000b\u00001(1\u0000\u0016)\f\u00001(6.98)\n",
      "E[\u0016] =\u000b\n",
      "\u000b+\f;V[\u0016] =\u000b\f\n",
      "(\u000b+\f)2(\u000b+\f+ 1)(6.99)\n",
      "where \u0000(\u0001)is the Gamma function deﬁned as\n",
      "\u0000(t) :=Z1\n",
      "0xt\u00001exp(\u0000x)dx; t> 0: (6.100)\n",
      "\u0000(t+ 1) =t\u0000(t): (6.101)\n",
      "Note that the fraction of Gamma functions in (6.98) normalizes the Beta\n",
      "distribution.\n",
      "Figure 6.11\n",
      "Examples of the\n",
      "Beta distribution for\n",
      "different values of \u000b\n",
      "and\f.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "µ0246810p(µ|α,β)α= 0.5 =β\n",
      "α= 1 =β\n",
      "α= 2,β= 0.3\n",
      "α= 4,β= 10\n",
      "α= 5,β= 1\n",
      "Intuitively,\u000bmoves probability mass toward 1, whereas\fmoves prob-\n",
      "ability mass toward 0. There are some special cases (Murphy, 2012):\n",
      "For\u000b= 1 =\f, we obtain the uniform distribution U[0;1].\n",
      "For\u000b;\f < 1, we get a bimodal distribution with spikes at 0and1.\n",
      "For\u000b;\f > 1, the distribution is unimodal.\n",
      "For\u000b;\f > 1and\u000b=\f, the distribution is unimodal, symmetric, and\n",
      "centered in the interval [0;1], i.e., the mode/mean is at1\n",
      "2.\n",
      "Remark. There is a whole zoo of distributions with names, and they are\n",
      "related in different ways to each other (Leemis and McQueston, 2008).\n",
      "It is worth keeping in mind that each named distribution is created for a\n",
      "particular reason, but may have other applications. Knowing the reason\n",
      "behind the creation of a particular distribution often allows insight into\n",
      "how to best use it. We introduced the preceding three distributions to be\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "208 Probability and Distributions\n",
      "able to illustrate the concepts of conjugacy (Section 6.6.1) and exponen-\n",
      "tial families (Section 6.6.3). }\n",
      "6.6.1 Conjugacy\n",
      "According to Bayes’ theorem (6.23), the posterior is proportional to the\n",
      "product of the prior and the likelihood. The speciﬁcation of the prior can\n",
      "be tricky for two reasons: First, the prior should encapsulate our knowl-\n",
      "edge about the problem before we see any data. This is often difﬁcult to\n",
      "describe. Second, it is often not possible to compute the posterior distribu-\n",
      "tion analytically. However, there are some priors that are computationally\n",
      "convenient: conjugate priors . conjugate prior\n",
      "Deﬁnition 6.13 (Conjugate Prior) .A prior is conjugate for the likelihood conjugate\n",
      "function if the posterior is of the same form/type as the prior.\n",
      "Conjugacy is particularly convenient because we can algebraically cal-\n",
      "culate our posterior distribution by updating the parameters of the prior\n",
      "distribution.\n",
      "Remark. When considering the geometry of probability distributions, con-\n",
      "jugate priors retain the same distance structure as the likelihood (Agarwal\n",
      "and Daum ´e III, 2010). }\n",
      "To introduce a concrete example of conjugate priors, we describe in Ex-\n",
      "ample 6.11 the Binomial distribution (deﬁned on discrete random vari-\n",
      "ables) and the Beta distribution (deﬁned on continuous random vari-\n",
      "ables).\n",
      "Example 6.11 (Beta-Binomial Conjugacy)\n",
      "Consider a Binomial random variable x\u0018Bin(N;\u0016)where\n",
      "p(xjN;\u0016) = \n",
      "N\n",
      "x!\n",
      "\u0016x(1\u0000\u0016)N\u0000x; x = 0;1;:::;N; (6.102)\n",
      "is the probability of ﬁnding xtimes the outcome “heads” in Ncoin ﬂips,\n",
      "where\u0016is the probability of a “head”. We place a Beta prior on the pa-\n",
      "rameter\u0016, that is,\u0016\u0018Beta(\u000b;\f), where\n",
      "p(\u0016j\u000b;\f) =\u0000(\u000b+\f)\n",
      "\u0000(\u000b)\u0000(\f)\u0016\u000b\u00001(1\u0000\u0016)\f\u00001: (6.103)\n",
      "If we now observe some outcome x=h, that is, we see hheads inNcoin\n",
      "ﬂips, we compute the posterior distribution on \u0016as\n",
      "p(\u0016jx=h;N;\u000b;\f )/p(xjN;\u0016)p(\u0016j\u000b;\f) (6.104a)\n",
      "/\u0016h(1\u0000\u0016)(N\u0000h)\u0016\u000b\u00001(1\u0000\u0016)\f\u00001(6.104b)\n",
      "=\u0016h+\u000b\u00001(1\u0000\u0016)(N\u0000h)+\f\u00001(6.104c)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.6 Conjugacy and the Exponential Family 209\n",
      "Table 6.2 Examples\n",
      "of conjugate priors\n",
      "for common\n",
      "likelihood functions.Likelihood Conjugate prior Posterior\n",
      "Bernoulli Beta Beta\n",
      "Binomial Beta Beta\n",
      "Gaussian Gaussian/inverse Gamma Gaussian/inverse Gamma\n",
      "Gaussian Gaussian/inverse Wishart Gaussian/inverse Wishart\n",
      "Multinomial Dirichlet Dirichlet\n",
      "/Beta(h+\u000b;N\u0000h+\f); (6.104d)\n",
      "i.e., the posterior distribution is a Beta distribution as the prior, i.e., the\n",
      "Beta prior is conjugate for the parameter \u0016in the Binomial likelihood\n",
      "function.\n",
      "In the following example, we will derive a result that is similar to the\n",
      "Beta-Binomial conjugacy result. Here we will show that the Beta distribu-\n",
      "tion is a conjugate prior for the Bernoulli distribution.\n",
      "Example 6.12 (Beta-Bernoulli Conjugacy)\n",
      "Letx2f0;1gbe distributed according to the Bernoulli distribution with\n",
      "parameter\u00122[0;1], that is,p(x= 1j\u0012) =\u0012. This can also be expressed\n",
      "asp(xj\u0012) =\u0012x(1\u0000\u0012)1\u0000x. Let\u0012be distributed according to a Beta distri-\n",
      "bution with parameters \u000b;\f, that is,p(\u0012j\u000b;\f)/\u0012\u000b\u00001(1\u0000\u0012)\f\u00001.\n",
      "Multiplying the Beta and the Bernoulli distributions, we get\n",
      "p(\u0012jx;\u000b;\f ) =p(xj\u0012)p(\u0012j\u000b;\f) (6.105a)\n",
      "/\u0012x(1\u0000\u0012)1\u0000x\u0012\u000b\u00001(1\u0000\u0012)\f\u00001(6.105b)\n",
      "=\u0012\u000b+x\u00001(1\u0000\u0012)\f+(1\u0000x)\u00001(6.105c)\n",
      "/p(\u0012j\u000b+x;\f+ (1\u0000x)): (6.105d)\n",
      "The last line is the Beta distribution with parameters (\u000b+x;\f+ (1\u0000x)).\n",
      "Table 6.2 lists examples for conjugate priors for the parameters of some\n",
      "standard likelihoods used in probabilistic modeling. Distributions such as The Gamma prior is\n",
      "conjugate for the\n",
      "precision (inverse\n",
      "variance) in the\n",
      "univariate Gaussian\n",
      "likelihood, and the\n",
      "Wishart prior is\n",
      "conjugate for the\n",
      "precision matrix\n",
      "(inverse covariance\n",
      "matrix) in the\n",
      "multivariate\n",
      "Gaussian likelihood.Multinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found\n",
      "in any statistical text, and are described in Bishop (2006), for example.\n",
      "The Beta distribution is the conjugate prior for the parameter \u0016in both\n",
      "the Binomial and the Bernoulli likelihood. For a Gaussian likelihood func-\n",
      "tion, we can place a conjugate Gaussian prior on the mean. The reason\n",
      "why the Gaussian likelihood appears twice in the table is that we need\n",
      "to distinguish the univariate from the multivariate case. In the univariate\n",
      "(scalar) case, the inverse Gamma is the conjugate prior for the variance.\n",
      "In the multivariate case, we use a conjugate inverse Wishart distribution\n",
      "as a prior on the covariance matrix. The Dirichlet distribution is the conju-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "210 Probability and Distributions\n",
      "gate prior for the multinomial likelihood function. For further details, we\n",
      "refer to Bishop (2006).\n",
      "6.6.2 Sufﬁcient Statistics\n",
      "Recall that a statistic of a random variable is a deterministic function of\n",
      "that random variable. For example, if x= [x1;:::;xN]>is a vector of\n",
      "univariate Gaussian random variables, that is, xn\u0018N\u0000\u0016; \u001b2\u0001\n",
      ", then the\n",
      "sample mean ^\u0016=1\n",
      "N(x1+\u0001\u0001\u0001+xN)is a statistic. Sir Ronald Fisher dis-\n",
      "covered the notion of sufﬁcient statistics : the idea that there are statistics sufﬁcient statistics\n",
      "that will contain all available information that can be inferred from data\n",
      "corresponding to the distribution under consideration. In other words, suf-\n",
      "ﬁcient statistics carry all the information needed to make inference about\n",
      "the population, that is, they are the statistics that are sufﬁcient to repre-\n",
      "sent the distribution.\n",
      "For a set of distributions parametrized by \u0012, letXbe a random variable\n",
      "with distribution p(xj\u00120)given an unknown \u00120. A vector\u001e(x)of statistics\n",
      "is called sufﬁcient statistics for \u00120if they contain all possible informa-\n",
      "tion about\u00120. To be more formal about “contain all possible information”,\n",
      "this means that the probability of xgiven\u0012can be factored into a part\n",
      "that does not depend on \u0012, and a part that depends on \u0012only via\u001e(x).\n",
      "The Fisher-Neyman factorization theorem formalizes this notion, which\n",
      "we state in Theorem 6.14 without proof.\n",
      "Theorem 6.14 (Fisher-Neyman) .[Theorem 6.5 in Lehmann and Casella\n",
      "(1998)] Let Xhave probability density function p(xj\u0012). Then the statistics Fisher-Neyman\n",
      "theorem \u001e(x)are sufﬁcient for \u0012if and only if p(xj\u0012)can be written in the form\n",
      "p(xj\u0012) =h(x)g\u0012(\u001e(x)); (6.106)\n",
      "whereh(x)is a distribution independent of \u0012andg\u0012captures all the depen-\n",
      "dence on\u0012via sufﬁcient statistics \u001e(x).\n",
      "Ifp(xj\u0012)does not depend on \u0012, then\u001e(x)is trivially a sufﬁcient statistic\n",
      "for any function \u001e. The more interesting case is that p(xj\u0012)is dependent\n",
      "only on\u001e(x)and notxitself. In this case, \u001e(x)is a sufﬁcient statistic for\n",
      "\u0012.\n",
      "In machine learning, we consider a ﬁnite number of samples from a\n",
      "distribution. One could imagine that for simple distributions (such as the\n",
      "Bernoulli in Example 6.8) we only need a small number of samples to\n",
      "estimate the parameters of the distributions. We could also consider the\n",
      "opposite problem: If we have a set of data (a sample from an unknown\n",
      "distribution), which distribution gives the best ﬁt? A natural question to\n",
      "ask is, as we observe more data, do we need more parameters \u0012to de-\n",
      "scribe the distribution? It turns out that the answer is yes in general, and\n",
      "this is studied in non-parametric statistics (Wasserman, 2007). A converse\n",
      "question is to consider which class of distributions have ﬁnite-dimensional\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.6 Conjugacy and the Exponential Family 211\n",
      "sufﬁcient statistics, that is the number of parameters needed to describe\n",
      "them does not increase arbitrarily. The answer is exponential family dis-\n",
      "tributions, described in the following section.\n",
      "6.6.3 Exponential Family\n",
      "There are three possible levels of abstraction we can have when con-\n",
      "sidering distributions (of discrete or continuous random variables). At\n",
      "level one (the most concrete end of the spectrum), we have a particu-\n",
      "lar named distribution with ﬁxed parameters, for example a univariate\n",
      "GaussianN\u00000;1\u0001\n",
      "with zero mean and unit variance. In machine learning,\n",
      "we often use the second level of abstraction, that is, we ﬁx the paramet-\n",
      "ric form (the univariate Gaussian) and infer the parameters from data. For\n",
      "example, we assume a univariate Gaussian N\u0000\u0016; \u001b2\u0001\n",
      "with unknown mean\n",
      "\u0016and unknown variance \u001b2, and use a maximum likelihood ﬁt to deter-\n",
      "mine the best parameters (\u0016;\u001b2). We will see an example of this when\n",
      "considering linear regression in Chapter 9. A third level of abstraction is\n",
      "to consider families of distributions, and in this book, we consider the ex-\n",
      "ponential family. The univariate Gaussian is an example of a member of\n",
      "the exponential family. Many of the widely used statistical models, includ-\n",
      "ing all the “named” models in Table 6.2, are members of the exponential\n",
      "family. They can all be uniﬁed into one concept (Brown, 1986).\n",
      "Remark. A brief historical anecdote: Like many concepts in mathemat-\n",
      "ics and science, exponential families were independently discovered at\n",
      "the same time by different researchers. In the years 1935–1936, Edwin\n",
      "Pitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in\n",
      "New York independently showed that the exponential families are the only\n",
      "families that enjoy ﬁnite-dimensional sufﬁcient statistics under repeated\n",
      "independent sampling (Lehmann and Casella, 1998). }\n",
      "Anexponential family is a family of probability distributions, parame- exponential family\n",
      "terized by\u00122RD, of the form\n",
      "p(xj\u0012) =h(x) exp (h\u0012;\u001e(x)i\u0000A(\u0012)); (6.107)\n",
      "where\u001e(x)is the vector of sufﬁcient statistics. In general, any inner prod-\n",
      "uct (Section 3.2) can be used in (6.107), and for concreteness we will use\n",
      "the standard dot product here ( h\u0012;\u001e(x)i=\u0012>\u001e(x)). Note that the form\n",
      "of the exponential family is essentially a particular expression of g\u0012(\u001e(x))\n",
      "in the Fisher-Neyman theorem (Theorem 6.14).\n",
      "The factorh(x)can be absorbed into the dot product term by adding\n",
      "another entry ( logh(x)) to the vector of sufﬁcient statistics \u001e(x), and\n",
      "constraining the corresponding parameter \u00120= 1. The termA(\u0012)is the\n",
      "normalization constant that ensures that the distribution sums up or inte-\n",
      "grates to one and is called the log-partition function . A good intuitive no- log-partition\n",
      "function tion of exponential families can be obtained by ignoring these two terms\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "212 Probability and Distributions\n",
      "and considering exponential families as distributions of the form\n",
      "p(xj\u0012)/exp\u0000\u0012>\u001e(x)\u0001: (6.108)\n",
      "For this form of parametrization, the parameters \u0012are called the natural natural parameters\n",
      "parameters . At ﬁrst glance, it seems that exponential families are a mun-\n",
      "dane transformation by adding the exponential function to the result of a\n",
      "dot product. However, there are many implications that allow for conve-\n",
      "nient modeling and efﬁcient computation based on the fact that we can\n",
      "capture information about data in \u001e(x).\n",
      "Example 6.13 (Gaussian as Exponential Family)\n",
      "Consider the univariate Gaussian distribution N\u0000\u0016; \u001b2\u0001\n",
      ". Let\u001e(x) =\u0014x\n",
      "x2\u0015\n",
      ".\n",
      "Then by using the deﬁnition of the exponential family,\n",
      "p(xj\u0012)/exp(\u00121x+\u00122x2): (6.109)\n",
      "Setting\n",
      "\u0012=\u0014\u0016\n",
      "\u001b2;\u00001\n",
      "2\u001b2\u0015>\n",
      "(6.110)\n",
      "and substituting into (6.109), we obtain\n",
      "p(xj\u0012)/exp\u0012\u0016x\n",
      "\u001b2\u0000x2\n",
      "2\u001b2\u0013\n",
      "/exp\u0012\n",
      "\u00001\n",
      "2\u001b2(x\u0000\u0016)2\u0013\n",
      ": (6.111)\n",
      "Therefore, the univariate Gaussian distribution is a member of the expo-\n",
      "nential family with sufﬁcient statistic \u001e(x) =\u0014x\n",
      "x2\u0015\n",
      ", and natural parame-\n",
      "ters given by\u0012in (6.110).\n",
      "Example 6.14 (Bernoulli as Exponential Family)\n",
      "Recall the Bernoulli distribution from Example 6.8\n",
      "p(xj\u0016) =\u0016x(1\u0000\u0016)1\u0000x; x2f0;1g: (6.112)\n",
      "This can be written in exponential family form\n",
      "p(xj\u0016) = exp\u0002log\u0000\u0016x(1\u0000\u0016)1\u0000x\u0001\u0003\n",
      "(6.113a)\n",
      "= exp [xlog\u0016+ (1\u0000x) log(1\u0000\u0016)] (6.113b)\n",
      "= exp [xlog\u0016\u0000xlog(1\u0000\u0016) + log(1\u0000\u0016)] (6.113c)\n",
      "= exph\n",
      "xlog\u0016\n",
      "1\u0000\u0016+ log(1\u0000\u0016)i\n",
      ": (6.113d)\n",
      "The last line (6.113d) can be identiﬁed as being in exponential family\n",
      "form (6.107) by observing that\n",
      "h(x) = 1 (6.114)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.6 Conjugacy and the Exponential Family 213\n",
      "\u0012= log\u0016\n",
      "1\u0000\u0016(6.115)\n",
      "\u001e(x) =x (6.116)\n",
      "A(\u0012) =\u0000log(1\u0000\u0016) = log(1 + exp( \u0012)): (6.117)\n",
      "The relationship between \u0012and\u0016is invertible so that\n",
      "\u0016=1\n",
      "1 + exp(\u0000\u0012): (6.118)\n",
      "The relation (6.118) is used to obtain the right equality of (6.117).\n",
      "Remark. The relationship between the original Bernoulli parameter \u0016and\n",
      "the natural parameter \u0012is known as the sigmoid or logistic function. Ob- sigmoid\n",
      "serve that\u00162(0;1)but\u00122R, and therefore the sigmoid function\n",
      "squeezes a real value into the range (0;1). This property is useful in ma-\n",
      "chine learning, for example it is used in logistic regression (Bishop, 2006,\n",
      "section 4.3.2), as well as as a nonlinear activation functions in neural net-\n",
      "works (Goodfellow et al., 2016, chapter 6). }\n",
      "It is often not obvious how to ﬁnd the parametric form of the conjugate\n",
      "distribution of a particular distribution (for example, those in Table 6.2).\n",
      "Exponential families provide a convenient way to ﬁnd conjugate pairs of\n",
      "distributions. Consider the random variable Xis a member of the expo-\n",
      "nential family (6.107):\n",
      "p(xj\u0012) =h(x) exp (h\u0012;\u001e(x)i\u0000A(\u0012)): (6.119)\n",
      "Every member of the exponential family has a conjugate prior (Brown,\n",
      "1986)\n",
      "1 =hc(\u0012) exp\u0012\u001c\u0014\n",
      "2\u0015\n",
      ";\u0014\u0012\n",
      "\u0000A(\u0012)\u0015\u001d\n",
      ")\u0013c(\n",
      "; (6.120)\n",
      "1\u0014ere\n",
      "2\u0015\n",
      "has dimension dim(\u0012) + 1 . The sufﬁcient statistics of\n",
      "the conjugate prior are\u0014\u0012\n",
      "\u0000A(\u0012)\u0015\n",
      ". By using the knowledge of the general\n",
      "form of conjugate priors for exponential families, we can derive functional\n",
      "forms of conjugate priors corresponding to particular distributions.\n",
      "Example 6.15\n",
      "Recall the exponential family form of the Bernoulli distribution (6.113d)\n",
      "p(xj\u0016) = exp\u0014\n",
      "xlog\u0016\n",
      "1\u0000\u0016+ log(1\u0000\u0016)\u0015\n",
      ": (6.121)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "214 Probability and Distributions\n",
      "The canonical conjugate prior has the form\n",
      "p(\u0016j\u000b;\f) =\u0016\n",
      "1\u0000\u0016exp\u0014\n",
      "\u000blog\u0016\n",
      ")\u0015\u0016+ (\f+\u000b) log(1\u0000\u0016)\u0000Ac(\n",
      ";\n",
      "(6.122)\n",
      ":= [\u000b;\f+\u000b]>andhc(\u0016) :=\u0016=(1\u0000\u0016). Equa-\n",
      "tion (6.122) then simpliﬁes to\n",
      "p(\u0016j\u000b;\f) = exp [(\u000b\u00001) log\u0016+ (\f\u00001) log(1\u0000\u0016)\u0000Ac(\u000b;\f)]:\n",
      "(6.123)\n",
      "Putting this in non-exponential family form yields\n",
      "p(\u0016j\u000b;\f)/\u0016\u000b\u00001(1\u0000\u0016)\f\u00001; (6.124)\n",
      "which we identify as the Beta distribution (6.98). In example 6.12, we\n",
      "assumed that the Beta distribution is the conjugate prior of the Bernoulli\n",
      "distribution and showed that it was indeed the conjugate prior. In this\n",
      "example, we derived the form of the Beta distribution by looking at the\n",
      "canonical conjugate prior of the Bernoulli distribution in exponential fam-\n",
      "ily form.\n",
      "As mentioned in the previous section, the main motivation for expo-\n",
      "nential families is that they have ﬁnite-dimensional sufﬁcient statistics.\n",
      "Additionally, conjugate distributions are easy to write down, and the con-\n",
      "jugate distributions also come from an exponential family. From an infer-\n",
      "ence perspective, maximum likelihood estimation behaves nicely because\n",
      "empirical estimates of sufﬁcient statistics are optimal estimates of the pop-\n",
      "ulation values of sufﬁcient statistics (recall the mean and covariance of a\n",
      "Gaussian). From an optimization perspective, the log-likelihood function\n",
      "is concave, allowing for efﬁcient optimization approaches to be applied\n",
      "(Chapter 7).\n",
      "6.7 Change of Variables/Inverse Transform\n",
      "It may seem that there are very many known distributions, but in reality\n",
      "the set of distributions for which we have names is quite limited. There-\n",
      "fore, it is often useful to understand how transformed random variables\n",
      "are distributed. For example, assuming that Xis a random variable dis-\n",
      "tributed according to the univariate normal distribution N\u00000;1\u0001\n",
      ", what is\n",
      "the distribution of X2? Another example, which is quite common in ma-\n",
      "chine learning, is, given that X1andX2are univariate standard normal,\n",
      "what is the distribution of1\n",
      "2(X1+X2)?\n",
      "One option to work out the distribution of1\n",
      "2(X1+X2)is to calculate\n",
      "the mean and variance of X1andX2and then combine them. As we saw\n",
      "in Section 6.4.4, we can calculate the mean and variance of resulting ran-\n",
      "dom variables when we consider afﬁne transformations of random vari-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.7 Change of Variables/Inverse Transform 215\n",
      "ables. However, we may not be able to obtain the functional form of the\n",
      "distribution under transformations. Furthermore, we may be interested\n",
      "in nonlinear transformations of random variables for which closed-form\n",
      "expressions are not readily available.\n",
      "Remark (Notation) .In this section, we will be explicit about random vari-\n",
      "ables and the values they take. Hence, recall that we use capital letters\n",
      "X;Y to denote random variables and small letters x;yto denote the val-\n",
      "ues in the target space Tthat the random variables take. We will explicitly\n",
      "write pmfs of discrete random variables XasP(X=x). For continuous\n",
      "random variables X(Section 6.2.2), the pdf is written as f(x)and the cdf\n",
      "is written as FX(x). }\n",
      "We will look at two approaches for obtaining distributions of transfor-\n",
      "mations of random variables: a direct approach using the deﬁnition of a\n",
      "cumulative distribution function and a change-of-variable approach that\n",
      "uses the chain rule of calculus (Section 5.2.2). The change-of-variable ap- Moment generating\n",
      "functions can also\n",
      "be used to study\n",
      "transformations of\n",
      "random\n",
      "variables (Casella\n",
      "and Berger, 2002,\n",
      "chapter 2).proach is widely used because it provides a “recipe” for attempting to\n",
      "compute the resulting distribution due to a transformation. We will ex-\n",
      "plain the techniques for univariate random variables, and will only brieﬂy\n",
      "provide the results for the general case of multivariate random variables.\n",
      "Transformations of discrete random variables can be understood di-\n",
      "rectly. Suppose that there is a discrete random variable Xwith pmfP(X=\n",
      "x)(Section 6.2.1), and an invertible function U(x). Consider the trans-\n",
      "formed random variable Y:=U(X), with pmfP(Y=y). Then\n",
      "P(Y=y) =P(U(X) =y) transformation of interest (6.125a)\n",
      "=P(X=U\u00001(y)) inverse (6.125b)\n",
      "where we can observe that x=U\u00001(y). Therefore, for discrete random\n",
      "variables, transformations directly change the individual events (with the\n",
      "probabilities appropriately transformed).\n",
      "6.7.1 Distribution Function Technique\n",
      "The distribution function technique goes back to ﬁrst principles, and uses\n",
      "the deﬁnition of a cdf FX(x) =P(X6x)and the fact that its differential\n",
      "is the pdff(x)(Wasserman, 2004, chapter 2). For a random variable X\n",
      "and a function U, we ﬁnd the pdf of the random variable Y:=U(X)by\n",
      "1. Finding the cdf:\n",
      "FY(y) =P(Y6y) (6.126)\n",
      "2. Differentiating the cdf FY(y)to get the pdf f(y).\n",
      "f(y) =d\n",
      "dyFY(y): (6.127)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "216 Probability and Distributions\n",
      "We also need to keep in mind that the domain of the random variable may\n",
      "have changed due to the transformation by U.\n",
      "Example 6.16\n",
      "LetXbe a continuous random variable with probability density function\n",
      "on06x61\n",
      "f(x) = 3x2: (6.128)\n",
      "We are interested in ﬁnding the pdf of Y=X2.\n",
      "The function fis an increasing function of x, and therefore the resulting\n",
      "value ofylies in the interval [0;1]. We obtain\n",
      "FY(y) =P(Y6y) de\fnition of cdf (6.129a)\n",
      "=P(X26y) transformation of interest (6.129b)\n",
      "=P(X6y1\n",
      "2) inverse (6.129c)\n",
      "=FX(y1\n",
      "2) de\fnition of cdf (6.129d)\n",
      "=Zy1\n",
      "2\n",
      "03t2dt cdf as a de\fnite integral (6.129e)\n",
      "=\u0002t3\u0003t=y1\n",
      "2\n",
      "t=0result of integration (6.129f)\n",
      "=y3\n",
      "2;06y61: (6.129g)\n",
      "Therefore, the cdf of Yis\n",
      "FY(y) =y3\n",
      "2 (6.130)\n",
      "for06y61. To obtain the pdf, we differentiate the cdf\n",
      "f(y) =d\n",
      "dyFY(y) =3\n",
      "2y1\n",
      "2 (6.131)\n",
      "for06y61.\n",
      "In Example 6.16, we considered a strictly monotonically increasing func-\n",
      "tionf(x) = 3x2. This means that we could compute an inverse function. Functions that have\n",
      "inverses are called\n",
      "bijective functions\n",
      "(Section 2.7).In general, we require that the function of interest y=U(x)has an in-\n",
      "versex=U\u00001(y). A useful result can be obtained by considering the cu-\n",
      "mulative distribution function FX(x)of a random variable X, and using\n",
      "it as the transformation U(x). This leads to the following theorem.\n",
      "Theorem 6.15. [Theorem 2.1.10 in Casella and Berger (2002)] Let Xbe a\n",
      "continuous random variable with a strictly monotonic cumulative distribu-\n",
      "tion function FX(x). Then the random variable Ydeﬁned as\n",
      "Y:=FX(X) (6.132)\n",
      "has a uniform distribution.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.7 Change of Variables/Inverse Transform 217\n",
      "Theorem 6.15 is known as the probability integral transform , and it is probability integral\n",
      "transform used to derive algorithms for sampling from distributions by transforming\n",
      "the result of sampling from a uniform random variable (Bishop, 2006).\n",
      "The algorithm works by ﬁrst generating a sample from a uniform distribu-\n",
      "tion, then transforming it by the inverse cdf (assuming this is available)\n",
      "to obtain a sample from the desired distribution. The probability integral\n",
      "transform is also used for hypothesis testing whether a sample comes from\n",
      "a particular distribution (Lehmann and Romano, 2005). The idea that the\n",
      "output of a cdf gives a uniform distribution also forms the basis of copu-\n",
      "las (Nelsen, 2006).\n",
      "6.7.2 Change of Variables\n",
      "The distribution function technique in Section 6.7.1 is derived from ﬁrst\n",
      "principles, based on the deﬁnitions of cdfs and using properties of in-\n",
      "verses, differentiation, and integration. This argument from ﬁrst principles\n",
      "relies on two facts:\n",
      "1. We can transform the cdf of Yinto an expression that is a cdf of X.\n",
      "2. We can differentiate the cdf to obtain the pdf.\n",
      "Let us break down the reasoning step by step, with the goal of understand-\n",
      "ing the more general change-of-variables approach in Theorem 6.16. Change of variables\n",
      "in probability relies\n",
      "on the\n",
      "change-of-variables\n",
      "method in\n",
      "calculus (Tandra,\n",
      "2014).Remark. The name “change of variables” comes from the idea of chang-\n",
      "ing the variable of integration when faced with a difﬁcult integral. For\n",
      "univariate functions, we use the substitution rule of integration,\n",
      "Z\n",
      "f(g(x))g0(x)dx=Z\n",
      "f(u)du; whereu=g(x): (6.133)\n",
      "The derivation of this rule is based on the chain rule of calculus (5.32) and\n",
      "by applying twice the fundamental theorem of calculus. The fundamental\n",
      "theorem of calculus formalizes the fact that integration and differentiation\n",
      "are somehow “inverses” of each other. An intuitive understanding of the\n",
      "rule can be obtained by thinking (loosely) about small changes (differen-\n",
      "tials) to the equation u=g(x), that is by considering \u0001u=g0(x)\u0001xas a\n",
      "differential of u=g(x). By substituting u=g(x), the argument inside the\n",
      "integral on the right-hand side of (6.133) becomes f(g(x)). By pretending\n",
      "that the term ducan be approximated by du\u0019\u0001u=g0(x)\u0001x, and that\n",
      "dx\u0019\u0001x, we obtain (6.133). }\n",
      "Consider a univariate random variable X, and an invertible function\n",
      "U, which gives us another random variable Y=U(X). We assume that\n",
      "random variable Xhas statesx2[a;b]. By the deﬁnition of the cdf, we\n",
      "have\n",
      "FY(y) =P(Y6y): (6.134)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "218 Probability and Distributions\n",
      "We are interested in a function Uof the random variable\n",
      "P(Y6y) =P(U(X)6y); (6.135)\n",
      "where we assume that the function Uis invertible. An invertible function\n",
      "on an interval is either strictly increasing or strictly decreasing. In the case\n",
      "thatUis strictly increasing, then its inverse U\u00001is also strictly increasing.\n",
      "By applying the inverse U\u00001to the arguments of P(U(X)6y), we obtain\n",
      "P(U(X)6y) =P(U\u00001(U(X))6U\u00001(y)) =P(X6U\u00001(y)):\n",
      "(6.136)\n",
      "The right-most term in (6.136) is an expression of the cdf of X. Recall the\n",
      "deﬁnition of the cdf in terms of the pdf\n",
      "P(X6U\u00001(y)) =ZU\u00001(y)\n",
      "af(x)dx: (6.137)\n",
      "Now we have an expression of the cdf of Yin terms ofx:\n",
      "FY(y) =ZU\u00001(y)\n",
      "af(x)dx: (6.138)\n",
      "To obtain the pdf, we differentiate (6.138) with respect to y:\n",
      "f(y) =d\n",
      "dyFy(y) =d\n",
      "dyZU\u00001(y)\n",
      "af(x)dx: (6.139)\n",
      "Note that the integral on the right-hand side is with respect to x, but we\n",
      "need an integral with respect to ybecause we are differentiating with\n",
      "respect toy. In particular, we use (6.133) to get the substitution\n",
      "Z\n",
      "f(U\u00001(y))U\u000010(y)dy=Z\n",
      "f(x)dxwherex=U\u00001(y):(6.140)\n",
      "Using (6.140) on the right-hand side of (6.139) gives us\n",
      "f(y) =d\n",
      "dyZU\u00001(y)\n",
      "afx(U\u00001(y))U\u000010(y)dy: (6.141)\n",
      "We then recall that differentiation is a linear operator and we use the\n",
      "subscriptxto remind ourselves that fx(U\u00001(y))is a function of xand not\n",
      "y. Invoking the fundamental theorem of calculus again gives us\n",
      "f(y) =fx(U\u00001(y))\u0001\u0012d\n",
      "dyU\u00001(y)\u0013\n",
      ": (6.142)\n",
      "Recall that we assumed that Uis a strictly increasing function. For decreas-\n",
      "ing functions, it turns out that we have a negative sign when we follow\n",
      "the same derivation. We introduce the absolute value of the differential to\n",
      "have the same expression for both increasing and decreasing U:\n",
      "f(y) =fx(U\u00001(y))\u0001\f\f\f\fd\n",
      "dyU\u00001(y)\f\f\f\f: (6.143)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.7 Change of Variables/Inverse Transform 219\n",
      "This is called the change-of-variable technique . The term\f\f\fd\n",
      "dyU\u00001(y)\f\f\fin change-of-variable\n",
      "technique(6.143) measures how much a unit volume changes when applying U\n",
      "(see also the deﬁnition of the Jacobian in Section 5.3).\n",
      "Remark. In comparison to the discrete case in (6.125b), we have an addi-\n",
      "tional factor\f\f\fd\n",
      "dyU\u00001(y)\f\f\f. The continuous case requires more care because\n",
      "P(Y=y) = 0 for ally. The probability density function f(y)does not\n",
      "have a description as a probability of an event involving y.}\n",
      "So far in this section, we have been studying univariate change of vari-\n",
      "ables. The case for multivariate random variables is analogous, but com-\n",
      "plicated by fact that the absolute value cannot be used for multivariate\n",
      "functions. Instead, we use the determinant of the Jacobian matrix. Recall\n",
      "from (5.58) that the Jacobian is a matrix of partial derivatives, and that\n",
      "the existence of a nonzero determinant shows that we can invert the Ja-\n",
      "cobian. Recall the discussion in Section 4.1 that the determinant arises\n",
      "because our differentials (cubes of volume) are transformed into paral-\n",
      "lelepipeds by the Jacobian. Let us summarize preceding the discussion in\n",
      "the following theorem, which gives us a recipe for multivariate change of\n",
      "variables.\n",
      "Theorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f(x)be the value\n",
      "of the probability density of the multivariate continuous random variable X.\n",
      "If the vector-valued function y=U(x)is differentiable and invertible for\n",
      "all values within the domain of x, then for corresponding values of y, the\n",
      "probability density of Y=U(X)is given by\n",
      "f(y) =fx(U\u00001(y))\u0001\f\f\f\fdet\u0012@\n",
      "@yU\u00001(y)\u0013\f\f\f\f: (6.144)\n",
      "The theorem looks intimidating at ﬁrst glance, but the key point is that\n",
      "a change of variable of a multivariate random variable follows the pro-\n",
      "cedure of the univariate change of variable. First we need to work out\n",
      "the inverse transform, and substitute that into the density of x. Then we\n",
      "calculate the determinant of the Jacobian and multiply the result. The\n",
      "following example illustrates the case of a bivariate random variable.\n",
      "Example 6.17\n",
      "Consider a bivariate random variable Xwith statesx=\u0014x1\n",
      "x2\u0015\n",
      "and proba-\n",
      "bility density function\n",
      "f\u0012\u0014x1\n",
      "x2\u0015\u0013\n",
      "=1\n",
      "2\u0019exp \n",
      "\u00001\n",
      "2\u0014x1\n",
      "x2\u0015>\u0014x1\n",
      "x2\u0015!\n",
      ": (6.145)\n",
      "We use the change-of-variable technique from Theorem 6.16 to derive the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "220 Probability and Distributions\n",
      "effect of a linear transformation (Section 2.7) of the random variable.\n",
      "Consider a matrix A2R2\u00022deﬁned as\n",
      "A=\u0014a b\n",
      "c d\u0015\n",
      ": (6.146)\n",
      "We are interested in ﬁnding the probability density function of the trans-\n",
      "formed bivariate random variable Ywith statesy=Ax.\n",
      "Recall that for change of variables we require the inverse transformation\n",
      "ofxas a function of y. Since we consider linear transformations, the\n",
      "inverse transformation is given by the matrix inverse (see Section 2.2.2).\n",
      "For2\u00022matrices, we can explicitly write out the formula, given by\n",
      "\u0014x1\n",
      "x2\u0015\n",
      "=A\u00001\u0014y1\n",
      "y2\u0015\n",
      "=1\n",
      "ad\u0000bc\u0014d\u0000b\n",
      "\u0000c a\u0015\u0014y1\n",
      "y2\u0015\n",
      ": (6.147)\n",
      "Observe that ad\u0000bcis the determinant (Section 4.1) of A. The corre-\n",
      "sponding probability density function is given by\n",
      "f(x) =f(A\u00001y) =1\n",
      "2\u0019exp\u0010\n",
      "\u00001\n",
      "2y>A\u0000>A\u00001y\u0011\n",
      ": (6.148)\n",
      "The partial derivative of a matrix times a vector with respect to the vector\n",
      "is the matrix itself (Section 5.5), and therefore\n",
      "@\n",
      "@yA\u00001y=A\u00001: (6.149)\n",
      "Recall from Section 4.1 that the determinant of the inverse is the inverse\n",
      "of the determinant so that the determinant of the Jacobian matrix is\n",
      "det\u0012@\n",
      "@yA\u00001y\u0013\n",
      "=1\n",
      "ad\u0000bc: (6.150)\n",
      "We are now able to apply the change-of-variable formula from Theo-\n",
      "rem 6.16 by multiplying (6.148) with (6.150), which yields\n",
      "f(y) =f(x)\f\f\f\fdet\u0012@\n",
      "@yA\u00001y\u0013\f\f\f\f(6.151a)\n",
      "=1\n",
      "2\u0019exp\u0010\n",
      "\u00001\n",
      "2y>A\u0000>A\u00001y\u0011\n",
      "jad\u0000bcj\u00001: (6.151b)\n",
      "While Example 6.17 is based on a bivariate random variable, which al-\n",
      "lows us to easily compute the matrix inverse, the preceding relation holds\n",
      "for higher dimensions.\n",
      "Remark. We saw in Section 6.5 that the density f(x)in (6.148) is actually\n",
      "the standard Gaussian distribution, and the transformed density f(y)is a\n",
      "bivariate Gaussian with covariance \u0006=AA>. }\n",
      "We will use the ideas in this chapter to describe probabilistic modeling\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "6.8 Further Reading 221\n",
      "in Section 8.4, as well as introduce a graphical language in Section 8.5. We\n",
      "will see direct machine learning applications of these ideas in Chapters 9\n",
      "and 11.\n",
      "6.8 Further Reading\n",
      "This chapter is rather terse at times. Grinstead and Snell (1997) and\n",
      "Walpole et al. (2011) provide more relaxed presentations that are suit-\n",
      "able for self-study. Readers interested in more philosophical aspects of\n",
      "probability should consider Hacking (2001), whereas an approach that\n",
      "is more related to software engineering is presented by Downey (2014).\n",
      "An overview of exponential families can be found in Barndorff-Nielsen\n",
      "(2014). We will see more about how to use probability distributions to\n",
      "model machine learning tasks in Chapter 8. Ironically, the recent surge\n",
      "in interest in neural networks has resulted in a broader appreciation of\n",
      "probabilistic models. For example, the idea of normalizing ﬂows (Jimenez\n",
      "Rezende and Mohamed, 2015) relies on change of variables for transform-\n",
      "ing random variables. An overview of methods for variational inference as\n",
      "applied to neural networks is described in chapters 16 to 20 of the book\n",
      "by Goodfellow et al. (2016).\n",
      "We side stepped a large part of the difﬁculty in continuous random vari-\n",
      "ables by avoiding measure theoretic questions (Billingsley, 1995; Pollard,\n",
      "2002), and by assuming without construction that we have real numbers,\n",
      "and ways of deﬁning sets on real numbers as well as their appropriate fre-\n",
      "quency of occurrence. These details do matter, for example, in the speciﬁ-\n",
      "cation of conditional probability p(yjx)for continuous random variables\n",
      "x;y(Proschan and Presnell, 1998). The lazy notation hides the fact that\n",
      "we want to specify that X=x(which is a set of measure zero). Fur-\n",
      "thermore, we are interested in the probability density function of y. A\n",
      "more precise notation would have to say Ey[f(y)j\u001b(x)], where we take\n",
      "the expectation over yof a test function fconditioned on the \u001b-algebra of\n",
      "x. A more technical audience interested in the details of probability the-\n",
      "ory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter,\n",
      "2004; Grimmett and Welsh, 2014), including some very technical discus-\n",
      "sions (Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel\n",
      "and Doksum, 2006; C ¸inlar, 2011). An alternative way to approach proba-\n",
      "bility is to start with the concept of expectation, and “work backward” to\n",
      "derive the necessary properties of a probability space (Whittle, 2000). As\n",
      "machine learning allows us to model more intricate distributions on ever\n",
      "more complex types of data, a developer of probabilistic machine learn-\n",
      "ing models would have to understand these more technical aspects. Ma-\n",
      "chine learning texts with a probabilistic modeling focus include the books\n",
      "by MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Bar-\n",
      "ber (2012); Murphy (2012).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "222 Probability and Distributions\n",
      "Exercises\n",
      "6.1 Consider the following bivariate distribution p(x;y)of two discrete random\n",
      "variablesXandY.\n",
      "Xx1x2x3x4x5Y\n",
      "y3y2y10.01 0.02 0.03 0.1 0.1\n",
      "0.05 0.1 0.05 0.07 0.2\n",
      "0.1 0.05 0.03 0.05 0.04\n",
      "Compute:\n",
      "a. The marginal distributions p(x)andp(y).\n",
      "b. The conditional distributions p(xjY=y1)andp(yjX=x3).\n",
      "6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4),\n",
      "0:4N\u0012\u0014\n",
      "10\n",
      "2\u0015\n",
      ";\u0014\n",
      "1 0\n",
      "0 1\u0015\u0013\n",
      "+ 0:6N\u0012\u0014\n",
      "0\n",
      "0\u0015\n",
      ";\u0014\n",
      "8:4 2:0\n",
      "2:0 1:7\u0015\u0013\n",
      ":\n",
      "a. Compute the marginal distributions for each dimension.\n",
      "b. Compute the mean, mode and median for each marginal distribution.\n",
      "c. Compute the mean and mode for the two-dimensional distribution.\n",
      "6.3 You have written a computer program that sometimes compiles and some-\n",
      "times not (code does not change). You decide to model the apparent stochas-\n",
      "ticity (success vs. no success) xof the compiler using a Bernoulli distribution\n",
      "with parameter \u0016:\n",
      "p(xj\u0016) =\u0016x(1\u0000\u0016)1\u0000x; x2f0;1g:\n",
      "Choose a conjugate prior for the Bernoulli likelihood and compute the pos-\n",
      "terior distribution p(\u0016jx1;:::;xN).\n",
      "6.4 There are two bags. The ﬁrst bag contains four mangos and two apples; the\n",
      "second bag contains four mangos and four apples.\n",
      "We also have a biased coin, which shows “heads” with probability 0.6 and\n",
      "“tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at\n",
      "random from bag 1; otherwise we pick a fruit at random from bag 2.\n",
      "Your friend ﬂips the coin (you cannot see the result), picks a fruit at random\n",
      "from the corresponding bag, and presents you a mango.\n",
      "What is the probability that the mango was picked from bag 2?\n",
      "Hint: Use Bayes’ theorem.\n",
      "6.5 Consider the time-series model\n",
      "xt+1=Axt+w;w\u0018N\u0000\n",
      "0;Q\u0001\n",
      "yt=Cxt+v;v\u0018N\u0000\n",
      "0;R\u0001\n",
      ";\n",
      "wherew;vare i.i.d. Gaussian noise variables. Further, assume that p(x0) =\n",
      "N\u0000\n",
      "\u00160;\u00060\u0001.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 223\n",
      "a. What is the form of p(x0;x1;:::;xT)? Justify your answer (you do not\n",
      "have to explicitly compute the joint distribution).\n",
      "b. Assume that p(xtjy1;:::;yt) =N\u0000\n",
      "\u0016t;\u0006t\u0001.\n",
      "1. Compute p(xt+1jy1;:::;yt).\n",
      "2. Compute p(xt+1;yt+1jy1;:::;yt).\n",
      "3. At timet+1, we observe the value yt+1=^y. Compute the conditional\n",
      "distribution p(xt+1jy1;:::;yt+1).\n",
      "6.6 Prove the relationship in (6.44), which relates the standard deﬁnition of the\n",
      "variance to the raw-score expression for the variance.\n",
      "6.7 Prove the relationship in (6.45), which relates the pairwise difference be-\n",
      "tween examples in a dataset with the raw-score expression for the variance.\n",
      "6.8 Express the Bernoulli distribution in the natural parameter form of the ex-\n",
      "ponential family, see (6.107).\n",
      "6.9 Express the Binomial distribution as an exponential family distribution. Also\n",
      "express the Beta distribution is an exponential family distribution. Show that\n",
      "the product of the Beta and the Binomial distribution is also a member of\n",
      "the exponential family.\n",
      "6.10 Derive the relationship in Section 6.5.2 in two ways:\n",
      "a. By completing the square\n",
      "b. By expressing the Gaussian in its exponential family form\n",
      "Theproduct of two Gaussians N\u0000\n",
      "xja;A\u0001\n",
      "N\u0000\n",
      "xjb;B\u0001is an unnormalized\n",
      "Gaussian distribution cN\u0000\n",
      "xjc;C\u0001with\n",
      "C= (A\u00001+B\u00001)\u00001\n",
      "c=C(A\u00001a+B\u00001b)\n",
      "c= (2\u0019)\u0000D\n",
      "2jA+Bj\u00001\n",
      "2exp\u0000\n",
      "\u00001\n",
      "2(a\u0000b)>(A+B)\u00001(a\u0000b)\u0001\n",
      ":\n",
      "Note that the normalizing constant citself can be considered a (normalized)\n",
      "Gaussian distribution either in aor inbwith an “inﬂated” covariance matrix\n",
      "A+B, i.e.,c=N\u0000\n",
      "ajb;A+B\u0001\n",
      "=N\u0000\n",
      "bja;A+B\u0001.\n",
      "6.11 Iterated Expectations.\n",
      "Consider two random variables x,ywith joint distribution p(x;y). Show that\n",
      "EX[x] =EY\u0002EX[xjy]\u0003\n",
      ":\n",
      "Here,EX[xjy]denotes the expected value of xunder the conditional distri-\n",
      "butionp(xjy).\n",
      "6.12 Manipulation of Gaussian Random Variables.\n",
      "Consider a Gaussian random variable x\u0018N\u0000\n",
      "xj\u0016x;\u0006x\u0001, wherex2RD.\n",
      "Furthermore, we have\n",
      "y=Ax+b+w;\n",
      "wherey2RE,A2RE\u0002D,b2RE, andw\u0018N\u0000\n",
      "wj0;Q\u0001is indepen-\n",
      "dent Gaussian noise. “Independent” implies that xandware independent\n",
      "random variables and that Qis diagonal.\n",
      "a. Write down the likelihood p(yjx).\n",
      "b. The distribution p(y) =R\n",
      "p(yjx)p(x)dxis Gaussian. Compute the mean\n",
      "\u0016yand the covariance \u0006y. Derive your result in detail.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "224 Probability and Distributions\n",
      "c. The random variable yis being transformed according to the measure-\n",
      "ment mapping\n",
      "z=Cy+v;\n",
      "wherez2RF,C2RF\u0002E, andv\u0018N\u0000\n",
      "vj0;R\u0001is independent Gaus-\n",
      "sian (measurement) noise.\n",
      "Write down p(zjy).\n",
      "Computep(z), i.e., the mean \u0016zand the covariance \u0006z. Derive your\n",
      "result in detail.\n",
      "d. Now, a value ^yis measured. Compute the posterior distribution p(xj^y).\n",
      "Hint for solution: This posterior is also Gaussian, i.e., we need to de-\n",
      "termine only its mean and covariance matrix. Start by explicitly com-\n",
      "puting the joint Gaussian p(x;y). This also requires us to compute the\n",
      "cross-covariances Covx;y[x;y]and Covy;x[y;x]. Then apply the rules\n",
      "for Gaussian conditioning.\n",
      "6.13 Probability Integral Transformation\n",
      "Given a continuous random variable X, with cdfFX(x), show that the ran-\n",
      "dom variable Y:=FX(X)is uniformly distributed (Theorem 6.15).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7\n",
      "Continuous Optimization\n",
      "Since machine learning algorithms are implemented on a computer, the\n",
      "mathematical formulations are expressed as numerical optimization meth-\n",
      "ods. This chapter describes the basic numerical methods for training ma-\n",
      "chine learning models. Training a machine learning model often boils\n",
      "down to ﬁnding a good set of parameters. The notion of “good” is de-\n",
      "termined by the objective function or the probabilistic model, which we\n",
      "will see examples of in the second part of this book. Given an objective\n",
      "function, ﬁnding the best value is done using optimization algorithms. Since we consider\n",
      "data and models in\n",
      "RD, the\n",
      "optimization\n",
      "problems we face\n",
      "arecontinuous\n",
      "optimization\n",
      "problems, as\n",
      "opposed to\n",
      "combinatorial\n",
      "optimization\n",
      "problems for\n",
      "discrete variables.This chapter covers two main branches of continuous optimization (Fig-\n",
      "ure 7.1): unconstrained and constrained optimization. We will assume in\n",
      "this chapter that our objective function is differentiable (see Chapter 5),\n",
      "hence we have access to a gradient at each location in the space to help us\n",
      "ﬁnd the optimum value. By convention, most objective functions in ma-\n",
      "chine learning are intended to be minimized, that is, the best value is the\n",
      "minimum value. Intuitively ﬁnding the best value is like ﬁnding the val-\n",
      "leys of the objective function, and the gradients point us uphill. The idea is\n",
      "to move downhill (opposite to the gradient) and hope to ﬁnd the deepest\n",
      "point. For unconstrained optimization, this is the only concept we need,\n",
      "but there are several design choices, which we discuss in Section 7.1. For\n",
      "constrained optimization, we need to introduce other concepts to man-\n",
      "age the constraints (Section 7.2). We will also introduce a special class\n",
      "of problems (convex optimization problems in Section 7.3) where we can\n",
      "make statements about reaching the global optimum.\n",
      "Consider the function in Figure 7.2. The function has a global minimum global minimum\n",
      "aroundx=\u00004:5, with a function value of approximately \u000047. Since\n",
      "the function is “smooth,” the gradients can be used to help ﬁnd the min-\n",
      "imum by indicating whether we should take a step to the right or left.\n",
      "This assumes that we are in the correct bowl, as there exists another local local minimum\n",
      "minimum aroundx= 0:7. Recall that we can solve for all the stationary\n",
      "points of a function by calculating its derivative and setting it to zero. For Stationary points\n",
      "are the real roots of\n",
      "the derivative, that\n",
      "is, points that have\n",
      "zero gradient.`(x) =x4+ 7x3+ 5x2\u000017x+ 3; (7.1)\n",
      "we obtain the corresponding gradient as\n",
      "d`(x)\n",
      "dx= 4x3+ 21x2+ 10x\u000017: (7.2)\n",
      "225\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "226 Continuous Optimization\n",
      "Figure 7.1 A mind\n",
      "map of the concepts\n",
      "related to\n",
      "optimization, as\n",
      "presented in this\n",
      "chapter. There are\n",
      "two main ideas:\n",
      "gradient descent\n",
      "and convex\n",
      "optimization.Continuous\n",
      "optimization\n",
      "Unconstrained\n",
      "optimization\n",
      "Constrained\n",
      "optimizationGradient descentStepsize\n",
      "Momentum\n",
      "Stochastic\n",
      "gradient\n",
      "descent\n",
      "Lagrange\n",
      "multipliers\n",
      "Convex optimization\n",
      "& dualityConvex\n",
      "Convex conjugateLinear\n",
      "programming\n",
      "Quadratic\n",
      "programmingChapter 10\n",
      "Dimension reduc.\n",
      "Chapter 11\n",
      "Density estimation\n",
      "Chapter 12\n",
      "Classiﬁcation\n",
      "Since this is a cubic equation, it has in general three solutions when set to\n",
      "zero. In the example, two of them are minimums and one is a maximum\n",
      "(aroundx=\u00001:4). To check whether a stationary point is a minimum\n",
      "or maximum, we need to take the derivative a second time and check\n",
      "whether the second derivative is positive or negative at the stationary\n",
      "point. In our case, the second derivative is\n",
      "d2`(x)\n",
      "dx2= 12x2+ 42x+ 10: (7.3)\n",
      "By substituting our visually estimated values of x=\u00004:5;\u00001:4;0:7;we\n",
      "will observe that as expected the middle point is a maximum\u0010\n",
      "d2`(x)\n",
      "dx2<0\u0011\n",
      "and the other two stationary points are minimums.\n",
      "Note that we have avoided analytically solving for values of xin the\n",
      "previous discussion, although for low-order polynomials such as the pre-\n",
      "ceding we could do so. In general, we are unable to ﬁnd analytic solu-\n",
      "tions, and hence we need to start at some value, say x0=\u00006, and follow\n",
      "the negative gradient. The negative gradient indicates that we should go\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.1 Optimization Using Gradient Descent 227\n",
      "Figure 7.2 Example\n",
      "objective function.\n",
      "Negative gradients\n",
      "are indicated by\n",
      "arrows, and the\n",
      "global minimum is\n",
      "indicated by the\n",
      "dashed blue line.\n",
      "−6−5−4−3−2−1 0 1 2\n",
      "Value of parameter−60−40−200204060Objectivex4+ 7x3+ 5x2−17x+ 3\n",
      "right, but not how far (this is called the step-size). Furthermore, if we According to the\n",
      "Abel–Rufﬁni\n",
      "theorem, there is in\n",
      "general no algebraic\n",
      "solution for\n",
      "polynomials of\n",
      "degree 5 or more\n",
      "(Abel, 1826).had started at the right side (e.g., x0= 0) the negative gradient would\n",
      "have led us to the wrong minimum. Figure 7.2 illustrates the fact that for\n",
      "x>\u00001, the negative gradient points toward the minimum on the right of\n",
      "the ﬁgure, which has a larger objective value.\n",
      "In Section 7.3, we will learn about a class of functions, called convex\n",
      "functions, that do not exhibit this tricky dependency on the starting point\n",
      "of the optimization algorithm. For convex functions, all local minimums\n",
      "are global minimum. It turns out that many machine learning objective For convex functions\n",
      "all local minima are\n",
      "global minimum.functions are designed such that they are convex, and we will see an ex-\n",
      "ample in Chapter 12.\n",
      "The discussion in this chapter so far was about a one-dimensional func-\n",
      "tion, where we are able to visualize the ideas of gradients, descent direc-\n",
      "tions, and optimal values. In the rest of this chapter we develop the same\n",
      "ideas in high dimensions. Unfortunately, we can only visualize the con-\n",
      "cepts in one dimension, but some concepts do not generalize directly to\n",
      "higher dimensions, therefore some care needs to be taken when reading.\n",
      "7.1 Optimization Using Gradient Descent\n",
      "We now consider the problem of solving for the minimum of a real-valued\n",
      "function\n",
      "min\n",
      "xf(x); (7.4)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "228 Continuous Optimization\n",
      "wheref:Rd!Ris an objective function that captures the machine\n",
      "learning problem at hand. We assume that our function fis differentiable,\n",
      "and we are unable to analytically ﬁnd a solution in closed form.\n",
      "Gradient descent is a ﬁrst-order optimization algorithm. To ﬁnd a local\n",
      "minimum of a function using gradient descent, one takes steps propor-\n",
      "tional to the negative of the gradient of the function at the current point.\n",
      "Recall from Section 5.1 that the gradient points in the direction of the We use the\n",
      "convention of row\n",
      "vectors for\n",
      "gradients.steepest ascent. Another useful intuition is to consider the set of lines\n",
      "where the function is at a certain value ( f(x) =cfor some value c2R),\n",
      "which are known as the contour lines. The gradient points in a direction\n",
      "that is orthogonal to the contour lines of the function we wish to optimize.\n",
      "Let us consider multivariate functions. Imagine a surface (described by\n",
      "the function f(x)) with a ball starting at a particular location x0. When\n",
      "the ball is released, it will move downhill in the direction of steepest de-\n",
      "scent. Gradient descent exploits the fact that f(x0)decreases fastest if one\n",
      "moves fromx0in the direction of the negative gradient \u0000((rf)(x0))>of\n",
      "fatx0. We assume in this book that the functions are differentiable, and\n",
      "refer the reader to more general settings in Section 7.4. Then, if\n",
      "((rf)(x0))>(7.5)\n",
      ">0, thenf(x1)6f(x0). Note that we use the\n",
      "transpose for the gradient since otherwise the dimensions will not work\n",
      "out.\n",
      "This observation allows us to deﬁne a simple gradient descent algo-\n",
      "rithm: If we want to ﬁnd a local optimum f(x\u0003)of a function f:Rn!\n",
      "R;x7!f(x), we start with an initial guess x0of the parameters we wish\n",
      "to optimize and then iterate according to\n",
      "i((rf)(xi))>: (7.6)\n",
      "i, the sequence f(x0)>f(x1)>:::converges to\n",
      "a local minimum.\n",
      "Example 7.1\n",
      "Consider a quadratic function in two dimensions\n",
      "f\u0012\u0014x1\n",
      "x2\u0015\u0013\n",
      "=1\n",
      "2\u0014x1\n",
      "x2\u0015>\u00142 1\n",
      "1 20\u0015\u0014x1\n",
      "x2\u0015\n",
      "\u0000\u00145\n",
      "3\u0015>\u0014x1\n",
      "x2\u0015\n",
      "(7.7)\n",
      "with gradient\n",
      "rf\u0012\u0014x1\n",
      "x2\u0015\u0013\n",
      "=\u0014x1\n",
      "x2\u0015>\u00142 1\n",
      "1 20\u0015\n",
      "\u0000\u00145\n",
      "3\u0015>\n",
      ": (7.8)\n",
      "Starting at the initial location x0= [\u00003;\u00001]>, we iteratively apply (7.6)\n",
      "to obtain a sequence of estimates that converge to the minimum value\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.1 Optimization Using Gradient Descent 229\n",
      "Figure 7.3 Gradient\n",
      "descent on a\n",
      "two-dimensional\n",
      "quadratic surface\n",
      "(shown as a\n",
      "heatmap). See\n",
      "Example 7.1 for a\n",
      "description.\n",
      "\u00004\u00002 0 2 4\n",
      "x1\u00002\u00001012x20.0\n",
      "10.0\n",
      "20.030.0\n",
      "40.040.0\n",
      "50.050.0\n",
      "60.0 70.0\n",
      "80.0\u0000150153045607590\n",
      "(illustrated in Figure 7.3). We can see (both from the ﬁgure and by plug-\n",
      "= 0:085) that the negative gradient at x0points\n",
      "north and east, leading to x1= [\u00001:98;1:21]>. Repeating that argument\n",
      "gives usx2= [\u00001:32;\u00000:42]>, and so on.\n",
      "Remark. Gradient descent can be relatively slow close to the minimum:\n",
      "Its asymptotic rate of convergence is inferior to many other methods. Us-\n",
      "ing the ball rolling down the hill analogy, when the surface is a long, thin\n",
      "valley, the problem is poorly conditioned (Trefethen and Bau III, 1997).\n",
      "For poorly conditioned convex problems, gradient descent increasingly\n",
      "“zigzags” as the gradients point nearly orthogonally to the shortest di-\n",
      "rection to a minimum point; see Figure 7.3. }\n",
      "7.1.1 Step-size\n",
      "As mentioned earlier, choosing a good step-size is important in gradient\n",
      "descent. If the step-size is too small, gradient descent can be slow. If the The step-size is also\n",
      "called the learning\n",
      "rate.step-size is chosen too large, gradient descent can overshoot, fail to con-\n",
      "verge, or even diverge. We will discuss the use of momentum in the next\n",
      "section. It is a method that smoothes out erratic behavior of gradient up-\n",
      "dates and dampens oscillations.\n",
      "Adaptive gradient methods rescale the step-size at each iteration, de-\n",
      "pending on local properties of the function. There are two simple heuris-\n",
      "tics (Toussaint, 2012):\n",
      "When the function value increases after a gradient step, the step-size\n",
      "was too large. Undo the step and decrease the step-size.\n",
      "When the function value decreases the step could have been larger. Try\n",
      "to increase the step-size.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "230 Continuous Optimization\n",
      "Although the “undo” step seems to be a waste of resources, using this\n",
      "heuristic guarantees monotonic convergence.\n",
      "Example 7.2 (Solving a Linear Equation System)\n",
      "When we solve linear equations of the form Ax=b, in practice we solve\n",
      "Ax\u0000b=0approximately by ﬁnding x\u0003that minimizes the squared error\n",
      "kAx\u0000bk2= (Ax\u0000b)>(Ax\u0000b) (7.9)\n",
      "if we use the Euclidean norm. The gradient of (7.9) with respect to xis\n",
      "rx= 2(Ax\u0000b)>A: (7.10)\n",
      "We can use this gradient directly in a gradient descent algorithm. How-\n",
      "ever, for this particular special case, it turns out that there is an analytic\n",
      "solution, which can be found by setting the gradient to zero. We will see\n",
      "more on solving squared error problems in Chapter 9.\n",
      "Remark. When applied to the solution of linear systems of equations Ax=\n",
      "b, gradient descent may converge slowly. The speed of convergence of gra-\n",
      "dient descent is dependent on the condition number \u0014=\u001b(A)max\n",
      "\u001b(A)min, which condition number\n",
      "is the ratio of the maximum to the minimum singular value (Section 4.5)\n",
      "ofA. The condition number essentially measures the ratio of the most\n",
      "curved direction versus the least curved direction, which corresponds to\n",
      "our imagery that poorly conditioned problems are long, thin valleys: They\n",
      "are very curved in one direction, but very ﬂat in the other. Instead of di-\n",
      "rectly solving Ax=b, one could instead solve P\u00001(Ax\u0000b) =0, where\n",
      "Pis called the preconditioner . The goal is to design P\u00001such thatP\u00001A preconditioner\n",
      "has a better condition number, but at the same time P\u00001is easy to com-\n",
      "pute. For further information on gradient descent, preconditioning, and\n",
      "convergence we refer to Boyd and Vandenberghe (2004, chapter 9). }\n",
      "7.1.2 Gradient Descent With Momentum\n",
      "As illustrated in Figure 7.3, the convergence of gradient descent may be\n",
      "very slow if the curvature of the optimization surface is such that there\n",
      "are regions that are poorly scaled. The curvature is such that the gradient\n",
      "descent steps hops between the walls of the valley and approaches the\n",
      "optimum in small steps. The proposed tweak to improve convergence is\n",
      "to give gradient descent some memory. Goh (2017) wrote\n",
      "an intuitive blog\n",
      "post on gradient\n",
      "descent with\n",
      "momentum.Gradient descent with momentum (Rumelhart et al., 1986) is a method\n",
      "that introduces an additional term to remember what happened in the\n",
      "previous iteration. This memory dampens oscillations and smoothes out\n",
      "the gradient updates. Continuing the ball analogy, the momentum term\n",
      "emulates the phenomenon of a heavy ball that is reluctant to change di-\n",
      "rections. The idea is to have a gradient update with memory to implement\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.1 Optimization Using Gradient Descent 231\n",
      "a moving average. The momentum-based method remembers the update\n",
      "\u0001xiat each iteration iand determines the next update as a linear combi-\n",
      "nation of the current and previous gradients\n",
      "i((rf)(xi))>+\u000b\u0001xi (7.11)\n",
      "i\u00001((rf)(xi\u00001))>; (7.12)\n",
      "where\u000b2[0;1]. Sometimes we will only know the gradient approxi-\n",
      "mately. In such cases, the momentum term is useful since it averages out\n",
      "different noisy estimates of the gradient. One particularly useful way to\n",
      "obtain an approximate gradient is by using a stochastic approximation,\n",
      "which we discuss next.\n",
      "7.1.3 Stochastic Gradient Descent\n",
      "Computing the gradient can be very time consuming. However, often it is\n",
      "possible to ﬁnd a “cheap” approximation of the gradient. Approximating\n",
      "the gradient is still useful as long as it points in roughly the same direction\n",
      "as the true gradient. stochastic gradient\n",
      "descent Stochastic gradient descent (often shortened as SGD) is a stochastic ap-\n",
      "proximation of the gradient descent method for minimizing an objective\n",
      "function that is written as a sum of differentiable functions. The word\n",
      "stochastic here refers to the fact that we acknowledge that we do not\n",
      "know the gradient precisely, but instead only know a noisy approxima-\n",
      "tion to it. By constraining the probability distribution of the approximate\n",
      "gradients, we can still theoretically guarantee that SGD will converge.\n",
      "In machine learning, given n= 1;:::;N data points, we often consider\n",
      "objective functions that are the sum of the losses Lnincurred by each\n",
      "examplen. In mathematical notation, we have the form\n",
      "L(\u0012) =NX\n",
      "n=1Ln(\u0012); (7.13)\n",
      "where\u0012is the vector of parameters of interest, i.e., we want to ﬁnd \u0012that\n",
      "minimizesL. An example from regression (Chapter 9) is the negative log-\n",
      "likelihood, which is expressed as a sum over log-likelihoods of individual\n",
      "examples so that\n",
      "L(\u0012) =\u0000NX\n",
      "n=1logp(ynjxn;\u0012); (7.14)\n",
      "wherexn2RDare the training inputs, ynare the training targets, and \u0012\n",
      "are the parameters of the regression model.\n",
      "Standard gradient descent, as introduced previously, is a “batch” opti-\n",
      "mization method, i.e., optimization is performed using the full training set\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "232 Continuous Optimization\n",
      "by updating the vector of parameters according to\n",
      "iNXL(\u0012i))>=\u0012i\u0000\n",
      "n=1(rLn(\u0012i))>(7.15)\n",
      "i. Evaluating the sum gradient may re-\n",
      "quire expensive evaluations of the gradients from all individual functions\n",
      "Ln. When the training set is enormous and/or no simple formulas exist,\n",
      "evaluating the sums of gradients becomes very expensive.\n",
      "Consider the termPN\n",
      "n=1(rLn(\u0012i))in (7.15). We can reduce the amount\n",
      "of computation by taking a sum over a smaller set of Ln. In contrast to\n",
      "batch gradient descent, which uses all Lnforn= 1;:::;N , we randomly\n",
      "choose a subset of Lnfor mini-batch gradient descent. In the extreme\n",
      "case, we randomly select only a single Lnto estimate the gradient. The\n",
      "key insight about why taking a subset of data is sensible is to realize that\n",
      "for gradient descent to converge, we only require that the gradient is an\n",
      "unbiased estimate of the true gradient. In fact the termPN\n",
      "n=1(rLn(\u0012i))\n",
      "in (7.15) is an empirical estimate of the expected value (Section 6.4.1) of\n",
      "the gradient. Therefore, any other unbiased empirical estimate of the ex-\n",
      "pected value, for example using any subsample of the data, would sufﬁce\n",
      "for convergence of gradient descent.\n",
      "Remark. When the learning rate decreases at an appropriate rate, and sub-\n",
      "ject to relatively mild assumptions, stochastic gradient descent converges\n",
      "almost surely to local minimum (Bottou, 1998). }\n",
      "Why should one consider using an approximate gradient? A major rea-\n",
      "son is practical implementation constraints, such as the size of central\n",
      "processing unit (CPU)/graphics processing unit (GPU) memory or limits\n",
      "on computational time. We can think of the size of the subset used to esti-\n",
      "mate the gradient in the same way that we thought of the size of a sample\n",
      "when estimating empirical means (Section 6.4.1). Large mini-batch sizes\n",
      "will provide accurate estimates of the gradient, reducing the variance in\n",
      "the parameter update. Furthermore, large mini-batches take advantage of\n",
      "highly optimized matrix operations in vectorized implementations of the\n",
      "cost and gradient. The reduction in variance leads to more stable conver-\n",
      "gence, but each gradient calculation will be more expensive.\n",
      "In contrast, small mini-batches are quick to estimate. If we keep the\n",
      "mini-batch size small, the noise in our gradient estimate will allow us to\n",
      "get out of some bad local optima, which we may otherwise get stuck in.\n",
      "In machine learning, optimization methods are used for training by min-\n",
      "imizing an objective function on the training data, but the overall goal\n",
      "is to improve generalization performance (Chapter 8). Since the goal in\n",
      "machine learning does not necessarily need a precise estimate of the min-\n",
      "imum of the objective function, approximate gradients using mini-batch\n",
      "approaches have been widely used. Stochastic gradient descent is very\n",
      "effective in large-scale machine learning problems (Bottou et al., 2018),\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.2 Constrained Optimization and Lagrange Multipliers 233\n",
      "Figure 7.4\n",
      "Illustration of\n",
      "constrained\n",
      "optimization. The\n",
      "unconstrained\n",
      "problem (indicated\n",
      "by the contour\n",
      "lines) has a\n",
      "minimum on the\n",
      "right side (indicated\n",
      "by the circle). The\n",
      "box constraints\n",
      "(\u000016x61and\n",
      "\u000016y61) require\n",
      "that the optimal\n",
      "solution is within\n",
      "the box, resulting in\n",
      "an optimal value\n",
      "indicated by the\n",
      "star.\n",
      "−3−2−1 0 1 2 3\n",
      "x1−3−2−10123x2\n",
      "such as training deep neural networks on millions of images (Dean et al.,\n",
      "2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih\n",
      "et al., 2015), or training of large-scale Gaussian process models (Hensman\n",
      "et al., 2013; Gal et al., 2014).\n",
      "7.2 Constrained Optimization and Lagrange Multipliers\n",
      "In the previous section, we considered the problem of solving for the min-\n",
      "imum of a function\n",
      "min\n",
      "xf(x); (7.16)\n",
      "wheref:RD!R.\n",
      "In this section, we have additional constraints. That is, for real-valued\n",
      "functionsgi:RD!Rfori= 1;:::;m , we consider the constrained\n",
      "optimization problem (see Figure 7.4 for an illustration)\n",
      "min\n",
      "xf(x) (7.17)\n",
      "subject to gi(x)60for alli= 1;:::;m:\n",
      "It is worth pointing out that the functions fandgicould be non-convex\n",
      "in general, and we will consider the convex case in the next section.\n",
      "One obvious, but not very practical, way of converting the constrained\n",
      "problem (7.17) into an unconstrained one is to use an indicator function\n",
      "J(x) =f(x) +mX\n",
      "i=11(gi(x)); (7.18)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "234 Continuous Optimization\n",
      "where 1(z)is an inﬁnite step function\n",
      "1(z) =(\n",
      "0ifz60\n",
      "1otherwise: (7.19)\n",
      "This gives inﬁnite penalty if the constraint is not satisﬁed, and hence\n",
      "would provide the same solution. However, this inﬁnite step function is\n",
      "equally difﬁcult to optimize. We can overcome this difﬁculty by introduc-\n",
      "ingLagrange multipliers . The idea of Lagrange multipliers is to replace the Lagrange multiplier\n",
      "step function with a linear function.\n",
      "We associate to problem (7.17) the Lagrangian by introducing the La- Lagrangian\n",
      "grange multipliers \u0015i>0corresponding to each inequality constraint re-\n",
      "spectively (Boyd and Vandenberghe, 2004, chapter 4) so that\n",
      "L(x;\u0015) =f(x) +mX\n",
      "i=1\u0015igi(x) (7.20a)\n",
      "=f(x) +\u0015>g(x); (7.20b)\n",
      "where in the last line we have concatenated all constraints gi(x)into a\n",
      "vectorg(x), and all the Lagrange multipliers into a vector \u00152Rm.\n",
      "We now introduce the idea of Lagrangian duality. In general, duality\n",
      "in optimization is the idea of converting an optimization problem in one\n",
      "set of variables x(called the primal variables), into another optimization\n",
      "problem in a different set of variables \u0015(called the dual variables). We\n",
      "introduce two different approaches to duality: In this section, we discuss\n",
      "Lagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality.\n",
      "Deﬁnition 7.1. The problem in (7.17)\n",
      "min\n",
      "xf(x) (7.21)\n",
      "subject to gi(x)60for alli= 1;:::;m\n",
      "is known as the primal problem , corresponding to the primal variables x. primal problem\n",
      "The associated Lagrangian dual problem is given by Lagrangian dual\n",
      "problem\n",
      "max\n",
      "\u00152RmD(\u0015)\n",
      "subject to\u0015>0;(7.22)\n",
      "where\u0015are the dual variables and D(\u0015) = minx2RdL(x;\u0015).\n",
      "Remark. In the discussion of Deﬁnition 7.1, we use two concepts that are\n",
      "also of independent interest (Boyd and Vandenberghe, 2004).\n",
      "First is the minimax inequality , which says that for any function with minimax inequality\n",
      "two arguments '(x;y), the maximin is less than the minimax, i.e.,\n",
      "max\n",
      "ymin\n",
      "x'(x;y)6min\n",
      "xmax\n",
      "y'(x;y): (7.23)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.2 Constrained Optimization and Lagrange Multipliers 235\n",
      "This inequality can be proved by considering the inequality\n",
      "For allx;y min\n",
      "x'(x;y)6max\n",
      "y'(x;y): (7.24)\n",
      "Note that taking the maximum over yof the left-hand side of (7.24) main-\n",
      "tains the inequality since the inequality is true for all y. Similarly, we can\n",
      "take the minimum over xof the right-hand side of (7.24) to obtain (7.23).\n",
      "The second concept is weak duality , which uses (7.23) to show that weak duality\n",
      "primal values are always greater than or equal to dual values. This is de-\n",
      "scribed in more detail in (7.27). }\n",
      "Recall that the difference between J(x)in (7.18) and the Lagrangian\n",
      "in (7.20b) is that we have relaxed the indicator function to a linear func-\n",
      "tion. Therefore, when \u0015>0, the Lagrangian L(x;\u0015)is a lower bound of\n",
      "J(x). Hence, the maximum of L(x;\u0015)with respect to \u0015is\n",
      "J(x) = max\n",
      "\u0015>0L(x;\u0015): (7.25)\n",
      "Recall that the original problem was minimizing J(x),\n",
      "min\n",
      "x2Rdmax\n",
      "\u0015>0L(x;\u0015): (7.26)\n",
      "By the minimax inequality (7.23), it follows that swapping the order of\n",
      "the minimum and maximum results in a smaller value, i.e.,\n",
      "min\n",
      "x2Rdmax\n",
      "\u0015>0L(x;\u0015)>max\n",
      "\u0015>0min\n",
      "x2RdL(x;\u0015): (7.27)\n",
      "This is also known as weak duality . Note that the inner part of the right- weak duality\n",
      "hand side is the dual objective function D(\u0015)and the deﬁnition follows.\n",
      "In contrast to the original optimization problem, which has constraints,\n",
      "minx2RdL(x;\u0015)is an unconstrained optimization problem for a given\n",
      "value of\u0015. If solving minx2RdL(x;\u0015)is easy, then the overall problem is\n",
      "easy to solve. We can see this by observing from (7.20b) that L(x;\u0015)is\n",
      "afﬁne with respect to \u0015. Therefore minx2RdL(x;\u0015)is a pointwise min-\n",
      "imum of afﬁne functions of \u0015, and hence D(\u0015)is concave even though\n",
      "f(\u0001)andgi(\u0001)may be nonconvex. The outer problem, maximization over\n",
      "\u0015, is the maximum of a concave function and can be efﬁciently computed.\n",
      "Assumingf(\u0001)andgi(\u0001)are differentiable, we ﬁnd the Lagrange dual\n",
      "problem by differentiating the Lagrangian with respect to x, setting the\n",
      "differential to zero, and solving for the optimal value. We will discuss two\n",
      "concrete examples in Sections 7.3.1 and 7.3.2, where f(\u0001)andgi(\u0001)are\n",
      "convex.\n",
      "Remark (Equality Constraints) .Consider (7.17) with additional equality\n",
      "constraints\n",
      "min\n",
      "xf(x)\n",
      "subject to gi(x)60for alli= 1;:::;m\n",
      "hj(x) = 0 for allj= 1;:::;n:(7.28)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "236 Continuous Optimization\n",
      "We can model equality constraints by replacing them with two inequality\n",
      "constraints. That is for each equality constraint hj(x) = 0 we equivalently\n",
      "replace it by two constraints hj(x)60andhj(x)>0. It turns out that\n",
      "the resulting Lagrange multipliers are then unconstrained.\n",
      "Therefore, we constrain the Lagrange multipliers corresponding to the\n",
      "inequality constraints in (7.28) to be non-negative, and leave the La-\n",
      "grange multipliers corresponding to the equality constraints unconstrained.\n",
      "}\n",
      "7.3 Convex Optimization\n",
      "We focus our attention of a particularly useful class of optimization prob-\n",
      "lems, where we can guarantee global optimality. When f(\u0001)is a convex\n",
      "function, and when the constraints involving g(\u0001)andh(\u0001)are convex sets,\n",
      "this is called a convex optimization problem . In this setting, we have strong convex optimization\n",
      "problem\n",
      "strong dualityduality : The optimal solution of the dual problem is the same as the opti-\n",
      "mal solution of the primal problem. The distinction between convex func-\n",
      "tions and convex sets are often not strictly presented in machine learning\n",
      "literature, but one can often infer the implied meaning from context.\n",
      "Deﬁnition 7.2. A setCis aconvex set if for anyx;y2Cand for any scalar convex set\n",
      "\u0012with 06\u001261, we have\n",
      "\u0012x+ (1\u0000\u0012)y2C: (7.29)\n",
      "Figure 7.5 Example\n",
      "of a convex set.\n",
      " Convex sets are sets such that a straight line connecting any two ele-\n",
      "ments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex\n",
      "and nonconvex sets, respectively.\n",
      "Figure 7.6 Example\n",
      "of a nonconvex set.\n",
      "Convex functions are functions such that a straight line between any\n",
      "two points of the function lie above the function. Figure 7.2 shows a non-\n",
      "convex function, and Figure 7.3 shows a convex function. Another convex\n",
      "function is shown in Figure 7.7.\n",
      "Deﬁnition 7.3. Let function f:RD!Rbe a function whose domain is a\n",
      "convex set. The function fis aconvex function if for allx;yin the domain\n",
      "convex functionoff, and for any scalar \u0012with 06\u001261, we have\n",
      "f(\u0012x+ (1\u0000\u0012)y)6\u0012f(x) + (1\u0000\u0012)f(y): (7.30)\n",
      "Remark. Aconcave function is the negative of a convex function. }\n",
      "concave functionThe constraints involving g(\u0001)andh(\u0001)in (7.28) truncate functions at a\n",
      "scalar value, resulting in sets. Another relation between convex functions\n",
      "and convex sets is to consider the set obtained by “ﬁlling in” a convex\n",
      "function. A convex function is a bowl-like object, and we imagine pouring\n",
      "water into it to ﬁll it up. This resulting ﬁlled-in set, called the epigraph of epigraph\n",
      "the convex function, is a convex set.\n",
      "If a function f:Rn!Ris differentiable, we can specify convexity in\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.3 Convex Optimization 237\n",
      "Figure 7.7 Example\n",
      "of a convex\n",
      "function.\n",
      "−3−2−1 0 1 2 3\n",
      "x010203040yy= 3x2−5x+ 2\n",
      "terms of its gradient rxf(x)(Section 5.2). A function f(x)is convex if\n",
      "and only if for any two points x;yit holds that\n",
      "f(y)>f(x) +rxf(x)>(y\u0000x): (7.31)\n",
      "If we further know that a function f(x)is twice differentiable, that is, the\n",
      "Hessian (5.147) exists for all values in the domain of x, then the function\n",
      "f(x)is convex if and only if r2\n",
      "xf(x)is positive semideﬁnite (Boyd and\n",
      "Vandenberghe, 2004).\n",
      "Example 7.3\n",
      "The negative entropy f(x) =xlog2xis convex for x>0. A visualization\n",
      "of the function is shown in Figure 7.8, and we can see that the function is\n",
      "convex. To illustrate the previous deﬁnitions of convexity, let us check the\n",
      "calculations for two points x= 2andx= 4. Note that to prove convexity\n",
      "off(x)we would need to check for all points x2R.\n",
      "Recall Deﬁnition 7.3. Consider a point midway between the two points\n",
      "(that is\u0012= 0:5); then the left-hand side is f(0:5\u00012 + 0:5\u00014) = 3 log23\u0019\n",
      "4:75. The right-hand side is 0:5(2 log22) + 0:5(4 log24) = 1 + 4 = 5 . And\n",
      "therefore the deﬁnition is satisﬁed.\n",
      "Sincef(x)is differentiable, we can alternatively use (7.31). Calculating\n",
      "the derivative of f(x), we obtain\n",
      "rx(xlog2x) = 1\u0001log2x+x\u00011\n",
      "xloge2= log2x+1\n",
      "loge2: (7.32)\n",
      "Using the same two test points x= 2 andx= 4, the left-hand side of\n",
      "(7.31) is given by f(4) = 8 . The right-hand side is\n",
      "f(x) +r>\n",
      "x(y\u0000x) =f(2) +rf(2)\u0001(4\u00002) (7.33a)\n",
      "= 2 + (1 +1\n",
      "loge2)\u00012\u00196:9: (7.33b)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "238 Continuous Optimization\n",
      "Figure 7.8 The\n",
      "negative entropy\n",
      "function (which is\n",
      "convex) and its\n",
      "tangent atx= 2.\n",
      "0 1 2 3 4 5\n",
      "x0510f(x)xlog2x\n",
      "tangent atx= 2\n",
      "We can check that a function or set is convex from ﬁrst principles by\n",
      "recalling the deﬁnitions. In practice, we often rely on operations that pre-\n",
      "serve convexity to check that a particular function or set is convex. Al-\n",
      "though the details are vastly different, this is again the idea of closure\n",
      "that we introduced in Chapter 2 for vector spaces.\n",
      "Example 7.4\n",
      "A nonnegative weighted sum of convex functions is convex. Observe that\n",
      "iffis a convex function, and \u000b>0is a nonnegative scalar, then the\n",
      "function\u000bfis convex. We can see this by multiplying \u000bto both sides of the\n",
      "equation in Deﬁnition 7.3, and recalling that multiplying a nonnegative\n",
      "number does not change the inequality.\n",
      "Iff1andf2are convex functions, then we have by the deﬁnition\n",
      "f1(\u0012x+ (1\u0000\u0012)y)6\u0012f1(x) + (1\u0000\u0012)f1(y) (7.34)\n",
      "f2(\u0012x+ (1\u0000\u0012)y)6\u0012f2(x) + (1\u0000\u0012)f2(y): (7.35)\n",
      "Summing up both sides gives us\n",
      "f1(\u0012x+ (1\u0000\u0012)y) +f2(\u0012x+ (1\u0000\u0012)y)\n",
      "6\u0012f1(x) + (1\u0000\u0012)f1(y) +\u0012f2(x) + (1\u0000\u0012)f2(y); (7.36)\n",
      "where the right-hand side can be rearranged to\n",
      "\u0012(f1(x) +f2(x)) + (1\u0000\u0012)(f1(y) +f2(y)); (7.37)\n",
      "completing the proof that the sum of convex functions is convex.\n",
      "Combining the preceding two facts, we see that \u000bf1(x) +\ff2(x)is\n",
      "convex for\u000b;\f>0. This closure property can be extended using a sim-\n",
      "ilar argument for nonnegative weighted sums of more than two convex\n",
      "functions.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.3 Convex Optimization 239\n",
      "Remark. The inequality in (7.30) is sometimes called Jensen’s inequality .Jensen’s inequality\n",
      "In fact, a whole class of inequalities for taking nonnegative weighted sums\n",
      "of convex functions are all called Jensen’s inequality. }\n",
      "In summary, a constrained optimization problem is called a convex opti- convex optimization\n",
      "problem mization problem if\n",
      "min\n",
      "xf(x)\n",
      "subject togi(x)60for alli= 1;:::;m\n",
      "hj(x) = 0 for allj= 1;:::;n;(7.38)\n",
      "where all functions f(x)andgi(x)are convex functions, and all hj(x) =\n",
      "0are convex sets. In the following, we will describe two classes of convex\n",
      "optimization problems that are widely used and well understood.\n",
      "7.3.1 Linear Programming\n",
      "Consider the special case when all the preceding functions are linear, i.e.,\n",
      "min\n",
      "x2Rdc>x (7.39)\n",
      "subject toAx6b;\n",
      "whereA2Rm\u0002dandb2Rm. This is known as a linear program . It hasd linear program\n",
      "Linear programs are\n",
      "one of the most\n",
      "widely used\n",
      "approaches in\n",
      "industry.variables and mlinear constraints. The Lagrangian is given by\n",
      "L(x;\u0015) =c>x+\u0015>(Ax\u0000b); (7.40)\n",
      "where\u00152Rmis the vector of non-negative Lagrange multipliers. Rear-\n",
      "ranging the terms corresponding to xyields\n",
      "L(x;\u0015) = (c+A>\u0015)>x\u0000\u0015>b: (7.41)\n",
      "Taking the derivative of L(x;\u0015)with respect to xand setting it to zero\n",
      "gives us\n",
      "c+A>\u0015=0: (7.42)\n",
      "Therefore, the dual Lagrangian is D(\u0015) =\u0000\u0015>b. Recall we would like\n",
      "to maximize D(\u0015). In addition to the constraint due to the derivative of\n",
      "L(x;\u0015)being zero, we also have the fact that \u0015>0, resulting in the\n",
      "following dual optimization problem It is convention to\n",
      "minimize the primal\n",
      "and maximize the\n",
      "dual.max\n",
      "\u00152Rm\u0000b>\u0015 (7.43)\n",
      "subject toc+A>\u0015=0\n",
      "\u0015>0:\n",
      "This is also a linear program, but with mvariables. We have the choice\n",
      "of solving the primal (7.39) or the dual (7.43) program depending on\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "240 Continuous Optimization\n",
      "whethermordis larger. Recall that dis the number of variables and mis\n",
      "the number of constraints in the primal linear program.\n",
      "Example 7.5 (Linear Program)\n",
      "Consider the linear program\n",
      "min\n",
      "x2R2\u0000\u00145\n",
      "3\u0015>\u0014x1\n",
      "x2\u0015\n",
      "subject to2\n",
      "666642 2\n",
      "2\u00004\n",
      "\u00002 1\n",
      "0\u00001\n",
      "0 13\n",
      "77775\u0014x1\n",
      "x2\u0015\n",
      "62\n",
      "6666433\n",
      "8\n",
      "5\n",
      "\u00001\n",
      "83\n",
      "77775(7.44)\n",
      "with two variables. This program is also shown in Figure 7.9. The objective\n",
      "function is linear, resulting in linear contour lines. The constraint set in\n",
      "standard form is translated into the legend. The optimal value must lie in\n",
      "the shaded (feasible) region, and is indicated by the star.\n",
      "Figure 7.9\n",
      "Illustration of a\n",
      "linear program. The\n",
      "unconstrained\n",
      "problem (indicated\n",
      "by the contour\n",
      "lines) has a\n",
      "minimum on the\n",
      "right side. The\n",
      "optimal value given\n",
      "the constraints are\n",
      "shown by the star.\n",
      "0 2 4 6 8 10 12 14 16\n",
      "x10246810x22x2≤33−2x1\n",
      "4x2≥2x1−8\n",
      "x2≤2x1−5\n",
      "x2≥1\n",
      "x2≤8\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.3 Convex Optimization 241\n",
      "7.3.2 Quadratic Programming\n",
      "Consider the case of a convex quadratic objective function, where the con-\n",
      "straints are afﬁne, i.e.,\n",
      "min\n",
      "x2Rd1\n",
      "2x>Qx+c>x (7.45)\n",
      "subject toAx6b;\n",
      "whereA2Rm\u0002d,b2Rm, andc2Rd. The square symmetric matrix Q2\n",
      "Rd\u0002dis positive deﬁnite, and therefore the objective function is convex.\n",
      "This is known as a quadratic program . Observe that it has dvariables and\n",
      "mlinear constraints.\n",
      "Example 7.6 (Quadratic Program)\n",
      "Consider the quadratic program\n",
      "min\n",
      "x2R21\n",
      "2\u0014x1\n",
      "x2\u0015>\u00142 1\n",
      "1 4\u0015\u0014x1\n",
      "x2\u0015\n",
      "+\u00145\n",
      "3\u0015>\u0014x1\n",
      "x2\u0015\n",
      "(7.46)\n",
      "subject to2\n",
      "6641 0\n",
      "\u00001 0\n",
      "0 1\n",
      "0\u000013\n",
      "775\u0014x1\n",
      "x2\u0015\n",
      "62\n",
      "6641\n",
      "1\n",
      "1\n",
      "13\n",
      "775(7.47)\n",
      "of two variables. The program is also illustrated in Figure 7.4. The objec-\n",
      "tive function is quadratic with a positive semideﬁnite matrix Q, resulting\n",
      "in elliptical contour lines. The optimal value must lie in the shaded (feasi-\n",
      "ble) region, and is indicated by the star.\n",
      "The Lagrangian is given by\n",
      "L(x;\u0015) =1\n",
      "2x>Qx+c>x+\u0015>(Ax\u0000b) (7.48a)\n",
      "=1\n",
      "2x>Qx+ (c+A>\u0015)>x\u0000\u0015>b; (7.48b)\n",
      "where again we have rearranged the terms. Taking the derivative of L(x;\u0015)\n",
      "with respect to xand setting it to zero gives\n",
      "Qx+ (c+A>\u0015) =0: (7.49)\n",
      "Assuming that Qis invertible, we get\n",
      "x=\u0000Q\u00001(c+A>\u0015): (7.50)\n",
      "Substituting (7.50) into the primal Lagrangian L(x;\u0015), we get the dual\n",
      "Lagrangian\n",
      "D(\u0015) =\u00001\n",
      "2(c+A>\u0015)>Q\u00001(c+A>\u0015)\u0000\u0015>b: (7.51)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "242 Continuous Optimization\n",
      "Therefore, the dual optimization problem is given by\n",
      "max\n",
      "\u00152Rm\u00001\n",
      "2(c+A>\u0015)>Q\u00001(c+A>\u0015)\u0000\u0015>b\n",
      "subject to\u0015>0:(7.52)\n",
      "We will see an application of quadratic programming in machine learning\n",
      "in Chapter 12.\n",
      "7.3.3 Legendre–Fenchel Transform and Convex Conjugate\n",
      "Let us revisit the idea of duality from Section 7.2, without considering\n",
      "constraints. One useful fact about a convex set is that it can be equiva-\n",
      "lently described by its supporting hyperplanes. A hyperplane is called a\n",
      "supporting hyperplane of a convex set if it intersects the convex set, and supporting\n",
      "hyperplane the convex set is contained on just one side of it. Recall that we can ﬁll up\n",
      "a convex function to obtain the epigraph, which is a convex set. Therefore,\n",
      "we can also describe convex functions in terms of their supporting hyper-\n",
      "planes. Furthermore, observe that the supporting hyperplane just touches\n",
      "the convex function, and is in fact the tangent to the function at that point.\n",
      "And recall that the tangent of a function f(x)at a given point x0is the\n",
      "evaluation of the gradient of that function at that pointdf(x)\n",
      "dx\f\f\f\n",
      "x=x0. In\n",
      "summary, because convex sets can be equivalently described by their sup-\n",
      "porting hyperplanes, convex functions can be equivalently described by a\n",
      "function of their gradient. The Legendre transform formalizes this concept. Legendre transform\n",
      "Physics students are\n",
      "often introduced to\n",
      "the Legendre\n",
      "transform as\n",
      "relating the\n",
      "Lagrangian and the\n",
      "Hamiltonian in\n",
      "classical mechanics.We begin with the most general deﬁnition, which unfortunately has a\n",
      "counter-intuitive form, and look at special cases to relate the deﬁnition to\n",
      "the intuition described in the preceding paragraph. The Legendre-Fenchel\n",
      "Legendre-Fenchel\n",
      "transformtransform is a transformation (in the sense of a Fourier transform) from\n",
      "a convex differentiable function f(x)to a function that depends on the\n",
      "tangentss(x) =rxf(x). It is worth stressing that this is a transformation\n",
      "of the function f(\u0001)and not the variable xor the function evaluated at x.\n",
      "The Legendre-Fenchel transform is also known as the convex conjugate (for convex conjugate\n",
      "reasons we will see soon) and is closely related to duality (Hiriart-Urruty\n",
      "and Lemar ´echal, 2001, chapter 5).\n",
      "Deﬁnition 7.4. The convex conjugate of a function f:RD!Ris a convex conjugate\n",
      "functionf\u0003deﬁned by\n",
      "f\u0003(s) = sup\n",
      "x2RD(hs;xi\u0000f(x)): (7.53)\n",
      "Note that the preceding convex conjugate deﬁnition does not need the\n",
      "functionfto be convex nor differentiable. In Deﬁnition 7.4, we have used\n",
      "a general inner product (Section 3.2) but in the rest of this section we\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.3 Convex Optimization 243\n",
      "will consider the standard dot product between ﬁnite-dimensional vectors\n",
      "(hs;xi=s>x) to avoid too many technical details.\n",
      "To understand Deﬁnition 7.4 in a geometric fashion, consider a nice This derivation is\n",
      "easiest to\n",
      "understand by\n",
      "drawing the\n",
      "reasoning as it\n",
      "progresses.simple one-dimensional convex and differentiable function, for example\n",
      "f(x) =x2. Note that since we are looking at a one-dimensional problem,\n",
      "hyperplanes reduce to a line. Consider a line y=sx+c. Recall that we are\n",
      "able to describe convex functions by their supporting hyperplanes, so let\n",
      "us try to describe this function f(x)by its supporting lines. Fix the gradi-\n",
      "ent of the line s2Rand for each point (x0;f(x0))on the graph of f, ﬁnd\n",
      "the minimum value of csuch that the line still intersects (x0;f(x0)). Note\n",
      "that the minimum value of cis the place where a line with slope s“just\n",
      "touches” the function f(x) =x2. The line passing through (x0;f(x0))\n",
      "with gradient sis given by\n",
      "y\u0000f(x0) =s(x\u0000x0): (7.54)\n",
      "They-intercept of this line is \u0000sx0+f(x0). The minimum of cfor which\n",
      "y=sx+cintersects with the graph of fis therefore\n",
      "inf\n",
      "x0\u0000sx0+f(x0): (7.55)\n",
      "The preceding convex conjugate is by convention deﬁned to be the nega-\n",
      "tive of this. The reasoning in this paragraph did not rely on the fact that\n",
      "we chose a one-dimensional convex and differentiable function, and holds\n",
      "forf:RD!R, which are nonconvex and non-differentiable.The classical\n",
      "Legendre transform\n",
      "is deﬁned on convex\n",
      "differentiable\n",
      "functions in RD.Remark. Convex differentiable functions such as the example f(x) =x2is\n",
      "a nice special case, where there is no need for the supremum, and there is\n",
      "a one-to-one correspondence between a function and its Legendre trans-\n",
      "form. Let us derive this from ﬁrst principles. For a convex differentiable\n",
      "function, we know that at x0the tangent touches f(x0)so that\n",
      "f(x0) =sx0+c: (7.56)\n",
      "Recall that we want to describe the convex function f(x)in terms of its\n",
      "gradientrxf(x), and thats=rxf(x0). We rearrange to get an expres-\n",
      "sion for\u0000cto obtain\n",
      "\u0000c=sx0\u0000f(x0): (7.57)\n",
      "Note that\u0000cchanges with x0and therefore with s, which is why we can\n",
      "think of it as a function of s, which we call\n",
      "f\u0003(s) :=sx0\u0000f(x0): (7.58)\n",
      "Comparing (7.58) with Deﬁnition 7.4, we see that (7.58) is a special case\n",
      "(without the supremum). }\n",
      "The conjugate function has nice properties; for example, for convex\n",
      "functions, applying the Legendre transform again gets us back to the orig-\n",
      "inal function. In the same way that the slope of f(x)iss, the slope of f\u0003(s)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "244 Continuous Optimization\n",
      "isx. The following two examples show common uses of convex conjugates\n",
      "in machine learning.\n",
      "Example 7.7 (Convex Conjugates)\n",
      "To illustrate the application of convex conjugates, consider the quadratic\n",
      "function\n",
      "f(y) =\u0015\n",
      "2y>K\u00001y (7.59)\n",
      "based on a positive deﬁnite matrix K2Rn\u0002n. We denote the primal\n",
      "variable to be y2Rnand the dual variable to be \u000b2Rn.\n",
      "Applying Deﬁnition 7.4, we obtain the function\n",
      "f\u0003(\u000b) = sup\n",
      "y2Rnhy;\u000bi\u0000\u0015\n",
      "2y>K\u00001y: (7.60)\n",
      "Since the function is differentiable, we can ﬁnd the maximum by taking\n",
      "the derivative and with respect to ysetting it to zero.\n",
      "@\u0002hy;\u000bi\u0000\u0015\n",
      "2y>K\u00001y\u0003\n",
      "@y= (\u000b\u0000\u0015K\u00001y)>(7.61)\n",
      "and hence when the gradient is zero we have y=1\n",
      "\u0015K\u000b. Substituting\n",
      "into (7.60) yields\n",
      "f\u0003(\u000b) =1\n",
      "\u0015\u000b>K\u000b\u0000\u0015\n",
      "2\u00121\n",
      "\u0015K\u000b\u0013>\n",
      "K\u00001\u00121\n",
      "\u0015K\u000b\u0013\n",
      "=1\n",
      "2\u0015\u000b>K\u000b:\n",
      "(7.62)\n",
      "Example 7.8\n",
      "In machine learning, we often use sums of functions; for example, the ob-\n",
      "jective function of the training set includes a sum of the losses for each ex-\n",
      "ample in the training set. In the following, we derive the convex conjugate\n",
      "of a sum of losses `(t), where`:R!R. This also illustrates the appli-\n",
      "cation of the convex conjugate to the vector case. Let L(t) =Pn\n",
      "i=1`i(ti).\n",
      "Then,\n",
      "L\u0003(z) = sup\n",
      "t2Rnhz;ti\u0000nX\n",
      "i=1`i(ti) (7.63a)\n",
      "= sup\n",
      "t2RnnX\n",
      "i=1ziti\u0000`i(ti) de\fnition of dot product (7.63b)\n",
      "=nX\n",
      "i=1sup\n",
      "t2Rnziti\u0000`i(ti) (7.63c)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "7.3 Convex Optimization 245\n",
      "=nX\n",
      "i=1`\u0003\n",
      "i(zi): de\fnition of conjugate (7.63d)\n",
      "Recall that in Section 7.2 we derived a dual optimization problem using\n",
      "Lagrange multipliers. Furthermore, for convex optimization problems we\n",
      "have strong duality, that is the solutions of the primal and dual problem\n",
      "match. The Legendre-Fenchel transform described here also can be used\n",
      "to derive a dual optimization problem. Furthermore, when the function\n",
      "is convex and differentiable, the supremum is unique. To further investi-\n",
      "gate the relation between these two approaches, let us consider a linear\n",
      "equality constrained convex optimization problem.\n",
      "Example 7.9\n",
      "Letf(y)andg(x)be convex functions, and Aa real matrix of appropriate\n",
      "dimensions such that Ax=y. Then\n",
      "min\n",
      "xf(Ax) +g(x) = min\n",
      "Ax=yf(y) +g(x): (7.64)\n",
      "By introducing the Lagrange multiplier ufor the constraints Ax=y,\n",
      "min\n",
      "Ax=yf(y) +g(x) = min\n",
      "x;ymax\n",
      "uf(y) +g(x) + (Ax\u0000y)>u (7.65a)\n",
      "= max\n",
      "umin\n",
      "x;yf(y) +g(x) + (Ax\u0000y)>u;(7.65b)\n",
      "where the last step of swapping max and min is due to the fact that f(y)\n",
      "andg(x)are convex functions. By splitting up the dot product term and\n",
      "collectingxandy,\n",
      "max\n",
      "umin\n",
      "x;yf(y) +g(x) + (Ax\u0000y)>u (7.66a)\n",
      "= max\n",
      "u\u0014\n",
      "min\n",
      "y\u0000y>u+f(y)\u0015\n",
      "+h\n",
      "min\n",
      "x(Ax)>u+g(x)i\n",
      "(7.66b)\n",
      "= max\n",
      "u\u0014\n",
      "min\n",
      "y\u0000y>u+f(y)\u0015\n",
      "+h\n",
      "min\n",
      "xx>A>u+g(x)i\n",
      "(7.66c)\n",
      "Recall the convex conjugate (Deﬁnition 7.4) and the fact that dot prod- For general inner\n",
      "products,A>is\n",
      "replaced by the\n",
      "adjointA\u0003.ucts are symmetric,\n",
      "max\n",
      "u\u0014\n",
      "min\n",
      "y\u0000y>u+f(y)\u0015\n",
      "+h\n",
      "min\n",
      "xx>A>u+g(x)i\n",
      "(7.67a)\n",
      "= max\n",
      "u\u0000f\u0003(u)\u0000g\u0003(\u0000A>u): (7.67b)\n",
      "Therefore, we have shown that\n",
      "min\n",
      "xf(Ax) +g(x) = max\n",
      "u\u0000f\u0003(u)\u0000g\u0003(\u0000A>u): (7.68)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "246 Continuous Optimization\n",
      "The Legendre-Fenchel conjugate turns out to be quite useful for ma-\n",
      "chine learning problems that can be expressed as convex optimization\n",
      "problems. In particular, for convex loss functions that apply independently\n",
      "to each example, the conjugate loss is a convenient way to derive a dual\n",
      "problem.\n",
      "7.4 Further Reading\n",
      "Continuous optimization is an active area of research, and we do not try\n",
      "to provide a comprehensive account of recent advances.\n",
      "From a gradient descent perspective, there are two major weaknesses\n",
      "which each have their own set of literature. The ﬁrst challenge is the fact\n",
      "that gradient descent is a ﬁrst-order algorithm, and does not use infor-\n",
      "mation about the curvature of the surface. When there are long valleys,\n",
      "the gradient points perpendicularly to the direction of interest. The idea\n",
      "of momentum can be generalized to a general class of acceleration meth-\n",
      "ods (Nesterov, 2018). Conjugate gradient methods avoid the issues faced\n",
      "by gradient descent by taking previous directions into account (Shewchuk,\n",
      "1994). Second-order methods such as Newton methods use the Hessian to\n",
      "provide information about the curvature. Many of the choices for choos-\n",
      "ing step-sizes and ideas like momentum arise by considering the curvature\n",
      "of the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton\n",
      "methods such as L-BFGS try to use cheaper computational methods to ap-\n",
      "proximate the Hessian (Nocedal and Wright, 2006). Recently there has\n",
      "been interest in other metrics for computing descent directions, result-\n",
      "ing in approaches such as mirror descent (Beck and Teboulle, 2003) and\n",
      "natural gradient (Toussaint, 2012).\n",
      "The second challenge is to handle non-differentiable functions. Gradi-\n",
      "ent methods are not well deﬁned when there are kinks in the function.\n",
      "In these cases, subgradient methods can be used (Shor, 1985). For fur-\n",
      "ther information and algorithms for optimizing non-differentiable func-\n",
      "tions, we refer to the book by Bertsekas (1999). There is a vast amount\n",
      "of literature on different approaches for numerically solving continuous\n",
      "optimization problems, including algorithms for constrained optimization\n",
      "problems. Good starting points to appreciate this literature are the books\n",
      "by Luenberger (1969) and Bonnans et al. (2006). A recent survey of con-\n",
      "tinuous optimization is provided by Bubeck (2015). Hugo Gonc ¸alves’\n",
      "blog is also a good\n",
      "resource for an\n",
      "easier introduction\n",
      "to Legendre–Fenchel\n",
      "transforms:\n",
      "https://tinyurl.\n",
      "com/ydaal7hjModern applications of machine learning often mean that the size of\n",
      "datasets prohibit the use of batch gradient descent, and hence stochastic\n",
      "gradient descent is the current workhorse of large-scale machine learning\n",
      "methods. Recent surveys of the literature include Hazan (2015) and Bot-\n",
      "tou et al. (2018).\n",
      "For duality and convex optimization, the book by Boyd and Vanden-\n",
      "berghe (2004) includes lectures and slides online. A more mathematical\n",
      "treatment is provided by Bertsekas (2009), and recent book by one of\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Exercises 247\n",
      "the key researchers in the area of optimization is Nesterov (2018). Con-\n",
      "vex optimization is based upon convex analysis, and the reader interested\n",
      "in more foundational results about convex functions is referred to Rock-\n",
      "afellar (1970), Hiriart-Urruty and Lemar ´echal (2001), and Borwein and\n",
      "Lewis (2006). Legendre–Fenchel transforms are also covered in the afore-\n",
      "mentioned books on convex analysis, but a more beginner-friendly pre-\n",
      "sentation is available at Zia et al. (2009). The role of Legendre–Fenchel\n",
      "transforms in the analysis of convex optimization algorithms is surveyed\n",
      "in Polyak (2016).\n",
      "Exercises\n",
      "7.1 Consider the univariate function\n",
      "f(x) =x3+ 6x2\u00003x\u00005:\n",
      "Find its stationary points and indicate whether they are maximum, mini-\n",
      "mum, or saddle points.\n",
      "7.2 Consider the update equation for stochastic gradient descent (Equation (7.15)).\n",
      "Write down the update when we use a mini-batch size of one.\n",
      "7.3 Consider whether the following statements are true or false:\n",
      "a. The intersection of any two convex sets is convex.\n",
      "b. The union of any two convex sets is convex.\n",
      "c. The difference of a convex set Afrom another convex set Bis convex.\n",
      "7.4 Consider whether the following statements are true or false:\n",
      "a. The sum of any two convex functions is convex.\n",
      "b. The difference of any two convex functions is convex.\n",
      "c. The product of any two convex functions is convex.\n",
      "d. The maximum of any two convex functions is convex.\n",
      "7.5 Express the following optimization problem as a standard linear program in\n",
      "matrix notation\n",
      "max\n",
      "x2R2;\u00182Rp>x+\u0018\n",
      "subject to the constraints that \u0018>0,x060andx163.\n",
      "7.6 Consider the linear program illustrated in Figure 7.9,\n",
      "min\n",
      "x2R2\u0000\u0014\n",
      "5\n",
      "3\u0015>\u0014\n",
      "x1\n",
      "x2\u0015\n",
      "subject to2\n",
      "666642 2\n",
      "2\u00004\n",
      "\u00002 1\n",
      "0\u00001\n",
      "0 13\n",
      "77775\u0014\n",
      "x1\n",
      "x2\u0015\n",
      "62\n",
      "6666433\n",
      "8\n",
      "5\n",
      "\u00001\n",
      "83\n",
      "77775\n",
      "Derive the dual linear program using Lagrange duality.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "248 Continuous Optimization\n",
      "7.7 Consider the quadratic program illustrated in Figure 7.4,\n",
      "min\n",
      "x2R21\n",
      "2\u0014\n",
      "x1\n",
      "x2\u0015>\u0014\n",
      "2 1\n",
      "1 4\u0015\u0014\n",
      "x1\n",
      "x2\u0015\n",
      "+\u0014\n",
      "5\n",
      "3\u0015>\u0014\n",
      "x1\n",
      "x2\u0015\n",
      "subject to2\n",
      "6641 0\n",
      "\u00001 0\n",
      "0 1\n",
      "0\u000013\n",
      "775\u0014\n",
      "x1\n",
      "x2\u0015\n",
      "62\n",
      "6641\n",
      "1\n",
      "1\n",
      "13\n",
      "775\n",
      "Derive the dual quadratic program using Lagrange duality.\n",
      "7.8 Consider the following convex optimization problem\n",
      "min\n",
      "w2RD1\n",
      "2w>w\n",
      "subject tow>x>1:\n",
      "Derive the Lagrangian dual by introducing the Lagrange multiplier \u0015.\n",
      "7.9 Consider the negative entropy of x2RD,\n",
      "f(x) =DX\n",
      "d=1xdlogxd:\n",
      "Derive the convex conjugate function f\u0003(s), by assuming the standard dot\n",
      "product.\n",
      "Hint: Take the gradient of an appropriate function and set the gradient to zero.\n",
      "7.10 Consider the function\n",
      "f(x) =1\n",
      "2x>Ax+b>x+c;\n",
      "whereAis strictly positive deﬁnite, which means that it is invertible. Derive\n",
      "the convex conjugate of f(x).\n",
      "Hint: Take the gradient of an appropriate function and set the gradient to zero.\n",
      "7.11 The hinge loss (which is the loss used by the support vector machine) is\n",
      "given by\n",
      "L(\u000b) = maxf0;1\u0000\u000bg;\n",
      "If we are interested in applying gradient methods such as L-BFGS, and do\n",
      "not want to resort to subgradient methods, we need to smooth the kink in\n",
      "the hinge loss. Compute the convex conjugate of the hinge loss L\u0003(\f)where\n",
      "\fis the dual variable. Add a `2proximal term, and compute the conjugate\n",
      "of the resulting function\n",
      "L\u0003(\f) +\n",
      "2\f2;\n",
      "is a given hyperparameter.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "Part II\n",
      "Central Machine Learning Problems\n",
      "249\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "\n",
      "8\n",
      "When Models Meet Data\n",
      "In the ﬁrst part of the book, we introduced the mathematics that form\n",
      "the foundations of many machine learning methods. The hope is that a\n",
      "reader would be able to learn the rudimentary forms of the language of\n",
      "mathematics from the ﬁrst part, which we will now use to describe and\n",
      "discuss machine learning. The second part of the book introduces four\n",
      "pillars of machine learning:\n",
      "Regression (Chapter 9)\n",
      "Dimensionality reduction (Chapter 10)\n",
      "Density estimation (Chapter 11)\n",
      "Classiﬁcation (Chapter 12)\n",
      "The main aim of this part of the book is to illustrate how the mathematical\n",
      "concepts introduced in the ﬁrst part of the book can be used to design\n",
      "machine learning algorithms that can be used to solve tasks within the\n",
      "remit of the four pillars. We do not intend to introduce advanced machine\n",
      "learning concepts, but instead to provide a set of practical methods that\n",
      "allow the reader to apply the knowledge they gained from the ﬁrst part\n",
      "of the book. It also provides a gateway to the wider machine learning\n",
      "literature for readers already familiar with the mathematics.\n",
      "8.1 Data, Models, and Learning\n",
      "It is worth at this point, to pause and consider the problem that a ma-\n",
      "chine learning algorithm is designed to solve. As discussed in Chapter 1,\n",
      "there are three major components of a machine learning system: data,\n",
      "models, and learning. The main question of machine learning is “What do\n",
      "we mean by good models?”. The word model has many subtleties, and we model\n",
      "will revisit it multiple times in this chapter. It is also not entirely obvious\n",
      "how to objectively deﬁne the word “good”. One of the guiding principles\n",
      "of machine learning is that good models should perform well on unseen\n",
      "data. This requires us to deﬁne some performance metrics, such as accu-\n",
      "racy or distance from ground truth, as well as ﬁguring out ways to do well\n",
      "under these performance metrics. This chapter covers a few necessary bits\n",
      "and pieces of mathematical and statistical language that are commonly\n",
      "251\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "252 When Models Meet Data\n",
      "Table 8.1 Example\n",
      "data from a\n",
      "ﬁctitious human\n",
      "resource database\n",
      "that is not in a\n",
      "numerical format.Name Gender Degree Postcode Age Annual salary\n",
      "Aditya M MSc W21BG 36 89563\n",
      "Bob M PhD EC1A1BA 47 123543\n",
      "Chlo´e F BEcon SW1A1BH 26 23989\n",
      "Daisuke M BSc SE207AT 68 138769\n",
      "Elisabeth F MBA SE10AA 33 113888\n",
      "used to talk about machine learning models. By doing so, we brieﬂy out-\n",
      "line the current best practices for training a model such that the resulting\n",
      "predictor does well on data that we have not yet seen.\n",
      "As mentioned in Chapter 1, there are two different senses in which we\n",
      "use the phrase “machine learning algorithm”: training and prediction. We\n",
      "will describe these ideas in this chapter, as well as the idea of selecting\n",
      "among different models. We will introduce the framework of empirical\n",
      "risk minimization in Section 8.2, the principle of maximum likelihood in\n",
      "Section 8.3, and the idea of probabilistic models in Section 8.4. We brieﬂy\n",
      "outline a graphical language for specifying probabilistic models in Sec-\n",
      "tion 8.5 and ﬁnally discuss model selection in Section 8.6. The rest of this\n",
      "section expands upon the three main components of machine learning:\n",
      "data, models and learning.\n",
      "8.1.1 Data as Vectors\n",
      "We assume that our data can be read by a computer, and represented ade-\n",
      "quately in a numerical format. Data is assumed to be tabular (Figure 8.1),\n",
      "where we think of each row of the table as representing a particular in-\n",
      "stance or example, and each column to be a particular feature. In recent Data is assumed to\n",
      "be in a tidy\n",
      "format (Wickham,\n",
      "2014; Codd, 1990).years, machine learning has been applied to many types of data that do not\n",
      "obviously come in the tabular numerical format, for example genomic se-\n",
      "quences, text and image contents of a webpage, and social media graphs.\n",
      "We do not discuss the important and challenging aspects of identifying\n",
      "good features. Many of these aspects depend on domain expertise and re-\n",
      "quire careful engineering, and, in recent years, they have been put under\n",
      "the umbrella of data science (Stray, 2016; Adhikari and DeNero, 2018).\n",
      "Even when we have data in tabular format, there are still choices to be\n",
      "made to obtain a numerical representation. For example, in Table 8.1, the\n",
      "gender column (a categorical variable) may be converted into numbers 0\n",
      "representing “Male” and 1representing “Female”. Alternatively, the gen-\n",
      "der could be represented by numbers \u00001;+1, respectively (as shown in\n",
      "Table 8.2). Furthermore, it is often important to use domain knowledge\n",
      "when constructing the representation, such as knowing that university\n",
      "degrees progress from bachelor’s to master’s to PhD or realizing that the\n",
      "postcode provided is not just a string of characters but actually encodes\n",
      "an area in London. In Table 8.2, we converted the data from Table 8.1\n",
      "to a numerical format, and each postcode is represented as two numbers,\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.1 Data, Models, and Learning 253\n",
      "Table 8.2 Example\n",
      "data from a\n",
      "ﬁctitious human\n",
      "resource database\n",
      "(see Table 8.1),\n",
      "converted to a\n",
      "numerical format.Gender ID Degree Latitude Longitude Age Annual Salary\n",
      "(in degrees) (in degrees) (in thousands)\n",
      "-1 2 51.5073 0.1290 36 89.563\n",
      "-1 3 51.5074 0.1275 47 123.543\n",
      "+1 1 51.5071 0.1278 26 23.989\n",
      "-1 1 51.5075 0.1281 68 138.769\n",
      "+1 2 51.5074 0.1278 33 113.888\n",
      "a latitude and longitude. Even numerical data that could potentially be\n",
      "directly read into a machine learning algorithm should be carefully con-\n",
      "sidered for units, scaling, and constraints. Without additional information,\n",
      "one should shift and scale all columns of the dataset such that they have\n",
      "an empirical mean of 0and an empirical variance of 1. For the purposes\n",
      "of this book, we assume that a domain expert already converted data ap-\n",
      "propriately, i.e., each input xnis aD-dimensional vector of real numbers,\n",
      "which are called features ,attributes , orcovariates . We consider a dataset to feature\n",
      "attribute\n",
      "covariatebe of the form as illustrated by Table 8.2. Observe that we have dropped\n",
      "the Name column of Table 8.1 in the new numerical representation. There\n",
      "are two main reasons why this is desirable: (1) we do not expect the iden-\n",
      "tiﬁer (the Name) to be informative for a machine learning task; and (2)\n",
      "we may wish to anonymize the data to help protect the privacy of the\n",
      "employees.\n",
      "In this part of the book, we will use Nto denote the number of exam-\n",
      "ples in a dataset and index the examples with lowercase n= 1;:::;N .\n",
      "We assume that we are given a set of numerical data, represented as an\n",
      "array of vectors (Table 8.2). Each row is a particular individual xn, often\n",
      "referred to as an example ordata point in machine learning. The subscript example\n",
      "data point nrefers to the fact that this is the nth example out of a total of Nexam-\n",
      "ples in the dataset. Each column represents a particular feature of interest\n",
      "about the example, and we index the features as d= 1;:::;D . Recall that\n",
      "data is represented as vectors, which means that each example (each data\n",
      "point) is aD-dimensional vector. The orientation of the table originates\n",
      "from the database community, but for some machine learning algorithms\n",
      "(e.g., in Chapter 10) it is more convenient to represent examples as col-\n",
      "umn vectors.\n",
      "Let us consider the problem of predicting annual salary from age, based\n",
      "on the data in Table 8.2. This is called a supervised learning problem\n",
      "where we have a labelyn(the salary) associated with each example xn label\n",
      "(the age). The label ynhas various other names, including target, re-\n",
      "sponse variable, and annotation. A dataset is written as a set of example-\n",
      "label pairsf(x1;y1);:::; (xn;yn);:::; (xN;yN)g. The table of examples\n",
      "fx1;:::;xNgis often concatenated, and written as X2RN\u0002D. Fig-\n",
      "ure 8.1 illustrates the dataset consisting of the two rightmost columns\n",
      "of Table 8.2, where x=age andy=salary.\n",
      "We use the concepts introduced in the ﬁrst part of the book to formalize\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "254 When Models Meet Data\n",
      "Figure 8.1 Toy data\n",
      "for linear regression.\n",
      "Training data in\n",
      "(xn;yn)pairs from\n",
      "the rightmost two\n",
      "columns of\n",
      "Table 8.2. We are\n",
      "interested in the\n",
      "salary of a person\n",
      "aged sixty (x= 60 )\n",
      "illustrated as a\n",
      "vertical dashed red\n",
      "line, which is not\n",
      "part of the training\n",
      "data.\n",
      "0 10 20 30 40 50 60 70 80\n",
      "x0255075100125150y\n",
      "?\n",
      "the machine learning problems such as that in the previous paragraph.\n",
      "Representing data as vectors xnallows us to use concepts from linear al-\n",
      "gebra (introduced in Chapter 2). In many machine learning algorithms,\n",
      "we need to additionally be able to compare two vectors. As we will see in\n",
      "Chapters 9 and 12, computing the similarity or distance between two ex-\n",
      "amples allows us to formalize the intuition that examples with similar fea-\n",
      "tures should have similar labels. The comparison of two vectors requires\n",
      "that we construct a geometry (explained in Chapter 3) and allows us to\n",
      "optimize the resulting learning problem using techniques from Chapter 7.\n",
      "Since we have vector representations of data, we can manipulate data to\n",
      "ﬁnd potentially better representations of it. We will discuss ﬁnding good\n",
      "representations in two ways: ﬁnding lower-dimensional approximations\n",
      "of the original feature vector, and using nonlinear higher-dimensional\n",
      "combinations of the original feature vector. In Chapter 10, we will see an\n",
      "example of ﬁnding a low-dimensional approximation of the original data\n",
      "space by ﬁnding the principal components. Finding principal components\n",
      "is closely related to concepts of eigenvalue and singular value decomposi-\n",
      "tion as introduced in Chapter 4. For the high-dimensional representation,\n",
      "we will see an explicit feature map \u001e(\u0001)that allows us to represent in- feature map\n",
      "putsxnusing a higher-dimensional representation \u001e(xn). The main mo-\n",
      "tivation for higher-dimensional representations is that we can construct\n",
      "new features as non-linear combinations of the original features, which in\n",
      "turn may make the learning problem easier. We will discuss the feature\n",
      "map in Section 9.2 and show how this feature map leads to a kernel in kernel\n",
      "Section 12.4. In recent years, deep learning methods (Goodfellow et al.,\n",
      "2016) have shown promise in using the data itself to learn new good fea-\n",
      "tures and have been very successful in areas, such as computer vision,\n",
      "speech recognition, and natural language processing. We will not cover\n",
      "neural networks in this part of the book, but the reader is referred to\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.1 Data, Models, and Learning 255\n",
      "Figure 8.2 Example\n",
      "function (black solid\n",
      "diagonal line) and\n",
      "its prediction at\n",
      "x= 60 , i.e.,\n",
      "f(60) = 100 .\n",
      "0 10 20 30 40 50 60 70 80\n",
      "x0255075100125150y\n",
      "Section 5.6 for the mathematical description of backpropagation, a key\n",
      "concept for training neural networks.\n",
      "8.1.2 Models as Functions\n",
      "Once we have data in an appropriate vector representation, we can get to\n",
      "the business of constructing a predictive function (known as a predictor ). predictor\n",
      "In Chapter 1, we did not yet have the language to be precise about models.\n",
      "Using the concepts from the ﬁrst part of the book, we can now introduce\n",
      "what “model” means. We present two major approaches in this book: a\n",
      "predictor as a function, and a predictor as a probabilistic model. We de-\n",
      "scribe the former here and the latter in the next subsection.\n",
      "A predictor is a function that, when given a particular input example\n",
      "(in our case, a vector of features), produces an output. For now, consider\n",
      "the output to be a single number, i.e., a real-valued scalar output. This can\n",
      "be written as\n",
      "f:RD!R; (8.1)\n",
      "where the input vector xisD-dimensional (has Dfeatures), and the func-\n",
      "tionfthen applied to it (written as f(x)) returns a real number. Fig-\n",
      "ure 8.2 illustrates a possible function that can be used to compute the\n",
      "value of the prediction for input values x.\n",
      "In this book, we do not consider the general case of all functions, which\n",
      "would involve the need for functional analysis. Instead, we consider the\n",
      "special case of linear functions\n",
      "f(x) =\u0012>x+\u00120 (8.2)\n",
      "for unknown \u0012and\u00120. This restriction means that the contents of Chap-\n",
      "ters 2 and 3 sufﬁce for precisely stating the notion of a predictor for\n",
      "the non-probabilistic (in contrast to the probabilistic view described next)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "256 When Models Meet Data\n",
      "Figure 8.3 Example\n",
      "function (black solid\n",
      "diagonal line) and\n",
      "its predictive\n",
      "uncertainty at\n",
      "x= 60 (drawn as a\n",
      "Gaussian).\n",
      "0 10 20 30 40 50 60 70 80\n",
      "x0255075100125150y\n",
      "view of machine learning. Linear functions strike a good balance between\n",
      "the generality of the problems that can be solved and the amount of back-\n",
      "ground mathematics that is needed.\n",
      "8.1.3 Models as Probability Distributions\n",
      "We often consider data to be noisy observations of some true underlying\n",
      "effect, and hope that by applying machine learning we can identify the\n",
      "signal from the noise. This requires us to have a language for quantify-\n",
      "ing the effect of noise. We often would also like to have predictors that\n",
      "express some sort of uncertainty, e.g., to quantify the conﬁdence we have\n",
      "about the value of the prediction for a particular test data point. As we\n",
      "have seen in Chapter 6, probability theory provides a language for quan-\n",
      "tifying uncertainty. Figure 8.3 illustrates the predictive uncertainty of the\n",
      "function as a Gaussian distribution.\n",
      "Instead of considering a predictor as a single function, we could con-\n",
      "sider predictors to be probabilistic models, i.e., models describing the dis-\n",
      "tribution of possible functions. We limit ourselves in this book to the spe-\n",
      "cial case of distributions with ﬁnite-dimensional parameters, which allows\n",
      "us to describe probabilistic models without needing stochastic processes\n",
      "and random measures. For this special case, we can think about prob-\n",
      "abilistic models as multivariate probability distributions, which already\n",
      "allow for a rich class of models.\n",
      "We will introduce how to use concepts from probability (Chapter 6) to\n",
      "deﬁne machine learning models in Section 8.4, and introduce a graphical\n",
      "language for describing probabilistic models in a compact way in Sec-\n",
      "tion 8.5.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.1 Data, Models, and Learning 257\n",
      "8.1.4 Learning is Finding Parameters\n",
      "The goal of learning is to ﬁnd a model and its corresponding parame-\n",
      "ters such that the resulting predictor will perform well on unseen data.\n",
      "There are conceptually three distinct algorithmic phases when discussing\n",
      "machine learning algorithms:\n",
      "1. Prediction or inference\n",
      "2. Training or parameter estimation\n",
      "3. Hyperparameter tuning or model selection\n",
      "The prediction phase is when we use a trained predictor on previously un-\n",
      "seen test data. In other words, the parameters and model choice is already\n",
      "ﬁxed and the predictor is applied to new vectors representing new input\n",
      "data points. As outlined in Chapter 1 and the previous subsection, we will\n",
      "consider two schools of machine learning in this book, corresponding to\n",
      "whether the predictor is a function or a probabilistic model. When we\n",
      "have a probabilistic model (discussed further in Section 8.4) the predic-\n",
      "tion phase is called inference.\n",
      "Remark. Unfortunately, there is no agreed upon naming for the different\n",
      "algorithmic phases. The word “inference” is sometimes also used to mean\n",
      "parameter estimation of a probabilistic model, and less often may be also\n",
      "used to mean prediction for non-probabilistic models. }\n",
      "The training or parameter estimation phase is when we adjust our pre-\n",
      "dictive model based on training data. We would like to ﬁnd good predic-\n",
      "tors given training data, and there are two main strategies for doing so:\n",
      "ﬁnding the best predictor based on some measure of quality (sometimes\n",
      "called ﬁnding a point estimate), or using Bayesian inference. Finding a\n",
      "point estimate can be applied to both types of predictors, but Bayesian\n",
      "inference requires probabilistic models.\n",
      "For the non-probabilistic model, we follow the principle of empirical risk empirical risk\n",
      "minimization minimization , which we describe in Section 8.2. Empirical risk minimiza-\n",
      "tion directly provides an optimization problem for ﬁnding good parame-\n",
      "ters. With a statistical model, the principle of maximum likelihood is used maximum likelihood\n",
      "to ﬁnd a good set of parameters (Section 8.3). We can additionally model\n",
      "the uncertainty of parameters using a probabilistic model, which we will\n",
      "look at in more detail in Section 8.4.\n",
      "We use numerical methods to ﬁnd good parameters that “ﬁt” the data,\n",
      "and most training methods can be thought of as hill-climbing approaches\n",
      "to ﬁnd the maximum of an objective, for example the maximum of a likeli-\n",
      "hood. To apply hill-climbing approaches we use the gradients described in The convention in\n",
      "optimization is to\n",
      "minimize objectives.\n",
      "Hence, there is often\n",
      "an extra minus sign\n",
      "in machine learning\n",
      "objectives.Chapter 5 and implement numerical optimization approaches from Chap-\n",
      "ter 7.\n",
      "As mentioned in Chapter 1, we are interested in learning a model based\n",
      "on data such that it performs well on future data. It is not enough for\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "258 When Models Meet Data\n",
      "the model to only ﬁt the training data well, the predictor needs to per-\n",
      "form well on unseen data. We simulate the behavior of our predictor on\n",
      "future unseen data using cross-validation (Section 8.2.4). As we will see cross-validation\n",
      "in this chapter, to achieve the goal of performing well on unseen data,\n",
      "we will need to balance between ﬁtting well on training data and ﬁnding\n",
      "“simple” explanations of the phenomenon. This trade-off is achieved us-\n",
      "ing regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In\n",
      "philosophy, this is considered to be neither induction nor deduction, but\n",
      "is called abduction . According to the Stanford Encyclopedia of Philosophy , abduction\n",
      "abduction is the process of inference to the best explanation (Douven,\n",
      "2017). A good movie title is\n",
      "“AI abduction”. We often need to make high-level modeling decisions about the struc-\n",
      "ture of the predictor, such as the number of components to use or the\n",
      "class of probability distributions to consider. The choice of the number of\n",
      "components is an example of a hyperparameter , and this choice can af- hyperparameter\n",
      "fect the performance of the model signiﬁcantly. The problem of choosing\n",
      "among different models is called model selection , which we describe in model selection\n",
      "Section 8.6. For non-probabilistic models, model selection is often done\n",
      "using nested cross-validation , which is described in Section 8.6.1. We also nested\n",
      "cross-validation use model selection to choose hyperparameters of our model.\n",
      "Remark. The distinction between parameters and hyperparameters is some-\n",
      "what arbitrary, and is mostly driven by the distinction between what can\n",
      "be numerically optimized versus what needs to use search techniques.\n",
      "Another way to consider the distinction is to consider parameters as the\n",
      "explicit parameters of a probabilistic model, and to consider hyperparam-\n",
      "eters (higher-level parameters) as parameters that control the distribution\n",
      "of these explicit parameters. }\n",
      "In the following sections, we will look at three ﬂavors of machine learn-\n",
      "ing: empirical risk minimization (Section 8.2), the principle of maximum\n",
      "likelihood (Section 8.3), and probabilistic modeling (Section 8.4).\n",
      "8.2 Empirical Risk Minimization\n",
      "After having all the mathematics under our belt, we are now in a posi-\n",
      "tion to introduce what it means to learn. The “learning” part of machine\n",
      "learning boils down to estimating parameters based on training data.\n",
      "In this section, we consider the case of a predictor that is a function,\n",
      "and consider the case of probabilistic models in Section 8.3. We describe\n",
      "the idea of empirical risk minimization, which was originally popularized\n",
      "by the proposal of the support vector machine (described in Chapter 12).\n",
      "However, its general principles are widely applicable and allow us to ask\n",
      "the question of what is learning without explicitly constructing probabilis-\n",
      "tic models. There are four main design choices, which we will cover in\n",
      "detail in the following subsections:\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.2 Empirical Risk Minimization 259\n",
      "Section 8.2.1 What is the set of functions we allow the predictor to take?\n",
      "Section 8.2.2 How do we measure how well the predictor performs on\n",
      "the training data?\n",
      "Section 8.2.3 How do we construct predictors from only training data\n",
      "that performs well on unseen test data?\n",
      "Section 8.2.4 What is the procedure for searching over the space of mod-\n",
      "els?\n",
      "8.2.1 Hypothesis Class of Functions\n",
      "Assume we are given Nexamplesxn2RDand corresponding scalar la-\n",
      "belsyn2R. We consider the supervised learning setting, where we obtain\n",
      "pairs (x1;y1);:::; (xN;yN). Given this data, we would like to estimate a\n",
      "predictorf(\u0001;\u0012) :RD!R, parametrized by \u0012. We hope to be able to ﬁnd\n",
      "a good parameter \u0012\u0003such that we ﬁt the data well, that is,\n",
      "f(xn;\u0012\u0003)\u0019ynfor alln= 1;:::;N: (8.3)\n",
      "In this section, we use the notation ^yn=f(xn;\u0012\u0003)to represent the output\n",
      "of the predictor.\n",
      "Remark. For ease of presentation, we will describe empirical risk mini-\n",
      "mization in terms of supervised learning (where we have labels). This\n",
      "simpliﬁes the deﬁnition of the hypothesis class and the loss function. It\n",
      "is also common in machine learning to choose a parametrized class of\n",
      "functions, for example afﬁne functions. }\n",
      "Example 8.1\n",
      "We introduce the problem of ordinary least-squares regression to illustrate\n",
      "empirical risk minimization. A more comprehensive account of regression\n",
      "is given in Chapter 9. When the label ynis real-valued, a popular choice\n",
      "of function class for predictors is the set of afﬁne functions. We choose a Afﬁne functions are\n",
      "often referred to as\n",
      "linear functions in\n",
      "machine learning.more compact notation for an afﬁne function by concatenating an addi-\n",
      "tional unit feature x(0)= 1toxn, i.e.,xn= [1;x(1)\n",
      "n;x(2)\n",
      "n;:::;x(D)\n",
      "n]>. The\n",
      "parameter vector is correspondingly \u0012= [\u00120;\u00121;\u00122;:::;\u0012D]>, allowing us\n",
      "to write the predictor as a linear function\n",
      "f(xn;\u0012) =\u0012>xn: (8.4)\n",
      "This linear predictor is equivalent to the afﬁne model\n",
      "f(xn;\u0012) =\u00120+DX\n",
      "d=1\u0012dx(d)\n",
      "n: (8.5)\n",
      "The predictor takes the vector of features representing a single example\n",
      "xnas input and produces a real-valued output, i.e., f:RD+1!R. The\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "260 When Models Meet Data\n",
      "previous ﬁgures in this chapter had a straight line as a predictor, which\n",
      "means that we have assumed an afﬁne function.\n",
      "Instead of a linear function, we may wish to consider non-linear func-\n",
      "tions as predictors. Recent advances in neural networks allow for efﬁcient\n",
      "computation of more complex non-linear function classes.\n",
      "Given the class of functions, we want to search for a good predictor.\n",
      "We now move on to the second ingredient of empirical risk minimization:\n",
      "how to measure how well the predictor ﬁts the training data.\n",
      "8.2.2 Loss Function for Training\n",
      "Consider the label ynfor a particular example; and the corresponding pre-\n",
      "diction ^ynthat we make based on xn. To deﬁne what it means to ﬁt the\n",
      "data well, we need to specify a loss function `(yn;^yn)that takes the ground loss function\n",
      "truth label and the prediction as input and produces a non-negative num-\n",
      "ber (referred to as the loss) representing how much error we have made\n",
      "on this particular prediction. Our goal for ﬁnding a good parameter vector The expression\n",
      "“error” is often used\n",
      "to mean loss.\u0012\u0003is to minimize the average loss on the set of Ntraining examples.\n",
      "One assumption that is commonly made in machine learning is that\n",
      "the set of examples (x1;y1);:::; (xN;yN)isindependent and identically independent and\n",
      "identically\n",
      "distributeddistributed . The word independent (Section 6.4.5) means that two data\n",
      "points (xi;yi)and(xj;yj)do not statistically depend on each other, mean-\n",
      "ing that the empirical mean is a good estimate of the population mean\n",
      "(Section 6.4.1). This implies that we can use the empirical mean of the\n",
      "loss on the training data. For a given training setf(x1;y1);:::; (xN;yN)g, training set\n",
      "we introduce the notation of an example matrix X:= [x1;:::;xN]>2\n",
      "RN\u0002Dand a label vector y:= [y1;:::;yN]>2RN. Using this matrix\n",
      "notation the average loss is given by\n",
      "Remp(f;X;y) =1\n",
      "NNX\n",
      "n=1`(yn;^yn); (8.6)\n",
      "where ^yn=f(xn;\u0012). Equation (8.6) is called the empirical risk and de- empirical risk\n",
      "pends on three arguments, the predictor fand the dataX;y. This general\n",
      "strategy for learning is called empirical risk minimization . empirical risk\n",
      "minimization\n",
      "Example 8.2 (Least-Squares Loss)\n",
      "Continuing the example of least-squares regression, we specify that we\n",
      "measure the cost of making an error during training using the squared\n",
      "loss`(yn;^yn) = (yn\u0000^yn)2. We wish to minimize the empirical risk (8.6),\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.2 Empirical Risk Minimization 261\n",
      "which is the average of the losses over the data\n",
      "min\n",
      "\u00122RD1\n",
      "NNX\n",
      "n=1(yn\u0000f(xn;\u0012))2; (8.7)\n",
      "where we substituted the predictor ^yn=f(xn;\u0012). By using our choice of\n",
      "a linear predictor f(xn;\u0012) =\u0012>xn, we obtain the optimization problem\n",
      "min\n",
      "\u00122RD1\n",
      "NNX\n",
      "n=1(yn\u0000\u0012>xn)2: (8.8)\n",
      "This equation can be equivalently expressed in matrix form\n",
      "min\n",
      "\u00122RD1\n",
      "Nky\u0000X\u0012k2: (8.9)\n",
      "This is known as the least-squares problem . There exists a closed-form an- least-squares\n",
      "problemalytic solution for this by solving the normal equations, which we will\n",
      "discuss in Section 9.2.\n",
      "We are not interested in a predictor that only performs well on the\n",
      "training data. Instead, we seek a predictor that performs well (has low\n",
      "risk) on unseen test data. More formally, we are interested in ﬁnding a\n",
      "predictorf(with parameters ﬁxed) that minimizes the expected risk expected risk\n",
      "Rtrue(f) =Ex;y[`(y;f(x))]; (8.10)\n",
      "whereyis the label and f(x)is the prediction based on the example x.\n",
      "The notation Rtrue(f)indicates that this is the true risk if we had access to\n",
      "an inﬁnite amount of data. The expectation is over the (inﬁnite) set of all Another phrase\n",
      "commonly used for\n",
      "expected risk is\n",
      "“population risk”.possible data and labels. There are two practical questions that arise from\n",
      "our desire to minimize expected risk, which we address in the following\n",
      "two subsections:\n",
      "How should we change our training procedure to generalize well?\n",
      "How do we estimate expected risk from (ﬁnite) data?\n",
      "Remark. Many machine learning tasks are speciﬁed with an associated\n",
      "performance measure, e.g., accuracy of prediction or root mean squared\n",
      "error. The performance measure could be more complex, be cost sensitive,\n",
      "and capture details about the particular application. In principle, the de-\n",
      "sign of the loss function for empirical risk minimization should correspond\n",
      "directly to the performance measure speciﬁed by the machine learning\n",
      "task. In practice, there is often a mismatch between the design of the loss\n",
      "function and the performance measure. This could be due to issues such\n",
      "as ease of implementation or efﬁciency of optimization. }\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "262 When Models Meet Data\n",
      "8.2.3 Regularization to Reduce Overﬁtting\n",
      "This section describes an addition to empirical risk minimization that al-\n",
      "lows it to generalize well (approximately minimizing expected risk). Re-\n",
      "call that the aim of training a machine learning predictor is so that we can\n",
      "perform well on unseen data, i.e., the predictor generalizes well. We sim-\n",
      "ulate this unseen data by holding out a proportion of the whole dataset.\n",
      "This hold out set is referred to as the test set . Given a sufﬁciently rich class test set\n",
      "Even knowing only\n",
      "the performance of\n",
      "the predictor on the\n",
      "test set leaks\n",
      "information (Blum\n",
      "and Hardt, 2015).of functions for the predictor f, we can essentially memorize the training\n",
      "data to obtain zero empirical risk. While this is great to minimize the loss\n",
      "(and therefore the risk) on the training data, we would not expect the\n",
      "predictor to generalize well to unseen data. In practice, we have only a\n",
      "ﬁnite set of data, and hence we split our data into a training and a test\n",
      "set. The training set is used to ﬁt the model, and the test set (not seen\n",
      "by the machine learning algorithm during training) is used to evaluate\n",
      "generalization performance. It is important for the user to not cycle back\n",
      "to a new round of training after having observed the test set. We use the\n",
      "subscripts trainand testto denote the training and test sets, respectively.\n",
      "We will revisit this idea of using a ﬁnite dataset to evaluate expected risk\n",
      "in Section 8.2.4.\n",
      "It turns out that empirical risk minimization can lead to overﬁtting , i.e., overﬁtting\n",
      "the predictor ﬁts too closely to the training data and does not general-\n",
      "ize well to new data (Mitchell, 1997). This general phenomenon of hav-\n",
      "ing very small average loss on the training set but large average loss on\n",
      "the test set tends to occur when we have little data and a complex hy-\n",
      "pothesis class. For a particular predictor f(with parameters ﬁxed), the\n",
      "phenomenon of overﬁtting occurs when the risk estimate from the train-\n",
      "ing data Remp(f;Xtrain;ytrain)underestimates the expected risk Rtrue(f).\n",
      "Since we estimate the expected risk Rtrue(f)by using the empirical risk\n",
      "on the test set Remp(f;Xtest;ytest)if the test risk is much larger than\n",
      "the training risk, this is an indication of overﬁtting. We revisit the idea of\n",
      "overﬁtting in Section 8.3.3.\n",
      "Therefore, we need to somehow bias the search for the minimizer of\n",
      "empirical risk by introducing a penalty term, which makes it harder for\n",
      "the optimizer to return an overly ﬂexible predictor. In machine learning,\n",
      "the penalty term is referred to as regularization . Regularization is a way regularization\n",
      "to compromise between accurate solution of empirical risk minimization\n",
      "and the size or complexity of the solution.\n",
      "Example 8.3 (Regularized Least Squares)\n",
      "Regularization is an approach that discourages complex or extreme solu-\n",
      "tions to an optimization problem. The simplest regularization strategy is\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.2 Empirical Risk Minimization 263\n",
      "to replace the least-squares problem\n",
      "min\n",
      "\u00121\n",
      "Nky\u0000X\u0012k2: (8.11)\n",
      "in the previous example with the “regularized” problem by adding a\n",
      "penalty term involving only \u0012:\n",
      "min\n",
      "\u00121\n",
      "Nky\u0000X\u0012k2+\u0015k\u0012k2: (8.12)\n",
      "The additional term k\u0012k2is called the regularizer , and the parameter regularizer\n",
      "\u0015is the regularization parameter . The regularization parameter trades regularization\n",
      "parameteroff minimizing the loss on the training set and the magnitude of the pa-\n",
      "rameters\u0012. It often happens that the magnitude of the parameter values\n",
      "becomes relatively large if we run into overﬁtting (Bishop, 2006).\n",
      "The regularization term is sometimes called the penalty term , which bi- penalty term\n",
      "ases the vector \u0012to be closer to the origin. The idea of regularization also\n",
      "appears in probabilistic models as the prior probability of the parameters.\n",
      "Recall from Section 6.6 that for the posterior distribution to be of the same\n",
      "form as the prior distribution, the prior and the likelihood need to be con-\n",
      "jugate. We will revisit this idea in Section 8.3.2. We will see in Chapter 12\n",
      "that the idea of the regularizer is equivalent to the idea of a large margin.\n",
      "8.2.4 Cross-Validation to Assess the Generalization Performance\n",
      "We mentioned in the previous section that we measure the generalization\n",
      "error by estimating it by applying the predictor on test data. This data is\n",
      "also sometimes referred to as the validation set . The validation set is a sub- validation set\n",
      "set of the available training data that we keep aside. A practical issue with\n",
      "this approach is that the amount of data is limited, and ideally we would\n",
      "use as much of the data available to train the model. This would require\n",
      "us to keep our validation set Vsmall, which then would lead to a noisy\n",
      "estimate (with high variance) of the predictive performance. One solu-\n",
      "tion to these contradictory objectives (large training set, large validation\n",
      "set) is to use cross-validation .K-fold cross-validation effectively partitions cross-validation\n",
      "the data into Kchunks,K\u00001of which form the training set R, and\n",
      "the last chunk serves as the validation set V(similar to the idea outlined\n",
      "previously). Cross-validation iterates through (ideally) all combinations\n",
      "of assignments of chunks to RandV; see Figure 8.4. This procedure is\n",
      "repeated for all Kchoices for the validation set, and the performance of\n",
      "the model from the Kruns is averaged.\n",
      "We partition our dataset into two sets D=R[V , such that they do not\n",
      "overlap (R\\V =;), whereVis the validation set, and train our model\n",
      "onR. After training, we assess the performance of the predictor fon the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "264 When Models Meet Data\n",
      "Figure 8.4K-fold\n",
      "cross-validation.\n",
      "The dataset is\n",
      "divided into K= 5\n",
      "chunks,K\u00001of\n",
      "which serve as the\n",
      "training set (blue)\n",
      "and one as the\n",
      "validation set\n",
      "(orange hatch).Training\n",
      "Validation\n",
      "validation setV(e.g., by computing root mean square error (RMSE) of\n",
      "the trained model on the validation set). More precisely, for each partition\n",
      "kthe training dataR(k)produces a predictor f(k), which is then applied\n",
      "to validation setV(k)to compute the empirical risk R(f(k);V(k)). We cycle\n",
      "through all possible partitionings of validation and training sets and com-\n",
      "pute the average generalization error of the predictor. Cross-validation\n",
      "approximates the expected generalization error\n",
      "EV[R(f;V)]\u00191\n",
      "KKX\n",
      "k=1R(f(k);V(k)); (8.13)\n",
      "whereR(f(k);V(k))is the risk (e.g., RMSE) on the validation set V(k)for\n",
      "predictorf(k). The approximation has two sources: ﬁrst, due to the ﬁnite\n",
      "training set, which results in not the best possible f(k); and second, due to\n",
      "the ﬁnite validation set, which results in an inaccurate estimation of the\n",
      "riskR(f(k);V(k)). A potential disadvantage of K-fold cross-validation is\n",
      "the computational cost of training the model Ktimes, which can be bur-\n",
      "densome if the training cost is computationally expensive. In practice, it\n",
      "is often not sufﬁcient to look at the direct parameters alone. For example,\n",
      "we need to explore multiple complexity parameters (e.g., multiple regu-\n",
      "larization parameters), which may not be direct parameters of the model.\n",
      "Evaluating the quality of the model, depending on these hyperparameters,\n",
      "may result in a number of training runs that is exponential in the number\n",
      "of model parameters. One can use nested cross-validation (Section 8.6.1)\n",
      "to search for good hyperparameters.\n",
      "However, cross-validation is an embarrassingly parallel problem, i.e., lit- embarrassingly\n",
      "parallel tle effort is needed to separate the problem into a number of parallel\n",
      "tasks. Given sufﬁcient computing resources (e.g., cloud computing, server\n",
      "farms), cross-validation does not require longer than a single performance\n",
      "assessment.\n",
      "In this section, we saw that empirical risk minimization is based on the\n",
      "following concepts: the hypothesis class of functions, the loss function and\n",
      "regularization. In Section 8.3, we will see the effect of using a probability\n",
      "distribution to replace the idea of loss functions and regularization.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.3 Parameter Estimation 265\n",
      "8.2.5 Further Reading\n",
      "Due to the fact that the original development of empirical risk minimiza-\n",
      "tion (Vapnik, 1998) was couched in heavily theoretical language, many\n",
      "of the subsequent developments have been theoretical. The area of study\n",
      "is called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statistical learning\n",
      "theory Hastie et al., 2001; von Luxburg and Sch ¨olkopf, 2011). A recent machine\n",
      "learning textbook that builds on the theoretical foundations and develops\n",
      "efﬁcient learning algorithms is Shalev-Shwartz and Ben-David (2014).\n",
      "The concept of regularization has its roots in the solution of ill-posed in-\n",
      "verse problems (Neumaier, 1998). The approach presented here is called\n",
      "Tikhonov regularization , and there is a closely related constrained version Tikhonov\n",
      "regularization called Ivanov regularization. Tikhonov regularization has deep relation-\n",
      "ships to the bias-variance trade-off and feature selection (B ¨uhlmann and\n",
      "Van De Geer, 2011). An alternative to cross-validation is bootstrap and\n",
      "jackknife (Efron and Tibshirani, 1993; Davidson and Hinkley, 1997; Hall,\n",
      "1992).\n",
      "Thinking about empirical risk minimization (Section 8.2) as “probabil-\n",
      "ity free” is incorrect. There is an underlying unknown probability distri-\n",
      "butionp(x;y)that governs the data generation. However, the approach\n",
      "of empirical risk minimization is agnostic to that choice of distribution.\n",
      "This is in contrast to standard statistical approaches that explicitly re-\n",
      "quire the knowledge of p(x;y). Furthermore, since the distribution is a\n",
      "joint distribution on both examples xand labelsy, the labels can be non-\n",
      "deterministic. In contrast to standard statistics we do not need to specify\n",
      "the noise distribution for the labels y.\n",
      "8.3 Parameter Estimation\n",
      "In Section 8.2, we did not explicitly model our problem using probability\n",
      "distributions. In this section, we will see how to use probability distribu-\n",
      "tions to model our uncertainty due to the observation process and our\n",
      "uncertainty in the parameters of our predictors. In Section 8.3.1, we in-\n",
      "troduce the likelihood, which is analogous to the concept of loss functions\n",
      "(Section 8.2.2) in empirical risk minimization. The concept of priors (Sec-\n",
      "tion 8.3.2) is analogous to the concept of regularization (Section 8.2.3).\n",
      "8.3.1 Maximum Likelihood Estimation\n",
      "The idea behind maximum likelihood estimation (MLE) is to deﬁne a func- maximum likelihood\n",
      "estimation tion of the parameters that enables us to ﬁnd a model that ﬁts the data\n",
      "well. The estimation problem is focused on the likelihood function, or likelihood\n",
      "more precisely its negative logarithm. For data represented by a random\n",
      "variablexand for a family of probability densities p(xj\u0012)parametrized\n",
      "by\u0012, the negative log-likelihood is given by negative\n",
      "log-likelihood\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "266 When Models Meet Data\n",
      "Lx(\u0012) =\u0000logp(xj\u0012): (8.14)\n",
      "The notationLx(\u0012)emphasizes the fact that the parameter \u0012is varying\n",
      "and the dataxis ﬁxed. We very often drop the reference to xwhen writing\n",
      "the negative log-likelihood, as it is really a function of \u0012, and write it as\n",
      "L(\u0012)when the random variable representing the uncertainty in the data\n",
      "is clear from the context.\n",
      "Let us interpret what the probability density p(xj\u0012)is modeling for a\n",
      "ﬁxed value of \u0012. It is a distribution that models the uncertainty of the data\n",
      "for a given parameter setting. For a given dataset x, the likelihood allows\n",
      "us to express preferences about different settings of the parameters \u0012, and\n",
      "we can choose the setting that more “likely” has generated the data.\n",
      "In a complementary view, if we consider the data to be ﬁxed (because\n",
      "it has been observed), and we vary the parameters \u0012, what doesL(\u0012)tell\n",
      "us? It tells us how likely a particular setting of \u0012is for the observations x.\n",
      "Based on this second view, the maximum likelihood estimator gives us the\n",
      "most likely parameter \u0012for the set of data.\n",
      "We consider the supervised learning setting, where we obtain pairs\n",
      "(x1;y1);:::; (xN;yN)withxn2RDand labelsyn2R. We are inter-\n",
      "ested in constructing a predictor that takes a feature vector xnas input\n",
      "and produces a prediction yn(or something close to it), i.e., given a vec-\n",
      "torxnwe want the probability distribution of the label yn. In other words,\n",
      "we specify the conditional probability distribution of the labels given the\n",
      "examples for the particular parameter setting \u0012.\n",
      "Example 8.4\n",
      "The ﬁrst example that is often used is to specify that the conditional\n",
      "probability of the labels given the examples is a Gaussian distribution. In\n",
      "other words, we assume that we can explain our observation uncertainty\n",
      "by independent Gaussian noise (refer to Section 6.5) with zero mean,\n",
      "\"n\u0018N\u00000; \u001b2\u0001\n",
      ". We further assume that the linear model x>\n",
      "n\u0012is used for\n",
      "prediction. This means we specify a Gaussian likelihood for each example\n",
      "label pair (xn;yn),\n",
      "p(ynjxn;\u0012) =N\u0000ynjx>\n",
      "n\u0012; \u001b2\u0001: (8.15)\n",
      "An illustration of a Gaussian likelihood for a given parameter \u0012is shown\n",
      "in Figure 8.3. We will see in Section 9.2 how to explicitly expand the\n",
      "preceding expression out in terms of the Gaussian distribution.\n",
      "We assume that the set of examples (x1;y1);:::; (xN;yN)areindependent independent and\n",
      "identically\n",
      "distributedand identically distributed (i.i.d.). The word “independent” (Section 6.4.5)\n",
      "implies that the likelihood involving the whole dataset ( Y=fy1;:::;yNg\n",
      "andX=fx1;:::;xNg) factorizes into a product of the likelihoods of\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.3 Parameter Estimation 267\n",
      "each individual example\n",
      "p(YjX;\u0012) =NY\n",
      "n=1p(ynjxn;\u0012); (8.16)\n",
      "wherep(ynjxn;\u0012)is a particular distribution (which was Gaussian in Ex-\n",
      "ample 8.4). The expression “identically distributed” means that each term\n",
      "in the product (8.16) is of the same distribution, and all of them share\n",
      "the same parameters. It is often easier from an optimization viewpoint to\n",
      "compute functions that can be decomposed into sums of simpler functions.\n",
      "Hence, in machine learning we often consider the negative log-likelihood Recall log(ab) =\n",
      "log(a) + log(b)\n",
      "L(\u0012) =\u0000logp(YjX;\u0012) =\u0000NX\n",
      "n=1logp(ynjxn;\u0012): (8.17)\n",
      "While it is temping to interpret the fact that \u0012is on the right of the condi-\n",
      "tioning inp(ynjxn;\u0012)(8.15), and hence should be interpreted as observed\n",
      "and ﬁxed, this interpretation is incorrect. The negative log-likelihood L(\u0012)\n",
      "is a function of \u0012. Therefore, to ﬁnd a good parameter vector \u0012that\n",
      "explains the data (x1;y1);:::; (xN;yN)well, minimize the negative log-\n",
      "likelihoodL(\u0012)with respect to \u0012.\n",
      "Remark. The negative sign in (8.17) is a historical artifact that is due\n",
      "to the convention that we want to maximize likelihood, but numerical\n",
      "optimization literature tends to study minimization of functions. }\n",
      "Example 8.5\n",
      "Continuing on our example of Gaussian likelihoods (8.15), the negative\n",
      "log-likelihood can be rewritten as\n",
      "L(\u0012) =\u0000NX\n",
      "n=1logp(ynjxn;\u0012) =\u0000NX\n",
      "n=1logN\u0000ynjx>\n",
      "n\u0012; \u001b2\u0001\n",
      "(8.18a)\n",
      "=\u0000NX\n",
      "n=1log1p\n",
      "2\u0019\u001b2exp\u0012\n",
      "\u0000(yn\u0000x>\n",
      "n\u0012)2\n",
      "2\u001b2\u0013\n",
      "(8.18b)\n",
      "=\u0000NX\n",
      "n=1log exp\u0012\n",
      "\u0000(yn\u0000x>\n",
      "n\u0012)2\n",
      "2\u001b2\u0013\n",
      "\u0000NX\n",
      "n=1log1p\n",
      "2\u0019\u001b2(8.18c)\n",
      "=1\n",
      "2\u001b2NX\n",
      "n=1(yn\u0000x>\n",
      "n\u0012)2\u0000NX\n",
      "n=1log1p\n",
      "2\u0019\u001b2: (8.18d)\n",
      "As\u001bis given, the second term in (8.18d) is constant, and minimizing L(\u0012)\n",
      "corresponds to solving the least-squares problem (compare with (8.8))\n",
      "expressed in the ﬁrst term.\n",
      "It turns out that for Gaussian likelihoods the resulting optimization\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "268 When Models Meet Data\n",
      "Figure 8.5 For the\n",
      "given data, the\n",
      "maximum likelihood\n",
      "estimate of the\n",
      "parameters results\n",
      "in the black\n",
      "diagonal line. The\n",
      "orange square\n",
      "shows the value of\n",
      "the maximum\n",
      "likelihood\n",
      "prediction at\n",
      "x= 60 .\n",
      "0 10 20 30 40 50 60 70 80\n",
      "x0255075100125150y\n",
      "Figure 8.6\n",
      "Comparing the\n",
      "predictions with the\n",
      "maximum likelihood\n",
      "estimate and the\n",
      "MAP estimate at\n",
      "x= 60 . The prior\n",
      "biases the slope to\n",
      "be less steep and the\n",
      "intercept to be\n",
      "closer to zero. In\n",
      "this example, the\n",
      "bias that moves the\n",
      "intercept closer to\n",
      "zero actually\n",
      "increases the slope.\n",
      "0 10 20 30 40 50 60 70 80\n",
      "x0255075100125150y\n",
      "MLE\n",
      "MAP\n",
      "problem corresponding to maximum likelihood estimation has a closed-\n",
      "form solution. We will see more details on this in Chapter 9. Figure 8.5\n",
      "shows a regression dataset and the function that is induced by the maxi-\n",
      "mum-likelihood parameters. Maximum likelihood estimation may suffer\n",
      "from overﬁtting (Section 8.3.3), analogous to unregularized empirical risk\n",
      "minimization (Section 8.2.3). For other likelihood functions, i.e., if we\n",
      "model our noise with non-Gaussian distributions, maximum likelihood es-\n",
      "timation may not have a closed-form analytic solution. In this case, we\n",
      "resort to numerical optimization methods discussed in Chapter 7.\n",
      "8.3.2 Maximum A Posteriori Estimation\n",
      "If we have prior knowledge about the distribution of the parameters \u0012, we\n",
      "can multiply an additional term to the likelihood. This additional term is\n",
      "a prior probability distribution on parameters p(\u0012). For a given prior, after\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.3 Parameter Estimation 269\n",
      "observing some data x, how should we update the distribution of \u0012? In\n",
      "other words, how should we represent the fact that we have more speciﬁc\n",
      "knowledge of \u0012after observing data x? Bayes’ theorem, as discussed in\n",
      "Section 6.3, gives us a principled tool to update our probability distribu-\n",
      "tions of random variables. It allows us to compute a posterior distribution posterior\n",
      "p(\u0012jx)(the more speciﬁc knowledge) on the parameters \u0012from general\n",
      "prior statements (prior distribution) p(\u0012)and the function p(xj\u0012)that prior\n",
      "links the parameters \u0012and the observed data x(called the likelihood ): likelihood\n",
      "p(\u0012jx) =p(xj\u0012)p(\u0012)\n",
      "p(x): (8.19)\n",
      "Recall that we are interested in ﬁnding the parameter \u0012that maximizes\n",
      "the posterior. Since the distribution p(x)does not depend on \u0012, we can\n",
      "ignore the value of the denominator for the optimization and obtain\n",
      "p(\u0012jx)/p(xj\u0012)p(\u0012): (8.20)\n",
      "The preceding proportion relation hides the density of the data p(x),\n",
      "which may be difﬁcult to estimate. Instead of estimating the minimum\n",
      "of the negative log-likelihood, we now estimate the minimum of the neg-\n",
      "ative log-posterior, which is referred to as maximum a posteriori estima- maximum a\n",
      "posteriori\n",
      "estimationtion(MAP estimation ). An illustration of the effect of adding a zero-mean\n",
      "MAP estimationGaussian prior is shown in Figure 8.6.\n",
      "Example 8.6\n",
      "In addition to the assumption of Gaussian likelihood in the previous exam-\n",
      "ple, we assume that the parameter vector is distributed as a multivariate\n",
      "Gaussian with zero mean, i.e., p(\u0012) =N\u00000;\u0006\u0001\n",
      ", where \u0006is the covari-\n",
      "ance matrix (Section 6.5). Note that the conjugate prior of a Gaussian\n",
      "is also a Gaussian (Section 6.6.1), and therefore we expect the posterior\n",
      "distribution to also be a Gaussian. We will see the details of maximum a\n",
      "posteriori estimation in Chapter 9.\n",
      "The idea of including prior knowledge about where good parameters\n",
      "lie is widespread in machine learning. An alternative view, which we saw\n",
      "in Section 8.2.3, is the idea of regularization, which introduces an addi-\n",
      "tional term that biases the resulting parameters to be close to the origin.\n",
      "Maximum a posteriori estimation can be considered to bridge the non-\n",
      "probabilistic and probabilistic worlds as it explicitly acknowledges the\n",
      "need for a prior distribution but it still only produces a point estimate\n",
      "of the parameters.\n",
      "Remark. The maximum likelihood estimate \u0012MLpossesses the following\n",
      "properties (Lehmann and Casella, 1998; Efron and Hastie, 2016):\n",
      "Asymptotic consistency: The MLE converges to the true value in the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "270 When Models Meet Data\n",
      "Figure 8.7 Model\n",
      "ﬁtting. In a\n",
      "parametrized class\n",
      "M\u0012of models, we\n",
      "optimize the model\n",
      "parameters\u0012to\n",
      "minimize the\n",
      "distance to the true\n",
      "(unknown) model\n",
      "M\u0003.M\u0012\n",
      "M\u0003M\u0012\u0003\n",
      "M\u00120\n",
      "limit of inﬁnitely many observations, plus a random error that is ap-\n",
      "proximately normal.\n",
      "The size of the samples necessary to achieve these properties can be\n",
      "quite large.\n",
      "The error’s variance decays in 1=N, whereNis the number of data\n",
      "points.\n",
      "Especially, in the “small” data regime, maximum likelihood estimation\n",
      "can lead to overﬁtting .\n",
      "}\n",
      "The principle of maximum likelihood estimation (and maximum a pos-\n",
      "teriori estimation) uses probabilistic modeling to reason about the uncer-\n",
      "tainty in the data and model parameters. However, we have not yet taken\n",
      "probabilistic modeling to its full extent. In this section, the resulting train-\n",
      "ing procedure still produces a point estimate of the predictor, i.e., training\n",
      "returns one single set of parameter values that represent the best predic-\n",
      "tor. In Section 8.4, we will take the view that the parameter values should\n",
      "also be treated as random variables, and instead of estimating “best” val-\n",
      "ues of that distribution, we will use the full parameter distribution when\n",
      "making predictions.\n",
      "8.3.3 Model Fitting\n",
      "Consider the setting where we are given a dataset, and we are interested\n",
      "in ﬁtting a parametrized model to the data. When we talk about “ﬁt-\n",
      "ting”, we typically mean optimizing/learning model parameters so that\n",
      "they minimize some loss function, e.g., the negative log-likelihood. With\n",
      "maximum likelihood (Section 8.3.1) and maximum a posteriori estima-\n",
      "tion (Section 8.3.2), we already discussed two commonly used algorithms\n",
      "for model ﬁtting.\n",
      "The parametrization of the model deﬁnes a model class M\u0012with which\n",
      "we can operate. For example, in a linear regression setting, we may deﬁne\n",
      "the relationship between inputs xand (noise-free) observations yto be\n",
      "y=ax+b, where\u0012:=fa;bgare the model parameters. In this case, the\n",
      "model parameters \u0012describe the family of afﬁne functions, i.e., straight\n",
      "lines with slope a, which are offset from 0byb. Assume the data comes\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.3 Parameter Estimation 271\n",
      "Figure 8.8 Fitting\n",
      "(by maximum\n",
      "likelihood) of\n",
      "different model\n",
      "classes to a\n",
      "regression dataset.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "(a) Overﬁtting\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (b) Underﬁtting.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (c) Fitting well.\n",
      "from a model M\u0003, which is unknown to us. For a given training dataset,\n",
      "we optimize\u0012so thatM\u0012is as close as possible to M\u0003, where the “close-\n",
      "ness” is deﬁned by the objective function we optimize (e.g., squared loss\n",
      "on the training data). Figure 8.7 illustrates a setting where we have a small\n",
      "model class (indicated by the circle M\u0012), and the data generation model\n",
      "M\u0003lies outside the set of considered models. We begin our parameter\n",
      "search atM\u00120. After the optimization, i.e., when we obtain the best pos-\n",
      "sible parameters \u0012\u0003, we distinguish three different cases: (i) overﬁtting,\n",
      "(ii) underﬁtting, and (iii) ﬁtting well. We will give a high-level intuition\n",
      "of what these three concepts mean.\n",
      "Roughly speaking, overﬁtting refers to the situation where the para- overﬁtting\n",
      "metrized model class is too rich to model the dataset generated by M\u0003,\n",
      "i.e.,M\u0012could model much more complicated datasets. For instance, if the\n",
      "dataset was generated by a linear function, and we deﬁne M\u0012to be the\n",
      "class of seventh-order polynomials, we could model not only linear func-\n",
      "tions, but also polynomials of degree two, three, etc. Models that over-\n",
      "ﬁt typically have a large number of parameters. An observation we often One way to detect\n",
      "overﬁtting in\n",
      "practice is to\n",
      "observe that the\n",
      "model has low\n",
      "training risk but\n",
      "high test risk during\n",
      "cross validation\n",
      "(Section 8.2.4).make is that the overly ﬂexible model class M\u0012uses all its modeling power\n",
      "to reduce the training error. If the training data is noisy, it will therefore\n",
      "ﬁnd some useful signal in the noise itself. This will cause enormous prob-\n",
      "lems when we predict away from the training data. Figure 8.8(a) gives an\n",
      "example of overﬁtting in the context of regression where the model pa-\n",
      "rameters are learned by means of maximum likelihood (see Section 8.3.1).\n",
      "We will discuss overﬁtting in regression more in Section 9.2.2.\n",
      "When we run into underﬁtting , we encounter the opposite problem underﬁtting\n",
      "where the model class M\u0012is not rich enough. For example, if our dataset\n",
      "was generated by a sinusoidal function, but \u0012only parametrizes straight\n",
      "lines, the best optimization procedure will not get us close to the true\n",
      "model. However, we still optimize the parameters and ﬁnd the best straight\n",
      "line that models the dataset. Figure 8.8(b) shows an example of a model\n",
      "that underﬁts because it is insufﬁciently ﬂexible. Models that underﬁt typ-\n",
      "ically have few parameters.\n",
      "The third case is when the parametrized model class is about right.\n",
      "Then, our model ﬁts well, i.e., it neither overﬁts nor underﬁts. This means\n",
      "our model class is just rich enough to describe the dataset we are given.\n",
      "Figure 8.8(c) shows a model that ﬁts the given dataset fairly well. Ideally,\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "272 When Models Meet Data\n",
      "this is the model class we would want to work with since it has good\n",
      "generalization properties.\n",
      "In practice, we often deﬁne very rich model classes M\u0012with many pa-\n",
      "rameters, such as deep neural networks. To mitigate the problem of over-\n",
      "ﬁtting, we can use regularization (Section 8.2.3) or priors (Section 8.3.2).\n",
      "We will discuss how to choose the model class in Section 8.6.\n",
      "8.3.4 Further Reading\n",
      "When considering probabilistic models, the principle of maximum likeli-\n",
      "hood estimation generalizes the idea of least-squares regression for linear\n",
      "models, which we will discuss in detail in Chapter 9. When restricting\n",
      "the predictor to have linear form with an additional nonlinear function '\n",
      "applied to the output, i.e.,\n",
      "p(ynjxn;\u0012) ='(\u0012>xn); (8.21)\n",
      "we can consider other models for other prediction tasks, such as binary\n",
      "classiﬁcation or modeling count data (McCullagh and Nelder, 1989). An\n",
      "alternative view of this is to consider likelihoods that are from the ex-\n",
      "ponential family (Section 6.6). The class of models, which have linear\n",
      "dependence between parameters and data, and have potentially nonlin-\n",
      "ear transformation '(called a link function ), is referred to as generalized link function\n",
      "generalized linear\n",
      "modellinear models (Agresti, 2002, chapter 4).\n",
      "Maximum likelihood estimation has a rich history, and was originally\n",
      "proposed by Sir Ronald Fisher in the 1930s. We will expand upon the idea\n",
      "of a probabilistic model in Section 8.4. One debate among researchers\n",
      "who use probabilistic models, is the discussion between Bayesian and fre-\n",
      "quentist statistics. As mentioned in Section 6.1.1, it boils down to the\n",
      "deﬁnition of probability. Recall from Section 6.1 that one can consider\n",
      "probability to be a generalization (by allowing uncertainty) of logical rea-\n",
      "soning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-\n",
      "lihood estimation is frequentist in nature, and the interested reader is\n",
      "pointed to Efron and Hastie (2016) for a balanced view of both Bayesian\n",
      "and frequentist statistics.\n",
      "There are some probabilistic models where maximum likelihood esti-\n",
      "mation may not be possible. The reader is referred to more advanced sta-\n",
      "tistical textbooks, e.g., Casella and Berger (2002), for approaches, such as\n",
      "method of moments, M-estimation, and estimating equations.\n",
      "8.4 Probabilistic Modeling and Inference\n",
      "In machine learning, we are frequently concerned with the interpretation\n",
      "and analysis of data, e.g., for prediction of future events and decision\n",
      "making. To make this task more tractable, we often build models that\n",
      "describe the generative process that generates the observed data. generative process\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.4 Probabilistic Modeling and Inference 273\n",
      "For example, we can describe the outcome of a coin-ﬂip experiment\n",
      "(“heads” or “tails”) in two steps. First, we deﬁne a parameter \u0016, which\n",
      "describes the probability of “heads” as the parameter of a Bernoulli distri-\n",
      "bution (Chapter 6); second, we can sample an outcome x2fhead, tailg\n",
      "from the Bernoulli distribution p(xj\u0016) =Ber(\u0016). The parameter \u0016gives\n",
      "rise to a speciﬁc dataset Xand depends on the coin used. Since \u0016is un-\n",
      "known in advance and can never be observed directly, we need mecha-\n",
      "nisms to learn something about \u0016given observed outcomes of coin-ﬂip\n",
      "experiments. In the following, we will discuss how probabilistic modeling\n",
      "can be used for this purpose.\n",
      "8.4.1 Probabilistic ModelsA probabilistic\n",
      "model is speciﬁed\n",
      "by the joint\n",
      "distribution of all\n",
      "random variables.Probabilistic models represent the uncertain aspects of an experiment as\n",
      "probability distributions. The beneﬁt of using probabilistic models is that\n",
      "they offer a uniﬁed and consistent set of tools from probability theory\n",
      "(Chapter 6) for modeling, inference, prediction, and model selection.\n",
      "In probabilistic modeling, the joint distribution p(x;\u0012)of the observed\n",
      "variablesxand the hidden parameters \u0012is of central importance: It en-\n",
      "capsulates information from the following:\n",
      "The prior and the likelihood (product rule, Section 6.3).\n",
      "The marginal likelihood p(x), which will play an important role in\n",
      "model selection (Section 8.6), can be computed by taking the joint dis-\n",
      "tribution and integrating out the parameters (sum rule, Section 6.3).\n",
      "The posterior, which can be obtained by dividing the joint by the marginal\n",
      "likelihood.\n",
      "Only the joint distribution has this property. Therefore, a probabilistic\n",
      "model is speciﬁed by the joint distribution of all its random variables.\n",
      "8.4.2 Bayesian Inference\n",
      "Parameter\n",
      "estimation can be\n",
      "phrased as an\n",
      "optimization\n",
      "problem.A key task in machine learning is to take a model and the data to uncover\n",
      "the values of the model’s hidden variables \u0012given the observed variables\n",
      "x. In Section 8.3.1, we already discussed two ways for estimating model\n",
      "parameters\u0012using maximum likelihood or maximum a posteriori esti-\n",
      "mation. In both cases, we obtain a single-best value for \u0012so that the key\n",
      "algorithmic problem of parameter estimation is solving an optimization\n",
      "problem. Once these point estimates \u0012\u0003are known, we use them to make\n",
      "predictions. More speciﬁcally, the predictive distribution will be p(xj\u0012\u0003),\n",
      "where we use \u0012\u0003in the likelihood function.\n",
      "As discussed in Section 6.3, focusing solely on some statistic of the pos-\n",
      "terior distribution (such as the parameter \u0012\u0003that maximizes the poste-\n",
      "rior) leads to loss of information, which can be critical in a system that\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "274 When Models Meet Data\n",
      "uses the prediction p(xj\u0012\u0003)to make decisions. These decision-making\n",
      "systems typically have different objective functions than the likelihood, a Bayesian inference\n",
      "is about learning the\n",
      "distribution of\n",
      "random variables.squared-error loss or a mis-classiﬁcation error. Therefore, having the full\n",
      "posterior distribution around can be extremely useful and leads to more\n",
      "robust decisions. Bayesian inference is about ﬁnding this posterior distri-Bayesian inference\n",
      "bution (Gelman et al., 2004). For a dataset X, a parameter prior p(\u0012), and\n",
      "a likelihood function, the posterior\n",
      "p(\u0012jX) =p(Xj\u0012)p(\u0012)\n",
      "p(X); p (X) =Z\n",
      "p(Xj\u0012)p(\u0012)d\u0012; (8.22)\n",
      "is obtained by applying Bayes’ theorem. The key idea is to exploit Bayes’ Bayesian inference\n",
      "inverts the\n",
      "relationship\n",
      "between parameters\n",
      "and the data.theorem to invert the relationship between the parameters \u0012and the data\n",
      "X(given by the likelihood) to obtain the posterior distribution p(\u0012jX).\n",
      "The implication of having a posterior distribution on the parameters is\n",
      "that it can be used to propagate uncertainty from the parameters to the\n",
      "data. More speciﬁcally, with a distribution p(\u0012)on the parameters our\n",
      "predictions will be\n",
      "p(x) =Z\n",
      "p(xj\u0012)p(\u0012)d\u0012=E\u0012[p(xj\u0012)]; (8.23)\n",
      "and they no longer depend on the model parameters \u0012, which have been\n",
      "marginalized/integrated out. Equation (8.23) reveals that the prediction\n",
      "is an average over all plausible parameter values \u0012, where the plausibility\n",
      "is encapsulated by the parameter distribution p(\u0012).\n",
      "Having discussed parameter estimation in Section 8.3 and Bayesian in-\n",
      "ference here, let us compare these two approaches to learning. Parameter\n",
      "estimation via maximum likelihood or MAP estimation yields a consistent\n",
      "point estimate \u0012\u0003of the parameters, and the key computational problem\n",
      "to be solved is optimization. In contrast, Bayesian inference yields a (pos-\n",
      "terior) distribution, and the key computational problem to be solved is\n",
      "integration. Predictions with point estimates are straightforward, whereas\n",
      "predictions in the Bayesian framework require solving another integration\n",
      "problem; see (8.23). However, Bayesian inference gives us a principled\n",
      "way to incorporate prior knowledge, account for side information, and\n",
      "incorporate structural knowledge, all of which is not easily done in the\n",
      "context of parameter estimation. Moreover, the propagation of parameter\n",
      "uncertainty to the prediction can be valuable in decision-making systems\n",
      "for risk assessment and exploration in the context of data-efﬁcient learn-\n",
      "ing (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018).\n",
      "While Bayesian inference is a mathematically principled framework for\n",
      "learning about parameters and making predictions, there are some prac-\n",
      "tical challenges that come with it because of the integration problems we\n",
      "need to solve; see (8.22) and (8.23). More speciﬁcally, if we do not choose\n",
      "a conjugate prior on the parameters (Section 6.6.1), the integrals in (8.22)\n",
      "and (8.23) are not analytically tractable, and we cannot compute the pos-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.4 Probabilistic Modeling and Inference 275\n",
      "terior, the predictions, or the marginal likelihood in closed form. In these\n",
      "cases, we need to resort to approximations. Here, we can use stochas-\n",
      "tic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks\n",
      "et al., 1996), or deterministic approximations, such as the Laplace ap-\n",
      "proximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational in-\n",
      "ference (Jordan et al., 1999; Blei et al., 2017), or expectation propaga-\n",
      "tion (Minka, 2001a).\n",
      "Despite these challenges, Bayesian inference has been successfully ap-\n",
      "plied to a variety of problems, including large-scale topic modeling (Hoff-\n",
      "man et al., 2013), click-through-rate prediction (Graepel et al., 2010),\n",
      "data-efﬁcient reinforcement learning in control systems (Deisenroth et al.,\n",
      "2015), online ranking systems (Herbrich et al., 2007), and large-scale rec-\n",
      "ommender systems. There are generic tools, such as Bayesian optimiza-\n",
      "tion (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that\n",
      "are very useful ingredients for an efﬁcient search of meta parameters of\n",
      "models or algorithms.\n",
      "Remark. In the machine learning literature, there can be a somewhat ar-\n",
      "bitrary separation between (random) “variables” and “parameters”. While\n",
      "parameters are estimated (e.g., via maximum likelihood), variables are\n",
      "usually marginalized out. In this book, we are not so strict with this sep-\n",
      "aration because, in principle, we can place a prior on any parameter and\n",
      "integrate it out, which would then turn the parameter into a random vari-\n",
      "able according to the aforementioned separation. }\n",
      "8.4.3 Latent-Variable Models\n",
      "In practice, it is sometimes useful to have additional latent variables z latent variable\n",
      "(besides the model parameters \u0012) as part of the model (Moustaki et al.,\n",
      "2015). These latent variables are different from the model parameters\n",
      "\u0012as they do not parametrize the model explicitly. Latent variables may\n",
      "describe the data-generating process, thereby contributing to the inter-\n",
      "pretability of the model. They also often simplify the structure of the\n",
      "model and allow us to deﬁne simpler and richer model structures. Sim-\n",
      "pliﬁcation of the model structure often goes hand in hand with a smaller\n",
      "number of model parameters (Paquet, 2008; Murphy, 2012). Learning in\n",
      "latent-variable models (at least via maximum likelihood) can be done in a\n",
      "principled way using the expectation maximization (EM) algorithm (Demp-\n",
      "ster et al., 1977; Bishop, 2006). Examples, where such latent variables\n",
      "are helpful, are principal component analysis for dimensionality reduc-\n",
      "tion (Chapter 10), Gaussian mixture models for density estimation (Chap-\n",
      "ter 11), hidden Markov models (Maybeck, 1979) or dynamical systems\n",
      "(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,\n",
      "and meta learning and task generalization (Hausman et al., 2018; Sæ-\n",
      "mundsson et al., 2018). Although the introduction of these latent variables\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "276 When Models Meet Data\n",
      "may make the model structure and the generative process easier, learning\n",
      "in latent-variable models is generally hard, as we will see in Chapter 11.\n",
      "Since latent-variable models also allow us to deﬁne the process that\n",
      "generates data from parameters, let us have a look at this generative pro-\n",
      "cess. Denoting data by x, the model parameters by \u0012and the latent vari-\n",
      "ables byz, we obtain the conditional distribution\n",
      "p(xjz;\u0012) (8.24)\n",
      "that allows us to generate data for any model parameters and latent vari-\n",
      "ables. Given that zare latent variables, we place a prior p(z)on them.\n",
      "As the models we discussed previously, models with latent variables\n",
      "can be used for parameter learning and inference within the frameworks\n",
      "we discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by\n",
      "means of maximum likelihood estimation or Bayesian inference), we fol-\n",
      "low a two-step procedure. First, we compute the likelihood p(xj\u0012)of the\n",
      "model, which does not depend on the latent variables. Second, we use this\n",
      "likelihood for parameter estimation or Bayesian inference, where we use\n",
      "exactly the same expressions as in Sections 8.3 and 8.4.2, respectively.\n",
      "Since the likelihood function p(xj\u0012)is the predictive distribution of the\n",
      "data given the model parameters, we need to marginalize out the latent\n",
      "variables so that\n",
      "p(xj\u0012) =Z\n",
      "p(xjz;\u0012)p(z)dz; (8.25)\n",
      "wherep(xjz;\u0012)is given in (8.24) and p(z)is the prior on the latent\n",
      "variables. Note that the likelihood must not depend on the latent variables The likelihood is a\n",
      "function of the data\n",
      "and the model\n",
      "parameters, but is\n",
      "independent of the\n",
      "latent variables.z, but it is only a function of the data xand the model parameters \u0012.\n",
      "The likelihood in (8.25) directly allows for parameter estimation via\n",
      "maximum likelihood. MAP estimation is also straightforward with an ad-\n",
      "ditional prior on the model parameters \u0012as discussed in Section 8.3.2.\n",
      "Moreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)\n",
      "in a latent-variable model works in the usual way: We place a prior p(\u0012)\n",
      "on the model parameters and use Bayes’ theorem to obtain a posterior\n",
      "distribution\n",
      "p(\u0012jX) =p(Xj\u0012)p(\u0012)\n",
      "p(X)(8.26)\n",
      "over the model parameters given a dataset X. The posterior in (8.26) can\n",
      "be used for predictions within a Bayesian inference framework; see (8.23).\n",
      "One challenge we have in this latent-variable model is that the like-\n",
      "lihoodp(Xj\u0012)requires the marginalization of the latent variables ac-\n",
      "cording to (8.25). Except when we choose a conjugate prior p(z)for\n",
      "p(xjz;\u0012), the marginalization in (8.25) is not analytically tractable, and\n",
      "we need to resort to approximations (Bishop, 2006; Paquet, 2008; Mur-\n",
      "phy, 2012; Moustaki et al., 2015).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.4 Probabilistic Modeling and Inference 277\n",
      "Similar to the parameter posterior (8.26) we can compute a posterior\n",
      "on the latent variables according to\n",
      "p(zjX) =p(Xjz)p(z)\n",
      "p(X); p (Xjz) =Z\n",
      "p(Xjz;\u0012)p(\u0012)d\u0012;(8.27)\n",
      "wherep(z)is the prior on the latent variables and p(Xjz)requires us to\n",
      "integrate out the model parameters \u0012.\n",
      "Given the difﬁculty of solving integrals analytically, it is clear that mar-\n",
      "ginalizing out both the latent variables and the model parameters at the\n",
      "same time is not possible in general (Bishop, 2006; Murphy, 2012). A\n",
      "quantity that is easier to compute is the posterior distribution on the latent\n",
      "variables, but conditioned on the model parameters, i.e.,\n",
      "p(zjX;\u0012) =p(Xjz;\u0012)p(z)\n",
      "p(Xj\u0012); (8.28)\n",
      "wherep(z)is the prior on the latent variables and p(Xjz;\u0012)is given\n",
      "in (8.24).\n",
      "In Chapters 10 and 11, we derive the likelihood functions for PCA and\n",
      "Gaussian mixture models, respectively. Moreover, we compute the poste-\n",
      "rior distributions (8.28) on the latent variables for both PCA and Gaussian\n",
      "mixture models.\n",
      "Remark. In the following chapters, we may not be drawing such a clear\n",
      "distinction between latent variables zand uncertain model parameters \u0012\n",
      "and call the model parameters “latent” or “hidden” as well because they\n",
      "are unobserved. In Chapters 10 and 11, where we use the latent variables\n",
      "z, we will pay attention to the difference as we will have two different\n",
      "types of hidden variables: model parameters \u0012and latent variables z.}\n",
      "We can exploit the fact that all the elements of a probabilistic model are\n",
      "random variables to deﬁne a uniﬁed language for representing them. In\n",
      "Section 8.5, we will see a concise graphical language for representing the\n",
      "structure of probabilistic models. We will use this graphical language to\n",
      "describe the probabilistic models in the subsequent chapters.\n",
      "8.4.4 Further Reading\n",
      "Probabilistic models in machine learning (Bishop, 2006; Barber, 2012;\n",
      "Murphy, 2012) provide a way for users to capture uncertainty about data\n",
      "and predictive models in a principled fashion. Ghahramani (2015) presents\n",
      "a short review of probabilistic models in machine learning. Given a proba-\n",
      "bilistic model, we may be lucky enough to be able to compute parameters\n",
      "of interest analytically. However, in general, analytic solutions are rare,\n",
      "and computational methods such as sampling (Gilks et al., 1996; Brooks\n",
      "et al., 2011) and variational inference (Jordan et al., 1999; Blei et al.,\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "278 When Models Meet Data\n",
      "2017) are used. Moustaki et al. (2015) and Paquet (2008) provide a good\n",
      "overview of Bayesian inference in latent-variable models.\n",
      "In recent years, several programming languages have been proposed\n",
      "that aim to treat the variables deﬁned in software as random variables\n",
      "corresponding to probability distributions. The objective is to be able to\n",
      "write complex functions of probability distributions, while under the hood\n",
      "the compiler automatically takes care of the rules of Bayesian inference.\n",
      "This rapidly changing ﬁeld is called probabilistic programming . probabilistic\n",
      "programming\n",
      "8.5 Directed Graphical Models\n",
      "In this section, we introduce a graphical language for specifying a prob-\n",
      "abilistic model, called the directed graphical model . It provides a compact directed graphical\n",
      "model and succinct way to specify probabilistic models, and allows the reader to\n",
      "visually parse dependencies between random variables. A graphical model\n",
      "visually captures the way in which the joint distribution over all random\n",
      "variables can be decomposed into a product of factors depending only on\n",
      "a subset of these variables. In Section 8.4, we identiﬁed the joint distri-\n",
      "bution of a probabilistic model as the key quantity of interest because it\n",
      "comprises information about the prior, the likelihood, and the posterior.\n",
      "However, the joint distribution by itself can be quite complicated, and Directed graphical\n",
      "models are also\n",
      "known as Bayesian\n",
      "networks.it does not tell us anything about structural properties of the probabilis-\n",
      "tic model. For example, the joint distribution p(a;b;c )does not tell us\n",
      "anything about independence relations. This is the point where graphical\n",
      "models come into play. This section relies on the concepts of independence\n",
      "and conditional independence, as described in Section 6.4.5.\n",
      "In agraphical model , nodes are random variables. In Figure 8.9(a), the graphical model\n",
      "nodes represent the random variables a;b;c . Edges represent probabilistic\n",
      "relations between variables, e.g., conditional probabilities.\n",
      "Remark. Not every distribution can be represented in a particular choice of\n",
      "graphical model. A discussion of this can be found in Bishop (2006). }\n",
      "Probabilistic graphical models have some convenient properties:\n",
      "They are a simple way to visualize the structure of a probabilistic model.\n",
      "They can be used to design or motivate new kinds of statistical models.\n",
      "Inspection of the graph alone gives us insight into properties, e.g., con-\n",
      "ditional independence.\n",
      "Complex computations for inference and learning in statistical models\n",
      "can be expressed in terms of graphical manipulations.\n",
      "8.5.1 Graph Semantics\n",
      "Directed graphical models /Bayesian networks are a method for representing directed graphical\n",
      "model/Bayesian\n",
      "networkconditional dependencies in a probabilistic model. They provide a visual\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.5 Directed Graphical Models 279\n",
      "description of the conditional probabilities, hence, providing a simple lan-\n",
      "guage for describing complex interdependence. The modular description With additional\n",
      "assumptions, the\n",
      "arrows can be used\n",
      "to indicate causal\n",
      "relationships (Pearl,\n",
      "2009).also entails computational simpliﬁcation. Directed links (arrows) between\n",
      "two nodes (random variables) indicate conditional probabilities. For ex-\n",
      "ample, the arrow between aandbin Figure 8.9(a) gives the conditional\n",
      "probabilityp(bja)ofbgivena.\n",
      "Figure 8.9\n",
      "Examples of\n",
      "directed graphical\n",
      "models.a b\n",
      "c\n",
      "(a) Fully connected.x1 x2\n",
      "x3 x4x5\n",
      "(b) Not fully connected.\n",
      "Directed graphical models can be derived from joint distributions if we\n",
      "know something about their factorization.\n",
      "Example 8.7\n",
      "Consider the joint distribution\n",
      "p(a;b;c ) =p(cja;b)p(bja)p(a) (8.29)\n",
      "of three random variables a;b;c . The factorization of the joint distribution\n",
      "in (8.29) tells us something about the relationship between the random\n",
      "variables:\n",
      "cdepends directly on aandb.\n",
      "bdepends directly on a.\n",
      "adepends neither on bnor onc.\n",
      "For the factorization in (8.29), we obtain the directed graphical model in\n",
      "Figure 8.9(a).\n",
      "In general, we can construct the corresponding directed graphical model\n",
      "from a factorized joint distribution as follows:\n",
      "1. Create a node for all random variables.\n",
      "2. For each conditional distribution, we add a directed link (arrow) to\n",
      "the graph from the nodes corresponding to the variables on which the\n",
      "distribution is conditioned.\n",
      "The graph layout\n",
      "depends on the\n",
      "factorization of the\n",
      "joint distribution.The graph layout depends on the choice of factorization of the joint dis-\n",
      "tribution.\n",
      "We discussed how to get from a known factorization of the joint dis-\n",
      "tribution to the corresponding directed graphical model. Now, we will do\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "280 When Models Meet Data\n",
      "exactly the opposite and describe how to extract the joint distribution of\n",
      "a set of random variables from a given graphical model.\n",
      "Example 8.8\n",
      "Looking at the graphical model in Figure 8.9(b), we exploit two proper-\n",
      "ties:\n",
      "The joint distribution p(x1;:::;x 5)we seek is the product of a set of\n",
      "conditionals, one for each node in the graph. In this particular example,\n",
      "we will need ﬁve conditionals.\n",
      "Each conditional depends only on the parents of the corresponding\n",
      "node in the graph. For example, x4will be conditioned on x2.\n",
      "These two properties yield the desired factorization of the joint distribu-\n",
      "tion\n",
      "p(x1;x2;x3;x4;x5) =p(x1)p(x5)p(x2jx5)p(x3jx1;x2)p(x4jx2):(8.30)\n",
      "In general, the joint distribution p(x) =p(x1;:::;xK)is given as\n",
      "p(x) =KY\n",
      "k=1p(xkjPak); (8.31)\n",
      "where Pa kmeans “the parent nodes of xk”. Parent nodes of xkare nodes\n",
      "that have arrows pointing to xk.\n",
      "We conclude this subsection with a concrete example of the coin-ﬂip\n",
      "experiment. Consider a Bernoulli experiment (Example 6.8) where the\n",
      "probability that the outcome xof this experiment is “heads” is\n",
      "p(xj\u0016) =Ber(\u0016): (8.32)\n",
      "We now repeat this experiment Ntimes and observe outcomes x1;:::;xN\n",
      "so that we obtain the joint distribution\n",
      "p(x1;:::;xNj\u0016) =NY\n",
      "n=1p(xnj\u0016): (8.33)\n",
      "The expression on the right-hand side is a product of Bernoulli distribu-\n",
      "tions on each individual outcome because the experiments are indepen-\n",
      "dent. Recall from Section 6.4.5 that statistical independence means that\n",
      "the distribution factorizes. To write the graphical model down for this set-\n",
      "ting, we make the distinction between unobserved/latent variables and\n",
      "observed variables. Graphically, observed variables are denoted by shaded\n",
      "nodes so that we obtain the graphical model in Figure 8.10(a). We see\n",
      "that the single parameter \u0016is the same for all xn,n= 1;:::;N as the\n",
      "outcomesxnare identically distributed. A more compact, but equivalent,\n",
      "graphical model for this setting is given in Figure 8.10(b), where we use\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.5 Directed Graphical Models 281\n",
      "Figure 8.10\n",
      "Graphical models\n",
      "for a repeated\n",
      "Bernoulli\n",
      "experiment.\u0016\n",
      "x1 xN\n",
      "(a) Version with xnexplicit.\u0016\n",
      "xn\n",
      "n= 1;:::;N\n",
      "(b) Version with\n",
      "plate notation.\u0016\n",
      "xn\f \u000b\n",
      "n= 1;:::;N\n",
      "(c) Hyperparameters \u000b\n",
      "and\fon the latent \u0016.\n",
      "theplate notation. The plate (box) repeats everything inside (in this case, plate\n",
      "the observations xn)Ntimes. Therefore, both graphical models are equiv-\n",
      "alent, but the plate notation is more compact. Graphical models immedi-\n",
      "ately allow us to place a hyperprior on \u0016. Ahyperprior is a second layer hyperprior\n",
      "of prior distributions on the parameters of the ﬁrst layer of priors. Fig-\n",
      "ure 8.10(c) places a Beta (\u000b;\f)prior on the latent variable \u0016. If we treat\n",
      "\u000band\fas deterministic parameters, i.e., not random variables, we omit\n",
      "the circle around it.\n",
      "8.5.2 Conditional Independence and d-Separation\n",
      "Directed graphical models allow us to ﬁnd conditional independence (Sec-\n",
      "tion 6.4.5) relationship properties of the joint distribution only by looking\n",
      "at the graph. A concept called d-separation (Pearl, 1988) is key to this. d-separation\n",
      "Consider a general directed graph in which A;B;Care arbitrary nonin-\n",
      "tersecting sets of nodes (whose union may be smaller than the complete\n",
      "set of nodes in the graph). We wish to ascertain whether a particular con-\n",
      "ditional independence statement, “ Ais conditionally independent of B\n",
      "givenC”, denoted by\n",
      "A? ?BjC; (8.34)\n",
      "is implied by a given directed acyclic graph. To do so, we consider all\n",
      "possible trails (paths that ignore the direction of the arrows) from any\n",
      "node inAto any nodes inB. Any such path is said to be blocked if it\n",
      "includes any node such that either of the following are true:\n",
      "The arrows on the path meet either head to tail or tail to tail at the\n",
      "node, and the node is in the set C.\n",
      "The arrows meet head to head at the node, and neither the node nor\n",
      "any of its descendants is in the set C.\n",
      "If all paths are blocked, then Ais said to be d-separated fromBbyC,\n",
      "and the joint distribution over all of the variables in the graph will satisfy\n",
      "A? ?BjC .\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "282 When Models Meet Data\n",
      "Figure 8.12 Three\n",
      "types of graphical\n",
      "models: (a) Directed\n",
      "graphical models\n",
      "(Bayesian\n",
      "networks);\n",
      "(b) Undirected\n",
      "graphical models\n",
      "(Markov random\n",
      "ﬁelds); (c) Factor\n",
      "graphs.a b\n",
      "c\n",
      "(a) Directed graphical modela b\n",
      "c\n",
      "(b) Undirected graphical\n",
      "modela b\n",
      "c\n",
      "(c) Factor graph\n",
      "Example 8.9 (Conditional Independence)\n",
      "Figure 8.11\n",
      "D-separation\n",
      "example.abc\n",
      "d\n",
      "e\n",
      "Consider the graphical model in Figure 8.11. Visual inspection gives us\n",
      "b? ?dja;c (8.35)\n",
      "a? ?cjb (8.36)\n",
      "b6? ?djc (8.37)\n",
      "a6? ?cjb;e (8.38)\n",
      "Directed graphical models allow a compact representation of proba-\n",
      "bilistic models, and we will see examples of directed graphical models in\n",
      "Chapters 9, 10, and 11. The representation, along with the concept of con-\n",
      "ditional independence, allows us to factorize the respective probabilistic\n",
      "models into expressions that are easier to optimize.\n",
      "The graphical representation of the probabilistic model allows us to\n",
      "visually see the impact of design choices we have made on the structure\n",
      "of the model. We often need to make high-level assumptions about the\n",
      "structure of the model. These modeling assumptions (hyperparameters)\n",
      "affect the prediction performance, but cannot be selected directly using\n",
      "the approaches we have seen so far. We will discuss different ways to\n",
      "choose the structure in Section 8.6.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.6 Model Selection 283\n",
      "8.5.3 Further Reading\n",
      "An introduction to probabilistic graphical models can be found in Bishop\n",
      "(2006, chapter 8), and an extensive description of the different applica-\n",
      "tions and corresponding algorithmic implications can be found in the book\n",
      "by Koller and Friedman (2009). There are three main types of probabilistic\n",
      "graphical models:\n",
      "directed graphical\n",
      "model Directed graphical models (Bayesian networks ); see Figure 8.12(a)\n",
      "Bayesian network\n",
      "undirected graphical\n",
      "modelUndirected graphical models (Markov random ﬁelds ); see Figure 8.12(b)\n",
      "Markov random\n",
      "ﬁeld\n",
      "factor graphFactor graphs ; see Figure 8.12(c)\n",
      "Graphical models allow for graph-based algorithms for inference and\n",
      "learning, e.g., via local message passing. Applications range from rank-\n",
      "ing in online games (Herbrich et al., 2007) and computer vision (e.g.,\n",
      "image segmentation, semantic labeling, image denoising, image restora-\n",
      "tion (Kittler and F ¨oglein, 1984; Sucar and Gillies, 1994; Shotton et al.,\n",
      "2006; Szeliski et al., 2008)) to coding theory (McEliece et al., 1998), solv-\n",
      "ing linear equation systems (Shental et al., 2008), and iterative Bayesian\n",
      "state estimation in signal processing (Bickson et al., 2007; Deisenroth and\n",
      "Mohamed, 2012).\n",
      "One topic that is particularly important in real applications that we do\n",
      "not discuss in this book is the idea of structured prediction (Bakir et al.,\n",
      "2007; Nowozin et al., 2014), which allows machine learning models to\n",
      "tackle predictions that are structured, for example sequences, trees, and\n",
      "graphs. The popularity of neural network models has allowed more ﬂex-\n",
      "ible probabilistic models to be used, resulting in many useful applica-\n",
      "tions of structured models (Goodfellow et al., 2016, chapter 16). In recent\n",
      "years, there has been a renewed interest in graphical models due to their\n",
      "applications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;\n",
      "Peters et al., 2017; Rosenbaum, 2017).\n",
      "8.6 Model Selection\n",
      "In machine learning, we often need to make high-level modeling decisions\n",
      "that critically inﬂuence the performance of the model. The choices we\n",
      "make (e.g., the functional form of the likelihood) inﬂuence the number\n",
      "and type of free parameters in the model and thereby also the ﬂexibility\n",
      "and expressivity of the model. More complex models are more ﬂexible in A polynomial\n",
      "y=a0+a1x+a2x2\n",
      "can also describe\n",
      "linear functions by\n",
      "settinga2= 0, i.e.,\n",
      "it is strictly more\n",
      "expressive than a\n",
      "ﬁrst-order\n",
      "polynomial.the sense that they can be used to describe more datasets. For instance, a\n",
      "polynomial of degree 1(a liney=a0+a1x) can only be used to describe\n",
      "linear relations between inputs xand observations y. A polynomial of\n",
      "degree 2can additionally describe quadratic relationships between inputs\n",
      "and observations.\n",
      "One would now think that very ﬂexible models are generally preferable\n",
      "to simple models because they are more expressive. A general problem\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "284 When Models Meet Data\n",
      "Figure 8.13 Nested\n",
      "cross-validation. We\n",
      "perform two levels\n",
      "ofK-fold\n",
      "cross-validation.All labeled data\n",
      "All training data Test data\n",
      "To train model Validation\n",
      "is that at training time we can only use the training set to evaluate the\n",
      "performance of the model and learn its parameters. However, the per-\n",
      "formance on the training set is not really what we are interested in. In\n",
      "Section 8.3, we have seen that maximum likelihood estimation can lead\n",
      "to overﬁtting, especially when the training dataset is small. Ideally, our\n",
      "model (also) works well on the test set (which is not available at training\n",
      "time). Therefore, we need some mechanisms for assessing how a model\n",
      "generalizes to unseen test data. Model selection is concerned with exactly\n",
      "this problem.\n",
      "8.6.1 Nested Cross-Validation\n",
      "We have already seen an approach (cross-validation in Section 8.2.4) that\n",
      "can be used for model selection. Recall that cross-validation provides an\n",
      "estimate of the generalization error by repeatedly splitting the dataset into\n",
      "training and validation sets. We can apply this idea one more time, i.e.,\n",
      "for each split, we can perform another round of cross-validation. This is\n",
      "sometimes referred to as nested cross-validation ; see Figure 8.13. The inner nested\n",
      "cross-validation level is used to estimate the performance of a particular choice of model\n",
      "or hyperparameter on a internal validation set. The outer level is used to\n",
      "estimate generalization performance for the best choice of model chosen\n",
      "by the inner loop. We can test different model and hyperparameter choices\n",
      "in the inner loop. To distinguish the two levels, the set used to estimate\n",
      "the generalization performance is often called the test set and the set used test set\n",
      "for choosing the best model is called the validation set . The inner loop validation set\n",
      "estimates the expected value of the generalization error for a given model\n",
      "(8.39), by approximating it using the empirical error on the validation set,\n",
      "i.e., The standard error\n",
      "is deﬁned as\u001bp\n",
      "K,\n",
      "whereKis the\n",
      "number of\n",
      "experiments and \u001b\n",
      "is the standard\n",
      "deviation of the risk\n",
      "of each experiment.EV[R(VjM)]\u00191\n",
      "KKX\n",
      "k=1R(V(k)jM); (8.39)\n",
      "where R(VjM)is the empirical risk (e.g., root mean square error) on the\n",
      "validation setVfor modelM. We repeat this procedure for all models and\n",
      "choose the model that performs best. Note that cross-validation not only\n",
      "gives us the expected generalization error, but we can also obtain high-\n",
      "order statistics, e.g., the standard error, an estimate of how uncertain the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.6 Model Selection 285\n",
      "Figure 8.14\n",
      "Bayesian inference\n",
      "embodies Occam’s\n",
      "razor. The\n",
      "horizontal axis\n",
      "describes the space\n",
      "of all possible\n",
      "datasetsD. The\n",
      "evidence (vertical\n",
      "axis) evaluates how\n",
      "well a model\n",
      "predicts available\n",
      "data. Since\n",
      "p(DjMi)needs to\n",
      "integrate to 1, we\n",
      "should choose the\n",
      "model with the\n",
      "greatest evidence.\n",
      "Adapted\n",
      "from MacKay\n",
      "(2003).Evidence\n",
      "D\n",
      "Cp(DjM1)\n",
      "p(DjM2)\n",
      "mean estimate is. Once the model is chosen, we can evaluate the ﬁnal\n",
      "performance on the test set.\n",
      "8.6.2 Bayesian Model Selection\n",
      "There are many approaches to model selection, some of which are covered\n",
      "in this section. Generally, they all attempt to trade off model complexity\n",
      "and data ﬁt. We assume that simpler models are less prone to overﬁtting\n",
      "than complex models, and hence the objective of model selection is to ﬁnd\n",
      "the simplest model that explains the data reasonably well. This concept is\n",
      "also known as Occam’s razor . Occam’s razor\n",
      "Remark. If we treat model selection as a hypothesis testing problem, we\n",
      "are looking for the simplest hypothesis that is consistent with the data (Mur-\n",
      "phy, 2012). }\n",
      "One may consider placing a prior on models that favors simpler models.\n",
      "However, it is not necessary to do this: An “automatic Occam’s Razor” is\n",
      "quantitatively embodied in the application of Bayesian probability (Smith\n",
      "and Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-\n",
      "ure 8.14, adapted from MacKay (2003), gives us the basic intuition why\n",
      "complex and very expressive models may turn out to be a less probable\n",
      "choice for modeling a given dataset D. Let us think of the horizontal axis These predictions\n",
      "are quantiﬁed by a\n",
      "normalized\n",
      "probability\n",
      "distribution onD,\n",
      "i.e., it needs to\n",
      "integrate/sum to 1.representing the space of all possible datasets D. If we are interested in\n",
      "the posterior probability p(MijD)of modelMigiven the dataD, we can\n",
      "employ Bayes’ theorem. Assuming a uniform prior p(M)over all mod-\n",
      "els, Bayes’ theorem rewards models in proportion to how much they pre-\n",
      "dicted the data that occurred. This prediction of the data given model\n",
      "Mi,p(DjMi), is called the evidence forMi. A simple model M1can only evidence\n",
      "predict a small number of datasets, which is shown by p(DjM1); a more\n",
      "powerful model M2that has, e.g., more free parameters than M1, is able\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "286 When Models Meet Data\n",
      "to predict a greater variety of datasets. This means, however, that M2\n",
      "does not predict the datasets in region Cas well asM1. Suppose that\n",
      "equal prior probabilities have been assigned to the two models. Then, if\n",
      "the dataset falls into region C, the less powerful model M1is the more\n",
      "probable model.\n",
      "Earlier in this chapter, we argued that models need to be able to explain\n",
      "the data, i.e., there should be a way to generate data from a given model.\n",
      "Furthermore, if the model has been appropriately learned from the data,\n",
      "then we expect that the generated data should be similar to the empirical\n",
      "data. For this, it is helpful to phrase model selection as a hierarchical\n",
      "inference problem, which allows us to compute the posterior distribution\n",
      "over models.\n",
      "Let us consider a ﬁnite number of models M=fM1;:::;MKg, where\n",
      "each model Mkpossesses parameters \u0012k. InBayesian model selection , we Bayesian model\n",
      "selection place a prior p(M)on the set of models. The corresponding generative\n",
      "generative processprocess that allows us to generate data from this model is\n",
      "Figure 8.15\n",
      "Illustration of the\n",
      "hierarchical\n",
      "generative process\n",
      "in Bayesian model\n",
      "selection. We place\n",
      "a priorp(M)on the\n",
      "set of models. For\n",
      "each model, there is\n",
      "a distribution\n",
      "p(\u0012jM)on the\n",
      "corresponding\n",
      "model parameters,\n",
      "which is used to\n",
      "generate the dataD.\n",
      "M\n",
      "\u0012\n",
      "DMk\u0018p(M) (8.40)\n",
      "\u0012k\u0018p(\u0012jMk) (8.41)\n",
      "D\u0018p(Dj\u0012k) (8.42)\n",
      "and illustrated in Figure 8.15. Given a training set D, we apply Bayes’\n",
      "theorem and compute the posterior distribution over models as\n",
      "p(MkjD)/p(Mk)p(DjMk): (8.43)\n",
      "Note that this posterior no longer depends on the model parameters \u0012k\n",
      "because they have been integrated out in the Bayesian setting since\n",
      "p(DjMk) =Z\n",
      "p(Dj\u0012k)p(\u0012kjMk)d\u0012k; (8.44)\n",
      "wherep(\u0012kjMk)is the prior distribution of the model parameters \u0012kof\n",
      "modelMk. The term (8.44) is referred to as the model evidence ormarginal\n",
      "model evidence\n",
      "marginal likelihoodlikelihood . From the posterior in (8.43), we determine the MAP estimate\n",
      "M\u0003= arg max\n",
      "Mkp(MkjD): (8.45)\n",
      "With a uniform prior p(Mk) =1\n",
      "K, which gives every model equal (prior)\n",
      "probability, determining the MAP estimate over models amounts to pick-\n",
      "ing the model that maximizes the model evidence (8.44).\n",
      "Remark (Likelihood and Marginal Likelihood) .There are some important\n",
      "differences between a likelihood and a marginal likelihood (evidence):\n",
      "While the likelihood is prone to overﬁtting, the marginal likelihood is typ-\n",
      "ically not as the model parameters have been marginalized out (i.e., we\n",
      "no longer have to ﬁt the parameters). Furthermore, the marginal likeli-\n",
      "hood automatically embodies a trade-off between model complexity and\n",
      "data ﬁt (Occam’s razor). }\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "8.6 Model Selection 287\n",
      "8.6.3 Bayes Factors for Model Comparison\n",
      "Consider the problem of comparing two probabilistic models M1;M2,\n",
      "given a datasetD. If we compute the posteriors p(M1jD)andp(M2jD),\n",
      "we can compute the ratio of the posteriors\n",
      "p(M1jD)\n",
      "p(M2jD)|{z}\n",
      "posterior odds=p(DjM1)p(M1)\n",
      "p(D)\n",
      "p(DjM2)p(M2)\n",
      "p(D)=p(M1)\n",
      "p(M2)|{z}\n",
      "prior oddsp(DjM1)\n",
      "p(DjM2)|{z}\n",
      "Bayes factor: (8.46)\n",
      "The ratio of the posteriors is also called the posterior odds . The ﬁrst frac- posterior odds\n",
      "tion on the right-hand side of (8.46), the prior odds , measures how much prior odds\n",
      "our prior (initial) beliefs favor M1overM2. The ratio of the marginal like-\n",
      "lihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor\n",
      "and measures how well the data Dis predicted by M1compared to M2.\n",
      "Remark. TheJeffreys-Lindley paradox states that the “Bayes factor always Jeffreys-Lindley\n",
      "paradox favors the simpler model since the probability of the data under a complex\n",
      "model with a diffuse prior will be very small” (Murphy, 2012). Here, a\n",
      "diffuse prior refers to a prior that does not favor speciﬁc models, i.e.,\n",
      "many models are a priori plausible under this prior. }\n",
      "If we choose a uniform prior over models, the prior odds term in (8.46)\n",
      "is1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes\n",
      "factor)\n",
      "p(DjM1)\n",
      "p(DjM2): (8.47)\n",
      "If the Bayes factor is greater than 1, we choose model M1, otherwise\n",
      "modelM2. In a similar way to frequentist statistics, there are guidelines\n",
      "on the size of the ratio that one should consider before ”signiﬁcance” of\n",
      "the result (Jeffreys, 1961).\n",
      "Remark (Computing the Marginal Likelihood) .The marginal likelihood\n",
      "plays an important role in model selection: We need to compute Bayes\n",
      "factors (8.46) and posterior distributions over models (8.43).\n",
      "Unfortunately, computing the marginal likelihood requires us to solve\n",
      "an integral (8.44). This integration is generally analytically intractable,\n",
      "and we will have to resort to approximation techniques, e.g., numerical\n",
      "integration (Stoer and Burlirsch, 2002), stochastic approximations using\n",
      "Monte Carlo (Murphy, 2012), or Bayesian Monte Carlo techniques (O’Hagan,\n",
      "1991; Rasmussen and Ghahramani, 2003).\n",
      "However, there are special cases in which we can solve it. In Section 6.6.1,\n",
      "we discussed conjugate models. If we choose a conjugate parameter prior\n",
      "p(\u0012), we can compute the marginal likelihood in closed form. In Chap-\n",
      "ter 9, we will do exactly this in the context of linear regression. }\n",
      "We have seen a brief introduction to the basic concepts of machine\n",
      "learning in this chapter. For the rest of this part of the book we will see\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "288 When Models Meet Data\n",
      "how the three different ﬂavors of learning in Sections 8.2, 8.3, and 8.4 are\n",
      "applied to the four pillars of machine learning (regression, dimensionality\n",
      "reduction, density estimation, and classiﬁcation).\n",
      "8.6.4 Further Reading\n",
      "We mentioned at the start of the section that there are high-level modeling\n",
      "choices that inﬂuence the performance of the model. Examples include the\n",
      "following:\n",
      "The degree of a polynomial in a regression setting\n",
      "The number of components in a mixture model\n",
      "The network architecture of a (deep) neural network\n",
      "The type of kernel in a support vector machine\n",
      "The dimensionality of the latent space in PCA\n",
      "The learning rate (schedule) in an optimization algorithm\n",
      "In parametric\n",
      "models, the number\n",
      "of parameters is\n",
      "often related to the\n",
      "complexity of the\n",
      "model class.Rasmussen and Ghahramani (2001) showed that the automatic Occam’s\n",
      "razor does not necessarily penalize the number of parameters in a model,\n",
      "but it is active in terms of the complexity of functions. They also showed\n",
      "that the automatic Occam’s razor also holds for Bayesian nonparametric\n",
      "models with many parameters, e.g., Gaussian processes.\n",
      "If we focus on the maximum likelihood estimate, there exist a number of\n",
      "heuristics for model selection that discourage overﬁtting. They are called\n",
      "information criteria, and we choose the model with the largest value. The\n",
      "Akaike information criterion (AIC) (Akaike, 1974) Akaike information\n",
      "criterion\n",
      "logp(xj\u0012)\u0000M (8.48)\n",
      "corrects for the bias of the maximum likelihood estimator by addition of\n",
      "a penalty term to compensate for the overﬁtting of more complex models\n",
      "with lots of parameters. Here, Mis the number of model parameters. The\n",
      "AIC estimates the relative information lost by a given model.\n",
      "TheBayesian information criterion (BIC) (Schwarz, 1978) Bayesian\n",
      "information\n",
      "criterion logp(x) = logZ\n",
      "p(xj\u0012)p(\u0012)d\u0012\u0019logp(xj\u0012)\u00001\n",
      "2MlogN (8.49)\n",
      "can be used for exponential family distributions. Here, Nis the number\n",
      "of data points and Mis the number of parameters. BIC penalizes model\n",
      "complexity more heavily than AIC.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9\n",
      "Linear Regression\n",
      "In the following, we will apply the mathematical concepts from Chap-\n",
      "ters 2, 5, 6, and 7 to solve linear regression (curve ﬁtting) problems. In\n",
      "regression , we aim to ﬁnd a function fthat maps inputs x2RDto corre- regression\n",
      "sponding function values f(x)2R. We assume we are given a set of train-\n",
      "ing inputsxnand corresponding noisy observations yn=f(xn)+\u000f, where\n",
      "\u000fis an i.i.d. random variable that describes measurement/observation\n",
      "noise and potentially unmodeled processes (which we will not consider\n",
      "further in this chapter). Throughout this chapter, we assume zero-mean\n",
      "Gaussian noise. Our task is to ﬁnd a function that not only models the\n",
      "training data, but generalizes well to predicting function values at input\n",
      "locations that are not part of the training data (see Chapter 8). An il-\n",
      "lustration of such a regression problem is given in Figure 9.1. A typical\n",
      "regression setting is given in Figure 9.1(a): For some input values xn, we\n",
      "observe (noisy) function values yn=f(xn) +\u000f. The task is to infer the\n",
      "functionfthat generated the data and generalizes well to function values\n",
      "at new input locations. A possible solution is given in Figure 9.1(b), where\n",
      "we also show three distributions centered at the function values f(x)that\n",
      "represent the noise in the data.\n",
      "Regression is a fundamental problem in machine learning, and regres-\n",
      "sion problems appear in a diverse range of research areas and applica-\n",
      "Figure 9.1\n",
      "(a) Dataset;\n",
      "(b) possible solution\n",
      "to the regression\n",
      "problem.\n",
      "−4−2 0 2 4\n",
      "x−0.4−0.20.00.20.4y\n",
      "(a) Regression problem: observed noisy func-\n",
      "tion values from which we wish to infer the\n",
      "underlying function that generated the data.\n",
      "−4−2 0 2 4\n",
      "x−0.4−0.20.00.20.4y\n",
      "(b) Regression solution: possible function\n",
      "that could have generated the data (blue)\n",
      "with indication of the measurement noise of\n",
      "the function value at the corresponding in-\n",
      "puts (orange distributions).\n",
      "289\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "290 Linear Regression\n",
      "tions, including time-series analysis (e.g., system identiﬁcation), control\n",
      "and robotics (e.g., reinforcement learning, forward/inverse model learn-\n",
      "ing), optimization (e.g., line searches, global optimization), and deep-\n",
      "learning applications (e.g., computer games, speech-to-text translation,\n",
      "image recognition, automatic video annotation). Regression is also a key\n",
      "ingredient of classiﬁcation algorithms. Finding a regression function re-\n",
      "quires solving a variety of problems, including the following:\n",
      "Choice of the model (type) and the parametrization of the regres-\n",
      "sion function. Given a dataset, what function classes (e.g., polynomi- Normally, the type\n",
      "of noise could also\n",
      "be a “model choice”,\n",
      "but we ﬁx the noise\n",
      "to be Gaussian in\n",
      "this chapter.als) are good candidates for modeling the data, and what particular\n",
      "parametrization (e.g., degree of the polynomial) should we choose?\n",
      "Model selection, as discussed in Section 8.6, allows us to compare var-\n",
      "ious models to ﬁnd the simplest model that explains the training data\n",
      "reasonably well.\n",
      "Finding good parameters. Having chosen a model of the regression\n",
      "function, how do we ﬁnd good model parameters? Here, we will need to\n",
      "look at different loss/objective functions (they determine what a “good”\n",
      "ﬁt is) and optimization algorithms that allow us to minimize this loss.\n",
      "Overﬁtting and model selection. Overﬁtting is a problem when the\n",
      "regression function ﬁts the training data “too well” but does not gen-\n",
      "eralize to unseen test data. Overﬁtting typically occurs if the underly-\n",
      "ing model (or its parametrization) is overly ﬂexible and expressive; see\n",
      "Section 8.6. We will look at the underlying reasons and discuss ways to\n",
      "mitigate the effect of overﬁtting in the context of linear regression.\n",
      "Relationship between loss functions and parameter priors. Loss func-\n",
      "tions (optimization objectives) are often motivated and induced by prob-\n",
      "abilistic models. We will look at the connection between loss functions\n",
      "and the underlying prior assumptions that induce these losses.\n",
      "Uncertainty modeling. In any practical setting, we have access to only\n",
      "a ﬁnite, potentially large, amount of (training) data for selecting the\n",
      "model class and the corresponding parameters. Given that this ﬁnite\n",
      "amount of training data does not cover all possible scenarios, we may\n",
      "want to describe the remaining parameter uncertainty to obtain a mea-\n",
      "sure of conﬁdence of the model’s prediction at test time; the smaller the\n",
      "training set, the more important uncertainty modeling. Consistent mod-\n",
      "eling of uncertainty equips model predictions with conﬁdence bounds.\n",
      "In the following, we will be using the mathematical tools from Chap-\n",
      "ters 3, 5, 6 and 7 to solve linear regression problems. We will discuss\n",
      "maximum likelihood and maximum a posteriori (MAP) estimation to ﬁnd\n",
      "optimal model parameters. Using these parameter estimates, we will have\n",
      "a brief look at generalization errors and overﬁtting. Toward the end of\n",
      "this chapter, we will discuss Bayesian linear regression, which allows us to\n",
      "reason about model parameters at a higher level, thereby removing some\n",
      "of the problems encountered in maximum likelihood and MAP estimation.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.1 Problem Formulation 291\n",
      "9.1 Problem Formulation\n",
      "Because of the presence of observation noise, we will adopt a probabilis-\n",
      "tic approach and explicitly model the noise using a likelihood function.\n",
      "More speciﬁcally, throughout this chapter, we consider a regression prob-\n",
      "lem with the likelihood function\n",
      "p(yjx) =N\u0000yjf(x); \u001b2\u0001: (9.1)\n",
      "Here,x2RDare inputs and y2Rare noisy function values (targets).\n",
      "With (9.1), the functional relationship between xandyis given as\n",
      "y=f(x) +\u000f; (9.2)\n",
      "where\u000f\u0018N\u00000; \u001b2\u0001\n",
      "is independent, identically distributed (i.i.d.) Gaus-\n",
      "sian measurement noise with mean 0and variance \u001b2. Our objective is\n",
      "to ﬁnd a function that is close (similar) to the unknown function fthat\n",
      "generated the data and that generalizes well.\n",
      "In this chapter, we focus on parametric models, i.e., we choose a para-\n",
      "metrized function and ﬁnd parameters \u0012that “work well” for modeling the\n",
      "data. For the time being, we assume that the noise variance \u001b2is known\n",
      "and focus on learning the model parameters \u0012. In linear regression, we\n",
      "consider the special case that the parameters \u0012appear linearly in our\n",
      "model. An example of linear regression is given by\n",
      "p(yjx;\u0012) =N\u0000yjx>\u0012; \u001b2\u0001\n",
      "(9.3)\n",
      "()y=x>\u0012+\u000f; \u000f\u0018N\u00000; \u001b2\u0001; (9.4)\n",
      "where\u00122RDare the parameters we seek. The class of functions de-\n",
      "scribed by (9.4) are straight lines that pass through the origin. In (9.4),\n",
      "we chose a parametrization f(x) =x>\u0012. A Dirac delta (delta\n",
      "function) is zero\n",
      "everywhere except\n",
      "at a single point,\n",
      "and its integral is 1.\n",
      "It can be considered\n",
      "a Gaussian in the\n",
      "limit of\u001b2!0.Thelikelihood in (9.3) is the probability density function of yevalu-\n",
      "likelihoodated atx>\u0012. Note that the only source of uncertainty originates from the\n",
      "observation noise (as xand\u0012are assumed known in (9.3)). Without ob-\n",
      "servation noise, the relationship between xandywould be deterministic\n",
      "and (9.3) would be a Dirac delta.\n",
      "Example 9.1\n",
      "Forx;\u00122Rthe linear regression model in (9.4) describes straight lines\n",
      "(linear functions), and the parameter \u0012is the slope of the line. Fig-\n",
      "ure 9.2(a) shows some example functions for different values of \u0012.\n",
      "Linear regression\n",
      "refers to models that\n",
      "are linear in the\n",
      "parameters.The linear regression model in (9.3)–(9.4) is not only linear in the pa-\n",
      "rameters, but also linear in the inputs x. Figure 9.2(a) shows examples\n",
      "of such functions. We will see later that y=\u001e>(x)\u0012for nonlinear trans-\n",
      "formations\u001eis also a linear regression model because “linear regression”\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "292 Linear Regression\n",
      "Figure 9.2 Linear\n",
      "regression example.\n",
      "(a) Example\n",
      "functions that fall\n",
      "into this category;\n",
      "(b) training set;\n",
      "(c) maximum\n",
      "likelihood estimate.\n",
      "−10 0 10\n",
      "x−20020y (a) Example functions (straight\n",
      "lines) that can be described us-\n",
      "ing the linear model in (9.4).\n",
      "−10−5 0 5 10\n",
      "x−10010y\n",
      "(b) Training set.\n",
      "−10−5 0 5 10\n",
      "x−10010y\n",
      " (c) Maximum likelihood esti-\n",
      "mate.\n",
      "refers to models that are “linear in the parameters”, i.e., models that de-\n",
      "scribe a function by a linear combination of input features. Here, a “fea-\n",
      "ture” is a representation \u001e(x)of the inputsx.\n",
      "In the following, we will discuss in more detail how to ﬁnd good pa-\n",
      "rameters\u0012and how to evaluate whether a parameter set “works well”.\n",
      "For the time being, we assume that the noise variance \u001b2is known.\n",
      "9.2 Parameter Estimation\n",
      "Consider the linear regression setting (9.4) and assume we are given a\n",
      "training setD:=f(x1;y1);:::; (xN;yN)gconsisting of Ninputsxn2 training set\n",
      "RDand corresponding observations/targets yn2R,n= 1;:::;N . The Figure 9.3\n",
      "Probabilistic\n",
      "graphical model for\n",
      "linear regression.\n",
      "Observed random\n",
      "variables are\n",
      "shaded,\n",
      "deterministic/\n",
      "known values are\n",
      "without circles.\n",
      "\u0012\n",
      "yn\u001b\n",
      "xn\n",
      "n= 1;:::;Ncorresponding graphical model is given in Figure 9.3. Note that yiandyj\n",
      "are conditionally independent given their respective inputs xi;xjso that\n",
      "the likelihood factorizes according to\n",
      "p(YjX;\u0012) =p(y1;:::;yNjx1;:::;xN;\u0012) (9.5a)\n",
      "=NY\n",
      "n=1p(ynjxn;\u0012) =NY\n",
      "n=1N\u0000ynjx>\n",
      "n\u0012; \u001b2\u0001; (9.5b)\n",
      "where we deﬁned X:=fx1;:::;xNgandY:=fy1;:::;yNgas the sets\n",
      "of training inputs and corresponding targets, respectively. The likelihood\n",
      "and the factors p(ynjxn;\u0012)are Gaussian due to the noise distribution;\n",
      "see (9.3).\n",
      "In the following, we will discuss how to ﬁnd optimal parameters \u0012\u00032\n",
      "RDfor the linear regression model (9.4). Once the parameters \u0012\u0003are\n",
      "found, we can predict function values by using this parameter estimate\n",
      "in (9.4) so that at an arbitrary test input x\u0003the distribution of the corre-\n",
      "sponding target y\u0003is\n",
      "p(y\u0003jx\u0003;\u0012\u0003) =N\u0000y\u0003jx>\n",
      "\u0003\u0012\u0003; \u001b2\u0001: (9.6)\n",
      "In the following, we will have a look at parameter estimation by maxi-\n",
      "mizing the likelihood, a topic that we already covered to some degree in\n",
      "Section 8.3.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.2 Parameter Estimation 293\n",
      "9.2.1 Maximum Likelihood Estimation\n",
      "A widely used approach to ﬁnding the desired parameters \u0012MLismaximum maximum likelihood\n",
      "estimation likelihood estimation , where we ﬁnd parameters \u0012MLthat maximize the\n",
      "likelihood (9.5b). Intuitively, maximizing the likelihood means maximiz- Maximizing the\n",
      "likelihood means\n",
      "maximizing the\n",
      "predictive\n",
      "distribution of the\n",
      "(training) data\n",
      "given the\n",
      "parameters.ing the predictive distribution of the training data given the model param-\n",
      "eters. We obtain the maximum likelihood parameters as\n",
      "\u0012ML= arg max\n",
      "\u0012p(YjX;\u0012): (9.7)\n",
      "The likelihood is not\n",
      "a probability\n",
      "distribution in the\n",
      "parameters.Remark. The likelihood p(yjx;\u0012)is not a probability distribution in \u0012: It\n",
      "is simply a function of the parameters \u0012but does not integrate to 1(i.e.,\n",
      "it is unnormalized), and may not even be integrable with respect to \u0012.\n",
      "However, the likelihood in (9.7) is a normalized probability distribution\n",
      "iny. }\n",
      "To ﬁnd the desired parameters \u0012MLthat maximize the likelihood, we\n",
      "typically perform gradient ascent (or gradient descent on the negative\n",
      "likelihood). In the case of linear regression we consider here, however, Since the logarithm\n",
      "is a (strictly)\n",
      "monotonically\n",
      "increasing function,\n",
      "the optimum of a\n",
      "functionfis\n",
      "identical to the\n",
      "optimum of logf.a closed-form solution exists, which makes iterative gradient descent un-\n",
      "necessary. In practice, instead of maximizing the likelihood directly, we\n",
      "apply the log-transformation to the likelihood function and minimize the\n",
      "negative log-likelihood.\n",
      "Remark (Log-Transformation) .Since the likelihood (9.5b) is a product of\n",
      "NGaussian distributions, the log-transformation is useful since (a) it does\n",
      "not suffer from numerical underﬂow, and (b) the differentiation rules will\n",
      "turn out simpler. More speciﬁcally, numerical underﬂow will be a prob-\n",
      "lem when we multiply Nprobabilities, where Nis the number of data\n",
      "points, since we cannot represent very small numbers, such as 10\u0000256.\n",
      "Furthermore, the log-transform will turn the product into a sum of log-\n",
      "probabilities such that the corresponding gradient is a sum of individual\n",
      "gradients, instead of a repeated application of the product rule (5.46) to\n",
      "compute the gradient of a product of Nterms. }\n",
      "To ﬁnd the optimal parameters \u0012MLof our linear regression problem,\n",
      "we minimize the negative log-likelihood\n",
      "\u0000logp(YjX;\u0012) =\u0000logNY\n",
      "n=1p(ynjxn;\u0012) =\u0000NX\n",
      "n=1logp(ynjxn;\u0012);(9.8)\n",
      "where we exploited that the likelihood (9.5b) factorizes over the number\n",
      "of data points due to our independence assumption on the training set.\n",
      "In the linear regression model (9.4), the likelihood is Gaussian (due to\n",
      "the Gaussian additive noise term), such that we arrive at\n",
      "logp(ynjxn;\u0012) =\u00001\n",
      "2\u001b2(yn\u0000x>\n",
      "n\u0012)2+const; (9.9)\n",
      "where the constant includes all terms independent of \u0012. Using (9.9) in the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "294 Linear Regression\n",
      "negative log-likelihood (9.8), we obtain (ignoring the constant terms)\n",
      "L(\u0012) :=1\n",
      "2\u001b2NX\n",
      "n=1(yn\u0000x>\n",
      "n\u0012)2(9.10a)\n",
      "=1\n",
      "2\u001b2(y\u0000X\u0012)>(y\u0000X\u0012) =1\n",
      "2\u001b2ky\u0000X\u0012k2; (9.10b)\n",
      "where we deﬁne the design matrix X:= [x1;:::;xN]>2RN\u0002Das the The negative\n",
      "log-likelihood\n",
      "function is also\n",
      "called error function .\n",
      "design matrixcollection of training inputs and y:= [y1;:::;yN]>2RNas a vector that\n",
      "collects all training targets. Note that the nth row in the design matrix X\n",
      "corresponds to the training input xn. In (9.10b), we used the fact that the\n",
      "The squared error is\n",
      "often used as a\n",
      "measure of distance.sum of squared errors between the observations ynand the corresponding\n",
      "model prediction x>\n",
      "n\u0012equals the squared distance between yandX\u0012.\n",
      "Recall from\n",
      "Section 3.1 that\n",
      "kxk2=x>xif we\n",
      "choose the dot\n",
      "product as the inner\n",
      "product.With (9.10b), we have now a concrete form of the negative log-likelihood\n",
      "function we need to optimize. We immediately see that (9.10b) is quadratic\n",
      "in\u0012. This means that we can ﬁnd a unique global solution \u0012MLfor mini-\n",
      "mizing the negative log-likelihood L. We can ﬁnd the global optimum by\n",
      "computing the gradient of L, setting it to 0and solving for \u0012.\n",
      "Using the results from Chapter 5, we compute the gradient of Lwith\n",
      "respect to the parameters as\n",
      "dL\n",
      "d\u0012=d\n",
      "d\u0012\u00121\n",
      "2\u001b2(y\u0000X\u0012)>(y\u0000X\u0012)\u0013\n",
      "(9.11a)\n",
      "=1\n",
      "2\u001b2d\n",
      "d\u0012\u0010\n",
      "y>y\u00002y>X\u0012+\u0012>X>X\u0012\u0011\n",
      "(9.11b)\n",
      "=1\n",
      "\u001b2(\u0000y>X+\u0012>X>X)2R1\u0002D: (9.11c)\n",
      "The maximum likelihood estimator \u0012MLsolvesdL\n",
      "d\u0012=0>(necessary opti-\n",
      "mality condition) and we obtain Ignoring the\n",
      "possibility of\n",
      "duplicate data\n",
      "points, rk(X) =D\n",
      "ifN>D, i.e., we\n",
      "do not have more\n",
      "parameters than\n",
      "data points.dL\n",
      "d\u0012=0>(9.11c)()\u0012>\n",
      "MLX>X=y>X (9.12a)\n",
      "()\u0012>\n",
      "ML=y>X(X>X)\u00001(9.12b)\n",
      "()\u0012ML= (X>X)\u00001X>y: (9.12c)\n",
      "We could right-multiply the ﬁrst equation by (X>X)\u00001becauseX>Xis\n",
      "positive deﬁnite if rk(X) =D, where rk(X)denotes the rank of X.\n",
      "Remark. Setting the gradient to 0>is a necessary and sufﬁcient condition,\n",
      "and we obtain a global minimum since the Hessian r2\n",
      "\u0012L(\u0012) =X>X2\n",
      "RD\u0002Dis positive deﬁnite. }\n",
      "Remark. The maximum likelihood solution in (9.12c) requires us to solve\n",
      "a system of linear equations of the form A\u0012=bwithA= (X>X)and\n",
      "b=X>y. }\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.2 Parameter Estimation 295\n",
      "Example 9.2 (Fitting Lines)\n",
      "Let us have a look at Figure 9.2, where we aim to ﬁt a straight line f(x) =\n",
      "\u0012x, where\u0012is an unknown slope, to a dataset using maximum likelihood\n",
      "estimation. Examples of functions in this model class (straight lines) are\n",
      "shown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we ﬁnd\n",
      "the maximum likelihood estimate of the slope parameter \u0012using (9.12c)\n",
      "and obtain the maximum likelihood linear function in Figure 9.2(c).\n",
      "Maximum Likelihood Estimation with Features\n",
      "So far, we considered the linear regression setting described in (9.4),\n",
      "which allowed us to ﬁt straight lines to data using maximum likelihood\n",
      "estimation. However, straight lines are not sufﬁciently expressive when it Linear regression\n",
      "refers to “linear-in-\n",
      "the-parameters”\n",
      "regression models,\n",
      "but the inputs can\n",
      "undergo any\n",
      "nonlinear\n",
      "transformation.comes to ﬁtting more interesting data. Fortunately, linear regression offers\n",
      "us a way to ﬁt nonlinear functions within the linear regression framework:\n",
      "Since “linear regression” only refers to “linear in the parameters”, we can\n",
      "perform an arbitrary nonlinear transformation \u001e(x)of the inputs xand\n",
      "then linearly combine the components of this transformation. The corre-\n",
      "sponding linear regression model is\n",
      "p(yjx;\u0012) =N\u0000yj\u001e>(x)\u0012; \u001b2\u0001\n",
      "()y=\u001e>(x)\u0012+\u000f=K\u00001X\n",
      "k=0\u0012k\u001ek(x) +\u000f;(9.13)\n",
      "where\u001e:RD!RKis a (nonlinear) transformation of the inputs xand\n",
      "\u001ek:RD!Ris thekth component of the feature vector \u001e. Note that the feature vector\n",
      "model parameters \u0012still appear only linearly.\n",
      "Example 9.3 (Polynomial Regression)\n",
      "We are concerned with a regression problem y=\u001e>(x)\u0012+\u000f, wherex2R\n",
      "and\u00122RK. A transformation that is often used in this context is\n",
      "\u001e(x) =2\n",
      "6664\u001e0(x)\n",
      "\u001e1(x)\n",
      "...\n",
      "\u001eK\u00001(x)3\n",
      "7775=2\n",
      "666666641\n",
      "x\n",
      "x2\n",
      "x3\n",
      "...\n",
      "xK\u000013\n",
      "777777752RK: (9.14)\n",
      "This means that we “lift” the original one-dimensional input space into\n",
      "aK-dimensional feature space consisting of all monomials xkfork=\n",
      "0;:::;K\u00001. With these features, we can model polynomials of degree\n",
      "6K\u00001within the framework of linear regression: A polynomial of degree\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "296 Linear Regression\n",
      "K\u00001is\n",
      "f(x) =K\u00001X\n",
      "k=0\u0012kxk=\u001e>(x)\u0012; (9.15)\n",
      "where\u001eis deﬁned in (9.14) and \u0012= [\u00120;:::;\u0012K\u00001]>2RKcontains the\n",
      "(linear) parameters \u0012k.\n",
      "Let us now have a look at maximum likelihood estimation of the param-\n",
      "eters\u0012in the linear regression model (9.13). We consider training inputs\n",
      "xn2RDand targets yn2R,n= 1;:::;N , and deﬁne the feature matrix feature matrix\n",
      "(design matrix ) as design matrix\n",
      "\b:=2\n",
      "64\u001e>(x1)\n",
      "...\n",
      "\u001e>(xN)3\n",
      "75=2\n",
      "6664\u001e0(x1)\u0001\u0001\u0001\u001eK\u00001(x1)\n",
      "\u001e0(x2)\u0001\u0001\u0001\u001eK\u00001(x2)\n",
      "......\n",
      "\u001e0(xN)\u0001\u0001\u0001\u001eK\u00001(xN)3\n",
      "77752RN\u0002K;(9.16)\n",
      "whereij=\u001ej(xi)and\u001ej:RD!R.\n",
      "Example 9.4 (Feature Matrix for Second-order Polynomials)\n",
      "For a second-order polynomial and Ntraining points xn2R;n=\n",
      "1;:::;N , the feature matrix is\n",
      "\b=2\n",
      "66641x1x2\n",
      "1\n",
      "1x2x2\n",
      "2.........\n",
      "1xNx2\n",
      "N3\n",
      "7775: (9.17)\n",
      "With the feature matrixdeﬁned in (9.16), the negative log-likelihood\n",
      "for the linear regression model (9.13) can be written as\n",
      "\u0000logp(YjX;\u0012) =1\n",
      "2\u001b2(y\u0012)>(y\u0012) +const: (9.18)\n",
      "Comparing (9.18) with the negative log-likelihood in (9.10b) for the “fea-\n",
      "ture-free” model, we immediately see we just need to replace Xwit.\n",
      "Since bothXanare independent of the parameters \u0012that we wish to\n",
      "optimize, we arrive immediately at the maximum likelihood estimate maximum likelihood\n",
      "estimate\n",
      "\u0012ML= )\u0000>y (9.19)\n",
      "for the linear regression problem with nonlinear features deﬁned in (9.13).\n",
      "Remark. When we were working without features, we required X>Xto\n",
      "be invertible, which is the case when rk(X) =D, i.e., the columns of X\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.2 Parameter Estimation 297\n",
      "are linearly independent. In (9.19), we therefore require2RK\u0002K\n",
      "to be invertible. This is the case if and only if rk) =K.}\n",
      "Example 9.5 (Maximum Likelihood Polynomial Fit)\n",
      "Figure 9.4\n",
      "Polynomial\n",
      "regression:\n",
      "(a) dataset\n",
      "consisting of\n",
      "(xn;yn)pairs,\n",
      "n= 1;:::; 10;\n",
      "(b) maximum\n",
      "likelihood\n",
      "polynomial of\n",
      "degree 4.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(a) Regression dataset.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (b) Polynomial of degree 4 determined by max-\n",
      "imum likelihood estimation.\n",
      "Consider the dataset in Figure 9.4(a). The dataset consists of N= 10\n",
      "pairs (xn;yn), wherexn\u0018U[\u00005;5]andyn=\u0000sin(xn=5) + cos(xn) +\u000f,\n",
      "where\u000f\u0018N\u00000;0:22\u0001\n",
      ".\n",
      "We ﬁt a polynomial of degree 4using maximum likelihood estimation,\n",
      "i.e., parameters \u0012MLare given in (9.19). The maximum likelihood estimate\n",
      "yields function values \u001e>(x\u0003)\u0012MLat any test location x\u0003. The result is\n",
      "shown in Figure 9.4(b).\n",
      "Estimating the Noise Variance\n",
      "Thus far, we assumed that the noise variance \u001b2is known. However, we\n",
      "can also use the principle of maximum likelihood estimation to obtain the\n",
      "maximum likelihood estimator \u001b2\n",
      "MLfor the noise variance. To do this, we\n",
      "follow the standard procedure: We write down the log-likelihood, com-\n",
      "pute its derivative with respect to \u001b2>0, set it to 0, and solve. The\n",
      "log-likelihood is given by\n",
      "logp(YjX;\u0012;\u001b2) =NX\n",
      "n=1logN\u0000ynj\u001e>(xn)\u0012; \u001b2\u0001\n",
      "(9.20a)\n",
      "=NX\n",
      "n=1\u0012\n",
      "\u00001\n",
      "2log(2\u0019)\u00001\n",
      "2log\u001b2\u00001\n",
      "2\u001b2(yn\u0000\u001e>(xn)\u0012)2\u0013\n",
      "(9.20b)\n",
      "=\u0000N\n",
      "2log\u001b2\u00001\n",
      "2\u001b2NX\n",
      "n=1(yn\u0000\u001e>(xn)\u0012)2\n",
      "|{z}\n",
      "=:s+const: (9.20c)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "298 Linear Regression\n",
      "The partial derivative of the log-likelihood with respect to \u001b2is then\n",
      "@logp(YjX;\u0012;\u001b2)\n",
      "@\u001b2=\u0000N\n",
      "2\u001b2+1\n",
      "2\u001b4s= 0 (9.21a)\n",
      "()N\n",
      "2\u001b2=s\n",
      "2\u001b4(9.21b)\n",
      "so that we identify\n",
      "\u001b2\n",
      "ML=s\n",
      "N=1\n",
      "NNX\n",
      "n=1(yn\u0000\u001e>(xn)\u0012)2: (9.22)\n",
      "Therefore, the maximum likelihood estimate of the noise variance is the\n",
      "empirical mean of the squared distances between the noise-free function\n",
      "values\u001e>(xn)\u0012and the corresponding noisy observations ynat input lo-\n",
      "cationsxn.\n",
      "9.2.2 Overﬁtting in Linear Regression\n",
      "We just discussed how to use maximum likelihood estimation to ﬁt lin-\n",
      "ear models (e.g., polynomials) to data. We can evaluate the quality of\n",
      "the model by computing the error/loss incurred. One way of doing this\n",
      "is to compute the negative log-likelihood (9.10b), which we minimized\n",
      "to determine the maximum likelihood estimator. Alternatively, given that\n",
      "the noise parameter \u001b2is not a free model parameter, we can ignore the\n",
      "scaling by 1=\u001b2, so that we end up with a squared-error-loss function\n",
      "ky\u0012k2. Instead of using this squared loss, we often use the root mean root mean square\n",
      "error square error (RMSE )\n",
      "RMSEr\n",
      "1\n",
      "Nky\u0012k2=vuut1\n",
      "NNX\n",
      "n=1(yn\u0000\u001e>(xn)\u0012)2; (9.23)\n",
      "which (a) allows us to compare errors of datasets with different sizes\n",
      "and (b) has the same scale and the same units as the observed func- The RMSE is\n",
      "normalized. tion values yn. For example, if we ﬁt a model that maps post-codes ( x\n",
      "is given in latitude, longitude) to house prices ( y-values are EUR) then\n",
      "the RMSE is also measured in EUR, whereas the squared error is given\n",
      "in EUR2. If we choose to include the factor \u001b2from the original negative The negative\n",
      "log-likelihood is\n",
      "unitless.log-likelihood (9.10b), then we end up with a unitless objective, i.e., in\n",
      "the preceding example, our objective would no longer be in EUR or EUR2.\n",
      "For model selection (see Section 8.6), we can use the RMSE (or the\n",
      "negative log-likelihood) to determine the best degree of the polynomial by\n",
      "ﬁnding the polynomial degree Mthat minimizes the objective. Given that\n",
      "the polynomial degree is a natural number, we can perform a brute-force\n",
      "search and enumerate all (reasonable) values of M. For a training set of\n",
      "sizeNit is sufﬁcient to test 06M6N\u00001. ForM <N , the maximum\n",
      "likelihood estimator is unique. For M>N, we have more parameters\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.2 Parameter Estimation 299\n",
      "Figure 9.5\n",
      "Maximum\n",
      "likelihood ﬁts for\n",
      "different polynomial\n",
      "degreesM.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "(a)M= 0\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (b)M= 1\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (c)M= 3\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "(d)M= 4\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (e)M= 6\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE (f)M= 9\n",
      "than data points, and would need to solve an underdetermined system of\n",
      "linear equations (in (9.19) would also no longer be invertible) so\n",
      "that there are inﬁnitely many possible maximum likelihood estimators.\n",
      "Figure 9.5 shows a number of polynomial ﬁts determined by maximum\n",
      "likelihood for the dataset from Figure 9.4(a) with N= 10 observations.\n",
      "We notice that polynomials of low degree (e.g., constants ( M= 0) or\n",
      "linear (M= 1)) ﬁt the data poorly and, hence, are poor representations\n",
      "of the true underlying function. For degrees M= 3;:::; 6, the ﬁts look\n",
      "plausible and smoothly interpolate the data. When we go to higher-degree The case of\n",
      "M=N\u00001is\n",
      "extreme in the sense\n",
      "that otherwise the\n",
      "null space of the\n",
      "corresponding\n",
      "system of linear\n",
      "equations would be\n",
      "non-trivial, and we\n",
      "would have\n",
      "inﬁnitely many\n",
      "optimal solutions to\n",
      "the linear regression\n",
      "problem.polynomials, we notice that they ﬁt the data better and better. In the ex-\n",
      "treme case of M=N\u00001 = 9 , the function will pass through every single\n",
      "data point. However, these high-degree polynomials oscillate wildly and\n",
      "are a poor representation of the underlying function that generated the\n",
      "data, such that we suffer from overﬁtting .\n",
      "overﬁtting\n",
      "Note that the noise\n",
      "variance\u001b2>0.Remember that the goal is to achieve good generalization by making\n",
      "accurate predictions for new (unseen) data. We obtain some quantita-\n",
      "tive insight into the dependence of the generalization performance on the\n",
      "polynomial of degree Mby considering a separate test set comprising 200\n",
      "data points generated using exactly the same procedure used to generate\n",
      "the training set. As test inputs, we chose a linear grid of 200points in the\n",
      "interval of [\u00005;5]. For each choice of M, we evaluate the RMSE (9.23) for\n",
      "both the training data and the test data.\n",
      "Looking now at the test error, which is a qualitive measure of the gen-\n",
      "eralization properties of the corresponding polynomial, we notice that ini-\n",
      "tially the test error decreases; see Figure 9.6 (orange). For fourth-order\n",
      "polynomials, the test error is relatively low and stays relatively constant up\n",
      "to degree 5. However, from degree 6onward the test error increases signif-\n",
      "icantly, and high-order polynomials have very bad generalization proper-\n",
      "ties. In this particular example, this also is evident from the corresponding\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "300 Linear Regression\n",
      "Figure 9.6 Training\n",
      "and test error.\n",
      "0 2 4 6 8 10\n",
      "Degree of polynomial0246810RMSETraining error\n",
      "Test error\n",
      "maximum likelihood ﬁts in Figure 9.5. Note that the training error (blue training error\n",
      "curve in Figure 9.6) never increases when the degree of the polynomial in-\n",
      "creases. In our example, the best generalization (the point of the smallest\n",
      "test error ) is obtained for a polynomial of degree M= 4. test error\n",
      "9.2.3 Maximum A Posteriori Estimation\n",
      "We just saw that maximum likelihood estimation is prone to overﬁtting.\n",
      "We often observe that the magnitude of the parameter values becomes\n",
      "relatively large if we run into overﬁtting (Bishop, 2006).\n",
      "To mitigate the effect of huge parameter values, we can place a prior\n",
      "distribution p(\u0012)on the parameters. The prior distribution explicitly en-\n",
      "codes what parameter values are plausible (before having seen any data).\n",
      "For example, a Gaussian prior p(\u0012) =N\u00000;1\u0001\n",
      "on a single parameter\n",
      "\u0012encodes that parameter values are expected lie in the interval [\u00002;2]\n",
      "(two standard deviations around the mean value). Once a dataset X;Y\n",
      "is available, instead of maximizing the likelihood we seek parameters that\n",
      "maximize the posterior distribution p(\u0012jX;Y). This procedure is called\n",
      "maximum a posteriori (MAP) estimation. maximum a\n",
      "posteriori\n",
      "MAPThe posterior over the parameters \u0012, given the training data X;Y, is\n",
      "obtained by applying Bayes’ theorem (Section 6.3) as\n",
      "p(\u0012jX;Y) =p(YjX;\u0012)p(\u0012)\n",
      "p(YjX ): (9.24)\n",
      "Since the posterior explicitly depends on the parameter prior p(\u0012), the\n",
      "prior will have an effect on the parameter vector we ﬁnd as the maximizer\n",
      "of the posterior. We will see this more explicitly in the following. The\n",
      "parameter vector \u0012MAPthat maximizes the posterior (9.24) is the MAP\n",
      "estimate.\n",
      "To ﬁnd the MAP estimate, we follow steps that are similar in ﬂavor\n",
      "to maximum likelihood estimation. We start with the log-transform and\n",
      "compute the log-posterior as\n",
      "logp(\u0012jX;Y) = logp(YjX;\u0012) + logp(\u0012) +const; (9.25)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.2 Parameter Estimation 301\n",
      "where the constant comprises the terms that are independent of \u0012. We see\n",
      "that the log-posterior in (9.25) is the sum of the log-likelihood p(YjX;\u0012)\n",
      "and the log-prior logp(\u0012)so that the MAP estimate will be a “compromise”\n",
      "between the prior (our suggestion for plausible parameter values before\n",
      "observing data) and the data-dependent likelihood.\n",
      "To ﬁnd the MAP estimate \u0012MAP, we minimize the negative log-posterior\n",
      "distribution with respect to \u0012, i.e., we solve\n",
      "\u0012MAP2arg min\n",
      "\u0012f\u0000logp(YjX;\u0012)\u0000logp(\u0012)g: (9.26)\n",
      "The gradient of the negative log-posterior with respect to \u0012is\n",
      "\u0000d logp(\u0012jX;Y)\n",
      "d\u0012=\u0000d logp(YjX;\u0012)\n",
      "d\u0012\u0000d logp(\u0012)\n",
      "d\u0012; (9.27)\n",
      "where we identify the ﬁrst term on the right-hand side as the gradient of\n",
      "the negative log-likelihood from (9.11c).\n",
      "With a (conjugate) Gaussian prior p(\u0012) =N\u00000; b2I\u0001\n",
      "on the parameters\n",
      "\u0012, the negative log-posterior for the linear regression setting (9.13), we\n",
      "obtain the negative log posterior\n",
      "\u0000logp(\u0012jX;Y) =1\n",
      "2\u001b2(y\u0012)>(y\u0012) +1\n",
      "2b2\u0012>\u0012+const:(9.28)\n",
      "Here, the ﬁrst term corresponds to the contribution from the log-likelihood,\n",
      "and the second term originates from the log-prior. The gradient of the log-\n",
      "posterior with respect to the parameters \u0012is then\n",
      "\u0000d logp(\u0012jX;Y)\n",
      "d\u0012=1\n",
      "\u001b2(\u0012\u0000y) +1\n",
      "b2\u0012>: (9.29)\n",
      "We will ﬁnd the MAP estimate \u0012MAPby setting this gradient to 0>and\n",
      "solving for\u0012MAP. We obtain\n",
      "1\n",
      "\u001b2(\u0012\u0000y) +1\n",
      "b2\u0012>=0>(9.30a)\n",
      "()\u0012>\u00121\n",
      "\u001b+1\n",
      "b2I\u0013\n",
      "\u00001\n",
      "\u001b2y=0>(9.30b)\n",
      "()\u0012>\u0012\n",
      "\b+\u001b2\n",
      "b2I\u0013\n",
      "=y (9.30c)\n",
      "()\u0012>=y\u0012\n",
      "\b+\u001b2\n",
      "b2I\u0013\u00001\n",
      "(9.30d)\n",
      "so that the MAP estimate is (by transposing both sides of the last equality)is symmetric,\n",
      "positive semi\n",
      "deﬁnite. The\n",
      "additional term\n",
      "in (9.31) is strictly\n",
      "positive deﬁnite so\n",
      "that the inverse\n",
      "exists.\u0012MAP=\u0012\n",
      "\b+\u001b2\n",
      "b2I\u0013\u00001\n",
      "\b>y: (9.31)\n",
      "Comparing the MAP estimate in (9.31) with the maximum likelihood es-\n",
      "timate in (9.19), we see that the only difference between both solutions\n",
      "is the additional term\u001b2\n",
      "b2Iin the inverse matrix. This term ensures that\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "302 Linear Regression\n",
      "\b+\u001b2\n",
      "b2Iis symmetric and strictly positive deﬁnite (i.e., its inverse\n",
      "exists and the MAP estimate is the unique solution of a system of linear\n",
      "equations). Moreover, it reﬂects the impact of the regularizer.\n",
      "Example 9.6 (MAP Estimation for Polynomial Regression)\n",
      "In the polynomial regression example from Section 9.2.1, we place a Gaus-\n",
      "sian priorp(\u0012) =N\u00000;I\u0001\n",
      "on the parameters \u0012and determine the MAP\n",
      "estimates according to (9.31). In Figure 9.7, we show both the maximum\n",
      "likelihood and the MAP estimates for polynomials of degree 6(left) and\n",
      "degree 8(right). The prior (regularizer) does not play a signiﬁcant role\n",
      "for the low-degree polynomial, but keeps the function relatively smooth\n",
      "for higher-degree polynomials. Although the MAP estimate can push the\n",
      "boundaries of overﬁtting, it is not a general solution to this problem, so\n",
      "we need a more principled approach to tackle overﬁtting.\n",
      "Figure 9.7\n",
      "Polynomial\n",
      "regression:\n",
      "maximum likelihood\n",
      "and MAP estimates.\n",
      "(a) Polynomials of\n",
      "degree 6;\n",
      "(b) polynomials of\n",
      "degree 8.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "MAP\n",
      "(a) Polynomials of degree 6.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "MAP (b) Polynomials of degree 8.\n",
      "9.2.4 MAP Estimation as Regularization\n",
      "Instead of placing a prior distribution on the parameters \u0012, it is also pos-\n",
      "sible to mitigate the effect of overﬁtting by penalizing the amplitude of\n",
      "the parameter by means of regularization . Inregularized least squares , we regularization\n",
      "regularized least\n",
      "squaresconsider the loss function\n",
      "ky\u0012k2+\u0015k\u0012k2\n",
      "2; (9.32)\n",
      "which we minimize with respect to \u0012(see Section 8.2.3). Here, the ﬁrst\n",
      "term is a data-ﬁt term (also called misﬁt term ), which is proportional to data-ﬁt term\n",
      "misﬁt term the negative log-likelihood; see (9.10b). The second term is called the\n",
      "regularizer , and the regularization parameter \u0015>0controls the “strict- regularizer\n",
      "regularization\n",
      "parameterness” of the regularization.\n",
      "Remark. Instead of the Euclidean norm k\u0001k2, we can choose any p-norm\n",
      "k\u0001kpin (9.32). In practice, smaller values for plead to sparser solutions.\n",
      "Here, “sparse” means that many parameter values \u0012d= 0, which is also\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.3 Bayesian Linear Regression 303\n",
      "useful for variable selection. For p= 1, the regularizer is called LASSO LASSO\n",
      "(least absolute shrinkage and selection operator) and was proposed by Tib-\n",
      "shirani (1996). }\n",
      "The regularizer \u0015k\u0012k2\n",
      "2in (9.32) can be interpreted as a negative log-\n",
      "Gaussian prior, which we use in MAP estimation; see (9.26). More specif-\n",
      "ically, with a Gaussian prior p(\u0012) =N\u00000; b2I\u0001\n",
      ", we obtain the negative\n",
      "log-Gaussian prior\n",
      "\u0000logp(\u0012) =1\n",
      "2b2k\u0012k2\n",
      "2+const (9.33)\n",
      "so that for\u0015=1\n",
      "2b2the regularization term and the negative log-Gaussian\n",
      "prior are identical.\n",
      "Given that the regularized least-squares loss function in (9.32) consists\n",
      "of terms that are closely related to the negative log-likelihood plus a neg-\n",
      "ative log-prior, it is not surprising that, when we minimize this loss, we\n",
      "obtain a solution that closely resembles the MAP estimate in (9.31). More\n",
      "speciﬁcally, minimizing the regularized least-squares loss function yields\n",
      "\u0012RLS= +\u0015I)\u0000>y; (9.34)\n",
      "which is identical to the MAP estimate in (9.31) for \u0015=\u001b2\n",
      "b2, where\u001b2is\n",
      "the noise variance and b2the variance of the (isotropic) Gaussian prior\n",
      "p(\u0012) =N\u00000; b2I\u0001\n",
      ". A point estimate is a\n",
      "single speciﬁc\n",
      "parameter value,\n",
      "unlike a distribution\n",
      "over plausible\n",
      "parameter settings.So far, we have covered parameter estimation using maximum likeli-\n",
      "hood and MAP estimation where we found point estimates \u0012\u0003that op-\n",
      "timize an objective function (likelihood or posterior). We saw that both\n",
      "maximum likelihood and MAP estimation can lead to overﬁtting. In the\n",
      "next section, we will discuss Bayesian linear regression, where we use\n",
      "Bayesian inference (Section 8.4) to ﬁnd a posterior distribution over the\n",
      "unknown parameters, which we subsequently use to make predictions.\n",
      "More speciﬁcally, for predictions we will average over all plausible sets of\n",
      "parameters instead of focusing on a point estimate.\n",
      "9.3 Bayesian Linear Regression\n",
      "Previously, we looked at linear regression models where we estimated the\n",
      "model parameters \u0012, e.g., by means of maximum likelihood or MAP esti-\n",
      "mation. We discovered that MLE can lead to severe overﬁtting, in particu-\n",
      "lar, in the small-data regime. MAP addresses this issue by placing a prior\n",
      "on the parameters that plays the role of a regularizer. Bayesian linear\n",
      "regression Bayesian linear regression pushes the idea of the parameter prior a step\n",
      "further and does not even attempt to compute a point estimate of the\n",
      "parameters, but instead the full posterior distribution over the parameters\n",
      "is taken into account when making predictions. This means we do not ﬁt\n",
      "any parameters, but we compute a mean over all plausible parameters\n",
      "settings (according to the posterior).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "304 Linear Regression\n",
      "9.3.1 Model\n",
      "In Bayesian linear regression, we consider the model\n",
      "prior p(\u0012) =N\u0000m0;S0\u0001;\n",
      "likelihood p(yjx;\u0012) =N\u0000yj\u001e>(x)\u0012; \u001b2\u0001;(9.35)\n",
      "where we now explicitly place a Gaussian prior p(\u0012) =N\u0000m0;S0\u0001\n",
      "on\u0012, Figure 9.8\n",
      "Graphical model for\n",
      "Bayesian linear\n",
      "regression.\n",
      "\u0012\n",
      "y\u001b\n",
      "xm0S0which turns the parameter vector into a random variable. This allows us\n",
      "to write down the corresponding graphical model in Figure 9.8, where we\n",
      "made the parameters of the Gaussian prior on \u0012explicit. The full proba-\n",
      "bilistic model, i.e., the joint distribution of observed and unobserved ran-\n",
      "dom variables, yand\u0012, respectively, is\n",
      "p(y;\u0012jx) =p(yjx;\u0012)p(\u0012): (9.36)\n",
      "9.3.2 Prior Predictions\n",
      "In practice, we are usually not so much interested in the parameter values\n",
      "\u0012themselves. Instead, our focus often lies in the predictions we make\n",
      "with those parameter values. In a Bayesian setting, we take the parameter\n",
      "distribution and average over all plausible parameter settings when we\n",
      "make predictions. More speciﬁcally, to make predictions at an input x\u0003,\n",
      "we integrate out \u0012and obtain\n",
      "p(y\u0003jx\u0003) =Z\n",
      "p(y\u0003jx\u0003;\u0012)p(\u0012)d\u0012=E\u0012[p(y\u0003jx\u0003;\u0012)]; (9.37)\n",
      "which we can interpret as the average prediction of y\u0003jx\u0003;\u0012for all plau-\n",
      "sible parameters \u0012according to the prior distribution p(\u0012). Note that pre-\n",
      "dictions using the prior distribution only require us to specify the input\n",
      "x\u0003, but no training data.\n",
      "In our model (9.35), we chose a conjugate (Gaussian) prior on \u0012so\n",
      "that the predictive distribution is Gaussian as well (and can be computed\n",
      "in closed form): With the prior distribution p(\u0012) =N\u0000m0;S0\u0001\n",
      ", we obtain\n",
      "the predictive distribution as\n",
      "p(y\u0003jx\u0003) =N\u0000\u001e>(x\u0003)m0;\u001e>(x\u0003)S0\u001e(x\u0003) +\u001b2\u0001; (9.38)\n",
      "where we exploited that (i) the prediction is Gaussian due to conjugacy\n",
      "(see Section 6.6) and the marginalization property of Gaussians (see Sec-\n",
      "tion 6.5), (ii) the Gaussian noise is independent so that\n",
      "V[y\u0003] =V\u0012[\u001e>(x\u0003)\u0012] +V\u000f[\u000f]; (9.39)\n",
      "and (iii)y\u0003is a linear transformation of \u0012so that we can apply the rules\n",
      "for computing the mean and covariance of the prediction analytically by\n",
      "using (6.50) and (6.51), respectively. In (9.38), the term \u001e>(x\u0003)S0\u001e(x\u0003)\n",
      "in the predictive variance explicitly accounts for the uncertainty associated\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.3 Bayesian Linear Regression 305\n",
      "with the parameters \u0012, whereas\u001b2is the uncertainty contribution due to\n",
      "the measurement noise.\n",
      "If we are interested in predicting noise-free function values f(x\u0003) =\n",
      "\u001e>(x\u0003)\u0012instead of the noise-corrupted targets y\u0003we obtain\n",
      "p(f(x\u0003)) =N\u0000\u001e>(x\u0003)m0;\u001e>(x\u0003)S0\u001e(x\u0003)\u0001; (9.40)\n",
      "which only differs from (9.38) in the omission of the noise variance \u001b2in\n",
      "the predictive variance.\n",
      "Remark (Distribution over Functions) .Since we can represent the distri- The parameter\n",
      "distribution p(\u0012)\n",
      "induces a\n",
      "distribution over\n",
      "functions.butionp(\u0012)using a set of samples \u0012iand every sample \u0012igives rise to a\n",
      "functionfi(\u0001) =\u0012>\n",
      "i\u001e(\u0001), it follows that the parameter distribution p(\u0012)\n",
      "induces a distribution p(f(\u0001))over functions. Here we use the notation (\u0001)\n",
      "to explicitly denote a functional relationship. }\n",
      "Example 9.7 (Prior over Functions)\n",
      "Figure 9.9 Prior\n",
      "over functions.\n",
      "(a) Distribution over\n",
      "functions\n",
      "represented by the\n",
      "mean function\n",
      "(black line) and the\n",
      "marginal\n",
      "uncertainties\n",
      "(shaded),\n",
      "representing the\n",
      "67% and 95%\n",
      "conﬁdence bounds,\n",
      "respectively;\n",
      "(b) samples from\n",
      "the prior over\n",
      "functions, which are\n",
      "induced by the\n",
      "samples from the\n",
      "parameter prior.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y(a) Prior distribution over functions.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y (b) Samples from the prior distribution over\n",
      "functions.\n",
      "Let us consider a Bayesian linear regression problem with polynomials\n",
      "of degree 5. We choose a parameter prior p(\u0012) =N\u00000;1\n",
      "4I\u0001\n",
      ". Figure 9.9\n",
      "visualizes the induced prior distribution over functions (shaded area: dark\n",
      "gray: 67% conﬁdence bound; light gray: 95% conﬁdence bound) induced\n",
      "by this parameter prior, including some function samples from this prior.\n",
      "A function sample is obtained by ﬁrst sampling a parameter vector\n",
      "\u0012i\u0018p(\u0012)and then computing fi(\u0001) =\u0012>\n",
      "i\u001e(\u0001). We used 200input lo-\n",
      "cationsx\u00032[\u00005;5]to which we apply the feature function \u001e(\u0001). The\n",
      "uncertainty (represented by the shaded area) in Figure 9.9 is solely due to\n",
      "the parameter uncertainty because we considered the noise-free predictive\n",
      "distribution (9.40).\n",
      "So far, we looked at computing predictions using the parameter prior\n",
      "p(\u0012). However, when we have a parameter posterior (given some train-\n",
      "ing dataX;Y), the same principles for prediction and inference hold\n",
      "as in (9.37) – we just need to replace the prior p(\u0012)with the posterior\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "306 Linear Regression\n",
      "p(\u0012jX;Y). In the following, we will derive the posterior distribution in\n",
      "detail before using it to make predictions.\n",
      "9.3.3 Posterior Distribution\n",
      "Given a training set of inputs xn2RDand corresponding observations\n",
      "yn2R,n= 1;:::;N , we compute the posterior over the parameters\n",
      "using Bayes’ theorem as\n",
      "p(\u0012jX;Y) =p(YjX;\u0012)p(\u0012)\n",
      "p(YjX ); (9.41)\n",
      "whereXis the set of training inputs and Ythe collection of correspond-\n",
      "ing training targets. Furthermore, p(YjX;\u0012)is the likelihood, p(\u0012)the\n",
      "parameter prior, and\n",
      "p(YjX ) =Z\n",
      "p(YjX;\u0012)p(\u0012)d\u0012=E\u0012[p(YjX;\u0012)] (9.42)\n",
      "themarginal likelihood /evidence , which is independent of the parameters marginal likelihood\n",
      "evidence \u0012and ensures that the posterior is normalized, i.e., it integrates to 1. We\n",
      "The marginal\n",
      "likelihood is the\n",
      "expected likelihood\n",
      "under the parameter\n",
      "prior.can think of the marginal likelihood as the likelihood averaged over all\n",
      "possible parameter settings (with respect to the prior distribution p(\u0012)).\n",
      "Theorem 9.1 (Parameter Posterior) .In our model (9.35) , the parameter\n",
      "posterior (9.41) can be computed in closed form as\n",
      "p(\u0012jX;Y) =N\u0000\u0012jmN;SN\u0001; (9.43a)\n",
      "SN= (S\u00001\n",
      "0+\u001b\u0000)\u00001; (9.43b)\n",
      "mN=SN(S\u00001\n",
      "0m0+\u001b\u0000>y); (9.43c)\n",
      "where the subscript Nindicates the size of the training set.\n",
      "Proof Bayes’ theorem tells us that the posterior p(\u0012jX;Y)is propor-\n",
      "tional to the product of the likelihood p(YjX;\u0012)and the prior p(\u0012):\n",
      "Posterior p(\u0012jX;Y) =p(YjX;\u0012)p(\u0012)\n",
      "p(YjX )(9.44a)\n",
      "Likelihood p(YjX;\u0012) =N\u0000y\u0012; \u001b2I\u0001\n",
      "(9.44b)\n",
      "Prior p(\u0012) =N\u0000\u0012jm0;S0\u0001: (9.44c)\n",
      "Instead of looking at the product of the prior and the likelihood, we\n",
      "can transform the problem into log-space and solve for the mean and\n",
      "covariance of the posterior by completing the squares.\n",
      "The sum of the log-prior and the log-likelihood is\n",
      "logN\u0000y\u0012; \u001b2I\u0001+ logN\u0000\u0012jm0;S0\u0001\n",
      "(9.45a)\n",
      "=\u00001\n",
      "2\u0000\u001b\u00002(y\u0012)>(y\u0012) + (\u0012\u0000m0)>S\u00001\n",
      "0(\u0012\u0000m0)\u0001+const\n",
      "(9.45b)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.3 Bayesian Linear Regression 307\n",
      "where the constant contains terms independent of \u0012. We will ignore the\n",
      "constant in the following. We now factorize (9.45b), which yields\n",
      "\u00001\n",
      "2\u0000\u001b\u00002y>y\u00002\u001b\u00002y\u0012+\u0012>\u001b\u0000\u0012+\u0012>S\u00001\n",
      "0\u0012\n",
      "\u00002m>\n",
      "0S\u00001\n",
      "0\u0012+m>\n",
      "0S\u00001\n",
      "0m0\u0001(9.46a)\n",
      "=\u00001\n",
      "2\u0000\u0012>(\u001b\u0000+S\u00001\n",
      "0)\u0012\u00002(\u001b\u0000>y+S\u00001\n",
      "0m0)>\u0012\u0001+const;\n",
      "(9.46b)\n",
      "where the constant contains the black terms in (9.46a), which are inde-\n",
      "pendent of\u0012. The orange terms are terms that are linear in \u0012, and the\n",
      "blue terms are the ones that are quadratic in \u0012. Inspecting (9.46b), we\n",
      "ﬁnd that this equation is quadratic in \u0012. The fact that the unnormalized\n",
      "log-posterior distribution is a (negative) quadratic form implies that the\n",
      "posterior is Gaussian, i.e.,\n",
      "p(\u0012jX;Y) = exp(log p(\u0012jX;Y))/exp(logp(YjX;\u0012) + logp(\u0012))\n",
      "(9.47a)\n",
      "/exp\u0010\n",
      "\u00001\n",
      "2\u0000\u0012>(\u001b\u0000+S\u00001\n",
      "0)\u0012\u00002(\u001b\u0000>y+S\u00001\n",
      "0m0)>\u0012\u0001\u0011\n",
      ";\n",
      "(9.47b)\n",
      "where we used (9.46b) in the last expression.\n",
      "The remaining task is it to bring this (unnormalized) Gaussian into the\n",
      "form that is proportional to N\u0000\u0012jmN;SN\u0001\n",
      ", i.e., we need to identify the\n",
      "meanmNand the covariance matrix SN. To do this, we use the concept\n",
      "ofcompleting the squares . The desired log-posterior is completing the\n",
      "squares\n",
      "logN\u0000\u0012jmN;SN\u0001=\u00001\n",
      "2(\u0012\u0000mN)>S\u00001\n",
      "N(\u0012\u0000mN) +const (9.48a)\n",
      "=\u00001\n",
      "2\u0000\u0012>S\u00001\n",
      "N\u0012\u00002m>\n",
      "NS\u00001\n",
      "N\u0012+m>\n",
      "NS\u00001\n",
      "NmN\u0001: (9.48b)\n",
      "Here, we factorized the quadratic form (\u0012\u0000mN)>S\u00001\n",
      "N(\u0012\u0000mN)into a Sincep(\u0012jX;Y) =\n",
      "N\u0000\n",
      "mN;SN\u0001\n",
      ", it\n",
      "holds that\n",
      "\u0012MAP=mN.term that is quadratic in \u0012alone (blue), a term that is linear in \u0012(orange),\n",
      "and a constant term (black). This allows us now to ﬁnd SNandmNby\n",
      "matching the colored expressions in (9.46b) and (9.48b), which yields\n",
      "S\u00001\n",
      "N>\u001b\u00002+S\u00001\n",
      "0 (9.49a)\n",
      "()SN= (\u001b\u0000+S\u00001\n",
      "0)\u00001(9.49b)\n",
      "and\n",
      "m>\n",
      "NS\u00001\n",
      "N= (\u001b\u0000>y+S\u00001\n",
      "0m0)>(9.50a)\n",
      "()mN=SN(\u001b\u0000>y+S\u00001\n",
      "0m0): (9.50b)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "308 Linear Regression\n",
      "Remark (General Approach to Completing the Squares) .If we are given\n",
      "an equation\n",
      "x>Ax\u00002a>x+const 1; (9.51)\n",
      "whereAis symmetric and positive deﬁnite, which we wish to bring into\n",
      "the form\n",
      "(x\u0000\u0016)>\u0006(x\u0000\u0016) +const 2; (9.52)\n",
      "we can do this by setting\n",
      "\u0006:=A; (9.53)\n",
      "\u0016:=\u0006\u00001a (9.54)\n",
      "and const 2=const 1\u0000\u0016>\u0006\u0016. }\n",
      "We can see that the terms inside the exponential in (9.47b) are of the\n",
      "form (9.51) with\n",
      "A:=\u001b\u0000+S\u00001\n",
      "0; (9.55)\n",
      "a:=\u001b\u0000>y+S\u00001\n",
      "0m0: (9.56)\n",
      "SinceA;acan be difﬁcult to identify in equations like (9.46a), it is of-\n",
      "ten helpful to bring these equations into the form (9.51) that decouples\n",
      "quadratic term, linear terms, and constants, which simpliﬁes ﬁnding the\n",
      "desired solution.\n",
      "9.3.4 Posterior Predictions\n",
      "In (9.37), we computed the predictive distribution of y\u0003at a test input\n",
      "x\u0003using the parameter prior p(\u0012). In principle, predicting with the pa-\n",
      "rameter posterior p(\u0012jX;Y)is not fundamentally different given that\n",
      "in our conjugate model the prior and posterior are both Gaussian (with\n",
      "different parameters). Therefore, by following the same reasoning as in\n",
      "Section 9.3.2, we obtain the (posterior) predictive distribution\n",
      "p(y\u0003jX;Y;x\u0003) =Z\n",
      "p(y\u0003jx\u0003;\u0012)p(\u0012jX;Y)d\u0012 (9.57a)\n",
      "=Z\n",
      "N\u0000y\u0003j\u001e>(x\u0003)\u0012; \u001b2\u0001N\u0000\u0012jmN;SN\u0001d\u0012(9.57b)\n",
      "=N\u0000y\u0003j\u001e>(x\u0003)mN;\u001e>(x\u0003)SN\u001e(x\u0003) +\u001b2\u0001:(9.57c)\n",
      "The term\u001e>(x\u0003)SN\u001e(x\u0003)reﬂects the posterior uncertainty associated E[y\u0003jX;Y;x\u0003] =\n",
      "\u001e>(x\u0003)mN=\n",
      "\u001e>(x\u0003)\u0012MAP.with the parameters \u0012. Note thatSNdepends on the training inputs\n",
      "through; see (9.43b). The predictive mean \u001e>(x\u0003)mNcoincides with\n",
      "the predictions made with the MAP estimate \u0012MAP.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.3 Bayesian Linear Regression 309\n",
      "Remark (Marginal Likelihood and Posterior Predictive Distribution) .By\n",
      "replacing the integral in (9.57a), the predictive distribution can be equiv-\n",
      "alently written as the expectation E\u0012jX;Y[p(y\u0003jx\u0003;\u0012)], where the expec-\n",
      "tation is taken with respect to the parameter posterior p(\u0012jX;Y).\n",
      "Writing the posterior predictive distribution in this way highlights a\n",
      "close resemblance to the marginal likelihood (9.42). The key difference\n",
      "between the marginal likelihood and the posterior predictive distribution\n",
      "are (i) the marginal likelihood can be thought of predicting the training\n",
      "targetsyand not the test targets y\u0003, and (ii) the marginal likelihood av-\n",
      "erages with respect to the parameter prior and not the parameter poste-\n",
      "rior. }\n",
      "Remark (Mean and Variance of Noise-Free Function Values) .In many\n",
      "cases, we are not interested in the predictive distribution p(y\u0003jX;Y;x\u0003)\n",
      "of a (noisy) observation y\u0003. Instead, we would like to obtain the distribu-\n",
      "tion of the (noise-free) function values f(x\u0003) =\u001e>(x\u0003)\u0012. We determine\n",
      "the corresponding moments by exploiting the properties of means and\n",
      "variances, which yields\n",
      "E[f(x\u0003)jX;Y] =E\u0012[\u001e>(x\u0003)\u0012jX;Y] =\u001e>(x\u0003)E\u0012[\u0012jX;Y]\n",
      "=\u001e>(x\u0003)mN=m>\n",
      "N\u001e(x\u0003);(9.58)\n",
      "V\u0012[f(x\u0003)jX;Y] =V\u0012[\u001e>(x\u0003)\u0012jX;Y]\n",
      "=\u001e>(x\u0003)V\u0012[\u0012jX;Y]\u001e(x\u0003)\n",
      "=\u001e>(x\u0003)SN\u001e(x\u0003):(9.59)\n",
      "We see that the predictive mean is the same as the predictive mean for\n",
      "noisy observations as the noise has mean 0, and the predictive variance\n",
      "only differs by \u001b2, which is the variance of the measurement noise: When\n",
      "we predict noisy function values, we need to include \u001b2as a source of\n",
      "uncertainty, but this term is not needed for noise-free predictions. Here,\n",
      "the only remaining uncertainty stems from the parameter posterior. }Integrating out\n",
      "parameters induces\n",
      "a distribution over\n",
      "functions.Remark (Distribution over Functions) .The fact that we integrate out the\n",
      "parameters\u0012induces a distribution over functions: If we sample \u0012i\u0018\n",
      "p(\u0012jX;Y)from the parameter posterior, we obtain a single function re-\n",
      "alization\u0012>\n",
      "i\u001e(\u0001). The mean function , i.e., the set of all expected function mean function\n",
      "values E\u0012[f(\u0001)j\u0012;X;Y], of this distribution over functions is m>\n",
      "N\u001e(\u0001).\n",
      "The (marginal) variance, i.e., the variance of the function f(\u0001), is given by\n",
      "\u001e>(\u0001)SN\u001e(\u0001). }\n",
      "Example 9.8 (Posterior over Functions)\n",
      "Let us revisit the Bayesian linear regression problem with polynomials\n",
      "of degree 5. We choose a parameter prior p(\u0012) =N\u00000;1\n",
      "4I\u0001\n",
      ". Figure 9.9\n",
      "visualizes the prior over functions induced by the parameter prior and\n",
      "sample functions from this prior.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "310 Linear Regression\n",
      "Figure 9.10 shows the posterior over functions that we obtain via\n",
      "Bayesian linear regression. The training dataset is shown in panel (a);\n",
      "panel (b) shows the posterior distribution over functions, including the\n",
      "functions we would obtain via maximum likelihood and MAP estimation.\n",
      "The function we obtain using the MAP estimate also corresponds to the\n",
      "posterior mean function in the Bayesian linear regression setting. Panel (c)\n",
      "shows some plausible realizations (samples) of functions under that pos-\n",
      "terior over functions.\n",
      "Figure 9.10\n",
      "Bayesian linear\n",
      "regression and\n",
      "posterior over\n",
      "functions.\n",
      "(a) training data;\n",
      "(b) posterior\n",
      "distribution over\n",
      "functions;\n",
      "(c) Samples from\n",
      "the posterior over\n",
      "functions.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(a) Training data.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "MAP\n",
      "BLR (b) Posterior over functions rep-\n",
      "resented by the marginal uncer-\n",
      "tainties (shaded) showing the\n",
      "67% and 95% predictive con-\n",
      "ﬁdence bounds, the maximum\n",
      "likelihood estimate (MLE) and\n",
      "the MAP estimate (MAP), the\n",
      "latter of which is identical to\n",
      "the posterior mean function.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(c) Samples from the posterior\n",
      "over functions, which are in-\n",
      "duced by the samples from the\n",
      "parameter posterior.\n",
      "Figure 9.11 shows some posterior distributions over functions induced\n",
      "by the parameter posterior. For different polynomial degrees M, the left\n",
      "panels show the maximum likelihood function \u0012>\n",
      "ML\u001e(\u0001), the MAP func-\n",
      "tion\u0012>\n",
      "MAP\u001e(\u0001)(which is identical to the posterior mean function), and the\n",
      "67% and 95% predictive conﬁdence bounds obtained by Bayesian linear\n",
      "regression, represented by the shaded areas.\n",
      "The right panels show samples from the posterior over functions: Here,\n",
      "we sampled parameters \u0012ifrom the parameter posterior and computed\n",
      "the function\u001e>(x\u0003)\u0012i, which is a single realization of a function under\n",
      "the posterior distribution over functions. For low-order polynomials, the\n",
      "parameter posterior does not allow the parameters to vary much: The\n",
      "sampled functions are nearly identical. When we make the model more\n",
      "ﬂexible by adding more parameters (i.e., we end up with a higher-order\n",
      "polynomial), these parameters are not sufﬁciently constrained by the pos-\n",
      "terior, and the sampled functions can be easily visually separated. We also\n",
      "see in the corresponding panels on the left how the uncertainty increases,\n",
      "especially at the boundaries.\n",
      "Although for a seventh-order polynomial the MAP estimate yields a rea-\n",
      "sonable ﬁt, the Bayesian linear regression model additionally tells us that\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.3 Bayesian Linear Regression 311\n",
      "Figure 9.11\n",
      "Bayesian linear\n",
      "regression. Left\n",
      "panels: Shaded\n",
      "areas indicate the\n",
      "67% (dark gray)\n",
      "and 95% (light\n",
      "gray) predictive\n",
      "conﬁdence bounds.\n",
      "The mean of the\n",
      "Bayesian linear\n",
      "regression model\n",
      "coincides with the\n",
      "MAP estimate. The\n",
      "predictive\n",
      "uncertainty is the\n",
      "sum of the noise\n",
      "term and the\n",
      "posterior parameter\n",
      "uncertainty, which\n",
      "depends on the\n",
      "location of the test\n",
      "input. Right panels:\n",
      "sampled functions\n",
      "from the posterior\n",
      "distribution.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "MAP\n",
      "BLR\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(a) Posterior distribution for polynomials of degree M= 3(left) and samples from the pos-\n",
      "terior over functions (right).\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "MAP\n",
      "BLR\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(b) Posterior distribution for polynomials of degree M= 5 (left) and samples from the\n",
      "posterior over functions (right).\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Training data\n",
      "MLE\n",
      "MAP\n",
      "BLR\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(c) Posterior distribution for polynomials of degree M= 7(left) and samples from the pos-\n",
      "terior over functions (right).\n",
      "the posterior uncertainty is huge. This information can be critical when\n",
      "we use these predictions in a decision-making system, where bad deci-\n",
      "sions can have signiﬁcant consequences (e.g., in reinforcement learning\n",
      "or robotics).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "312 Linear Regression\n",
      "9.3.5 Computing the Marginal Likelihood\n",
      "In Section 8.6.2, we highlighted the importance of the marginal likelihood\n",
      "for Bayesian model selection. In the following, we compute the marginal\n",
      "likelihood for Bayesian linear regression with a conjugate Gaussian prior\n",
      "on the parameters, i.e., exactly the setting we have been discussing in this\n",
      "chapter.\n",
      "Just to recap, we consider the following generative process:\n",
      "\u0012\u0018N\u0000m0;S0\u0001\n",
      "(9.60a)\n",
      "ynjxn;\u0012\u0018N\u0000x>\n",
      "n\u0012; \u001b2\u0001; (9.60b)\n",
      "n= 1;:::;N . The marginal likelihood is given by The marginal\n",
      "likelihood can be\n",
      "interpreted as the\n",
      "expected likelihood\n",
      "under the prior, i.e.,\n",
      "E\u0012[p(YjX;\u0012)].p(YjX ) =Z\n",
      "p(YjX;\u0012)p(\u0012)d\u0012 (9.61a)\n",
      "=Z\n",
      "N\u0000yjX\u0012; \u001b2I\u0001N\u0000\u0012jm0;S0\u0001d\u0012; (9.61b)\n",
      "where we integrate out the model parameters \u0012. We compute the marginal\n",
      "likelihood in two steps: First, we show that the marginal likelihood is\n",
      "Gaussian (as a distribution in y); second, we compute the mean and co-\n",
      "variance of this Gaussian.\n",
      "1. The marginal likelihood is Gaussian: From Section 6.5.2, we know that\n",
      "(i) the product of two Gaussian random variables is an (unnormalized)\n",
      "Gaussian distribution, and (ii) a linear transformation of a Gaussian\n",
      "random variable is Gaussian distributed. In (9.61b), we require a linear\n",
      "transformation to bring N\u0000yjX\u0012; \u001b2I\u0001\n",
      "into the formN\u0000\u0012j\u0016;\u0006\u0001\n",
      "for\n",
      "some\u0016;\u0006. Once this is done, the integral can be solved in closed form.\n",
      "The result is the normalizing constant of the product of the two Gaus-\n",
      "sians. The normalizing constant itself has Gaussian shape; see (6.76).\n",
      "2. Mean and covariance. We compute the mean and covariance matrix\n",
      "of the marginal likelihood by exploiting the standard results for means\n",
      "and covariances of afﬁne transformations of random variables; see Sec-\n",
      "tion 6.4.4. The mean of the marginal likelihood is computed as\n",
      "E[YjX ] =E\u0012;\u000f[X\u0012+\u000f] =XE\u0012[\u0012] =Xm 0: (9.62)\n",
      "Note that\u000f\u0018N\u00000; \u001b2I\u0001\n",
      "is a vector of i.i.d. random variables. The\n",
      "covariance matrix is given as\n",
      "Cov[YjX] = Cov\u0012;\u000f[X\u0012+\u000f] = Cov\u0012[X\u0012] +\u001b2I (9.63a)\n",
      "=XCov\u0012[\u0012]X>+\u001b2I=XS 0X>+\u001b2I:(9.63b)\n",
      "Hence, the marginal likelihood is\n",
      "p(YjX ) = (2\u0019)\u0000N\n",
      "2det(XS 0X>+\u001b2I)\u00001\n",
      "2 (9.64a)\n",
      "\u0001exp\u0000\u00001\n",
      "2(y\u0000Xm 0)>(XS 0X>+\u001b2I)\u00001(y\u0000Xm 0)\u0001\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.4 Maximum Likelihood as Orthogonal Projection 313\n",
      "Figure 9.12\n",
      "Geometric\n",
      "interpretation of\n",
      "least squares.\n",
      "(a) Dataset;\n",
      "(b) maximum\n",
      "likelihood solution\n",
      "interpreted as a\n",
      "projection.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "(a) Regression dataset consisting of noisy ob-\n",
      "servationsyn(blue) of function values f(xn)\n",
      "at input locations xn.\n",
      "−4−2 0 2 4\n",
      "x−4−2024y\n",
      "Projection\n",
      "Observations\n",
      "Maximum likelihood estimate(b) The orange dots are the projections of\n",
      "the noisy observations (blue dots) onto the\n",
      "line\u0012MLx. The maximum likelihood solution to\n",
      "a linear regression problem ﬁnds a subspace\n",
      "(line) onto which the overall projection er-\n",
      "ror (orange lines) of the observations is mini-\n",
      "mized.\n",
      "=N\u0000yjXm 0;XS 0X>+\u001b2I\u0001: (9.64b)\n",
      "Given the close connection with the posterior predictive distribution (see\n",
      "Remark on Marginal Likelihood and Posterior Predictive Distribution ear-\n",
      "lier in this section), the functional form of the marginal likelihood should\n",
      "not be too surprising.\n",
      "9.4 Maximum Likelihood as Orthogonal Projection\n",
      "Having crunched through much algebra to derive maximum likelihood\n",
      "and MAP estimates, we will now provide a geometric interpretation of\n",
      "maximum likelihood estimation. Let us consider a simple linear regression\n",
      "setting\n",
      "y=x\u0012+\u000f; \u000f\u0018N\u00000; \u001b2\u0001; (9.65)\n",
      "in which we consider linear functions f:R!Rthat go through the\n",
      "origin (we omit features here for clarity). The parameter \u0012determines the\n",
      "slope of the line. Figure 9.12(a) shows a one-dimensional dataset.\n",
      "With a training data set f(x1;y1);:::; (xN;yN)gwe recall the results\n",
      "from Section 9.2.1 and obtain the maximum likelihood estimator for the\n",
      "slope parameter as\n",
      "\u0012ML= (X>X)\u00001X>y=X>y\n",
      "X>X2R; (9.66)\n",
      "whereX= [x1;:::;xN]>2RN,y= [y1;:::;yN]>2RN.\n",
      "This means for the training inputs Xwe obtain the optimal (maximum\n",
      "likelihood) reconstruction of the training targets as\n",
      "X\u0012ML=XX>y\n",
      "X>X=XX>\n",
      "X>Xy; (9.67)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "314 Linear Regression\n",
      "i.e., we obtain the approximation with the minimum least-squares error\n",
      "betweenyandX\u0012.\n",
      "As we are looking for a solution of y=X\u0012, we can think of linear\n",
      "regression as a problem for solving systems of linear equations. There- Linear regression\n",
      "can be thought of as\n",
      "a method for solving\n",
      "systems of linear\n",
      "equations.fore, we can relate to concepts from linear algebra and analytic geometry\n",
      "that we discussed in Chapters 2 and 3. In particular, looking carefully\n",
      "at (9.67) we see that the maximum likelihood estimator \u0012MLin our ex-\n",
      "ample from (9.65) effectively does an orthogonal projection of yonto\n",
      "the one-dimensional subspace spanned by X. Recalling the results on or- Maximum\n",
      "likelihood linear\n",
      "regression performs\n",
      "an orthogonal\n",
      "projection.thogonal projections from Section 3.8, we identifyXX>\n",
      "X>Xas the projection\n",
      "matrix,\u0012MLas the coordinates of the projection onto the one-dimensional\n",
      "subspace of RNspanned byXandX\u0012MLas the orthogonal projection of\n",
      "yonto this subspace.\n",
      "Therefore, the maximum likelihood solution provides also a geometri-\n",
      "cally optimal solution by ﬁnding the vectors in the subspace spanned by\n",
      "Xthat are “closest” to the corresponding observations y, where “clos-\n",
      "est” means the smallest (squared) distance of the function values ynto\n",
      "xn\u0012. This is achieved by orthogonal projections. Figure 9.12(b) shows the\n",
      "projection of the noisy observations onto the subspace that minimizes the\n",
      "squared distance between the original dataset and its projection (note that\n",
      "thex-coordinate is ﬁxed), which corresponds to the maximum likelihood\n",
      "solution.\n",
      "In the general linear regression case where\n",
      "y=\u001e>(x)\u0012+\u000f; \u000f\u0018N\u00000; \u001b2\u0001\n",
      "(9.68)\n",
      "with vector-valued features \u001e(x)2RK, we again can interpret the maxi-\n",
      "mum likelihood result\n",
      "y\u0012ML; (9.69)\n",
      "\u0012ML= )\u0000>y (9.70)\n",
      "as a projection onto a K-dimensional subspace of RN, which is spanned\n",
      "by the columns of the feature matrix; see Section 3.8.2.\n",
      "If the feature functions \u001ekthat we use to construct the feature ma-\n",
      "triare orthonormal (see Section 3.7), we obtain a special case where\n",
      "the columns ofform an orthonormal basis (see Section 3.5), such that\n",
      "\b=I. This will then lead to the projection\n",
      "\b)\u0000>>y= KX\n",
      "k=1\u001ek\u001e>\n",
      "k!\n",
      "y (9.71)\n",
      "so that the maximum likelihood projection is simply the sum of projections\n",
      "ofyonto the individual basis vectors \u001ek, i.e., the columns of. Further-\n",
      "more, the coupling between different features has disappeared due to the\n",
      "orthogonality of the basis. Many popular basis functions in signal process-\n",
      "ing, such as wavelets and Fourier bases, are orthogonal basis functions.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "9.5 Further Reading 315\n",
      "When the basis is not orthogonal, one can convert a set of linearly inde-\n",
      "pendent basis functions to an orthogonal basis by using the Gram-Schmidt\n",
      "process; see Section 3.8.3 and (Strang, 2003).\n",
      "9.5 Further Reading\n",
      "In this chapter, we discussed linear regression for Gaussian likelihoods\n",
      "and conjugate Gaussian priors on the parameters of the model. This al-\n",
      "lowed for closed-form Bayesian inference. However, in some applications\n",
      "we may want to choose a different likelihood function. For example, in\n",
      "a binary classiﬁcation setting, we observe only two possible (categorical) classiﬁcation\n",
      "outcomes, and a Gaussian likelihood is inappropriate in this setting. In-\n",
      "stead, we can choose a Bernoulli likelihood that will return a probability of\n",
      "the predicted label to be 1(or0). We refer to the books by Barber (2012),\n",
      "Bishop (2006), and Murphy (2012) for an in-depth introduction to classiﬁ-\n",
      "cation problems. A different example where non-Gaussian likelihoods are\n",
      "important is count data. Counts are non-negative integers, and in this case\n",
      "a Binomial or Poisson likelihood would be a better choice than a Gaussian.\n",
      "All these examples fall into the category of generalized linear models , a ﬂex- generalized linear\n",
      "model ible generalization of linear regression that allows for response variables\n",
      "that have error distributions other than a Gaussian distribution. The GLM Generalized linear\n",
      "models are the\n",
      "building blocks of\n",
      "deep neural\n",
      "networks.generalizes linear regression by allowing the linear model to be related\n",
      "to the observed values via a smooth and invertible function \u001b(\u0001)that may\n",
      "be nonlinear so that y=\u001b(f(x)), wheref(x) =\u0012>\u001e(x)is the linear\n",
      "regression model from (9.13). We can therefore think of a generalized\n",
      "linear model in terms of function composition y=\u001b\u000ef, wherefis a\n",
      "linear regression model and \u001bthe activation function. Note that although\n",
      "we are talking about “generalized linear models”, the outputs yare no\n",
      "longer linear in the parameters \u0012. Inlogistic regression , we choose the logistic regression\n",
      "logistic sigmoid \u001b(f) =1\n",
      "1+exp(\u0000f)2[0;1], which can be interpreted as the logistic sigmoid\n",
      "probability of observing y= 1of a Bernoulli random variable y2f0;1g.\n",
      "The function \u001b(\u0001)is called transfer function oractivation function , and its transfer function\n",
      "activation function inverse is called the canonical link function . From this perspective, it is\n",
      "canonical link\n",
      "function\n",
      "For ordinary linear\n",
      "regression the\n",
      "activation function\n",
      "would simply be the\n",
      "identity.also clear that generalized linear models are the building blocks of (deep)\n",
      "feedforward neural networks: If we consider a generalized linear model\n",
      "y=\u001b(Ax+b), whereAis a weight matrix and ba bias vector, we iden-\n",
      "tify this generalized linear model as a single-layer neural network with\n",
      "activation function \u001b(\u0001). We can now recursively compose these functions\n",
      "via\n",
      "A great post on the\n",
      "relation between\n",
      "GLMs and deep\n",
      "networks is\n",
      "available at\n",
      "https://tinyurl.\n",
      "com/glm-dnn .xk+1=fk(xk)\n",
      "fk(xk) =\u001bk(Akxk+bk)(9.72)\n",
      "fork= 0;:::;K\u00001, wherex0are the input features and xK=yare\n",
      "the observed outputs, such that fK\u00001\u000e\u0001\u0001\u0001\u000ef0is aK-layer deep neural\n",
      "network. Therefore, the building blocks of this deep neural network are\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "316 Linear Regression\n",
      "the generalized linear models deﬁned in (9.72). Neural networks (Bishop,\n",
      "1995; Goodfellow et al., 2016) are signiﬁcantly more expressive and ﬂexi-\n",
      "ble than linear regression models. However, maximum likelihood parame-\n",
      "ter estimation is a non-convex optimization problem, and marginalization\n",
      "of the parameters in a fully Bayesian setting is analytically intractable.\n",
      "We brieﬂy hinted at the fact that a distribution over parameters in-\n",
      "duces a distribution over regression functions. Gaussian processes (Ras- Gaussian process\n",
      "mussen and Williams, 2006) are regression models where the concept of\n",
      "a distribution over function is central. Instead of placing a distribution\n",
      "over parameters, a Gaussian process places a distribution directly on the\n",
      "space of functions without the “detour” via the parameters. To do so, the\n",
      "Gaussian process exploits the kernel trick (Sch¨olkopf and Smola, 2002), kernel trick\n",
      "which allows us to compute inner products between two function values\n",
      "f(xi);f(xj)only by looking at the corresponding input xi;xj. A Gaus-\n",
      "sian process is closely related to both Bayesian linear regression and sup-\n",
      "port vector regression but can also be interpreted as a Bayesian neural\n",
      "network with a single hidden layer where the number of units tends to\n",
      "inﬁnity (Neal, 1996; Williams, 1997). Excellent introductions to Gaussian\n",
      "processes can be found in MacKay (1998) and Rasmussen and Williams\n",
      "(2006).\n",
      "We focused on Gaussian parameter priors in the discussions in this chap-\n",
      "ter, because they allow for closed-form inference in linear regression mod-\n",
      "els. However, even in a regression setting with Gaussian likelihoods, we\n",
      "may choose a non-Gaussian prior. Consider a setting, where the inputs are\n",
      "x2RDand our training set is small and of size N\u001cD. This means that\n",
      "the regression problem is underdetermined. In this case, we can choose\n",
      "a parameter prior that enforces sparsity, i.e., a prior that tries to set as\n",
      "many parameters to 0as possible ( variable selection ). This prior provides variable selection\n",
      "a stronger regularizer than the Gaussian prior, which often leads to an in-\n",
      "creased prediction accuracy and interpretability of the model. The Laplace\n",
      "prior is one example that is frequently used for this purpose. A linear re-\n",
      "gression model with the Laplace prior on the parameters is equivalent to\n",
      "linear regression with L1 regularization ( LASSO ) (Tibshirani, 1996). The LASSO\n",
      "Laplace distribution is sharply peaked at zero (its ﬁrst derivative is discon-\n",
      "tinuous) and it concentrates its probability mass closer to zero than the\n",
      "Gaussian distribution, which encourages parameters to be 0. Therefore,\n",
      "the nonzero parameters are relevant for the regression problem, which is\n",
      "the reason why we also speak of “variable selection”.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10\n",
      "Dimensionality Reduction with Principal\n",
      "Component Analysis\n",
      "Working directly with high-dimensional data, such as images, comes with A640\u0002480pixel\n",
      "color image is a data\n",
      "point in a\n",
      "million-dimensional\n",
      "space, where every\n",
      "pixel responds to\n",
      "three dimensions,\n",
      "one for each color\n",
      "channel (red, green,\n",
      "blue).some difﬁculties: It is hard to analyze, interpretation is difﬁcult, visualiza-\n",
      "tion is nearly impossible, and (from a practical point of view) storage of\n",
      "the data vectors can be expensive. However, high-dimensional data often\n",
      "has properties that we can exploit. For example, high-dimensional data is\n",
      "often overcomplete, i.e., many dimensions are redundant and can be ex-\n",
      "plained by a combination of other dimensions. Furthermore, dimensions\n",
      "in high-dimensional data are often correlated so that the data possesses an\n",
      "intrinsic lower-dimensional structure. Dimensionality reduction exploits\n",
      "structure and correlation and allows us to work with a more compact rep-\n",
      "resentation of the data, ideally without losing information. We can think\n",
      "of dimensionality reduction as a compression technique, similar to jpeg or\n",
      "mp3, which are compression algorithms for images and music.\n",
      "In this chapter, we will discuss principal component analysis (PCA), an principal component\n",
      "analysis\n",
      "PCAalgorithm for linear dimensionality reduction . PCA, proposed by Pearson\n",
      "dimensionality\n",
      "reduction(1901) and Hotelling (1933), has been around for more than 100years\n",
      "and is still one of the most commonly used techniques for data compres-\n",
      "sion and data visualization. It is also used for the identiﬁcation of simple\n",
      "patterns, latent factors, and structures of high-dimensional data. In the\n",
      "Figure 10.1\n",
      "Illustration:\n",
      "dimensionality\n",
      "reduction. (a) The\n",
      "original dataset\n",
      "does not vary much\n",
      "along thex2\n",
      "direction. (b) The\n",
      "data from (a) can be\n",
      "represented using\n",
      "thex1-coordinate\n",
      "alone with nearly no\n",
      "loss.\n",
      "−5.0−2.5 0.0 2.5 5.0\n",
      "x1−4−2024x2\n",
      "(a) Dataset with x1andx2coordinates.\n",
      "−5.0−2.5 0.0 2.5 5.0\n",
      "x1−4−2024x2\n",
      " (b) Compressed dataset where only the x1coor-\n",
      "dinate is relevant.\n",
      "317\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "318 Dimensionality Reduction with Principal Component Analysis\n",
      "signal processing community, PCA is also known as the Karhunen-Lo `eve Karhunen-Lo `eve\n",
      "transform transform . In this chapter, we derive PCA from ﬁrst principles, drawing on\n",
      "our understanding of basis and basis change (Sections 2.6.1 and 2.7.2),\n",
      "projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distribu-\n",
      "tions (Section 6.5), and constrained optimization (Section 7.2).\n",
      "Dimensionality reduction generally exploits a property of high-dimen-\n",
      "sional data (e.g., images) that it often lies on a low-dimensional subspace.\n",
      "Figure 10.1 gives an illustrative example in two dimensions. Although\n",
      "the data in Figure 10.1(a) does not quite lie on a line, the data does not\n",
      "vary much in the x2-direction, so that we can express it as if it were on\n",
      "a line – with nearly no loss; see Figure 10.1(b). To describe the data in\n",
      "Figure 10.1(b), only the x1-coordinate is required, and the data lies in a\n",
      "one-dimensional subspace of R2.\n",
      "10.1 Problem Setting\n",
      "In PCA, we are interested in ﬁnding projections ~xnof data points xnthat\n",
      "are as similar to the original data points as possible, but which have a sig-\n",
      "niﬁcantly lower intrinsic dimensionality. Figure 10.1 gives an illustration\n",
      "of what this could look like.\n",
      "More concretely, we consider an i.i.d. dataset X=fx1;:::;xNg,xn2\n",
      "RD, with mean 0that possesses the data covariance matrix (6.42) data covariance\n",
      "matrix\n",
      "S=1\n",
      "NNX\n",
      "n=1xnx>\n",
      "n: (10.1)\n",
      "Furthermore, we assume there exists a low-dimensional compressed rep-\n",
      "resentation (code)\n",
      "zn=B>xn2RM(10.2)\n",
      "ofxn, where we deﬁne the projection matrix\n",
      "B:= [b1;:::;bM]2RD\u0002M: (10.3)\n",
      "We assume that the columns of Bare orthonormal (Deﬁnition 3.7) so that\n",
      "b>\n",
      "ibj= 0if and only if i6=jandb>\n",
      "ibi= 1. We seek an M-dimensional The columns\n",
      "b1;:::;bMofB\n",
      "form a basis of the\n",
      "M-dimensional\n",
      "subspace in which\n",
      "the projected data\n",
      "~x=BB>x2RD\n",
      "live.subspaceU\u0012RD,dim(U) =M <D onto which we project the data. We\n",
      "denote the projected data by ~xn2U, and their coordinates (with respect\n",
      "to the basis vectors b1;:::;bMofU) byzn. Our aim is to ﬁnd projections\n",
      "~xn2RD(or equivalently the codes znand the basis vectors b1;:::;bM)\n",
      "so that they are as similar to the original data xnand minimize the loss\n",
      "due to compression.\n",
      "Example 10.1 (Coordinate Representation/Code)\n",
      "Consider R2with the canonical basis e1= [1;0]>,e2= [0;1]>. From\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.1 Problem Setting 319\n",
      "Figure 10.2\n",
      "Graphical\n",
      "illustration of PCA.\n",
      "In PCA, we ﬁnd a\n",
      "compressed version\n",
      "zof original data x.\n",
      "The compressed\n",
      "data can be\n",
      "reconstructed into\n",
      "~x, which lives in the\n",
      "original data space,\n",
      "but has an intrinsic\n",
      "lower-dimensional\n",
      "representation than\n",
      "x.x ~xzOriginal\n",
      "CompressedReconstructed\n",
      "RDRD\n",
      "RM\n",
      "Chapter 2, we know that x2R2can be represented as a linear combina-\n",
      "tion of these basis vectors, e.g.,\n",
      "\u00145\n",
      "3\u0015\n",
      "= 5e1+ 3e2: (10.4)\n",
      "However, when we consider vectors of the form\n",
      "~x=\u00140\n",
      "z\u0015\n",
      "2R2; z2R; (10.5)\n",
      "they can always be written as 0e1+ze2. To represent these vectors it is\n",
      "sufﬁcient to remember/store the coordinate/code zof~xwith respect to\n",
      "thee2vector. The dimension of a\n",
      "vector space\n",
      "corresponds to the\n",
      "number of its basis\n",
      "vectors (see\n",
      "Section 2.6.1).More precisely, the set of ~xvectors (with the standard vector addition\n",
      "and scalar multiplication) forms a vector subspace U(see Section 2.4)\n",
      "with dim(U) = 1 becauseU= span[e2].\n",
      "In Section 10.2, we will ﬁnd low-dimensional representations that re-\n",
      "tain as much information as possible and minimize the compression loss.\n",
      "An alternative derivation of PCA is given in Section 10.3, where we will\n",
      "be looking at minimizing the squared reconstruction error kxn\u0000~xnk2be-\n",
      "tween the original data xnand its projection ~xn.\n",
      "Figure 10.2 illustrates the setting we consider in PCA, where zrepre-\n",
      "sents the lower-dimensional representation of the compressed data ~xand\n",
      "plays the role of a bottleneck, which controls how much information can\n",
      "ﬂow between xand~x. In PCA, we consider a linear relationship between\n",
      "the original data xand its low-dimensional code zso thatz=B>xand\n",
      "~x=Bzfor a suitable matrix B. Based on the motivation of thinking\n",
      "of PCA as a data compression technique, we can interpret the arrows in\n",
      "Figure 10.2 as a pair of operations representing encoders and decoders.\n",
      "The linear mapping represented by Bcan be thought of as a decoder,\n",
      "which maps the low-dimensional code z2RMback into the original data\n",
      "spaceRD. Similarly,B>can be thought of an encoder, which encodes the\n",
      "original dataxas a low-dimensional (compressed) code z.\n",
      "Throughout this chapter, we will use the MNIST digits dataset as a re-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "320 Dimensionality Reduction with Principal Component Analysis\n",
      "Figure 10.3\n",
      "Examples of\n",
      "handwritten digits\n",
      "from the MNIST\n",
      "dataset. http:\n",
      "//yann.lecun.\n",
      "com/exdb/mnist/ .\n",
      "occurring example, which contains 60;000examples of handwritten digits\n",
      "0through 9. Each digit is a grayscale image of size 28\u000228, i.e., it contains\n",
      "784pixels so that we can interpret every image in this dataset as a vector\n",
      "x2R784. Examples of these digits are shown in Figure 10.3.\n",
      "10.2 Maximum Variance Perspective\n",
      "Figure 10.1 gave an example of how a two-dimensional dataset can be\n",
      "represented using a single coordinate. In Figure 10.1(b), we chose to ig-\n",
      "nore thex2-coordinate of the data because it did not add too much in-\n",
      "formation so that the compressed data is similar to the original data in\n",
      "Figure 10.1(a). We could have chosen to ignore the x1-coordinate, but\n",
      "then the compressed data had been very dissimilar from the original data,\n",
      "and much information in the data would have been lost.\n",
      "If we interpret information content in the data as how “space ﬁlling”\n",
      "the dataset is, then we can describe the information contained in the data\n",
      "by looking at the spread of the data. From Section 6.4.1, we know that the\n",
      "variance is an indicator of the spread of the data, and we can derive PCA as\n",
      "a dimensionality reduction algorithm that maximizes the variance in the\n",
      "low-dimensional representation of the data to retain as much information\n",
      "as possible. Figure 10.4 illustrates this.\n",
      "Considering the setting discussed in Section 10.1, our aim is to ﬁnd\n",
      "a matrixB(see (10.3)) that retains as much information as possible\n",
      "when compressing data by projecting it onto the subspace spanned by\n",
      "the columnsb1;:::;bMofB. Retaining most information after data com-\n",
      "pression is equivalent to capturing the largest amount of variance in the\n",
      "low-dimensional code (Hotelling, 1933).\n",
      "Remark. (Centered Data) For the data covariance matrix in (10.1), we\n",
      "assumed centered data. We can make this assumption without loss of gen-\n",
      "erality: Let us assume that \u0016is the mean of the data. Using the properties\n",
      "of the variance, which we discussed in Section 6.4.4, we obtain\n",
      "Vz[z] =Vx[B>(x\u0000\u0016)] =Vx[B>x\u0000B>\u0016] =Vx[B>x];(10.6)\n",
      "i.e., the variance of the low-dimensional code does not depend on the\n",
      "mean of the data. Therefore, we assume without loss of generality that the\n",
      "data has mean 0for the remainder of this section. With this assumption\n",
      "the mean of the low-dimensional code is also 0sinceEz[z] =Ex[B>x] =\n",
      "B>Ex[x] =0. }\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.2 Maximum Variance Perspective 321\n",
      "Figure 10.4 PCA\n",
      "ﬁnds a\n",
      "lower-dimensional\n",
      "subspace (line) that\n",
      "maintains as much\n",
      "variance (spread of\n",
      "the data) as possible\n",
      "when the data\n",
      "(blue) is projected\n",
      "onto this subspace\n",
      "(orange).\n",
      "10.2.1 Direction with Maximal Variance\n",
      "We maximize the variance of the low-dimensional code using a sequential\n",
      "approach. We start by seeking a single vector b12RDthat maximizes the The vectorb1will\n",
      "be the ﬁrst column\n",
      "of the matrixBand\n",
      "therefore the ﬁrst of\n",
      "Morthonormal\n",
      "basis vectors that\n",
      "span the\n",
      "lower-dimensional\n",
      "subspace.variance of the projected data, i.e., we aim to maximize the variance of\n",
      "the ﬁrst coordinate z1ofz2RMso that\n",
      "V1:=V[z1] =1\n",
      "NNX\n",
      "n=1z2\n",
      "1n (10.7)\n",
      "is maximized, where we exploited the i.i.d. assumption of the data and\n",
      "deﬁnedz1nas the ﬁrst coordinate of the low-dimensional representation\n",
      "zn2RMofxn2RD. Note that ﬁrst component of znis given by\n",
      "z1n=b>\n",
      "1xn; (10.8)\n",
      "i.e., it is the coordinate of the orthogonal projection of xnonto the one-\n",
      "dimensional subspace spanned by b1(Section 3.8). We substitute (10.8)\n",
      "into (10.7), which yields\n",
      "V1=1\n",
      "NNX\n",
      "n=1(b>\n",
      "1xn)2=1\n",
      "NNX\n",
      "n=1b>\n",
      "1xnx>\n",
      "nb1 (10.9a)\n",
      "=b>\n",
      "1 \n",
      "1\n",
      "NNX\n",
      "n=1xnx>\n",
      "n!\n",
      "b1=b>\n",
      "1Sb1; (10.9b)\n",
      "whereSis the data covariance matrix deﬁned in (10.1). In (10.9a), we\n",
      "have used the fact that the dot product of two vectors is symmetric with\n",
      "respect to its arguments, that is, b>\n",
      "1xn=x>\n",
      "nb1.\n",
      "Notice that arbitrarily increasing the magnitude of the vector b1in-\n",
      "creasesV1, that is, a vector b1that is two times longer can result in V1\n",
      "that is potentially four times larger. Therefore, we restrict all solutions to kb1k2= 1\n",
      "() kb1k= 1. kb1k2= 1, which results in a constrained optimization problem in which\n",
      "we seek the direction along which the data varies most.\n",
      "With the restriction of the solution space to unit vectors the vector b1\n",
      "that points in the direction of maximum variance can be found by the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "322 Dimensionality Reduction with Principal Component Analysis\n",
      "constrained optimization problem\n",
      "max\n",
      "b1b>\n",
      "1Sb1\n",
      "subject tokb1k2= 1:(10.10)\n",
      "Following Section 7.2, we obtain the Lagrangian\n",
      "L(b1;\u0015) =b>\n",
      "1Sb1+\u00151(1\u0000b>\n",
      "1b1) (10.11)\n",
      "to solve this constrained optimization problem. The partial derivatives of\n",
      "Lwith respect to b1and\u00151are\n",
      "@L\n",
      "@b1= 2b>\n",
      "1S\u00002\u00151b>\n",
      "1;@L\n",
      "@\u00151= 1\u0000b>\n",
      "1b1; (10.12)\n",
      "respectively. Setting these partial derivatives to 0gives us the relations\n",
      "Sb1=\u00151b1; (10.13)\n",
      "b>\n",
      "1b1= 1: (10.14)\n",
      "By comparing this with the deﬁnition of an eigenvalue decomposition\n",
      "(Section 4.4), we see that b1is an eigenvector of the data covariance\n",
      "matrixS, and the Lagrange multiplier \u00151plays the role of the correspond-\n",
      "ing eigenvalue. This eigenvector property (10.13) allows us to rewrite our The quantityp\u00151is\n",
      "also called the\n",
      "loading of the unit\n",
      "vectorb1and\n",
      "represents the\n",
      "standard deviation\n",
      "of the data\n",
      "accounted for by the\n",
      "principal subspace\n",
      "span[b1].variance objective (10.10) as\n",
      "V1=b>\n",
      "1Sb1=\u00151b>\n",
      "1b1=\u00151; (10.15)\n",
      "i.e., the variance of the data projected onto a one-dimensional subspace\n",
      "equals the eigenvalue that is associated with the basis vector b1that spans\n",
      "this subspace. Therefore, to maximize the variance of the low-dimensional\n",
      "code, we choose the basis vector associated with the largest eigenvalue\n",
      "of the data covariance matrix. This eigenvector is called the ﬁrst principalprincipal component\n",
      "component . We can determine the effect/contribution of the principal com-\n",
      "ponentb1in the original data space by mapping the coordinate z1nback\n",
      "into data space, which gives us the projected data point\n",
      "~xn=b1z1n=b1b>\n",
      "1xn2RD(10.16)\n",
      "in the original data space.\n",
      "Remark. Although ~xnis aD-dimensional vector, it only requires a single\n",
      "coordinatez1nto represent it with respect to the basis vector b12RD.}\n",
      "10.2.2M-dimensional Subspace with Maximal Variance\n",
      "Assume we have found the ﬁrst m\u00001principal components as the m\u00001\n",
      "eigenvectors of Sthat are associated with the largest m\u00001eigenvalues.\n",
      "SinceSis symmetric, the spectral theorem (Theorem 4.15) states that we\n",
      "can use these eigenvectors to construct an orthonormal eigenbasis of an\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.2 Maximum Variance Perspective 323\n",
      "(m\u00001)-dimensional subspace of RD. Generally, the mth principal com-\n",
      "ponent can be found by subtracting the effect of the ﬁrst m\u00001principal\n",
      "componentsb1;:::;bm\u00001from the data, thereby trying to ﬁnd principal\n",
      "components that compress the remaining information. We then arrive at\n",
      "the new data matrix\n",
      "^X:=X\u0000m\u00001X\n",
      "i=1bib>\n",
      "iX=X\u0000Bm\u00001X; (10.17)\n",
      "whereX= [x1;:::;xN]2RD\u0002Ncontains the data points as column The matrix ^X:=\n",
      "[^x1;:::; ^xN]2\n",
      "RD\u0002Nin (10.17)\n",
      "contains the\n",
      "information in the\n",
      "data that has not yet\n",
      "been compressed.vectors andBm\u00001:=Pm\u00001\n",
      "i=1bib>\n",
      "iis a projection matrix that projects onto\n",
      "the subspace spanned by b1;:::;bm\u00001.\n",
      "Remark (Notation) .Throughout this chapter, we do not follow the con-\n",
      "vention of collecting data x1;:::;xNas the rows of the data matrix, but\n",
      "we deﬁne them to be the columns of X. This means that our data ma-\n",
      "trixXis aD\u0002Nmatrix instead of the conventional N\u0002Dmatrix. The\n",
      "reason for our choice is that the algebra operations work out smoothly\n",
      "without the need to either transpose the matrix or to redeﬁne vectors as\n",
      "row vectors that are left-multiplied onto matrices. }\n",
      "To ﬁnd themth principal component, we maximize the variance\n",
      "Vm=V[zm] =1\n",
      "NNX\n",
      "n=1z2\n",
      "mn=1\n",
      "NNX\n",
      "n=1(b>\n",
      "m^xn)2=b>\n",
      "m^Sbm; (10.18)\n",
      "subject tokbmk2= 1, where we followed the same steps as in (10.9b)\n",
      "and deﬁned ^Sas the data covariance matrix of the transformed dataset\n",
      "^X:=f^x1;:::; ^xNg. As previously, when we looked at the ﬁrst principal\n",
      "component alone, we solve a constrained optimization problem and dis-\n",
      "cover that the optimal solution bmis the eigenvector of ^Sthat is associated\n",
      "with the largest eigenvalue of ^S.\n",
      "It turns out that bmis also an eigenvector of S. More generally, the sets\n",
      "of eigenvectors of Sand^Sare identical. Since both Sand^Sare sym-\n",
      "metric, we can ﬁnd an ONB of eigenvectors (spectral theorem 4.15), i.e.,\n",
      "there existDdistinct eigenvectors for both Sand^S. Next, we show that\n",
      "every eigenvector of Sis an eigenvector of ^S. Assume we have already\n",
      "found eigenvectors b1;:::;bm\u00001of^S. Consider an eigenvector biofS,\n",
      "i.e.,Sbi=\u0015ibi. In general,\n",
      "^Sbi=1\n",
      "N^X^X>bi=1\n",
      "N(X\u0000Bm\u00001X)(X\u0000Bm\u00001X)>bi(10.19a)\n",
      "= (S\u0000SBm\u00001\u0000Bm\u00001S+Bm\u00001SBm\u00001)bi: (10.19b)\n",
      "We distinguish between two cases. If i>m, i.e.,biis an eigenvector\n",
      "that is not among the ﬁrst m\u00001principal components, then biis orthogo-\n",
      "nal to the ﬁrst m\u00001principal components and Bm\u00001bi=0. Ifi<m , i.e.,\n",
      "biis among the ﬁrst m\u00001principal components, then biis a basis vector\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "324 Dimensionality Reduction with Principal Component Analysis\n",
      "of the principal subspace onto which Bm\u00001projects. Since b1;:::;bm\u00001\n",
      "are an ONB of this principal subspace, we obtain Bm\u00001bi=bi. The two\n",
      "cases can be summarized as follows:\n",
      "Bm\u00001bi=biifi<m;Bm\u00001bi=0ifi>m: (10.20)\n",
      "In the case i>m, by using (10.20) in (10.19b), we obtain ^Sbi= (S\u0000\n",
      "Bm\u00001S)bi=Sbi=\u0015ibi;i.e.,biis also an eigenvector of ^Swith eigen-\n",
      "value\u0015i. Speciﬁcally,\n",
      "^Sbm=Sbm=\u0015mbm: (10.21)\n",
      "Equation (10.21) reveals that bmis not only an eigenvector of Sbut also\n",
      "of^S. Speciﬁcally, \u0015mis the largest eigenvalue of ^Sand\u0015mis themth\n",
      "largest eigenvalue of S, and both have the associated eigenvector bm.\n",
      "In the casei<m , by using (10.20) in (10.19b), we obtain\n",
      "^Sbi= (S\u0000SBm\u00001\u0000Bm\u00001S+Bm\u00001SBm\u00001)bi=0= 0bi(10.22)\n",
      "This means that b1;:::;bm\u00001are also eigenvectors of ^S, but they are as-\n",
      "sociated with eigenvalue 0so thatb1;:::;bm\u00001span the null space of ^S.\n",
      "Overall, every eigenvector of Sis also an eigenvector of ^S. However,\n",
      "if the eigenvectors of Sare part of the (m\u00001)dimensional principal\n",
      "subspace, then the associated eigenvalue of ^Sis0. This derivation\n",
      "shows that there is\n",
      "an intimate\n",
      "connection between\n",
      "theM-dimensional\n",
      "subspace with\n",
      "maximal variance\n",
      "and the eigenvalue\n",
      "decomposition. We\n",
      "will revisit this\n",
      "connection in\n",
      "Section 10.4.With the relation (10.21) and b>\n",
      "mbm= 1, the variance of the data pro-\n",
      "jected onto the mth principal component is\n",
      "Vm=b>\n",
      "mSbm(10.21)=\u0015mb>\n",
      "mbm=\u0015m: (10.23)\n",
      "This means that the variance of the data, when projected onto an M-\n",
      "dimensional subspace, equals the sum of the eigenvalues that are associ-\n",
      "ated with the corresponding eigenvectors of the data covariance matrix.\n",
      "Example 10.2 (Eigenvalues of MNIST “8”)\n",
      "Figure 10.5\n",
      "Properties of the\n",
      "training data of\n",
      "MNIST “8”. (a)\n",
      "Eigenvalues sorted\n",
      "in descending order;\n",
      "(b) Variance\n",
      "captured by the\n",
      "principal\n",
      "components\n",
      "associated with the\n",
      "largest eigenvalues.\n",
      "0 50 100 150 200\n",
      "Index01020304050Eigenvalue\n",
      "(a) Eigenvalues (sorted in descending order) of\n",
      "the data covariance matrix of all digits “8” in\n",
      "the MNIST training set.\n",
      "0 50 100 150 200\n",
      "Number of principal components100200300400500Captured variance(b) Variance captured by the principal compo-\n",
      "nents.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.3 Projection Perspective 325\n",
      "Figure 10.6\n",
      "Illustration of the\n",
      "projection\n",
      "approach: Find a\n",
      "subspace (line) that\n",
      "minimizes the\n",
      "length of the\n",
      "difference vector\n",
      "between projected\n",
      "(orange) and\n",
      "original (blue) data.\n",
      "Taking all digits “8” in the MNIST training data, we compute the eigen-\n",
      "values of the data covariance matrix. Figure 10.5(a) shows the 200largest\n",
      "eigenvalues of the data covariance matrix. We see that only a few of\n",
      "them have a value that differs signiﬁcantly from 0. Therefore, most of\n",
      "the variance, when projecting data onto the subspace spanned by the cor-\n",
      "responding eigenvectors, is captured by only a few principal components,\n",
      "as shown in Figure 10.5(b).\n",
      "Overall, to ﬁnd an M-dimensional subspace of RDthat retains as much\n",
      "information as possible, PCA tells us to choose the columns of the matrix\n",
      "Bin (10.3) as the Meigenvectors of the data covariance matrix Sthat\n",
      "are associated with the Mlargest eigenvalues. The maximum amount of\n",
      "variance PCA can capture with the ﬁrst Mprincipal components is\n",
      "VM=MX\n",
      "m=1\u0015m; (10.24)\n",
      "where the\u0015mare theMlargest eigenvalues of the data covariance matrix\n",
      "S. Consequently, the variance lost by data compression via PCA is\n",
      "JM:=DX\n",
      "j=M+1\u0015j=VD\u0000VM: (10.25)\n",
      "Instead of these absolute quantities, we can deﬁne the relative variance\n",
      "captured asVM\n",
      "VD, and the relative variance lost by compression as 1\u0000VM\n",
      "VD.\n",
      "10.3 Projection Perspective\n",
      "In the following, we will derive PCA as an algorithm that directly mini-\n",
      "mizes the average reconstruction error. This perspective allows us to in-\n",
      "terpret PCA as implementing an optimal linear auto-encoder. We will draw\n",
      "heavily from Chapters 2 and 3.\n",
      "In the previous section, we derived PCA by maximizing the variance\n",
      "in the projected space to retain as much information as possible. In the\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "326 Dimensionality Reduction with Principal Component Analysis\n",
      "Figure 10.7\n",
      "Simpliﬁed\n",
      "projection setting.\n",
      "(a) A vectorx2R2\n",
      "(red cross) shall be\n",
      "projected onto a\n",
      "one-dimensional\n",
      "subspaceU\u0012R2\n",
      "spanned byb. (b)\n",
      "shows the difference\n",
      "vectors between x\n",
      "and some\n",
      "candidates ~x.\n",
      "−1.0−0.5 0.0 0.5 1.0 1.5 2.0\n",
      "x1−0.50.00.51.01.52.02.5x2\n",
      "bU\n",
      "(a) Setting.\n",
      "−1.0−0.5 0.0 0.5 1.0 1.5 2.0\n",
      "x1−0.50.00.51.01.52.02.5x2\n",
      "bU (b) Differences x\u0000~xifor50different ~xiare\n",
      "shown by the red lines.\n",
      "following, we will look at the difference vectors between the original data\n",
      "xnand their reconstruction ~xnand minimize this distance so that xnand\n",
      "~xnare as close as possible. Figure 10.6 illustrates this setting.\n",
      "10.3.1 Setting and Objective\n",
      "Assume an (ordered) orthonormal basis (ONB) B= (b1;:::;bD)ofRD,\n",
      "i.e.,b>\n",
      "ibj= 1if and only if i=jand0otherwise.\n",
      "From Section 2.5 we know that for a basis (b1;:::;bD)ofRDanyx2\n",
      "RDcan be written as a linear combination of the basis vectors of RD, i.e.,\n",
      "Vectors ~x2Ucould\n",
      "be vectors on a\n",
      "plane in R3. The\n",
      "dimensionality of\n",
      "the plane is 2, but\n",
      "the vectors still have\n",
      "three coordinates\n",
      "with respect to the\n",
      "standard basis of\n",
      "R3.x=DX\n",
      "d=1\u0010dbd=MX\n",
      "m=1\u0010mbm+DX\n",
      "j=M+1\u0010jbj (10.26)\n",
      "for suitable coordinates \u0010d2R.\n",
      "We are interested in ﬁnding vectors ~x2RD, which live in lower-\n",
      "dimensional subspace U\u0012RD,dim(U) =M, so that\n",
      "~x=MX\n",
      "m=1zmbm2U\u0012RD(10.27)\n",
      "is as similar to xas possible. Note that at this point we need to assume\n",
      "that the coordinates zmof~xand\u0010mofxare not identical.\n",
      "In the following, we use exactly this kind of representation of ~xto ﬁnd\n",
      "optimal coordinates zand basis vectors b1;:::;bMsuch that ~xis as sim-\n",
      "ilar to the original data point xas possible, i.e., we aim to minimize the\n",
      "(Euclidean) distance kx\u0000~xk. Figure 10.7 illustrates this setting.\n",
      "Without loss of generality, we assume that the dataset X=fx1;:::;xNg,\n",
      "xn2RD, is centered at 0, i.e.,E[X] =0. Without the zero-mean assump-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.3 Projection Perspective 327\n",
      "tion, we would arrive at exactly the same solution, but the notation would\n",
      "be substantially more cluttered.\n",
      "We are interested in ﬁnding the best linear projection of Xonto a lower-\n",
      "dimensional subspace UofRDwith dim(U) =Mand orthonormal basis\n",
      "vectorsb1;:::;bM. We will call this subspace Utheprincipal subspace .principal subspace\n",
      "The projections of the data points are denoted by\n",
      "~xn:=MX\n",
      "m=1zmnbm=Bzn2RD; (10.28)\n",
      "wherezn:= [z1n;:::;zMn]>2RMis the coordinate vector of ~xnwith\n",
      "respect to the basis (b1;:::;bM). More speciﬁcally, we are interested in\n",
      "having the ~xnas similar toxnas possible.\n",
      "The similarity measure we use in the following is the squared distance\n",
      "(Euclidean norm) kx\u0000~xk2betweenxand~x. We therefore deﬁne our ob-\n",
      "jective as minimizing the average squared Euclidean distance ( reconstruction reconstruction error\n",
      "error ) (Pearson, 1901)\n",
      "JM:=1\n",
      "NNX\n",
      "n=1kxn\u0000~xnk2; (10.29)\n",
      "where we make it explicit that the dimension of the subspace onto which\n",
      "we project the data is M. In order to ﬁnd this optimal linear projection,\n",
      "we need to ﬁnd the orthonormal basis of the principal subspace and the\n",
      "coordinateszn2RMof the projections with respect to this basis.\n",
      "To ﬁnd the coordinates znand the ONB of the principal subspace, we\n",
      "follow a two-step approach. First, we optimize the coordinates znfor a\n",
      "given ONB (b1;:::;bM); second, we ﬁnd the optimal ONB.\n",
      "10.3.2 Finding Optimal Coordinates\n",
      "Let us start by ﬁnding the optimal coordinates z1n;:::;zMnof the projec-\n",
      "tions ~xnforn= 1;:::;N . Consider Figure 10.7(b), where the principal\n",
      "subspace is spanned by a single vector b. Geometrically speaking, ﬁnding\n",
      "the optimal coordinates zcorresponds to ﬁnding the representation of the\n",
      "linear projection ~xwith respect to bthat minimizes the distance between\n",
      "~x\u0000x. From Figure 10.7(b), it is clear that this will be the orthogonal\n",
      "projection, and in the following we will show exactly this.\n",
      "We assume an ONB (b1;:::;bM)ofU\u0012RD. To ﬁnd the optimal co-\n",
      "ordinateszmwith respect to this basis, we require the partial derivatives\n",
      "@JM\n",
      "@zin=@JM\n",
      "@~xn@~xn\n",
      "@zin; (10.30a)\n",
      "@JM\n",
      "@~xn=\u00002\n",
      "N(xn\u0000~xn)>2R1\u0002D; (10.30b)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "328 Dimensionality Reduction with Principal Component Analysis\n",
      "Figure 10.8\n",
      "Optimal projection\n",
      "of a vectorx2R2\n",
      "onto a\n",
      "one-dimensional\n",
      "subspace\n",
      "(continuation from\n",
      "Figure 10.7).\n",
      "(a) Distances\n",
      "kx\u0000~xkfor some\n",
      "~x2U.\n",
      "(b) Orthogonal\n",
      "projection and\n",
      "optimal coordinates.\n",
      "−1.0−0.5 0.0 0.5 1.0 1.5 2.0\n",
      "x11.251.501.752.002.252.502.753.003.25/bardblx−˜x/bardbl\n",
      "(a) Distanceskx\u0000~xkfor some ~x=z1b2\n",
      "U= span[b]; see panel (b) for the setting.\n",
      "−1.0−0.5 0.0 0.5 1.0 1.5 2.0\n",
      "x1−0.50.00.51.01.52.02.5x2\n",
      "bU\n",
      "˜x(b) The vector ~xthat minimizes the distance\n",
      "in panel (a) is its orthogonal projection onto\n",
      "U. The coordinate of the projection ~xwith\n",
      "respect to the basis vector bthat spansU\n",
      "is the factor we need to scale bin order to\n",
      "“reach” ~x.\n",
      "@~xn\n",
      "@zin(10.28)=@\n",
      "@zin MX\n",
      "m=1zmnbm!\n",
      "=bi (10.30c)\n",
      "fori= 1;:::;M , such that we obtain\n",
      "@JM\n",
      "@zin(10.30b)\n",
      "(10.30c)=\u00002\n",
      "N(xn\u0000~xn)>bi(10.28)=\u00002\n",
      "N \n",
      "xn\u0000MX\n",
      "m=1zmnbm!>\n",
      "bi\n",
      "(10.31a)\n",
      "ONB=\u00002\n",
      "N(x>\n",
      "nbi\u0000zinb>\n",
      "ibi) =\u00002\n",
      "N(x>\n",
      "nbi\u0000zin): (10.31b)\n",
      "sinceb>\n",
      "ibi= 1. Setting this partial derivative to 0yields immediately the The coordinates of\n",
      "the optimal\n",
      "projection ofxn\n",
      "with respect to the\n",
      "basis vectors\n",
      "b1;:::;bMare the\n",
      "coordinates of the\n",
      "orthogonal\n",
      "projection ofxn\n",
      "onto the principal\n",
      "subspace.optimal coordinates\n",
      "zin=x>\n",
      "nbi=b>\n",
      "ixn (10.32)\n",
      "fori= 1;:::;M andn= 1;:::;N . This means that the optimal co-\n",
      "ordinateszinof the projection ~xnare the coordinates of the orthogonal\n",
      "projection (see Section 3.8) of the original data point xnonto the one-\n",
      "dimensional subspace that is spanned by bi. Consequently:\n",
      "The optimal linear projection ~xnofxnis an orthogonal projection.\n",
      "The coordinates of ~xnwith respect to the basis (b1;:::;bM)are the\n",
      "coordinates of the orthogonal projection of xnonto the principal sub-\n",
      "space.\n",
      "An orthogonal projection is the best linear mapping given the objec-\n",
      "tive (10.29).\n",
      "The coordinates \u0010mofxin (10.26) and the coordinates zmof~xin (10.27)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.3 Projection Perspective 329\n",
      "must be identical for m= 1;:::;M sinceU?= span[bM+1;:::;bD]is\n",
      "the orthogonal complement (see Section 3.6) of U= span[b1;:::;bM].\n",
      "Remark (Orthogonal Projections with Orthonormal Basis Vectors) .Let us\n",
      "brieﬂy recap orthogonal projections from Section 3.8. If (b1;:::;bD)is an\n",
      "orthonormal basis of RDthen b>\n",
      "jxis the\n",
      "coordinate of the\n",
      "orthogonal\n",
      "projection ofxonto\n",
      "the subspace\n",
      "spanned bybj.~x=bj(b>\n",
      "jbj)\u00001b>\n",
      "jx=bjb>\n",
      "jx2RD(10.33)\n",
      "is the orthogonal projection of xonto the subspace spanned by the jth ba-\n",
      "sis vector, and zj=b>\n",
      "jxis the coordinate of this projection with respect to\n",
      "the basis vector bjthat spans that subspace since zjbj=~x. Figure 10.8(b)\n",
      "illustrates this setting.\n",
      "More generally, if we aim to project onto an M-dimensional subspace\n",
      "ofRD, we obtain the orthogonal projection of xonto theM-dimensional\n",
      "subspace with orthonormal basis vectors b1;:::;bMas\n",
      "~x=B(B>B|{z}\n",
      "=I)\u00001B>x=BB>x; (10.34)\n",
      "where we deﬁned B:= [b1;:::;bM]2RD\u0002M. The coordinates of this\n",
      "projection with respect to the ordered basis (b1;:::;bM)arez:=B>x\n",
      "as discussed in Section 3.8.\n",
      "We can think of the coordinates as a representation of the projected\n",
      "vector in a new coordinate system deﬁned by (b1;:::;bM). Note that al-\n",
      "though ~x2RD, we only need Mcoordinates z1;:::;zMto represent\n",
      "this vector; the other D\u0000Mcoordinates with respect to the basis vectors\n",
      "(bM+1;:::;bD)are always 0. }\n",
      "So far we have shown that for a given ONB we can ﬁnd the optimal\n",
      "coordinates of ~xby an orthogonal projection onto the principal subspace.\n",
      "In the following, we will determine what the best basis is.\n",
      "10.3.3 Finding the Basis of the Principal Subspace\n",
      "To determine the basis vectors b1;:::;bMof the principal subspace, we\n",
      "rephrase the loss function (10.29) using the results we have so far. This\n",
      "will make it easier to ﬁnd the basis vectors. To reformulate the loss func-\n",
      "tion, we exploit our results from before and obtain\n",
      "~xn=MX\n",
      "m=1zmnbm(10.32)=MX\n",
      "m=1(x>\n",
      "nbm)bm: (10.35)\n",
      "We now exploit the symmetry of the dot product, which yields\n",
      "~xn= MX\n",
      "m=1bmb>\n",
      "m!\n",
      "xn: (10.36)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "330 Dimensionality Reduction with Principal Component Analysis\n",
      "Figure 10.9\n",
      "Orthogonal\n",
      "projection and\n",
      "displacement\n",
      "vectors. When\n",
      "projecting data\n",
      "pointsxn(blue)\n",
      "onto subspace U1,\n",
      "we obtain ~xn\n",
      "(orange). The\n",
      "displacement vector\n",
      "~xn\u0000xnlies\n",
      "completely in the\n",
      "orthogonal\n",
      "complement U2of\n",
      "U1.\n",
      "−5 0 5\n",
      "x1−6−4−20246x2\n",
      "UU⊥\n",
      "Since we can generally write the original data point xnas a linear combi-\n",
      "nation of all basis vectors, it holds that\n",
      "xn=DX\n",
      "d=1zdnbd(10.32)=DX\n",
      "d=1(x>\n",
      "nbd)bd= DX\n",
      "d=1bdb>\n",
      "d!\n",
      "xn (10.37a)\n",
      "= MX\n",
      "m=1bmb>\n",
      "m!\n",
      "xn+ DX\n",
      "j=M+1bjb>\n",
      "j!\n",
      "xn; (10.37b)\n",
      "where we split the sum with Dterms into a sum over Mand a sum\n",
      "overD\u0000Mterms. With this result, we ﬁnd that the displacement vector\n",
      "xn\u0000~xn, i.e., the difference vector between the original data point and its\n",
      "projection, is\n",
      "xn\u0000~xn= DX\n",
      "j=M+1bjb>\n",
      "j!\n",
      "xn (10.38a)\n",
      "=DX\n",
      "j=M+1(x>\n",
      "nbj)bj: (10.38b)\n",
      "This means the difference is exactly the projection of the data point onto\n",
      "the orthogonal complement of the principal subspace: We identify the ma-\n",
      "trixPD\n",
      "j=M+1bjb>\n",
      "jin (10.38a) as the projection matrix that performs this\n",
      "projection. Hence the displacement vector xn\u0000~xnlies in the subspace\n",
      "that is orthogonal to the principal subspace as illustrated in Figure 10.9.\n",
      "Remark (Low-Rank Approximation) .In (10.38a), we saw that the projec-\n",
      "tion matrix, which projects xonto ~x, is given by\n",
      "MX\n",
      "m=1bmb>\n",
      "m=BB>: (10.39)\n",
      "By construction as a sum of rank-one matrices bmb>\n",
      "mwe see thatBB>is\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.3 Projection Perspective 331\n",
      "symmetric and has rank M. Therefore, the average squared reconstruction\n",
      "error can also be written as\n",
      "1\n",
      "NNX\n",
      "n=1kxn\u0000~xnk2=1\n",
      "NNX\n",
      "2n\u0000BB>xn\n",
      "(10.40a)\n",
      "=1\n",
      "NNX\n",
      "2I\u0000BB>)xn\n",
      ": (10.40b)\n",
      "Finding orthonormal basis vectors b1;:::;bM, which minimize the differ- PCA ﬁnds the best\n",
      "rank-M\n",
      "approximation of\n",
      "the identity matrix.ence between the original data xnand their projections ~xn, is equivalent\n",
      "to ﬁnding the best rank- Mapproximation BB>of the identity matrix I\n",
      "(see Section 4.6). }\n",
      "Now we have all the tools to reformulate the loss function (10.29).\n",
      "JM=1\n",
      "NNX\n",
      "n=1kxn\u0000~xnk2(10.38b)=1\n",
      "NNX\n",
      "DX1\n",
      "j=M+1(b>\n",
      "2xn)bj\n",
      ":(10.41)\n",
      "We now explicitly compute the squared norm and exploit the fact that the\n",
      "bjform an ONB, which yields\n",
      "JM=1\n",
      "NNX\n",
      "n=1DX\n",
      "j=M+1(b>\n",
      "jxn)2=1\n",
      "NNX\n",
      "n=1DX\n",
      "j=M+1b>\n",
      "jxnb>\n",
      "jxn (10.42a)\n",
      "=1\n",
      "NNX\n",
      "n=1DX\n",
      "j=M+1b>\n",
      "jxnx>\n",
      "nbj; (10.42b)\n",
      "where we exploited the symmetry of the dot product in the last step to\n",
      "writeb>\n",
      "jxn=x>\n",
      "nbj. We now swap the sums and obtain\n",
      "JM=DX\n",
      "j=M+1b>\n",
      "j \n",
      "1\n",
      "NNX\n",
      "n=1xnx>\n",
      "n!\n",
      "|{z}\n",
      "=:Sbj=DX\n",
      "j=M+1b>\n",
      "jSbj (10.43a)\n",
      "=DX\n",
      "j=M+1tr(b>\n",
      "jSbj) =DX\n",
      "j=M+1tr(Sbjb>\n",
      "j) =tr\u0010\u0010DX\n",
      "j=M+1bjb>\n",
      "j\u0011\n",
      "|{z}\n",
      "projection matrixS\u0011\n",
      ";\n",
      "(10.43b)\n",
      "where we exploited the property that the trace operator tr (\u0001)(see (4.18))\n",
      "is linear and invariant to cyclic permutations of its arguments. Since we\n",
      "assumed that our dataset is centered, i.e., E[X] =0, we identifySas the\n",
      "data covariance matrix. Since the projection matrix in (10.43b) is con-\n",
      "structed as a sum of rank-one matrices bjb>\n",
      "jit itself is of rank D\u0000M.\n",
      "Equation (10.43a) implies that we can formulate the average squared\n",
      "reconstruction error equivalently as the covariance matrix of the data,\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "332 Dimensionality Reduction with Principal Component Analysis\n",
      "projected onto the orthogonal complement of the principal subspace. Min-\n",
      "imizing the average squared reconstruction error is therefore equivalent to Minimizing the\n",
      "average squared\n",
      "reconstruction error\n",
      "is equivalent to\n",
      "minimizing the\n",
      "projection of the\n",
      "data covariance\n",
      "matrix onto the\n",
      "orthogonal\n",
      "complement of the\n",
      "principal subspace.minimizing the variance of the data when projected onto the subspace we\n",
      "ignore, i.e., the orthogonal complement of the principal subspace. Equiva-\n",
      "lently, we maximize the variance of the projection that we retain in the\n",
      "principal subspace, which links the projection loss immediately to the\n",
      "maximum-variance formulation of PCA discussed in Section 10.2. But this\n",
      "then also means that we will obtain the same solution that we obtained\n",
      "Minimizing the\n",
      "average squared\n",
      "reconstruction error\n",
      "is equivalent to\n",
      "maximizing the\n",
      "variance of the\n",
      "projected data.for the maximum-variance perspective. Therefore, we omit a derivation\n",
      "that is identical to the one presented in Section 10.2 and summarize the\n",
      "results from earlier in the light of the projection perspective.\n",
      "The average squared reconstruction error, when projecting onto the M-\n",
      "dimensional principal subspace, is\n",
      "JM=DX\n",
      "j=M+1\u0015j; (10.44)\n",
      "where\u0015jare the eigenvalues of the data covariance matrix. Therefore,\n",
      "to minimize (10.44) we need to select the smallest D\u0000Meigenvalues,\n",
      "which then implies that their corresponding eigenvectors are the basis of\n",
      "the orthogonal complement of the principal subspace. Consequently, this\n",
      "means that the basis of the principal subspace comprises the eigenvectors\n",
      "b1;:::;bMthat are associated with the largest Meigenvalues of the data\n",
      "covariance matrix.\n",
      "Example 10.3 (MNIST Digits Embedding)\n",
      "Figure 10.10\n",
      "Embedding of\n",
      "MNIST digits 0\n",
      "(blue) and 1\n",
      "(orange) in a\n",
      "two-dimensional\n",
      "principal subspace\n",
      "using PCA. Four\n",
      "embeddings of the\n",
      "digits “0” and “1” in\n",
      "the principal\n",
      "subspace are\n",
      "highlighted in red\n",
      "with their\n",
      "corresponding\n",
      "original digit.\n",
      "Figure 10.10 visualizes the training data of the MMIST digits “0” and “1”\n",
      "embedded in the vector subspace spanned by the ﬁrst two principal com-\n",
      "ponents. We observe a relatively clear separation between “0”s (blue dots)\n",
      "and “1”s (orange dots), and we see the variation within each individual\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.4 Eigenvector Computation and Low-Rank Approximations 333\n",
      "cluster. Four embeddings of the digits “0” and “1” in the principal subspace\n",
      "are highlighted in red with their corresponding original digit. The ﬁgure\n",
      "reveals that the variation within the set of “0” is signiﬁcantly greater than\n",
      "the variation within the set of “1”.\n",
      "10.4 Eigenvector Computation and Low-Rank Approximations\n",
      "In the previous sections, we obtained the basis of the principal subspace\n",
      "as the eigenvectors that are associated with the largest eigenvalues of the\n",
      "data covariance matrix\n",
      "S=1\n",
      "NNX\n",
      "n=1xnx>\n",
      "n=1\n",
      "NXX>; (10.45)\n",
      "X= [x1;:::;xN]2RD\u0002N: (10.46)\n",
      "Note thatXis aD\u0002Nmatrix, i.e., it is the transpose of the “typical”\n",
      "data matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and\n",
      "the corresponding eigenvectors) of S, we can follow two approaches: Use\n",
      "eigendecomposition\n",
      "or SVD to compute\n",
      "eigenvectors.We perform an eigendecomposition (see Section 4.2) and compute the\n",
      "eigenvalues and eigenvectors of Sdirectly.\n",
      "We use a singular value decomposition (see Section 4.5). Since Sis\n",
      "symmetric and factorizes into XX>(ignoring the factor1\n",
      "N), the eigen-\n",
      "values ofSare the squared singular values of X.\n",
      "More speciﬁcally, the SVD of Xis given by\n",
      "X|{z}\n",
      "D\u0002N=U|{z}\n",
      "D\u0002D\u0006|{z}\n",
      "D\u0002NV>\n",
      "|{z}\n",
      "N\u0002N; (10.47)\n",
      "whereU2RD\u0002DandV>2RN\u0002Nare orthogonal matrices and \u00062\n",
      "RD\u0002Nis a matrix whose only nonzero entries are the singular values \u001bii>\n",
      "0. It then follows that\n",
      "S=1\n",
      "NXX>=1\n",
      "NU\u0006V>V|{z}\n",
      "=IN\u0006>U>=1\n",
      "NU\u0006\u0006>U>: (10.48)\n",
      "With the results from Section 4.5, we get that the columns of Uare the The columns of U\n",
      "are the eigenvectors\n",
      "ofS.eigenvectors of XX>(and therefore S). Furthermore, the eigenvalues\n",
      "\u0015dofSare related to the singular values of Xvia\n",
      "\u0015d=\u001b2\n",
      "d\n",
      "N: (10.49)\n",
      "This relationship between the eigenvalues of Sand the singular values\n",
      "ofXprovides the connection between the maximum variance view (Sec-\n",
      "tion 10.2) and the singular value decomposition.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "334 Dimensionality Reduction with Principal Component Analysis\n",
      "10.4.1 PCA Using Low-Rank Matrix Approximations\n",
      "To maximize the variance of the projected data (or minimize the average\n",
      "squared reconstruction error), PCA chooses the columns of Uin (10.48)\n",
      "to be the eigenvectors that are associated with the Mlargest eigenvalues\n",
      "of the data covariance matrix Sso that we identify Uas the projection ma-\n",
      "trixBin (10.3), which projects the original data onto a lower-dimensional\n",
      "subspace of dimension M. The Eckart-Young theorem (Theorem 4.25 in Eckart-Young\n",
      "theorem Section 4.6) offers a direct way to estimate the low-dimensional represen-\n",
      "tation. Consider the best rank- Mapproximation\n",
      "~XM:= argminrk(A)6MkX\u0000Ak22RD\u0002N(10.50)\n",
      "ofX, wherek\u0001k2is the spectral norm deﬁned in (4.93). The Eckart-Young\n",
      "theorem states that ~XMis given by truncating the SVD at the top- M\n",
      "singular value. In other words, we obtain\n",
      "~XM=UM|{z}\n",
      "D\u0002M\u0006M|{z}\n",
      "M\u0002MV>\n",
      "M|{z}\n",
      "M\u0002N2RD\u0002N(10.51)\n",
      "with orthogonal matrices UM:= [u1;:::;uM]2RD\u0002MandVM:=\n",
      "[v1;:::;vM]2RN\u0002Mand a diagonal matrix \u0006M2RM\u0002Mwhose diago-\n",
      "nal entries are the Mlargest singular values of X.\n",
      "10.4.2 Practical Aspects\n",
      "Finding eigenvalues and eigenvectors is also important in other funda-\n",
      "mental machine learning methods that require matrix decompositions. In\n",
      "theory, as we discussed in Section 4.2, we can solve for the eigenvalues as\n",
      "roots of the characteristic polynomial. However, for matrices larger than\n",
      "4\u00024this is not possible because we would need to ﬁnd the roots of a poly-\n",
      "nomial of degree 5or higher. However, the Abel-Rufﬁni theorem (Rufﬁni, Abel-Rufﬁni\n",
      "theorem 1799; Abel, 1826) states that there exists no algebraic solution to this\n",
      "problem for polynomials of degree 5or more. Therefore, in practice, wenp.linalg.eigh\n",
      "or\n",
      "np.linalg.svdsolve for eigenvalues or singular values using iterative methods, which are\n",
      "implemented in all modern packages for linear algebra.\n",
      "In many applications (such as PCA presented in this chapter), we only\n",
      "require a few eigenvectors. It would be wasteful to compute the full de-\n",
      "composition, and then discard all eigenvectors with eigenvalues that are\n",
      "beyond the ﬁrst few. It turns out that if we are interested in only the ﬁrst\n",
      "few eigenvectors (with the largest eigenvalues), then iterative processes,\n",
      "which directly optimize these eigenvectors, are computationally more efﬁ-\n",
      "cient than a full eigendecomposition (or SVD). In the extreme case of only\n",
      "needing the ﬁrst eigenvector, a simple method called the power iteration power iteration\n",
      "is very efﬁcient. Power iteration chooses a random vector x0that is not in\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.5 PCA in High Dimensions 335\n",
      "the null space of Sand follows the iteration\n",
      "xk+1=Sxk\n",
      "kSxkk; k = 0;1;::: : (10.52)\n",
      "This means the vector xkis multiplied by Sin every iteration and then IfSis invertible, it\n",
      "is sufﬁcient to\n",
      "ensure thatx06=0.normalized, i.e., we always have kxkk= 1. This sequence of vectors con-\n",
      "verges to the eigenvector associated with the largest eigenvalue of S. The\n",
      "original Google PageRank algorithm (Page et al., 1999) uses such an al-\n",
      "gorithm for ranking web pages based on their hyperlinks.\n",
      "10.5 PCA in High Dimensions\n",
      "In order to do PCA, we need to compute the data covariance matrix. In D\n",
      "dimensions, the data covariance matrix is a D\u0002Dmatrix. Computing the\n",
      "eigenvalues and eigenvectors of this matrix is computationally expensive\n",
      "as it scales cubically in D. Therefore, PCA, as we discussed earlier, will be\n",
      "infeasible in very high dimensions. For example, if our xnare images with\n",
      "10;000pixels (e.g., 100\u0002100pixel images), we would need to compute\n",
      "the eigendecomposition of a 10;000\u000210;000covariance matrix. In the\n",
      "following, we provide a solution to this problem for the case that we have\n",
      "substantially fewer data points than dimensions, i.e., N\u001cD.\n",
      "Assume we have a centered dataset x1;:::;xN,xn2RD. Then the\n",
      "data covariance matrix is given as\n",
      "S=1\n",
      "NXX>2RD\u0002D; (10.53)\n",
      "whereX= [x1;:::;xN]is aD\u0002Nmatrix whose columns are the data\n",
      "points.\n",
      "We now assume that N\u001cD, i.e., the number of data points is smaller\n",
      "than the dimensionality of the data. If there are no duplicate data points,\n",
      "the rank of the covariance matrix SisN, so it hasD\u0000N+1many eigen-\n",
      "values that are 0. Intuitively, this means that there are some redundancies.\n",
      "In the following, we will exploit this and turn the D\u0002Dcovariance matrix\n",
      "into anN\u0002Ncovariance matrix whose eigenvalues are all positive.\n",
      "In PCA, we ended up with the eigenvector equation\n",
      "Sbm=\u0015mbm; m = 1;:::;M; (10.54)\n",
      "wherebmis a basis vector of the principal subspace. Let us rewrite this\n",
      "equation a bit: With Sdeﬁned in (10.53), we obtain\n",
      "Sbm=1\n",
      "NXX>bm=\u0015mbm: (10.55)\n",
      "We now multiply X>2RN\u0002Dfrom the left-hand side, which yields\n",
      "1\n",
      "NX>X|{z}\n",
      "N\u0002NX>bm|{z}\n",
      "=:cm=\u0015mX>bm()1\n",
      "NX>Xcm=\u0015mcm;(10.56)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "336 Dimensionality Reduction with Principal Component Analysis\n",
      "and we get a new eigenvector/eigenvalue equation: \u0015mremains eigen-\n",
      "value, which conﬁrms our results from Section 4.5.3 that the nonzero\n",
      "eigenvalues of XX>equal the nonzero eigenvalues of X>X. We obtain\n",
      "the eigenvector of the matrix1\n",
      "NX>X2RN\u0002Nassociated with \u0015mas\n",
      "cm:=X>bm. Assuming we have no duplicate data points, this matrix\n",
      "has rankNand is invertible. This also implies that1\n",
      "NX>Xhas the same\n",
      "(nonzero) eigenvalues as the data covariance matrix S. But this is now an\n",
      "N\u0002Nmatrix, so that we can compute the eigenvalues and eigenvectors\n",
      "much more efﬁciently than for the original D\u0002Ddata covariance matrix.\n",
      "Now that we have the eigenvectors of1\n",
      "NX>X, we are going to re-\n",
      "cover the original eigenvectors, which we still need for PCA. Currently,\n",
      "we know the eigenvectors of1\n",
      "NX>X. If we left-multiply our eigenvalue/\n",
      "eigenvector equation with X, we get\n",
      "1\n",
      "NXX>\n",
      "|{z}\n",
      "SXcm=\u0015mXcm (10.57)\n",
      "and we recover the data covariance matrix again. This now also means\n",
      "that we recover Xcmas an eigenvector of S.\n",
      "Remark. If we want to apply the PCA algorithm that we discussed in Sec-\n",
      "tion 10.6, we need to normalize the eigenvectors XcmofSso that they\n",
      "have norm 1. }\n",
      "10.6 Key Steps of PCA in Practice\n",
      "In the following, we will go through the individual steps of PCA using a\n",
      "running example, which is summarized in Figure 10.11. We are given a\n",
      "two-dimensional dataset (Figure 10.11(a)), and we want to use PCA to\n",
      "project it onto a one-dimensional subspace.\n",
      "1.Mean subtraction We start by centering the data by computing the\n",
      "mean\u0016of the dataset and subtracting it from every single data point.\n",
      "This ensures that the dataset has mean 0(Figure 10.11(b)). Mean sub-\n",
      "traction is not strictly necessary but reduces the risk of numerical prob-\n",
      "lems.\n",
      "2.Standardization Divide the data points by the standard deviation \u001bd\n",
      "of the dataset for every dimension d= 1;:::;D . Now the data is unit\n",
      "free, and it has variance 1along each axis, which is indicated by the\n",
      "two arrows in Figure 10.11(c). This step completes the standardization standardization\n",
      "of the data.\n",
      "3.Eigendecomposition of the covariance matrix Compute the data\n",
      "covariance matrix and its eigenvalues and corresponding eigenvectors.\n",
      "Since the covariance matrix is symmetric, the spectral theorem (The-\n",
      "orem 4.15) states that we can ﬁnd an ONB of eigenvectors. In Fig-\n",
      "ure 10.11(d), the eigenvectors are scaled by the magnitude of the cor-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.6 Key Steps of PCA in Practice 337\n",
      "Figure 10.11 Steps\n",
      "of PCA. (a) Original\n",
      "dataset;\n",
      "(b) centering;\n",
      "(c) divide by\n",
      "standard deviation;\n",
      "(d) eigendecomposi-\n",
      "tion; (e) projection;\n",
      "(f) mapping back to\n",
      "original data space.\n",
      "0 5\n",
      "x1−2.50.02.55.0x2\n",
      "(a) Original dataset.\n",
      "0 5\n",
      "x1−2.50.02.55.0x2\n",
      " (b) Step 1: Centering by sub-\n",
      "tracting the mean from each\n",
      "data point.\n",
      "0 5\n",
      "x1−2.50.02.55.0x2\n",
      "(c) Step 2: Dividing by the\n",
      "standard deviation to make\n",
      "the data unit free. Data has\n",
      "variance 1along each axis.\n",
      "0 5\n",
      "x1−2.50.02.55.0x2\n",
      "(d) Step 3: Compute eigenval-\n",
      "ues and eigenvectors (arrows)\n",
      "of the data covariance matrix\n",
      "(ellipse).\n",
      "0 5\n",
      "x1−2.50.02.55.0x2\n",
      "(e) Step 4: Project data onto\n",
      "the principal subspace.\n",
      "0 5\n",
      "x1−2.50.02.55.0x2\n",
      "(f) Undo the standardization\n",
      "and move projected data back\n",
      "into the original data space\n",
      "from (a).\n",
      "responding eigenvalue. The longer vector spans the principal subspace,\n",
      "which we denote by U. The data covariance matrix is represented by\n",
      "the ellipse.\n",
      "4.Projection We can project any data point x\u00032RDonto the principal\n",
      "subspace: To get this right, we need to standardize x\u0003using the mean\n",
      "\u0016dand standard deviation \u001bdof the training data in the dth dimension,\n",
      "respectively, so that\n",
      "x(d)\n",
      "\u0003 x(d)\n",
      "\u0003\u0000\u0016d\n",
      "\u001bd; d = 1;:::;D; (10.58)\n",
      "wherex(d)\n",
      "\u0003is thedth component of x\u0003. We obtain the projection as\n",
      "~x\u0003=BB>x\u0003 (10.59)\n",
      "with coordinates\n",
      "z\u0003=B>x\u0003 (10.60)\n",
      "with respect to the basis of the principal subspace. Here, Bis the ma-\n",
      "trix that contains the eigenvectors that are associated with the largest\n",
      "eigenvalues of the data covariance matrix as columns. PCA returns the\n",
      "coordinates (10.60), not the projections x\u0003.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "338 Dimensionality Reduction with Principal Component Analysis\n",
      "Having standardized our dataset, (10.59) only yields the projections in\n",
      "the context of the standardized dataset. To obtain our projection in the\n",
      "original data space (i.e., before standardization), we need to undo the\n",
      "standardization (10.58) and multiply by the standard deviation before\n",
      "adding the mean so that we obtain\n",
      "~x(d)\n",
      "\u0003 ~x(d)\n",
      "\u0003\u001bd+\u0016d; d = 1;:::;D: (10.61)\n",
      "Figure 10.11(f) illustrates the projection in the original data space.\n",
      "Example 10.4 (MNIST Digits: Reconstruction)\n",
      "In the following, we will apply PCA to the MNIST digits dataset, which\n",
      "contains 60;000examples of handwritten digits 0through 9. Each digit is\n",
      "an image of size 28\u000228, i.e., it contains 784pixels so that we can interpret\n",
      "every image in this dataset as a vector x2R784. Examples of these digits\n",
      "are shown in Figure 10.3.\n",
      "Figure 10.12 Effect\n",
      "of increasing the\n",
      "number of principal\n",
      "components on\n",
      "reconstruction.\n",
      "Original\n",
      "PCs: 1\n",
      "PCs: 10\n",
      "PCs: 100\n",
      "PCs: 500\n",
      "For illustration purposes, we apply PCA to a subset of the MNIST digits,\n",
      "and we focus on the digit “8”. We used 5,389 training images of the digit\n",
      "“8” and determined the principal subspace as detailed in this chapter. We\n",
      "then used the learned projection matrix to reconstruct a set of test im-\n",
      "ages, which is illustrated in Figure 10.12. The ﬁrst row of Figure 10.12\n",
      "shows a set of four original digits from the test set. The following rows\n",
      "show reconstructions of exactly these digits when using a principal sub-\n",
      "space of dimensions 1,10,100, and 500, respectively. We see that even\n",
      "with a single-dimensional principal subspace we get a halfway decent re-\n",
      "construction of the original digits, which, however, is blurry and generic.\n",
      "With an increasing number of principal components (PCs), the reconstruc-\n",
      "tions become sharper and more details are accounted for. With 500prin-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.7 Latent Variable Perspective 339\n",
      "cipal components, we effectively obtain a near-perfect reconstruction. If\n",
      "we were to choose 784PCs, we would recover the exact digit without any\n",
      "compression loss.\n",
      "Figure 10.13 shows the average squared reconstruction error, which is\n",
      "1\n",
      "NNX\n",
      "n=1kxn\u0000~xnk2=DX\n",
      "i=M+1\u0015i; (10.62)\n",
      "as a function of the number Mof principal components. We can see that\n",
      "the importance of the principal components drops off rapidly, and only\n",
      "marginal gains can be achieved by adding more PCs. This matches exactly\n",
      "our observation in Figure 10.5, where we discovered that the most of the\n",
      "variance of the projected data is captured by only a few principal compo-\n",
      "nents. With about 550PCs, we can essentially fully reconstruct the training\n",
      "data that contains the digit “8” (some pixels around the boundaries show\n",
      "no variation across the dataset as they are always black).\n",
      "Figure 10.13\n",
      "Average squared\n",
      "reconstruction error\n",
      "as a function of the\n",
      "number of principal\n",
      "components. The\n",
      "average squared\n",
      "reconstruction error\n",
      "is the sum of the\n",
      "eigenvalues in the\n",
      "orthogonal\n",
      "complement of the\n",
      "principal subspace.\n",
      "0 200 400 600 800\n",
      "Number of PCs0100200300400500Average squared reconstruction error\n",
      "10.7 Latent Variable Perspective\n",
      "In the previous sections, we derived PCA without any notion of a prob-\n",
      "abilistic model using the maximum-variance and the projection perspec-\n",
      "tives. On the one hand, this approach may be appealing as it allows us to\n",
      "sidestep all the mathematical difﬁculties that come with probability the-\n",
      "ory, but on the other hand, a probabilistic model would offer us more ﬂex-\n",
      "ibility and useful insights. More speciﬁcally, a probabilistic model would\n",
      "Come with a likelihood function, and we can explicitly deal with noisy\n",
      "observations (which we did not even discuss earlier)\n",
      "Allow us to do Bayesian model comparison via the marginal likelihood\n",
      "as discussed in Section 8.6\n",
      "View PCA as a generative model, which allows us to simulate new data\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "340 Dimensionality Reduction with Principal Component Analysis\n",
      "Allow us to make straightforward connections to related algorithms\n",
      "Deal with data dimensions that are missing at random by applying\n",
      "Bayes’ theorem\n",
      "Give us a notion of the novelty of a new data point\n",
      "Give us a principled way to extend the model, e.g., to a mixture of PCA\n",
      "models\n",
      "Have the PCA we derived in earlier sections as a special case\n",
      "Allow for a fully Bayesian treatment by marginalizing out the model\n",
      "parameters\n",
      "By introducing a continuous-valued latent variable z2RMit is possible\n",
      "to phrase PCA as a probabilistic latent-variable model. Tipping and Bishop\n",
      "(1999) proposed this latent-variable model as probabilistic PCA (PPCA ). probabilistic PCA\n",
      "PPCA PPCA addresses most of the aforementioned issues, and the PCA solution\n",
      "that we obtained by maximizing the variance in the projected space or\n",
      "by minimizing the reconstruction error is obtained as the special case of\n",
      "maximum likelihood estimation in a noise-free setting.\n",
      "10.7.1 Generative Process and Probabilistic Model\n",
      "In PPCA, we explicitly write down the probabilistic model for linear di-\n",
      "mensionality reduction. For this we assume a continuous latent variable\n",
      "z2RMwith a standard-normal prior p(z) =N\u00000;I\u0001\n",
      "and a linear rela-\n",
      "tionship between the latent variables and the observed xdata where\n",
      "x=Bz+\u0016+\u000f2RD; (10.63)\n",
      "where\u000f\u0018 N\u00000; \u001b2I\u0001\n",
      "is Gaussian observation noise and B2RD\u0002M\n",
      "and\u00162RDdescribe the linear/afﬁne mapping from latent to observed\n",
      "variables. Therefore, PPCA links latent and observed variables via\n",
      "p(xjz;B;\u0016;\u001b2) =N\u0000xjBz+\u0016; \u001b2I\u0001: (10.64)\n",
      "Overall, PPCA induces the following generative process:\n",
      "zn\u0018N\u0000zj0;I\u0001\n",
      "(10.65)\n",
      "xnjzn\u0018N\u0000xjBzn+\u0016; \u001b2I\u0001\n",
      "(10.66)\n",
      "To generate a data point that is typical given the model parameters, we\n",
      "follow an ancestral sampling scheme: We ﬁrst sample a latent variable zn ancestral sampling\n",
      "fromp(z). Then we use znin (10.64) to sample a data point conditioned\n",
      "on the sampled zn, i.e.,xn\u0018p(xjzn;B;\u0016;\u001b2).\n",
      "This generative process allows us to write down the probabilistic model\n",
      "(i.e., the joint distribution of all random variables; see Section 8.4) as\n",
      "p(x;zjB;\u0016;\u001b2) =p(xjz;B;\u0016;\u001b2)p(z); (10.67)\n",
      "which immediately gives rise to the graphical model in Figure 10.14 using\n",
      "the results from Section 8.5.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.7 Latent Variable Perspective 341\n",
      "Figure 10.14\n",
      "Graphical model for\n",
      "probabilistic PCA.\n",
      "The observations xn\n",
      "explicitly depend on\n",
      "corresponding\n",
      "latent variables\n",
      "zn\u0018N\u0000\n",
      "0;I\u0001\n",
      ". The\n",
      "model parameters\n",
      "B;\u0016and the\n",
      "likelihood\n",
      "parameter\u001bare\n",
      "shared across the\n",
      "dataset.xnBzn\n",
      "\u001b\u0016\n",
      "n= 1;:::;N\n",
      "Remark. Note the direction of the arrow that connects the latent variables\n",
      "zand the observed data x: The arrow points from ztox, which means\n",
      "that the PPCA model assumes a lower-dimensional latent cause zfor high-\n",
      "dimensional observations x. In the end, we are obviously interested in\n",
      "ﬁnding something out about zgiven some observations. To get there we\n",
      "will apply Bayesian inference to “invert” the arrow implicitly and go from\n",
      "observations to latent variables. }\n",
      "Example 10.5 (Generating New Data Using Latent Variables)\n",
      "Figure 10.15\n",
      "Generating new\n",
      "MNIST digits. The\n",
      "latent variables z\n",
      "can be used to\n",
      "generate new data\n",
      "~x=Bz. The closer\n",
      "we stay to the\n",
      "training data, the\n",
      "more realistic the\n",
      "generated data.\n",
      "Figure 10.15 shows the latent coordinates of the MNIST digits “8” found\n",
      "by PCA when using a two-dimensional principal subspace (blue dots). We\n",
      "can query any vector z\u0003in this latent space and generate an image ~x\u0003=\n",
      "Bz\u0003that resembles the digit “8”. We show eight of such generated images\n",
      "with their corresponding latent space representation. Depending on where\n",
      "we query the latent space, the generated images look different (shape,\n",
      "rotation, size, etc.). If we query away from the training data, we see more\n",
      "and more artifacts, e.g., the top-left and top-right digits. Note that the\n",
      "intrinsic dimensionality of these generated images is only two.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "342 Dimensionality Reduction with Principal Component Analysis\n",
      "10.7.2 Likelihood and Joint DistributionThe likelihood does\n",
      "not depend on the\n",
      "latent variables z.Using the results from Chapter 6, we obtain the likelihood of this proba-\n",
      "bilistic model by integrating out the latent variable z(see Section 8.4.3)\n",
      "so that\n",
      "p(xjB;\u0016;\u001b2) =Z\n",
      "p(xjz;B;\u0016;\u001b2)p(z)dz (10.68a)\n",
      "=Z\n",
      "N\u0000xjBz+\u0016; \u001b2I\u0001N\u0000zj0;I\u0001dz:(10.68b)\n",
      "From Section 6.5, we know that the solution to this integral is a Gaussian\n",
      "distribution with mean\n",
      "Ex[x] =Ez[Bz+\u0016] +E\u000f[\u000f] =\u0016 (10.69)\n",
      "and with covariance matrix\n",
      "V[x] =Vz[Bz+\u0016] +V\u000f[\u000f] =Vz[Bz] +\u001b2I (10.70a)\n",
      "=BVz[z]B>+\u001b2I=BB>+\u001b2I: (10.70b)\n",
      "The likelihood in (10.68b) can be used for maximum likelihood or MAP\n",
      "estimation of the model parameters.\n",
      "Remark. We cannot use the conditional distribution in (10.64) for maxi-\n",
      "mum likelihood estimation as it still depends on the latent variables. The\n",
      "likelihood function we require for maximum likelihood (or MAP) estima-\n",
      "tion should only be a function of the data xand the model parameters,\n",
      "but must not depend on the latent variables. }\n",
      "From Section 6.5, we know that a Gaussian random variable zand\n",
      "a linear/afﬁne transformation x=Bzof it are jointly Gaussian dis-\n",
      "tributed. We already know the marginals p(z) =N\u0000zj0;I\u0001\n",
      "andp(x) =\n",
      "N\u0000xj\u0016;BB>+\u001b2I\u0001\n",
      ". The missing cross-covariance is given as\n",
      "Cov[x;z] = Covz[Bz+\u0016] =BCovz[z;z] =B: (10.71)\n",
      "Therefore, the probabilistic model of PPCA, i.e., the joint distribution of\n",
      "latent and observed random variables is explicitly given by\n",
      "p(x;zjB;\u0016;\u001b2) =N\u0012\u0014x\n",
      "z\u0015\f\f\f\f\u0014\u0016\n",
      "0\u0015\n",
      ";\u0014BB>+\u001b2I B\n",
      "B>I\u0015\u0013\n",
      ";(10.72)\n",
      "with a mean vector of length D+Mand a covariance matrix of size\n",
      "(D+M)\u0002(D+M).\n",
      "10.7.3 Posterior Distribution\n",
      "The joint Gaussian distribution p(x;zjB;\u0016;\u001b2)in (10.72) allows us to\n",
      "determine the posterior distribution p(zjx)immediately by applying the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.8 Further Reading 343\n",
      "rules of Gaussian conditioning from Section 6.5.1. The posterior distribu-\n",
      "tion of the latent variable given an observation xis then\n",
      "p(zjx) =N\u0000zjm;C\u0001; (10.73)\n",
      "m=B>(BB>+\u001b2I)\u00001(x\u0000\u0016); (10.74)\n",
      "C=I\u0000B>(BB>+\u001b2I)\u00001B: (10.75)\n",
      "Note that the posterior covariance does not depend on the observed data\n",
      "x. For a new observation x\u0003in data space, we use (10.73) to determine\n",
      "the posterior distribution of the corresponding latent variable z\u0003. The co-\n",
      "variance matrix Callows us to assess how conﬁdent the embedding is. A\n",
      "covariance matrix Cwith a small determinant (which measures volumes)\n",
      "tells us that the latent embedding z\u0003is fairly certain. If we obtain a pos-\n",
      "terior distribution p(z\u0003jx\u0003)with much variance, we may be faced with\n",
      "an outlier. However, we can explore this posterior distribution to under-\n",
      "stand what other data points xare plausible under this posterior. To do\n",
      "this, we exploit the generative process underlying PPCA, which allows us\n",
      "to explore the posterior distribution on the latent variables by generating\n",
      "new data that is plausible under this posterior:\n",
      "1. Sample a latent variable z\u0003\u0018p(zjx\u0003)from the posterior distribution\n",
      "over the latent variables (10.73).\n",
      "2. Sample a reconstructed vector ~x\u0003\u0018p(xjz\u0003;B;\u0016;\u001b2) from (10.64).\n",
      "If we repeat this process many times, we can explore the posterior dis-\n",
      "tribution (10.73) on the latent variables z\u0003and its implications on the\n",
      "observed data. The sampling process effectively hypothesizes data, which\n",
      "is plausible under the posterior distribution.\n",
      "10.8 Further Reading\n",
      "We derived PCA from two perspectives: (a) maximizing the variance in the\n",
      "projected space; (b) minimizing the average reconstruction error. How-\n",
      "ever, PCA can also be interpreted from different perspectives. Let us recap\n",
      "what we have done: We took high-dimensional data x2RDand used\n",
      "a matrixB>to ﬁnd a lower-dimensional representation z2RM. The\n",
      "columns ofBare the eigenvectors of the data covariance matrix Sthat are\n",
      "associated with the largest eigenvalues. Once we have a low-dimensional\n",
      "representation z, we can get a high-dimensional version of it (in the orig-\n",
      "inal data space) as x\u0019~x=Bz=BB>x2RD, whereBB>is a\n",
      "projection matrix.\n",
      "We can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder\n",
      "ure 10.16. An auto-encoder encodes the data xn2RDto acodezn2RMcode\n",
      "and decodes it to a ~xnsimilar toxn. The mapping from the data to the\n",
      "code is called the encoder , and the mapping from the code back to the orig- encoder\n",
      "inal data space is called the decoder . If we consider linear mappings where decoder\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "344 Dimensionality Reduction with Principal Component Analysis\n",
      "Figure 10.16 PCA\n",
      "can be viewed as a\n",
      "linear auto-encoder.\n",
      "It encodes the\n",
      "high-dimensional\n",
      "dataxinto a\n",
      "lower-dimensional\n",
      "representation\n",
      "(code)z2RMand\n",
      "decodeszusing a\n",
      "decoder. The\n",
      "decoded vector ~xis\n",
      "the orthogonal\n",
      "projection of the\n",
      "original dataxonto\n",
      "theM-dimensional\n",
      "principal subspace.B>\n",
      "x ~xzB\n",
      "Encoder DecoderOriginal\n",
      "CodeRDRD\n",
      "RM\n",
      "the code is given by zn=B>xn2RMand we are interested in minimiz-\n",
      "ing the average squared error between the data xnand its reconstruction\n",
      "~xn=Bzn,n= 1;:::;N , we obtain\n",
      "1\n",
      "NNX\n",
      "n=1kxn\u0000~xnk2=1\n",
      "NNX\n",
      "2n\u0000BB>xn\n",
      ": (10.76)\n",
      "This means we end up with the same objective function as in (10.29) that\n",
      "we discussed in Section 10.3 so that we obtain the PCA solution when we\n",
      "minimize the squared auto-encoding loss. If we replace the linear map-\n",
      "ping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.\n",
      "A prominent example of this is a deep auto-encoder where the linear func-\n",
      "tions are replaced with deep neural networks. In this context, the encoder\n",
      "is also known as a recognition network orinference network , whereas the recognition network\n",
      "inference network decoder is also called a generator .\n",
      "generatorAnother interpretation of PCA is related to information theory. We can\n",
      "think of the code as a smaller or compressed version of the original data\n",
      "point. When we reconstruct our original data using the code, we do not\n",
      "get the exact data point back, but a slightly distorted or noisy version\n",
      "of it. This means that our compression is “lossy”. Intuitively, we want The code is a\n",
      "compressed version\n",
      "of the original data.to maximize the correlation between the original data and the lower-\n",
      "dimensional code. More formally, this is related to the mutual information.\n",
      "We would then get the same solution to PCA we discussed in Section 10.3\n",
      "by maximizing the mutual information, a core concept in information the-\n",
      "ory (MacKay, 2003).\n",
      "In our discussion on PPCA, we assumed that the parameters of the\n",
      "model, i.e.,B;\u0016, and the likelihood parameter \u001b2, are known. Tipping\n",
      "and Bishop (1999) describe how to derive maximum likelihood estimates\n",
      "for these parameters in the PPCA setting (note that we use a different\n",
      "notation in this chapter). The maximum likelihood parameters, when pro-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.8 Further Reading 345\n",
      "jectingD-dimensional data onto an M-dimensional subspace, are\n",
      "\u0016ML=1\n",
      "NNX\n",
      "n=1xn; (10.77)\n",
      "BML=T(\u0003\u0000\u001b2I)1\n",
      "2R; (10.78)\n",
      "\u001b2\n",
      "ML=1\n",
      "D\u0000MDX\n",
      "j=M+1\u0015j; (10.79)\n",
      "whereT2RD\u0002McontainsMeigenvectors of the data covariance matrix, The matrix \u0003\u0000\u001b2I\n",
      "in (10.78) is\n",
      "guaranteed to be\n",
      "positive semideﬁnite\n",
      "as the smallest\n",
      "eigenvalue of the\n",
      "data covariance\n",
      "matrix is bounded\n",
      "from below by the\n",
      "noise variance \u001b2.\u0003= diag(\u00151;:::;\u0015M)2RM\u0002Mis a diagonal matrix with the eigenvalues\n",
      "associated with the principal axes on its diagonal, and R2RM\u0002Mis\n",
      "an arbitrary orthogonal matrix. The maximum likelihood solution BMLis\n",
      "unique up to an arbitrary orthogonal transformation, e.g., we can right-\n",
      "multiplyBMLwith any rotation matrix Rso that (10.78) essentially is a\n",
      "singular value decomposition (see Section 4.5). An outline of the proof is\n",
      "given by Tipping and Bishop (1999).\n",
      "The maximum likelihood estimate for \u0016given in (10.77) is the sample\n",
      "mean of the data. The maximum likelihood estimator for the observation\n",
      "noise variance \u001b2given in (10.79) is the average variance in the orthog-\n",
      "onal complement of the principal subspace, i.e., the average leftover vari-\n",
      "ance that we cannot capture with the ﬁrst Mprincipal components is\n",
      "treated as observation noise.\n",
      "In the noise-free limit where \u001b!0, PPCA and PCA provide identical\n",
      "solutions: Since the data covariance matrix Sis symmetric, it can be di-\n",
      "agonalized (see Section 4.4), i.e., there exists a matrix Tof eigenvectors\n",
      "ofSso that\n",
      "S=T\u0003T\u00001: (10.80)\n",
      "In the PPCA model, the data covariance matrix is the covariance matrix of\n",
      "the Gaussian likelihood p(xjB;\u0016;\u001b2), which isBB>+\u001b2I, see (10.70b).\n",
      "For\u001b!0, we obtainBB>so that this data covariance must equal the\n",
      "PCA data covariance (and its factorization given in (10.80)) so that\n",
      "Cov[X] =T\u0003T\u00001=BB>()B=T\u00031\n",
      "2R; (10.81)\n",
      "i.e., we obtain the maximum likelihood estimate in (10.78) for \u001b= 0.\n",
      "From (10.78) and (10.80), it becomes clear that (P)PCA performs a de-\n",
      "composition of the data covariance matrix.\n",
      "In a streaming setting, where data arrives sequentially, it is recom-\n",
      "mended to use the iterative expectation maximization (EM) algorithm for\n",
      "maximum likelihood estimation (Roweis, 1998).\n",
      "To determine the dimensionality of the latent variables (the length of\n",
      "the code, the dimensionality of the lower-dimensional subspace onto which\n",
      "we project the data), Gavish and Donoho (2014) suggest the heuristic\n",
      "that, if we can estimate the noise variance \u001b2of the data, we should\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "346 Dimensionality Reduction with Principal Component Analysis\n",
      "discard all singular values smaller than4\u001bp\n",
      "Dp\n",
      "3. Alternatively, we can use\n",
      "(nested) cross-validation (Section 8.6.1) or Bayesian model selection cri-\n",
      "teria (discussed in Section 8.6.2) to determine a good estimate of the\n",
      "intrinsic dimensionality of the data (Minka, 2001b).\n",
      "Similar to our discussion on linear regression in Chapter 9, we can place\n",
      "a prior distribution on the parameters of the model and integrate them\n",
      "out. By doing so, we (a) avoid point estimates of the parameters and the\n",
      "issues that come with these point estimates (see Section 8.6) and (b) al-\n",
      "low for an automatic selection of the appropriate dimensionality Mof the\n",
      "latent space. In this Bayesian PCA , which was proposed by Bishop (1999), Bayesian PCA\n",
      "a priorp(\u0016;B;\u001b2)is placed on the model parameters. The generative\n",
      "process allows us to integrate the model parameters out instead of condi-\n",
      "tioning on them, which addresses overﬁtting issues. Since this integration\n",
      "is analytically intractable, Bishop (1999) proposes to use approximate in-\n",
      "ference methods, such as MCMC or variational inference. We refer to the\n",
      "work by Gilks et al. (1996) and Blei et al. (2017) for more details on these\n",
      "approximate inference techniques.\n",
      "In PPCA, we considered the linear model p(xnjzn) =N\u0000xnjBzn+\n",
      "\u0016; \u001b2I\u0001\n",
      "with priorp(zn) =N\u00000;I\u0001\n",
      ", where all observation dimensions\n",
      "are affected by the same amount of noise. If we allow each observation\n",
      "dimensiondto have a different variance \u001b2\n",
      "d, we obtain factor analysis factor analysis\n",
      "(FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA\n",
      "gives the likelihood some more ﬂexibility than PPCA, but still forces the\n",
      "data to be explained by the model parameters B;\u0016.However, FA no An overly ﬂexible\n",
      "likelihood would be\n",
      "able to explain more\n",
      "than just the noise.longer allows for a closed-form maximum likelihood solution so that we\n",
      "need to use an iterative scheme, such as the expectation maximization\n",
      "algorithm, to estimate the model parameters. While in PPCA all station-\n",
      "ary points are global optima, this no longer holds for FA. Compared to\n",
      "PPCA, FA does not change if we scale the data, but it does return different\n",
      "solutions if we rotate the data.\n",
      "An algorithm that is also closely related to PCA is independent com- independent\n",
      "component analysis ponent analysis (ICA(Hyvarinen et al., 2001)). Starting again with the\n",
      "ICAlatent-variable perspective p(xnjzn) =N\u0000xnjBzn+\u0016; \u001b2I\u0001\n",
      "we now\n",
      "change the prior on znto non-Gaussian distributions. ICA can be used\n",
      "forblind-source separation . Imagine you are in a busy train station with blind-source\n",
      "separation many people talking. Your ears play the role of microphones, and they\n",
      "linearly mix different speech signals in the train station. The goal of blind-\n",
      "source separation is to identify the constituent parts of the mixed signals.\n",
      "As discussed previously in the context of maximum likelihood estimation\n",
      "for PPCA, the original PCA solution is invariant to any rotation. Therefore,\n",
      "PCA can identify the best lower-dimensional subspace in which the sig-\n",
      "nals live, but not the signals themselves (Murphy, 2012). ICA addresses\n",
      "this issue by modifying the prior distribution p(z)on the latent sources\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "10.8 Further Reading 347\n",
      "to require non-Gaussian priors p(z). We refer to the books by Hyvarinen\n",
      "et al. (2001) and Murphy (2012) for more details on ICA.\n",
      "PCA, factor analysis, and ICA are three examples for dimensionality re-\n",
      "duction with linear models. Cunningham and Ghahramani (2015) provide\n",
      "a broader survey of linear dimensionality reduction.\n",
      "The (P)PCA model we discussed here allows for several important ex-\n",
      "tensions. In Section 10.5, we explained how to do PCA when the in-\n",
      "put dimensionality Dis signiﬁcantly greater than the number Nof data\n",
      "points. By exploiting the insight that PCA can be performed by computing\n",
      "(many) inner products, this idea can be pushed to the extreme by consid-\n",
      "ering inﬁnite-dimensional features. The kernel trick is the basis of kernel kernel trick\n",
      "kernel PCA PCAand allows us to implicitly compute inner products between inﬁnite-\n",
      "dimensional features (Sch ¨olkopf et al., 1998; Sch ¨olkopf and Smola, 2002).\n",
      "There are nonlinear dimensionality reduction techniques that are de-\n",
      "rived from PCA (Burges (2010) provides a good overview). The auto-\n",
      "encoder perspective of PCA that we discussed previously in this section\n",
      "can be used to render PCA as a special case of a deep auto-encoder . In the deep auto-encoder\n",
      "deep auto-encoder, both the encoder and the decoder are represented by\n",
      "multilayer feedforward neural networks, which themselves are nonlinear\n",
      "mappings. If we set the activation functions in these neural networks to be\n",
      "the identity, the model becomes equivalent to PCA. A different approach to\n",
      "nonlinear dimensionality reduction is the Gaussian process latent-variable Gaussian process\n",
      "latent-variable\n",
      "modelmodel (GP-LVM ) proposed by Lawrence (2005). The GP-LVM starts off with\n",
      "GP-LVMthe latent-variable perspective that we used to derive PPCA and replaces\n",
      "the linear relationship between the latent variables zand the observations\n",
      "xwith a Gaussian process (GP). Instead of estimating the parameters of\n",
      "the mapping (as we do in PPCA), the GP-LVM marginalizes out the model\n",
      "parameters and makes point estimates of the latent variables z. Similar\n",
      "to Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence Bayesian GP-LVM\n",
      "(2010) maintains a distribution on the latent variables zand uses approx-\n",
      "imate inference to integrate them out as well.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "11\n",
      "Density Estimation with Gaussian Mixture\n",
      "Models\n",
      "In earlier chapters, we covered already two fundamental problems in\n",
      "machine learning: regression (Chapter 9) and dimensionality reduction\n",
      "(Chapter 10). In this chapter, we will have a look at a third pillar of ma-\n",
      "chine learning: density estimation. On our journey, we introduce impor-\n",
      "tant concepts, such as the expectation maximization (EM) algorithm and\n",
      "a latent variable perspective of density estimation with mixture models.\n",
      "When we apply machine learning to data we often aim to represent\n",
      "data in some way. A straightforward way is to take the data points them-\n",
      "selves as the representation of the data; see Figure 11.1 for an example.\n",
      "However, this approach may be unhelpful if the dataset is huge or if we\n",
      "are interested in representing characteristics of the data. In density esti-\n",
      "mation, we represent the data compactly using a density from a paramet-\n",
      "ric family, e.g., a Gaussian or Beta distribution. For example, we may be\n",
      "looking for the mean and variance of a dataset in order to represent the\n",
      "data compactly using a Gaussian distribution. The mean and variance can\n",
      "be found using tools we discussed in Section 8.3: maximum likelihood or\n",
      "maximum a posteriori estimation. We can then use the mean and variance\n",
      "of this Gaussian to represent the distribution underlying the data, i.e., we\n",
      "think of the dataset to be a typical realization from this distribution if we\n",
      "were to sample from it.\n",
      "Figure 11.1\n",
      "Two-dimensional\n",
      "dataset that cannot\n",
      "be meaningfully\n",
      "represented by a\n",
      "Gaussian.\n",
      "−5 0 5\n",
      "x1−4−2024x2\n",
      "348\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "11.1 Gaussian Mixture Model 349\n",
      "In practice, the Gaussian (or similarly all other distributions we encoun-\n",
      "tered so far) have limited modeling capabilities. For example, a Gaussian\n",
      "approximation of the density that generated the data in Figure 11.1 would\n",
      "be a poor approximation. In the following, we will look at a more ex-\n",
      "pressive family of distributions, which we can use for density estimation:\n",
      "mixture models . mixture model\n",
      "Mixture models can be used to describe a distribution p(x)by a convex\n",
      "combination of Ksimple (base) distributions\n",
      "p(x) =KX\n",
      "k=1\u0019kpk(x) (11.1)\n",
      "06\u0019k61;KX\n",
      "k=1\u0019k= 1; (11.2)\n",
      "where the components pkare members of a family of basic distributions,\n",
      "e.g., Gaussians, Bernoullis, or Gammas, and the \u0019karemixture weights .mixture weight\n",
      "Mixture models are more expressive than the corresponding base distri-\n",
      "butions because they allow for multimodal data representations, i.e., they\n",
      "can describe datasets with multiple “clusters”, such as the example in Fig-\n",
      "ure 11.1.\n",
      "We will focus on Gaussian mixture models (GMMs), where the basic\n",
      "distributions are Gaussians. For a given dataset, we aim to maximize the\n",
      "likelihood of the model parameters to train the GMM. For this purpose,\n",
      "we will use results from Chapter 5, Chapter 6, and Section 7.2. However,\n",
      "unlike other applications we discussed earlier (linear regression or PCA),\n",
      "we will not ﬁnd a closed-form maximum likelihood solution. Instead, we\n",
      "will arrive at a set of dependent simultaneous equations, which we can\n",
      "only solve iteratively.\n",
      "11.1 Gaussian Mixture Model\n",
      "AGaussian mixture model is a density model where we combine a ﬁnite Gaussian mixture\n",
      "model number ofKGaussian distributions N\u0000xj\u0016k;\u0006k\u0001\n",
      "so that\n",
      "p(xj\u0012) =KX\n",
      "k=1\u0019kN\u0000xj\u0016k;\u0006k\u0001\n",
      "(11.3)\n",
      "06\u0019k61;KX\n",
      "k=1\u0019k= 1; (11.4)\n",
      "where we deﬁned \u0012:=f\u0016k;\u0006k;\u0019k:k= 1;:::;Kgas the collection of\n",
      "all parameters of the model. This convex combination of Gaussian distri-\n",
      "bution gives us signiﬁcantly more ﬂexibility for modeling complex densi-\n",
      "ties than a simple Gaussian distribution (which we recover from (11.3) for\n",
      "K= 1). An illustration is given in Figure 11.2, displaying the weighted\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "350 Density Estimation with Gaussian Mixture Models\n",
      "Figure 11.2\n",
      "Gaussian mixture\n",
      "model. The\n",
      "Gaussian mixture\n",
      "distribution (black)\n",
      "is composed of a\n",
      "convex combination\n",
      "of Gaussian\n",
      "distributions and is\n",
      "more expressive\n",
      "than any individual\n",
      "component. Dashed\n",
      "lines represent the\n",
      "weighted Gaussian\n",
      "components.\n",
      "\u00004\u00002 0 2 4 6 8\n",
      "x0:000:050:100:150:200:250:30p(x)Component 1\n",
      "Component 2\n",
      "Component 3\n",
      "GMM density\n",
      "components and the mixture density, which is given as\n",
      "p(xj\u0012) = 0:5N\u0000xj\u00002;1\n",
      "2\u0001+ 0:2N\u0000xj1;2\u0001+ 0:3N\u0000xj4;1\u0001:(11.5)\n",
      "11.2 Parameter Learning via Maximum Likelihood\n",
      "Assume we are given a dataset X=fx1;:::;xNg, wherexn; n =\n",
      "1;:::;N , are drawn i.i.d. from an unknown distribution p(x). Our ob-\n",
      "jective is to ﬁnd a good approximation/representation of this unknown\n",
      "distribution p(x)by means of a GMM with Kmixture components. The\n",
      "parameters of the GMM are the Kmeans\u0016k, the covariances \u0006k, and\n",
      "mixture weights \u0019k. We summarize all these free parameters in \u0012:=\n",
      "f\u0019k;\u0016k;\u0006k:k= 1;:::;Kg.\n",
      "Example 11.1 (Initial Setting)\n",
      "Figure 11.3 Initial\n",
      "setting: GMM\n",
      "(black) with\n",
      "mixture three\n",
      "mixture components\n",
      "(dashed) and seven\n",
      "data points (discs).\n",
      "−5 0 5 10 15\n",
      "x0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density\n",
      "Throughout this chapter, we will have a simple running example that\n",
      "helps us illustrate and visualize important concepts.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.2 Parameter Learning via Maximum Likelihood 351\n",
      "We consider a one-dimensional dataset X=f\u00003;\u00002:5;\u00001;0;2;4;5g\n",
      "consisting of seven data points and wish to ﬁnd a GMM with K= 3\n",
      "components that models the density of the data. We initialize the mixture\n",
      "components as\n",
      "p1(x) =N\u0000xj\u00004;1\u0001\n",
      "(11.6)\n",
      "p2(x) =N\u0000xj0;0:2\u0001\n",
      "(11.7)\n",
      "p3(x) =N\u0000xj8;3\u0001\n",
      "(11.8)\n",
      "and assign them equal weights \u00191=\u00192=\u00193=1\n",
      "3. The corresponding\n",
      "model (and the data points) are shown in Figure 11.3.\n",
      "In the following, we detail how to obtain a maximum likelihood esti-\n",
      "mate\u0012MLof the model parameters \u0012. We start by writing down the like-\n",
      "lihood, i.e., the predictive distribution of the training data given the pa-\n",
      "rameters. We exploit our i.i.d. assumption, which leads to the factorized\n",
      "likelihood\n",
      "p(Xj\u0012) =NY\n",
      "n=1p(xnj\u0012); p(xnj\u0012) =KX\n",
      "k=1\u0019kN\u0000xnj\u0016k;\u0006k\u0001;(11.9)\n",
      "where every individual likelihood term p(xnj\u0012)is a Gaussian mixture\n",
      "density. Then we obtain the log-likelihood as\n",
      "logp(Xj\u0012) =NX\n",
      "n=1logp(xnj\u0012) =NX\n",
      "n=1logKX\n",
      "k=1\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "|{z }\n",
      "=:L:(11.10)\n",
      "We aim to ﬁnd parameters \u0012\u0003\n",
      "MLthat maximize the log-likelihood Ldeﬁned\n",
      "in (11.10). Our “normal” procedure would be to compute the gradient\n",
      "dL=d\u0012of the log-likelihood with respect to the model parameters \u0012, set\n",
      "it to0, and solve for \u0012. However, unlike our previous examples for max-\n",
      "imum likelihood estimation (e.g., when we discussed linear regression in\n",
      "Section 9.2), we cannot obtain a closed-form solution. However, we can\n",
      "exploit an iterative scheme to ﬁnd good model parameters \u0012ML, which will\n",
      "turn out to be the EM algorithm for GMMs. The key idea is to update one\n",
      "model parameter at a time while keeping the others ﬁxed.\n",
      "Remark. If we were to consider a single Gaussian as the desired density,\n",
      "the sum over kin (11.10) vanishes, and the logcan be applied directly to\n",
      "the Gaussian component, such that we get\n",
      "logN\u0000xj\u0016;\u0006\u0001=\u0000D\n",
      "2log(2\u0019)\u00001\n",
      "2log det( \u0006)\u00001\n",
      "2(x\u0000\u0016)>\u0006\u00001(x\u0000\u0016):\n",
      "(11.11)\n",
      "This simple form allows us to ﬁnd closed-form maximum likelihood esti-\n",
      "mates of\u0016and\u0006, as discussed in Chapter 8. In (11.10), we cannot move\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "352 Density Estimation with Gaussian Mixture Models\n",
      "theloginto the sum over kso that we cannot obtain a simple closed-form\n",
      "maximum likelihood solution. }\n",
      "Any local optimum of a function exhibits the property that its gradi-\n",
      "ent with respect to the parameters must vanish (necessary condition); see\n",
      "Chapter 7. In our case, we obtain the following necessary conditions when\n",
      "we optimize the log-likelihood in (11.10) with respect to the GMM param-\n",
      "eters\u0016k;\u0006k;\u0019k:\n",
      "@L\n",
      "@\u0016k=0>()NX\n",
      "n=1@logp(xnj\u0012)\n",
      "@\u0016k=0>; (11.12)\n",
      "@L\n",
      "@\u0006k=0()NX\n",
      "n=1@logp(xnj\u0012)\n",
      "@\u0006k=0; (11.13)\n",
      "@L\n",
      "@\u0019k= 0()NX\n",
      "n=1@logp(xnj\u0012)\n",
      "@\u0019k= 0: (11.14)\n",
      "For all three necessary conditions, by applying the chain rule (see Sec-\n",
      "tion 5.2.2), we require partial derivatives of the form\n",
      "@logp(xnj\u0012)\n",
      "@\u0012=1\n",
      "p(xnj\u0012)@p(xnj\u0012)\n",
      "@\u0012; (11.15)\n",
      "where\u0012=f\u0016k;\u0006k;\u0019k;k= 1;:::;Kgare the model parameters and\n",
      "1\n",
      "p(xnj\u0012)=1\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001: (11.16)\n",
      "In the following, we will compute the partial derivatives (11.12) through\n",
      "(11.14). But before we do this, we introduce a quantity that will play a\n",
      "central role in the remainder of this chapter: responsibilities.\n",
      "11.2.1 Responsibilities\n",
      "We deﬁne the quantity\n",
      "rnk:=\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001 (11.17)\n",
      "as the responsibility of thekth mixture component for the nth data point. responsibility\n",
      "The responsibility rnkof thekth mixture component for data point xnis\n",
      "proportional to the likelihood\n",
      "p(xnj\u0019k;\u0016k;\u0006k) =\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "(11.18)\n",
      "of the mixture component given the data point. Therefore, mixture com- rnfollows a\n",
      "Boltzmann/Gibbs\n",
      "distribution.ponents have a high responsibility for a data point when the data point\n",
      "could be a plausible sample from that mixture component. Note that\n",
      "rn:= [rn1;:::;rnK]>2RKis a (normalized) probability vector, i.e.,\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.2 Parameter Learning via Maximum Likelihood 353\n",
      "P\n",
      "krnk= 1 withrnk>0. This probability vector distributes probabil-\n",
      "ity mass among the Kmixture components, and we can think of rnas a\n",
      "“soft assignment” of xnto theKmixture components. Therefore, the re- The responsibility\n",
      "rnkis the\n",
      "probability that the\n",
      "kth mixture\n",
      "component\n",
      "generated the nth\n",
      "data point.sponsibility rnkfrom (11.17) represents the probability that xnhas been\n",
      "generated by the kth mixture component.\n",
      "Example 11.2 (Responsibilities)\n",
      "For our example from Figure 11.3, we compute the responsibilities rnk\n",
      "2\n",
      "6666666641:0 0:0 0:0\n",
      "1:0 0:0 0:0\n",
      "0:057 0:943 0:0\n",
      "0:001 0:999 0:0\n",
      "0:0 0:066 0:934\n",
      "0:0 0:0 1:0\n",
      "0:0 0:0 1:03\n",
      "7777777752RN\u0002K: (11.19)\n",
      "Here thenth row tells us the responsibilities of all mixture components\n",
      "forxn. The sum of all Kresponsibilities for a data point (sum of every\n",
      "row) is 1. Thekth column gives us an overview of the responsibility of\n",
      "thekth mixture component. We can see that the third mixture component\n",
      "(third column) is not responsible for any of the ﬁrst four data points, but\n",
      "takes much responsibility of the remaining data points. The sum of all\n",
      "entries of a column gives us the values Nk, i.e., the total responsibility of\n",
      "thekth mixture component. In our example, we get N1= 2:058; N 2=\n",
      "2:008; N3= 2:934.\n",
      "In the following, we determine the updates of the model parameters\n",
      "\u0016k;\u0006k;\u0019kfor given responsibilities. We will see that the update equa-\n",
      "tions all depend on the responsibilities, which makes a closed-form solu-\n",
      "tion to the maximum likelihood estimation problem impossible. However,\n",
      "for given responsibilities we will be updating one model parameter at a\n",
      "time, while keeping the others ﬁxed. After this, we will recompute the\n",
      "responsibilities. Iterating these two steps will eventually converge to a lo-\n",
      "cal optimum and is a speciﬁc instantiation of the EM algorithm. We will\n",
      "discuss this in some more detail in Section 11.3.\n",
      "11.2.2 Updating the Means\n",
      "Theorem 11.1 (Update of the GMM Means) .The update of the mean pa-\n",
      "rameters\u0016k,k= 1;:::;K , of the GMM is given by\n",
      "\u0016new\n",
      "k=PN\n",
      "n=1rnkxnPN\n",
      "n=1rnk; (11.20)\n",
      "where the responsibilities rnkare deﬁned in (11.17) .\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "354 Density Estimation with Gaussian Mixture Models\n",
      "Remark. The update of the means \u0016kof the individual mixture compo-\n",
      "nents in (11.20) depends on all means, covariance matrices \u0006k, and mix-\n",
      "ture weights \u0019kviarnkgiven in (11.17). Therefore, we cannot obtain a\n",
      "closed-form solution for all \u0016kat once. }\n",
      "Proof From (11.15), we see that the gradient of the log-likelihood with\n",
      "respect to the mean parameters \u0016k,k= 1;:::;K , requires us to compute\n",
      "the partial derivative\n",
      "@p(xnj\u0012)\n",
      "@\u0016k=KX\n",
      "j=1\u0019j@N\u0000xnj\u0016j;\u0006j\u0001\n",
      "@\u0016k=\u0019k@N\u0000xnj\u0016k;\u0006k\u0001\n",
      "@\u0016k(11.21a)\n",
      "=\u0019k(xn\u0000\u0016k)>\u0006\u00001\n",
      "kN\u0000xnj\u0016k;\u0006k\u0001; (11.21b)\n",
      "where we exploited that only the kth mixture component depends on \u0016k.\n",
      "We use our result from (11.21b) in (11.15) and put everything together\n",
      "so that the desired partial derivative of Lwith respect to \u0016kis given as\n",
      "@L\n",
      "@\u0016k=NX\n",
      "n=1@logp(xnj\u0012)\n",
      "@\u0016k=NX\n",
      "n=11\n",
      "p(xnj\u0012)@p(xnj\u0012)\n",
      "@\u0016k(11.22a)\n",
      "=NX\n",
      "n=1(xn\u0000\u0016k)>\u0006\u00001\n",
      "k\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001\n",
      "|{z}\n",
      "=rnk(11.22b)\n",
      "=NX\n",
      "n=1rnk(xn\u0000\u0016k)>\u0006\u00001\n",
      "k: (11.22c)\n",
      "Here we used the identity from (11.16) and the result of the partial deriva-\n",
      "tive in (11.21b) to get to (11.22b). The values rnkare the responsibilities\n",
      "we deﬁned in (11.17).\n",
      "We now solve (11.22c) for \u0016new\n",
      "kso that@L(\u0016new\n",
      "k)\n",
      "@\u0016k=0>and obtain\n",
      "NX\n",
      "n=1rnkxn=NX\n",
      "n=1rnk\u0016new\n",
      "k()\u0016new\n",
      "k=PN\n",
      "n=1rnkxn\n",
      "PN\n",
      "n=1rnk=1\n",
      "NkNX\n",
      "n=1rnkxn;\n",
      "(11.23)\n",
      "where we deﬁned\n",
      "Nk:=NX\n",
      "n=1rnk (11.24)\n",
      "as the total responsibility of the kth mixture component for the entire\n",
      "dataset. This concludes the proof of Theorem 11.1.\n",
      "Intuitively, (11.20) can be interpreted as an importance-weighted Monte\n",
      "Carlo estimate of the mean, where the importance weights of data point\n",
      "xnare the responsibilities rnkof thekth cluster for xn,k= 1;:::;K .\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.2 Parameter Learning via Maximum Likelihood 355\n",
      "Therefore, the mean \u0016kis pulled toward a data point xnwith strength Figure 11.4 Update\n",
      "of the mean\n",
      "parameter of\n",
      "mixture component\n",
      "in a GMM. The\n",
      "mean\u0016is being\n",
      "pulled toward\n",
      "individual data\n",
      "points with the\n",
      "weights given by the\n",
      "corresponding\n",
      "responsibilities.\n",
      "r1r2\n",
      "r3x1x2x3\n",
      "\u0016given byrnk. The means are pulled stronger toward data points for which\n",
      "the corresponding mixture component has a high responsibility, i.e., a high\n",
      "likelihood. Figure 11.4 illustrates this. We can also interpret the mean up-\n",
      "date in (11.20) as the expected value of all data points under the distri-\n",
      "bution given by\n",
      "rk:= [r1k;:::;rNk]>=Nk; (11.25)\n",
      "which is a normalized probability vector, i.e.,\n",
      "\u0016k Erk[X]: (11.26)\n",
      "Example 11.3 (Mean Updates)\n",
      "Figure 11.5 Effect\n",
      "of updating the\n",
      "mean values in a\n",
      "GMM. (a) GMM\n",
      "before updating the\n",
      "mean values;\n",
      "(b) GMM after\n",
      "updating the mean\n",
      "values\u0016kwhile\n",
      "retaining the\n",
      "variances and\n",
      "mixture weights.\n",
      "−5 0 5 10 15\n",
      "x0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density (a) GMM density and individual components\n",
      "prior to updating the mean values.\n",
      "−5 0 5 10 15\n",
      "x0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density(b) GMM density and individual components\n",
      "after updating the mean values.\n",
      "In our example from Figure 11.3, the mean values are updated as fol-\n",
      "lows:\n",
      "\u00161:\u00004!\u00002:7 (11.27)\n",
      "\u00162: 0!\u00000:4 (11.28)\n",
      "\u00163: 8!3:7 (11.29)\n",
      "Here we see that the means of the ﬁrst and third mixture component\n",
      "move toward the regime of the data, whereas the mean of the second\n",
      "component does not change so dramatically. Figure 11.5 illustrates this\n",
      "change, where Figure 11.5(a) shows the GMM density prior to updating\n",
      "the means and Figure 11.5(b) shows the GMM density after updating the\n",
      "mean values \u0016k.\n",
      "The update of the mean parameters in (11.20) look fairly straight-\n",
      "forward. However, note that the responsibilities rnkare a function of\n",
      "\u0019j;\u0016j;\u0006jfor allj= 1;:::;K , such that the updates in (11.20) depend\n",
      "on all parameters of the GMM, and a closed-form solution, which we ob-\n",
      "tained for linear regression in Section 9.2 or PCA in Chapter 10, cannot\n",
      "be obtained.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "356 Density Estimation with Gaussian Mixture Models\n",
      "11.2.3 Updating the Covariances\n",
      "Theorem 11.2 (Updates of the GMM Covariances) .The update of the co-\n",
      "variance parameters \u0006k,k= 1;:::;K of the GMM is given by\n",
      "\u0006new\n",
      "k=1\n",
      "NkNX\n",
      "n=1rnk(xn\u0000\u0016k)(xn\u0000\u0016k)>; (11.30)\n",
      "wherernkandNkare deﬁned in (11.17) and(11.24) , respectively.\n",
      "Proof To prove Theorem 11.2, our approach is to compute the partial\n",
      "derivatives of the log-likelihood Lwith respect to the covariances \u0006k, set\n",
      "them to 0, and solve for \u0006k. We start with our general approach\n",
      "@L\n",
      "@\u0006k=NX\n",
      "n=1@logp(xnj\u0012)\n",
      "@\u0006k=NX\n",
      "n=11\n",
      "p(xnj\u0012)@p(xnj\u0012)\n",
      "@\u0006k: (11.31)\n",
      "We already know 1=p(xnj\u0012)from (11.16). To obtain the remaining par-\n",
      "tial derivative @p(xnj\u0012)=@\u0006k, we write down the deﬁnition of the Gaus-\n",
      "sian distribution p(xnj\u0012)(see (11.9)) and drop all terms but the kth. We\n",
      "then obtain\n",
      "@p(xnj\u0012)\n",
      "@\u0006k(11.32a)\n",
      "=@\n",
      "@\u0006k\u0012\n",
      "\u0019k(2\u0019)\u0000D\n",
      "2det(\u0006k)\u00001\n",
      "2exp\u0000\u00001\n",
      "2(xn\u0000\u0016k)>\u0006\u00001\n",
      "k(xn\u0000\u0016k)\u0001\u0013\n",
      "(11.32b)\n",
      "=\u0019k(2\u0019)\u0000D\n",
      "2\u0014@\n",
      "@\u0006kdet(\u0006k)\u00001\n",
      "2exp\u0000\u00001\n",
      "2(xn\u0000\u0016k)>\u0006\u00001\n",
      "k(xn\u0000\u0016k)\u0001\n",
      "+ det( \u0006k)\u00001\n",
      "2@\n",
      "@\u0006kexp\u0000\u00001\n",
      "2(xn\u0000\u0016k)>\u0006\u00001\n",
      "k(xn\u0000\u0016k)\u0001\u0015\n",
      ":(11.32c)\n",
      "We now use the identities\n",
      "@\n",
      "@\u0006kdet(\u0006k)\u00001\n",
      "2(5.101)=\u00001\n",
      "2det(\u0006k)\u00001\n",
      "2\u0006\u00001\n",
      "k; (11.33)\n",
      "@\n",
      "@\u0006k(xn\u0000\u0016k)>\u0006\u00001\n",
      "k(xn\u0000\u0016k)(5.103)=\u0000\u0006\u00001\n",
      "k(xn\u0000\u0016k)(xn\u0000\u0016k)>\u0006\u00001\n",
      "k\n",
      "(11.34)\n",
      "and obtain (after some rearranging) the desired partial derivative required\n",
      "in (11.31) as\n",
      "@p(xnj\u0012)\n",
      "@\u0006k=\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "\u0001\u0002\u00001\n",
      "2(\u0006\u00001\n",
      "k\u0000\u0006\u00001\n",
      "k(xn\u0000\u0016k)(xn\u0000\u0016k)>\u0006\u00001\n",
      "k)\u0003:(11.35)\n",
      "Putting everything together, the partial derivative of the log-likelihood\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.2 Parameter Learning via Maximum Likelihood 357\n",
      "with respect to \u0006kis given by\n",
      "@L\n",
      "@\u0006k=NX\n",
      "n=1@logp(xnj\u0012)\n",
      "@\u0006k=NX\n",
      "n=11\n",
      "p(xnj\u0012)@p(xnj\u0012)\n",
      "@\u0006k(11.36a)\n",
      "=NX\n",
      "n=1\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001\n",
      "|{z}\n",
      "=rnk\n",
      "\u0001\u0002\u00001\n",
      "2(\u0006\u00001\n",
      "k\u0000\u0006\u00001\n",
      "k(xn\u0000\u0016k)(xn\u0000\u0016k)>\u0006\u00001\n",
      "k)\u0003\n",
      "(11.36b)\n",
      "=\u00001\n",
      "2NX\n",
      "n=1rnk(\u0006\u00001\n",
      "k\u0000\u0006\u00001\n",
      "k(xn\u0000\u0016k)(xn\u0000\u0016k)>\u0006\u00001\n",
      "k) (11.36c)\n",
      "=\u00001\n",
      "2\u0006\u00001\n",
      "kNX\n",
      "n=1rnk\n",
      "|{z}\n",
      "=Nk+1\n",
      "2\u0006\u00001\n",
      "k NX\n",
      "n=1rnk(xn\u0000\u0016k)(xn\u0000\u0016k)>!\n",
      "\u0006\u00001\n",
      "k:\n",
      "(11.36d)\n",
      "We see that the responsibilities rnkalso appear in this partial derivative.\n",
      "Setting this partial derivative to 0, we obtain the necessary optimality\n",
      "condition\n",
      "Nk\u0006\u00001\n",
      "k=\u0006\u00001\n",
      "k NX\n",
      "n=1rnk(xn\u0000\u0016k)(xn\u0000\u0016k)>!\n",
      "\u0006\u00001\n",
      "k (11.37a)\n",
      "()NkI= NX\n",
      "n=1rnk(xn\u0000\u0016k)(xn\u0000\u0016k)>!\n",
      "\u0006\u00001\n",
      "k: (11.37b)\n",
      "By solving for \u0006k, we obtain\n",
      "\u0006new\n",
      "k=1\n",
      "NkNX\n",
      "n=1rnk(xn\u0000\u0016k)(xn\u0000\u0016k)>; (11.38)\n",
      "whererkis the probability vector deﬁned in (11.25). This gives us a sim-\n",
      "ple update rule for \u0006kfork= 1;:::;K and proves Theorem 11.2.\n",
      "Similar to the update of \u0016kin (11.20), we can interpret the update of\n",
      "the covariance in (11.30) as an importance-weighted expected value of\n",
      "the square of the centered data ~Xk:=fx1\u0000\u0016k;:::;xN\u0000\u0016kg.\n",
      "Example 11.4 (Variance Updates)\n",
      "In our example from Figure 11.3, the variances are updated as follows:\n",
      "\u001b2\n",
      "1: 1!0:14 (11.39)\n",
      "\u001b2\n",
      "2: 0:2!0:44 (11.40)\n",
      "\u001b2\n",
      "3: 3!1:53 (11.41)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "358 Density Estimation with Gaussian Mixture Models\n",
      "Here we see that the variances of the ﬁrst and third component shrink\n",
      "signiﬁcantly, whereas the variance of the second component increases\n",
      "slightly.\n",
      "Figure 11.6 illustrates this setting. Figure 11.6(a) is identical (but\n",
      "zoomed in) to Figure 11.5(b) and shows the GMM density and its indi-\n",
      "vidual components prior to updating the variances. Figure 11.6(b) shows\n",
      "the GMM density after updating the variances.\n",
      "Figure 11.6 Effect\n",
      "of updating the\n",
      "variances in a GMM.\n",
      "(a) GMM before\n",
      "updating the\n",
      "variances; (b) GMM\n",
      "after updating the\n",
      "variances while\n",
      "retaining the means\n",
      "and mixture\n",
      "weights.\n",
      "−4−2 0 2 4 6 8\n",
      "x0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density\n",
      "(a) GMM density and individual components\n",
      "prior to updating the variances.\n",
      "−4−2 0 2 4 6 8\n",
      "x0.000.050.100.150.200.250.300.35p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density(b) GMM density and individual components\n",
      "after updating the variances.\n",
      "Similar to the update of the mean parameters, we can interpret (11.30)\n",
      "as a Monte Carlo estimate of the weighted covariance of data points xn\n",
      "associated with the kth mixture component, where the weights are the\n",
      "responsibilities rnk. As with the updates of the mean parameters, this up-\n",
      "date depends on all \u0019j;\u0016j;\u0006j; j= 1;:::;K , through the responsibilities\n",
      "rnk, which prohibits a closed-form solution.\n",
      "11.2.4 Updating the Mixture Weights\n",
      "Theorem 11.3 (Update of the GMM Mixture Weights) .The mixture weights\n",
      "of the GMM are updated as\n",
      "\u0019new\n",
      "k=Nk\n",
      "N; k = 1;:::;K; (11.42)\n",
      "whereNis the number of data points and Nkis deﬁned in (11.24) .\n",
      "Proof To ﬁnd the partial derivative of the log-likelihood with respect\n",
      "to the weight parameters \u0019k,k= 1;:::;K , we account for the con-\n",
      "straintP\n",
      "k\u0019k= 1 by using Lagrange multipliers (see Section 7.2). The\n",
      "Lagrangian is\n",
      "L=L+\u0015 KX\n",
      "k=1\u0019k\u00001!\n",
      "(11.43a)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.2 Parameter Learning via Maximum Likelihood 359\n",
      "=NX\n",
      "n=1logKX\n",
      "k=1\u0019kN\u0000xnj\u0016k;\u0006k\u0001+\u0015 KX\n",
      "k=1\u0019k\u00001!\n",
      "; (11.43b)\n",
      "whereLis the log-likelihood from (11.10) and the second term encodes\n",
      "for the equality constraint that all the mixture weights need to sum up to\n",
      "1. We obtain the partial derivative with respect to \u0019kas\n",
      "@L\n",
      "@\u0019k=NX\n",
      "n=1N\u0000xnj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001+\u0015 (11.44a)\n",
      "=1\n",
      "\u0019kNX\n",
      "n=1\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001\n",
      "|{z }\n",
      "=Nk+\u0015=Nk\n",
      "\u0019k+\u0015; (11.44b)\n",
      "and the partial derivative with respect to the Lagrange multiplier \u0015as\n",
      "@L\n",
      "@\u0015=KX\n",
      "k=1\u0019k\u00001: (11.45)\n",
      "Setting both partial derivatives to 0(necessary condition for optimum)\n",
      "yields the system of equations\n",
      "\u0019k=\u0000Nk\n",
      "\u0015; (11.46)\n",
      "1 =KX\n",
      "k=1\u0019k: (11.47)\n",
      "Using (11.46) in (11.47) and solving for \u0019k, we obtain\n",
      "KX\n",
      "k=1\u0019k= 1() \u0000KX\n",
      "k=1Nk\n",
      "\u0015= 1() \u0000N\n",
      "\u0015= 1()\u0015=\u0000N:\n",
      "(11.48)\n",
      "This allows us to substitute \u0000Nfor\u0015in (11.46) to obtain\n",
      "\u0019new\n",
      "k=Nk\n",
      "N; (11.49)\n",
      "which gives us the update for the weight parameters \u0019kand proves Theo-\n",
      "rem 11.3.\n",
      "We can identify the mixture weight in (11.42) as the ratio of the to-\n",
      "tal responsibility of the kth cluster and the number of data points. Since\n",
      "N=P\n",
      "kNk, the number of data points can also be interpreted as the\n",
      "total responsibility of all mixture components together, such that \u0019kis the\n",
      "relative importance of the kth mixture component for the dataset.\n",
      "Remark. SinceNk=PN\n",
      "i=1rnk, the update equation (11.42) for the mix-\n",
      "ture weights \u0019kalso depends on all \u0019j;\u0016j;\u0006j;j= 1;:::;K via the re-\n",
      "sponsibilities rnk. }\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "360 Density Estimation with Gaussian Mixture Models\n",
      "Example 11.5 (Weight Parameter Updates)\n",
      "Figure 11.7 Effect\n",
      "of updating the\n",
      "mixture weights in a\n",
      "GMM. (a) GMM\n",
      "before updating the\n",
      "mixture weights;\n",
      "(b) GMM after\n",
      "updating the\n",
      "mixture weights\n",
      "while retaining the\n",
      "means and\n",
      "variances. Note the\n",
      "different scales of\n",
      "the vertical axes.\n",
      "−4−2 0 2 4 6 8\n",
      "x0.000.050.100.150.200.250.300.35p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density(a) GMM density and individual components\n",
      "prior to updating the mixture weights.\n",
      "−4−2 0 2 4 6 8\n",
      "x0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density(b) GMM density and individual components\n",
      "after updating the mixture weights.\n",
      "In our running example from Figure 11.3, the mixture weights are up-\n",
      "dated as follows:\n",
      "\u00191:1\n",
      "3!0:29 (11.50)\n",
      "\u00192:1\n",
      "3!0:29 (11.51)\n",
      "\u00193:1\n",
      "3!0:42 (11.52)\n",
      "Here we see that the third component gets more weight/importance,\n",
      "while the other components become slightly less important. Figure 11.7\n",
      "illustrates the effect of updating the mixture weights. Figure 11.7(a) is\n",
      "identical to Figure 11.6(b) and shows the GMM density and its individual\n",
      "components prior to updating the mixture weights. Figure 11.7(b) shows\n",
      "the GMM density after updating the mixture weights.\n",
      "Overall, having updated the means, the variances, and the weights\n",
      "once, we obtain the GMM shown in Figure 11.7(b). Compared with the\n",
      "initialization shown in Figure 11.3, we can see that the parameter updates\n",
      "caused the GMM density to shift some of its mass toward the data points.\n",
      "After updating the means, variances, and weights once, the GMM ﬁt\n",
      "in Figure 11.7(b) is already remarkably better than its initialization from\n",
      "Figure 11.3. This is also evidenced by the log-likelihood values, which in-\n",
      "creased from 28:3(initialization) to 14:4after one complete update cycle.\n",
      "11.3 EM Algorithm\n",
      "Unfortunately, the updates in (11.20), (11.30), and (11.42) do not consti-\n",
      "tute a closed-form solution for the updates of the parameters \u0016k;\u0006k;\u0019k\n",
      "of the mixture model because the responsibilities rnkdepend on those pa-\n",
      "rameters in a complex way. However, the results suggest a simple iterative\n",
      "scheme for ﬁnding a solution to the parameters estimation problem via\n",
      "maximum likelihood. The expectation maximization algorithm ( EM algo- EM algorithm\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.3 EM Algorithm 361\n",
      "rithm ) was proposed by Dempster et al. (1977) and is a general iterative\n",
      "scheme for learning parameters (maximum likelihood or MAP) in mixture\n",
      "models and, more generally, latent-variable models.\n",
      "In our example of the Gaussian mixture model, we choose initial values\n",
      "for\u0016k;\u0006k;\u0019kand alternate until convergence between\n",
      "E-step: Evaluate the responsibilities rnk(posterior probability of data\n",
      "pointnbelonging to mixture component k).\n",
      "M-step: Use the updated responsibilities to reestimate the parameters\n",
      "\u0016k;\u0006k;\u0019k.\n",
      "Every step in the EM algorithm increases the log-likelihood function (Neal\n",
      "and Hinton, 1999). For convergence, we can check the log-likelihood or\n",
      "the parameters directly. A concrete instantiation of the EM algorithm for\n",
      "estimating the parameters of a GMM is as follows:\n",
      "1. Initialize\u0016k;\u0006k;\u0019k.\n",
      "2.E-step: Evaluate responsibilities rnkfor every data point xnusing cur-\n",
      "rent parameters \u0019k;\u0016k;\u0006k:\n",
      "rnk=\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "P\n",
      "j\u0019jN\u0000xnj\u0016j;\u0006j\u0001: (11.53)\n",
      "3.M-step: Reestimate parameters \u0019k;\u0016k;\u0006kusing the current responsi-\n",
      "bilitiesrnk(from E-step): Having updated the\n",
      "means\u0016k\n",
      "in (11.54), they are\n",
      "subsequently used\n",
      "in (11.55) to update\n",
      "the corresponding\n",
      "covariances.\u0016k=1\n",
      "NkNX\n",
      "n=1rnkxn; (11.54)\n",
      "\u0006k=1\n",
      "NkNX\n",
      "n=1rnk(xn\u0000\u0016k)(xn\u0000\u0016k)>; (11.55)\n",
      "\u0019k=Nk\n",
      "N: (11.56)\n",
      "Example 11.6 (GMM Fit)\n",
      "Figure 11.8 EM\n",
      "algorithm applied to\n",
      "the GMM from\n",
      "Figure 11.2. (a)\n",
      "Final GMM ﬁt;\n",
      "(b) negative\n",
      "log-likelihood as a\n",
      "function of the EM\n",
      "iteration.\n",
      "−5 0 5 10 15\n",
      "x0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\n",
      "1)\n",
      "π2N(x|µ2,σ2\n",
      "2)\n",
      "π3N(x|µ3,σ2\n",
      "3)\n",
      "GMM density\n",
      "(a) Final GMM ﬁt. After ﬁve iterations, the EM\n",
      "algorithm converges and returns this GMM.\n",
      "0 1 2 3 4 5\n",
      "Iteration1416182022242628Negative log-likelihood\n",
      "(b) Negative log-likelihood as a function of the\n",
      "EM iterations.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "362 Density Estimation with Gaussian Mixture Models\n",
      "Figure 11.9\n",
      "Illustration of the\n",
      "EM algorithm for\n",
      "ﬁtting a Gaussian\n",
      "mixture model with\n",
      "three components to\n",
      "a two-dimensional\n",
      "dataset. (a) Dataset;\n",
      "(b) negative\n",
      "log-likelihood\n",
      "(lower is better) as\n",
      "a function of the EM\n",
      "iterations. The red\n",
      "dots indicate the\n",
      "iterations for which\n",
      "the mixture\n",
      "components of the\n",
      "corresponding GMM\n",
      "ﬁts are shown in (c)\n",
      "through (f). The\n",
      "yellow discs indicate\n",
      "the means of the\n",
      "Gaussian mixture\n",
      "components.\n",
      "Figure 11.10(a)\n",
      "shows the ﬁnal\n",
      "GMM ﬁt.\n",
      "−10−5 0 5 10\n",
      "x1−10−50510x2\n",
      "(a) Dataset.\n",
      "0 20 40 60\n",
      "EM iteration104\n",
      "4×1036×103Negative log-likelihood (b) Negative log-likelihood.\n",
      "−10−5 0 5 10\n",
      "x1−10−50510x2\n",
      "(c) EM initialization.\n",
      "−10−5 0 5 10\n",
      "x1−10−50510x2\n",
      " (d) EM after one iteration.\n",
      "−10−5 0 5 10\n",
      "x1−10−50510x2\n",
      "(e) EM after 10iterations.\n",
      "−10−5 0 5 10\n",
      "x1−10−50510x2\n",
      " (f) EM after 62iterations.\n",
      "When we run EM on our example from Figure 11.3, we obtain the ﬁnal\n",
      "result shown in Figure 11.8(a) after ﬁve iterations, and Figure 11.8(b)\n",
      "shows how the negative log-likelihood evolves as a function of the EM\n",
      "iterations. The ﬁnal GMM is given as\n",
      "p(x) = 0:29N\u0000xj\u00002:75;0:06\u0001+ 0:28N\u0000xj\u00000:50;0:25\u0001\n",
      "+ 0:43N\u0000xj3:64;1:63\u0001:(11.57)\n",
      "We applied the EM algorithm to the two-dimensional dataset shown\n",
      "in Figure 11.1 with K= 3 mixture components. Figure 11.9 illustrates\n",
      "some steps of the EM algorithm and shows the negative log-likelihood as\n",
      "a function of the EM iteration (Figure 11.9(b)). Figure 11.10(a) shows\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.4 Latent-Variable Perspective 363\n",
      "Figure 11.10 GMM\n",
      "ﬁt and\n",
      "responsibilities\n",
      "when EM converges.\n",
      "(a) GMM ﬁt when\n",
      "EM converges;\n",
      "(b) each data point\n",
      "is colored according\n",
      "to the\n",
      "responsibilities of\n",
      "the mixture\n",
      "components.\n",
      "−5 0 5\n",
      "x1−6−4−20246x2\n",
      "(a) GMM ﬁt after 62iterations.\n",
      "−5 0 5\n",
      "x1−6−4−20246x2 (b) Dataset colored according to the respon-\n",
      "sibilities of the mixture components.\n",
      "the corresponding ﬁnal GMM ﬁt. Figure 11.10(b) visualizes the ﬁnal re-\n",
      "sponsibilities of the mixture components for the data points. The dataset is\n",
      "colored according to the responsibilities of the mixture components when\n",
      "EM converges. While a single mixture component is clearly responsible\n",
      "for the data on the left, the overlap of the two data clusters on the right\n",
      "could have been generated by two mixture components. It becomes clear\n",
      "that there are data points that cannot be uniquely assigned to a single\n",
      "component (either blue or yellow), such that the responsibilities of these\n",
      "two clusters for those points are around 0:5.\n",
      "11.4 Latent-Variable Perspective\n",
      "We can look at the GMM from the perspective of a discrete latent-variable\n",
      "model, i.e., where the latent variable zcan attain only a ﬁnite set of val-\n",
      "ues. This is in contrast to PCA, where the latent variables were continuous-\n",
      "valued numbers in RM.\n",
      "The advantages of the probabilistic perspective are that (i) it will jus-\n",
      "tify some ad hoc decisions we made in the previous sections, (ii) it allows\n",
      "for a concrete interpretation of the responsibilities as posterior probabil-\n",
      "ities, and (iii) the iterative algorithm for updating the model parameters\n",
      "can be derived in a principled manner as the EM algorithm for maximum\n",
      "likelihood parameter estimation in latent-variable models.\n",
      "11.4.1 Generative Process and Probabilistic Model\n",
      "To derive the probabilistic model for GMMs, it is useful to think about the\n",
      "generative process, i.e., the process that allows us to generate data, using\n",
      "a probabilistic model.\n",
      "We assume a mixture model with Kcomponents and that a data point\n",
      "xcan be generated by exactly one mixture component. We introduce a\n",
      "binary indicator variable zk2f0;1gwith two states (see Section 6.2) that\n",
      "indicates whether the kth mixture component generated that data point\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "364 Density Estimation with Gaussian Mixture Models\n",
      "so that\n",
      "p(xjzk= 1) =N\u0000xj\u0016k;\u0006k\u0001: (11.58)\n",
      "We deﬁnez:= [z1;:::;zK]>2RKas a probability vector consisting of\n",
      "K\u00001many 0s and exactly one 1. For example, for K= 3, a validzwould\n",
      "bez= [z1;z2;z3]>= [0;1;0]>, which would select the second mixture\n",
      "component since z2= 1.\n",
      "Remark. Sometimes this kind of probability distribution is called “multi-\n",
      "noulli”, a generalization of the Bernoulli distribution to more than two\n",
      "values (Murphy, 2012). }\n",
      "The properties of zimply thatPK\n",
      "k=1zk= 1. Therefore,zis aone-hot one-hot encoding\n",
      "encoding (also: 1-of-Krepresentation ). 1-of-K\n",
      "representation Thus far, we assumed that the indicator variables zkare known. How-\n",
      "ever, in practice, this is not the case, and we place a prior distribution\n",
      "p(z) =\u0019= [\u00191;:::;\u0019K]>;KX\n",
      "k=1\u0019k= 1; (11.59)\n",
      "on the latent variable z. Then thekth entry\n",
      "\u0019k=p(zk= 1) (11.60)\n",
      "of this probability vector describes the probability that the kth mixture\n",
      "component generated data point x. Figure 11.11\n",
      "Graphical model for\n",
      "a GMM with a single\n",
      "data point.\n",
      "\u0019\n",
      "z\n",
      "x \u0006k\u0016k\n",
      "k= 1;:::;KRemark (Sampling from a GMM) .The construction of this latent-variable\n",
      "model (see the corresponding graphical model in Figure 11.11) lends it-\n",
      "self to a very simple sampling procedure (generative process) to generate\n",
      "data:\n",
      "1. Samplez(i)\u0018p(z).\n",
      "2. Samplex(i)\u0018p(xjz(i)= 1).\n",
      "In the ﬁrst step, we select a mixture component i(via the one-hot encod-\n",
      "ingz) at random according to p(z) =\u0019; in the second step we draw a\n",
      "sample from the corresponding mixture component. When we discard the\n",
      "samples of the latent variable so that we are left with the x(i), we have\n",
      "valid samples from the GMM. This kind of sampling, where samples of\n",
      "random variables depend on samples from the variable’s parents in the\n",
      "graphical model, is called ancestral sampling . } ancestral sampling\n",
      "Generally, a probabilistic model is deﬁned by the joint distribution of\n",
      "the data and the latent variables (see Section 8.4). With the prior p(z)\n",
      "deﬁned in (11.59) and (11.60) and the conditional p(xjz)from (11.58),\n",
      "we obtain all Kcomponents of this joint distribution via\n",
      "p(x;zk= 1) =p(xjzk= 1)p(zk= 1) =\u0019kN\u0000xj\u0016k;\u0006k\u0001\n",
      "(11.61)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.4 Latent-Variable Perspective 365\n",
      "fork= 1;:::;K , so that\n",
      "p(x;z) =2\n",
      "64p(x;z1= 1)\n",
      "...\n",
      "p(x;zK= 1)3\n",
      "75=2\n",
      "64\u00191N\u0000xj\u00161;\u00061\u0001\n",
      "...\n",
      "\u0019KN\u0000xj\u0016K;\u0006K\u00013\n",
      "75; (11.62)\n",
      "which fully speciﬁes the probabilistic model.\n",
      "11.4.2 Likelihood\n",
      "To obtain the likelihood p(xj\u0012)in a latent-variable model, we need to\n",
      "marginalize out the latent variables (see Section 8.4.3). In our case, this\n",
      "can be done by summing out all latent variables from the joint p(x;z)\n",
      "in (11.62) so that\n",
      "p(xj\u0012) =X\n",
      "zp(xj\u0012;z)p(zj\u0012);\u0012:=f\u0016k;\u0006k;\u0019k:k= 1;:::;Kg:\n",
      "(11.63)\n",
      "We now explicitly condition on the parameters \u0012of the probabilistic model,\n",
      "which we previously omitted. In (11.63), we sum over all Kpossible one-\n",
      "hot encodings of z, which is denoted byP\n",
      "z. Since there is only a single\n",
      "nonzero single entry in each zthere are only Kpossible conﬁgurations/\n",
      "settings ofz. For example, if K= 3, thenzcan have the conﬁgurations\n",
      "2\n",
      "41\n",
      "0\n",
      "03\n",
      "5;2\n",
      "40\n",
      "1\n",
      "03\n",
      "5;2\n",
      "40\n",
      "0\n",
      "13\n",
      "5: (11.64)\n",
      "Summing over all possible conﬁgurations of zin (11.63) is equivalent to\n",
      "looking at the nonzero entry of the z-vector and writing\n",
      "p(xj\u0012) =X\n",
      "zp(xj\u0012;z)p(zj\u0012) (11.65a)\n",
      "=KX\n",
      "k=1p(xj\u0012;zk= 1)p(zk= 1j\u0012) (11.65b)\n",
      "so that the desired marginal distribution is given as\n",
      "p(xj\u0012)(11.65b)=KX\n",
      "k=1p(xj\u0012;zk= 1)p(zk= 1j\u0012) (11.66a)\n",
      "=KX\n",
      "k=1\u0019kN\u0000xj\u0016k;\u0006k\u0001; (11.66b)\n",
      "which we identify as the GMM model from (11.3). Given a dataset X, we\n",
      "immediately obtain the likelihood\n",
      "p(Xj\u0012) =NY\n",
      "n=1p(xnj\u0012)(11.66b)=NY\n",
      "n=1KX\n",
      "k=1\u0019kN\u0000xnj\u0016k;\u0006k\u0001;(11.67)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "366 Density Estimation with Gaussian Mixture Models\n",
      "Figure 11.12\n",
      "Graphical model for\n",
      "a GMM with Ndata\n",
      "points.\u0019\n",
      "zn\n",
      "xn \u0006k\u0016k\n",
      "n= 1;:::;Nk= 1;:::;K\n",
      "which is exactly the GMM likelihood from (11.9). Therefore, the latent-\n",
      "variable model with latent indicators zkis an equivalent way of thinking\n",
      "about a Gaussian mixture model.\n",
      "11.4.3 Posterior Distribution\n",
      "Let us have a brief look at the posterior distribution on the latent variable\n",
      "z. According to Bayes’ theorem, the posterior of the kth component having\n",
      "generated data point x\n",
      "p(zk= 1jx) =p(zk= 1)p(xjzk= 1)\n",
      "p(x); (11.68)\n",
      "where the marginal p(x)is given in (11.66b). This yields the posterior\n",
      "distribution for the kth indicator variable zk\n",
      "p(zk= 1jx) =p(zk= 1)p(xjzk= 1)\n",
      "PK\n",
      "j=1p(zj= 1)p(xjzj= 1)=\u0019kN\u0000xj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xj\u0016j;\u0006j\u0001;\n",
      "(11.69)\n",
      "which we identify as the responsibility of the kth mixture component for\n",
      "data pointx. Note that we omitted the explicit conditioning on the GMM\n",
      "parameters \u0019k;\u0016k;\u0006kwherek= 1;:::;K .\n",
      "11.4.4 Extension to a Full Dataset\n",
      "Thus far, we have only discussed the case where the dataset consists only\n",
      "of a single data point x. However, the concepts of the prior and posterior\n",
      "can be directly extended to the case of Ndata pointsX:=fx1;:::;xNg.\n",
      "In the probabilistic interpretation of the GMM, every data point xnpos-\n",
      "sesses its own latent variable\n",
      "zn= [zn1;:::;znK]>2RK: (11.70)\n",
      "Previously (when we only considered a single data point x), we omitted\n",
      "the indexn, but now this becomes important.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.4 Latent-Variable Perspective 367\n",
      "We share the same prior distribution \u0019across all latent variables zn.\n",
      "The corresponding graphical model is shown in Figure 11.12, where we\n",
      "use the plate notation.\n",
      "The conditional distribution p(x1;:::;xNjz1;:::;zN)factorizes over\n",
      "the data points and is given as\n",
      "p(x1;:::;xNjz1;:::;zN) =NY\n",
      "n=1p(xnjzn): (11.71)\n",
      "To obtain the posterior distribution p(znk= 1jxn), we follow the same\n",
      "reasoning as in Section 11.4.3 and apply Bayes’ theorem to obtain\n",
      "p(znk= 1jxn) =p(xnjznk= 1)p(znk= 1)\n",
      "PK\n",
      "j=1p(xnjznj= 1)p(znj= 1)(11.72a)\n",
      "=\u0019kN\u0000xnj\u0016k;\u0006k\u0001\n",
      "PK\n",
      "j=1\u0019jN\u0000xnj\u0016j;\u0006j\u0001=rnk: (11.72b)\n",
      "This means that p(zk= 1jxn)is the (posterior) probability that the kth\n",
      "mixture component generated data point xnand corresponds to the re-\n",
      "sponsibility rnkwe introduced in (11.17). Now the responsibilities also\n",
      "have not only an intuitive but also a mathematically justiﬁed interpreta-\n",
      "tion as posterior probabilities.\n",
      "11.4.5 EM Algorithm Revisited\n",
      "The EM algorithm that we introduced as an iterative scheme for maximum\n",
      "likelihood estimation can be derived in a principled way from the latent-\n",
      "variable perspective. Given a current setting \u0012(t)of model parameters, the\n",
      "E-step calculates the expected log-likelihood\n",
      "Q(\u0012j\u0012(t)) =Ezjx;\u0012(t)[logp(x;zj\u0012)] (11.73a)\n",
      "=Z\n",
      "logp(x;zj\u0012)p(zjx;\u0012(t))dz; (11.73b)\n",
      "where the expectation of logp(x;zj\u0012)is taken with respect to the poste-\n",
      "riorp(zjx;\u0012(t))of the latent variables. The M-step selects an updated set\n",
      "of model parameters \u0012(t+1)by maximizing (11.73b).\n",
      "Although an EM iteration does increase the log-likelihood, there are\n",
      "no guarantees that EM converges to the maximum likelihood solution.\n",
      "It is possible that the EM algorithm converges to a local maximum of\n",
      "the log-likelihood. Different initializations of the parameters \u0012could be\n",
      "used in multiple EM runs to reduce the risk of ending up in a bad local\n",
      "optimum. We do not go into further details here, but refer to the excellent\n",
      "expositions by Rogers and Girolami (2016) and Bishop (2006).\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "368 Density Estimation with Gaussian Mixture Models\n",
      "11.5 Further Reading\n",
      "The GMM can be considered a generative model in the sense that it is\n",
      "straightforward to generate new data using ancestral sampling (Bishop,\n",
      "2006). For given GMM parameters \u0019k;\u0016k;\u0006k,k= 1;:::;K , we sample\n",
      "an indexkfrom the probability vector [\u00191;:::;\u0019K]>and then sample a\n",
      "data pointx\u0018N\u0000\u0016k;\u0006k\u0001\n",
      ". If we repeat this Ntimes, we obtain a dataset\n",
      "that has been generated by a GMM. Figure 11.1 was generated using this\n",
      "procedure.\n",
      "Throughout this chapter, we assumed that the number of components\n",
      "Kis known. In practice, this is often not the case. However, we could use\n",
      "nested cross-validation, as discussed in Section 8.6.1, to ﬁnd good models.\n",
      "Gaussian mixture models are closely related to the K-means clustering\n",
      "algorithm.K-means also uses the EM algorithm to assign data points to\n",
      "clusters. If we treat the means in the GMM as cluster centers and ignore\n",
      "the covariances (or set them to I), we arrive at K-means. As also nicely\n",
      "described by MacKay (2003), K-means makes a “hard” assignment of data\n",
      "points to cluster centers \u0016k, whereas a GMM makes a “soft” assignment\n",
      "via the responsibilities.\n",
      "We only touched upon the latent-variable perspective of GMMs and the\n",
      "EM algorithm. Note that EM can be used for parameter learning in general\n",
      "latent-variable models, e.g., nonlinear state-space models (Ghahramani\n",
      "and Roweis, 1999; Roweis and Ghahramani, 1999) and for reinforcement\n",
      "learning as discussed by Barber (2012). Therefore, the latent-variable per-\n",
      "spective of a GMM is useful to derive the corresponding EM algorithm in\n",
      "a principled way (Bishop, 2006; Barber, 2012; Murphy, 2012).\n",
      "We only discussed maximum likelihood estimation (via the EM algo-\n",
      "rithm) for ﬁnding GMM parameters. The standard criticisms of maximum\n",
      "likelihood also apply here:\n",
      "As in linear regression, maximum likelihood can suffer from severe\n",
      "overﬁtting. In the GMM case, this happens when the mean of a mix-\n",
      "ture component is identical to a data point and the covariance tends to\n",
      "0. Then, the likelihood approaches inﬁnity. Bishop (2006) and Barber\n",
      "(2012) discuss this issue in detail.\n",
      "We only obtain a point estimate of the parameters \u0019k;\u0016k;\u0006kfork=\n",
      "1;:::;K , which does not give any indication of uncertainty in the pa-\n",
      "rameter values. A Bayesian approach would place a prior on the param-\n",
      "eters, which can be used to obtain a posterior distribution on the param-\n",
      "eters. This posterior allows us to compute the model evidence (marginal\n",
      "likelihood), which can be used for model comparison, which gives us a\n",
      "principled way to determine the number of mixture components. Un-\n",
      "fortunately, closed-form inference is not possible in this setting because\n",
      "there is no conjugate prior for this model. However, approximations,\n",
      "such as variational inference, can be used to obtain an approximate\n",
      "posterior (Bishop, 2006).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "11.5 Further Reading 369\n",
      "Figure 11.13\n",
      "Histogram (orange\n",
      "bars) and kernel\n",
      "density estimation\n",
      "(blue line). The\n",
      "kernel density\n",
      "estimator produces\n",
      "a smooth estimate\n",
      "of the underlying\n",
      "density, whereas the\n",
      "histogram is an\n",
      "unsmoothed count\n",
      "measure of how\n",
      "many data points\n",
      "(black) fall into a\n",
      "single bin.\n",
      "\u00004\u00002 0 2 4 6 8\n",
      "x0:000:050:100:150:200:250:30p(x)\n",
      "Data\n",
      "KDE\n",
      "Histogram In this chapter, we discussed mixture models for density estimation.\n",
      "There is a plethora of density estimation techniques available. In practice,\n",
      "we often use histograms and kernel density estimation. histogram\n",
      "Histograms provide a nonparametric way to represent continuous den-\n",
      "sities and have been proposed by Pearson (1895). A histogram is con-\n",
      "structed by “binning” the data space and count, how many data points fall\n",
      "into each bin. Then a bar is drawn at the center of each bin, and the height\n",
      "of the bar is proportional to the number of data points within that bin. The\n",
      "bin size is a critical hyperparameter, and a bad choice can lead to overﬁt-\n",
      "ting and underﬁtting. Cross-validation, as discussed in Section 8.2.4, can\n",
      "be used to determine a good bin size. kernel density\n",
      "estimation Kernel density estimation , independently proposed by Rosenblatt (1956)\n",
      "and Parzen (1962), is a nonparametric way for density estimation. Given\n",
      "Ni.i.d. samples, the kernel density estimator represents the underlying\n",
      "distribution as\n",
      "p(x) =1\n",
      "NhNX\n",
      "n=1k\u0012x\u0000xn\n",
      "h\u0013\n",
      "; (11.74)\n",
      "wherekis a kernel function, i.e., a nonnegative function that integrates to\n",
      "1andh >0is a smoothing/bandwidth parameter, which plays a similar\n",
      "role as the bin size in histograms. Note that we place a kernel on every\n",
      "single data point xnin the dataset. Commonly used kernel functions are\n",
      "the uniform distribution and the Gaussian distribution. Kernel density esti-\n",
      "mates are closely related to histograms, but by choosing a suitable kernel,\n",
      "we can guarantee smoothness of the density estimate. Figure 11.13 illus-\n",
      "trates the difference between a histogram and a kernel density estimator\n",
      "(with a Gaussian-shaped kernel) for a given dataset of 250data points.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "12\n",
      "Classiﬁcation with Support Vector Machines\n",
      "In many situations, we want our machine learning algorithm to predict\n",
      "one of a number of (discrete) outcomes. For example, an email client sorts\n",
      "mail into personal mail and junk mail, which has two outcomes. Another\n",
      "example is a telescope that identiﬁes whether an object in the night sky\n",
      "is a galaxy, star, or planet. There are usually a small number of outcomes,\n",
      "and more importantly there is usually no additional structure on these\n",
      "outcomes. In this chapter, we consider predictors that output binary val- An example of\n",
      "structure is if the\n",
      "outcomes were\n",
      "ordered, like in the\n",
      "case of small,\n",
      "medium, and large\n",
      "t-shirts.ues, i.e., there are only two possible outcomes. This machine learning task\n",
      "is called binary classiﬁcation . This is in contrast to Chapter 9, where we\n",
      "binary classiﬁcationconsidered a prediction problem with continuous-valued outputs.\n",
      "For binary classiﬁcation, the set of possible values that the label/output\n",
      "can attain is binary, and for this chapter we denote them by f+1;\u00001g. In\n",
      "other words, we consider predictors of the form\n",
      "f:RD!f+1;\u00001g: (12.1)\n",
      "Recall from Chapter 8 that we represent each example (data point) xn\n",
      "as a feature vector of Dreal numbers. The labels are often referred to as Input example xn\n",
      "may also be referred\n",
      "to as inputs, data\n",
      "points, features, or\n",
      "instances.the positive and negative classes , respectively. One should be careful not\n",
      "classto infer intuitive attributes of positiveness of the +1class. For example,\n",
      "in a cancer detection task, a patient with cancer is often labeled +1. In\n",
      "principle, any two distinct values can be used, e.g., fTrue;Falseg,f0;1g\n",
      "orfred;blueg. The problem of binary classiﬁcation is well studied, and For probabilistic\n",
      "models, it is\n",
      "mathematically\n",
      "convenient to use\n",
      "f0;1gas a binary\n",
      "representation; see\n",
      "the remark after\n",
      "Example 6.12.we defer a survey of other approaches to Section 12.6.\n",
      "We present an approach known as the support vector machine (SVM),\n",
      "which solves the binary classiﬁcation task. As in regression, we have a su-\n",
      "pervised learning task, where we have a set of examples xn2RDalong\n",
      "with their corresponding (binary) labels yn2f+1;\u00001g. Given a train-\n",
      "ing data set consisting of example–label pairs f(x1;y1);:::; (xN;yN)g, we\n",
      "would like to estimate parameters of the model that will give the smallest\n",
      "classiﬁcation error. Similar to Chapter 9, we consider a linear model, and\n",
      "hide away the nonlinearity in a transformation \u001eof the examples (9.13).\n",
      "We will revisit \u001ein Section 12.4.\n",
      "The SVM provides state-of-the-art results in many applications, with\n",
      "sound theoretical guarantees (Steinwart and Christmann, 2008). There\n",
      "are two main reasons why we chose to illustrate binary classiﬁcation using\n",
      "370\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "Classiﬁcation with Support Vector Machines 371\n",
      "Figure 12.1\n",
      "Example 2D data,\n",
      "illustrating the\n",
      "intuition of data\n",
      "where we can ﬁnd a\n",
      "linear classiﬁer that\n",
      "separates orange\n",
      "crosses from blue\n",
      "discs.\n",
      "x(1)x(2)\n",
      "SVMs. First, the SVM allows for a geometric way to think about supervised\n",
      "machine learning. While in Chapter 9 we considered the machine learning\n",
      "problem in terms of probabilistic models and attacked it using maximum\n",
      "likelihood estimation and Bayesian inference, here we will consider an\n",
      "alternative approach where we reason geometrically about the machine\n",
      "learning task. It relies heavily on concepts, such as inner products and\n",
      "projections, which we discussed in Chapter 3. The second reason why we\n",
      "ﬁnd SVMs instructive is that in contrast to Chapter 9, the optimization\n",
      "problem for SVM does not admit an analytic solution so that we need to\n",
      "resort to a variety of optimization tools introduced in Chapter 7.\n",
      "The SVM view of machine learning is subtly different from the max-\n",
      "imum likelihood view of Chapter 9. The maximum likelihood view pro-\n",
      "poses a model based on a probabilistic view of the data distribution, from\n",
      "which an optimization problem is derived. In contrast, the SVM view starts\n",
      "by designing a particular function that is to be optimized during training,\n",
      "based on geometric intuitions. We have seen something similar already\n",
      "in Chapter 10, where we derived PCA from geometric principles. In the\n",
      "SVM case, we start by designing a loss function that is to be minimized\n",
      "on training data, following the principles of empirical risk minimization\n",
      "(Section 8.2).\n",
      "Let us derive the optimization problem corresponding to training an\n",
      "SVM on example–label pairs. Intuitively, we imagine binary classiﬁcation\n",
      "data, which can be separated by a hyperplane as illustrated in Figure 12.1.\n",
      "Here, every example xn(a vector of dimension 2) is a two-dimensional\n",
      "location (x(1)\n",
      "nandx(2)\n",
      "n), and the corresponding binary label ynis one of\n",
      "two different symbols (orange cross or blue disc). “Hyperplane” is a word\n",
      "that is commonly used in machine learning, and we encountered hyper-\n",
      "planes already in Section 2.8. A hyperplane is an afﬁne subspace of di-\n",
      "mensionD\u00001(if the corresponding vector space is of dimension D).\n",
      "The examples consist of two classes (there are two possible labels) that\n",
      "have features (the components of the vector representing the example)\n",
      "arranged in such a way as to allow us to separate/classify them by draw-\n",
      "ing a straight line.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "372 Classiﬁcation with Support Vector Machines\n",
      "In the following, we formalize the idea of ﬁnding a linear separator\n",
      "of the two classes. We introduce the idea of the margin and then extend\n",
      "linear separators to allow for examples to fall on the “wrong” side, incur-\n",
      "ring a classiﬁcation error. We present two equivalent ways of formalizing\n",
      "the SVM: the geometric view (Section 12.2.4) and the loss function view\n",
      "(Section 12.2.5). We derive the dual version of the SVM using Lagrange\n",
      "multipliers (Section 7.2). The dual SVM allows us to observe a third way\n",
      "of formalizing the SVM: in terms of the convex hulls of the examples of\n",
      "each class (Section 12.3.2). We conclude by brieﬂy describing kernels and\n",
      "how to numerically solve the nonlinear kernel-SVM optimization problem.\n",
      "12.1 Separating Hyperplanes\n",
      "Given two examples represented as vectors xiandxj, one way to compute\n",
      "the similarity between them is using an inner product hxi;xji. Recall from\n",
      "Section 3.2 that inner products are closely related to the angle between\n",
      "two vectors. The value of the inner product between two vectors depends\n",
      "on the length (norm) of each vector. Furthermore, inner products allow\n",
      "us to rigorously deﬁne geometric concepts such as orthogonality and pro-\n",
      "jections.\n",
      "The main idea behind many classiﬁcation algorithms is to represent\n",
      "data in RDand then partition this space, ideally in a way that examples\n",
      "with the same label (and no other examples) are in the same partition.\n",
      "In the case of binary classiﬁcation, the space would be divided into two\n",
      "parts corresponding to the positive and negative classes, respectively. We\n",
      "consider a particularly convenient partition, which is to (linearly) split\n",
      "the space into two halves using a hyperplane. Let example x2RDbe an\n",
      "element of the data space. Consider a function\n",
      "f:RD!R (12.2a)\n",
      "x7!f(x) :=hw;xi+b; (12.2b)\n",
      "parametrized by w2RDandb2R. Recall from Section 2.8 that hy-\n",
      "perplanes are afﬁne subspaces. Therefore, we deﬁne the hyperplane that\n",
      "separates the two classes in our binary classiﬁcation problem as\n",
      "\bx2RD:f(x) = 0\t: (12.3)\n",
      "An illustration of the hyperplane is shown in Figure 12.2, where the\n",
      "vectorwis a vector normal to the hyperplane and bthe intercept. We can\n",
      "derive thatwis a normal vector to the hyperplane in (12.3) by choosing\n",
      "any two examples xaandxbon the hyperplane and showing that the\n",
      "vector between them is orthogonal to w. In the form of an equation,\n",
      "f(xa)\u0000f(xb) =hw;xai+b\u0000(hw;xbi+b) (12.4a)\n",
      "=hw;xa\u0000xbi; (12.4b)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.1 Separating Hyperplanes 373\n",
      "Figure 12.2\n",
      "Equation of a\n",
      "separating\n",
      "hyperplane (12.3).\n",
      "(a) The standard\n",
      "way of representing\n",
      "the equation in 3D.\n",
      "(b) For ease of\n",
      "drawing, we look at\n",
      "the hyperplane edge\n",
      "on.w\n",
      "(a) Separating hyperplane in 3Dw\n",
      ".\n",
      "0.Positive\n",
      ".\n",
      "Negativeb\n",
      "(b) Projection of the setting in (a) onto\n",
      "a plane\n",
      "where the second line is obtained by the linearity of the inner product\n",
      "(Section 3.2). Since we have chosen xaandxbto be on the hyperplane,\n",
      "this implies that f(xa) = 0 andf(xb) = 0 and hencehw;xa\u0000xbi= 0.\n",
      "Recall that two vectors are orthogonal when their inner product is zero. wis orthogonal to\n",
      "any vector on the\n",
      "hyperplane.Therefore, we obtain that wis orthogonal to any vector on the hyperplane.\n",
      "Remark. Recall from Chapter 2 that we can think of vectors in different\n",
      "ways. In this chapter, we think of the parameter vector was an arrow\n",
      "indicating a direction, i.e., we consider wto be a geometric vector. In\n",
      "contrast, we think of the example vector xas a data point (as indicated\n",
      "by its coordinates), i.e., we consider xto be the coordinates of a vector\n",
      "with respect to the standard basis. }\n",
      "When presented with a test example, we classify the example as pos-\n",
      "itive or negative depending on the side of the hyperplane on which it\n",
      "occurs. Note that (12.3) not only deﬁnes a hyperplane; it additionally de-\n",
      "ﬁnes a direction. In other words, it deﬁnes the positive and negative side\n",
      "of the hyperplane. Therefore, to classify a test example xtest, we calcu-\n",
      "late the value of the function f(xtest)and classify the example as +1if\n",
      "f(xtest)>0and\u00001otherwise. Thinking geometrically, the positive ex-\n",
      "amples lie “above” the hyperplane and the negative examples “below” the\n",
      "hyperplane.\n",
      "When training the classiﬁer, we want to ensure that the examples with\n",
      "positive labels are on the positive side of the hyperplane, i.e.,\n",
      "hw;xni+b>0 whenyn= +1 (12.5)\n",
      "and the examples with negative labels are on the negative side, i.e.,\n",
      "hw;xni+b<0 whenyn=\u00001: (12.6)\n",
      "Refer to Figure 12.2 for a geometric intuition of positive and negative\n",
      "examples. These two conditions are often presented in a single equation\n",
      "yn(hw;xni+b)>0: (12.7)\n",
      "Equation (12.7) is equivalent to (12.5) and (12.6) when we multiply both\n",
      "sides of (12.5) and (12.6) with yn= 1andyn=\u00001, respectively.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "374 Classiﬁcation with Support Vector Machines\n",
      "Figure 12.3\n",
      "Possible separating\n",
      "hyperplanes. There\n",
      "are many linear\n",
      "classiﬁers (green\n",
      "lines) that separate\n",
      "orange crosses from\n",
      "blue discs.\n",
      "x(1)x(2)\n",
      "12.2 Primal Support Vector Machine\n",
      "Based on the concept of distances from points to a hyperplane, we now\n",
      "are in a position to discuss the support vector machine. For a dataset\n",
      "f(x1;y1);:::; (xN;yN)gthat is linearly separable, we have inﬁnitely many\n",
      "candidate hyperplanes (refer to Figure 12.3), and therefore classiﬁers,\n",
      "that solve our classiﬁcation problem without any (training) errors. To ﬁnd\n",
      "a unique solution, one idea is to choose the separating hyperplane that\n",
      "maximizes the margin between the positive and negative examples. In\n",
      "other words, we want the positive and negative examples to be separated\n",
      "by a large margin (Section 12.2.1). In the following, we compute the dis- A classiﬁer with\n",
      "large margin turns\n",
      "out to generalize\n",
      "well (Steinwart and\n",
      "Christmann, 2008).tance between an example and a hyperplane to derive the margin. Recall\n",
      "that the closest point on the hyperplane to a given point (example xn) is\n",
      "obtained by the orthogonal projection (Section 3.8).\n",
      "12.2.1 Concept of the Margin\n",
      "The concept of the margin is intuitively simple: It is the distance of the margin\n",
      "separating hyperplane to the closest examples in the dataset, assuming There could be two\n",
      "or more closest\n",
      "examples to a\n",
      "hyperplane.that the dataset is linearly separable. However, when trying to formalize\n",
      "this distance, there is a technical wrinkle that may be confusing. The tech-\n",
      "nical wrinkle is that we need to deﬁne a scale at which to measure the\n",
      "distance. A potential scale is to consider the scale of the data, i.e., the raw\n",
      "values ofxn. There are problems with this, as we could change the units\n",
      "of measurement of xnand change the values in xn, and, hence, change\n",
      "the distance to the hyperplane. As we will see shortly, we deﬁne the scale\n",
      "based on the equation of the hyperplane (12.3) itself.\n",
      "Consider a hyperplane hw;xi+b, and an example xaas illustrated in\n",
      "Figure 12.4. Without loss of generality, we can consider the example xa\n",
      "to be on the positive side of the hyperplane, i.e., hw;xai+b > 0. We\n",
      "would like to compute the distance r >0ofxafrom the hyperplane. We\n",
      "do so by considering the orthogonal projection (Section 3.8) of xaonto\n",
      "the hyperplane, which we denote by x0\n",
      "a. Sincewis orthogonal to the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.2 Primal Support Vector Machine 375\n",
      "Figure 12.4 Vector\n",
      "addition to express\n",
      "distance to\n",
      "hyperplane:\n",
      "xa=x0\n",
      "a+rw\n",
      "kwk.\n",
      ".0.xa\n",
      "w.x0\n",
      "ar\n",
      "hyperplane, we know that the distance ris just a scaling of this vector w.\n",
      "If the length of wis known, then we can use this scaling factor rfactor\n",
      "to work out the absolute distance between xaandx0\n",
      "a. For convenience,\n",
      "we choose to use a vector of unit length (its norm is 1) and obtain this\n",
      "by dividingwby its norm,w\n",
      "kwk. Using vector addition (Section 2.4), we\n",
      "obtain\n",
      "xa=x0\n",
      "a+rw\n",
      "kwk: (12.8)\n",
      "Another way of thinking about ris that it is the coordinate of xain the\n",
      "subspace spanned by w=kwk. We have now expressed the distance of xa\n",
      "from the hyperplane as r, and if we choose xato be the point closest to\n",
      "the hyperplane, this distance ris the margin.\n",
      "Recall that we would like the positive examples to be further than r\n",
      "from the hyperplane, and the negative examples to be further than dis-\n",
      "tancer(in the negative direction) from the hyperplane. Analogously to\n",
      "the combination of (12.5) and (12.6) into (12.7), we formulate this ob-\n",
      "jective as\n",
      "yn(hw;xni+b)>r: (12.9)\n",
      "In other words, we combine the requirements that examples are at least\n",
      "raway from the hyperplane (in the positive and negative direction) into\n",
      "one single inequality.\n",
      "Since we are interested only in the direction, we add an assumption to\n",
      "our model that the parameter vector wis of unit length, i.e., kwk= 1,\n",
      "where we use the Euclidean norm kwk=p\n",
      "w>w(Section 3.1). This We will see other\n",
      "choices of inner\n",
      "products\n",
      "(Section 3.2) in\n",
      "Section 12.4.assumption also allows a more intuitive interpretation of the distance r\n",
      "(12.8) since it is the scaling factor of a vector of length 1.\n",
      "Remark. A reader familiar with other presentations of the margin would\n",
      "notice that our deﬁnition of kwk= 1 is different from the standard\n",
      "presentation if the SVM was the one provided by Sch ¨olkopf and Smola\n",
      "(2002), for example. In Section 12.2.3, we will show the equivalence of\n",
      "both approaches. }\n",
      "Collecting the three requirements into a single constrained optimization\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "376 Classiﬁcation with Support Vector Machines\n",
      "Figure 12.5\n",
      "Derivation of the\n",
      "margin:r=1\n",
      "kwk..xa\n",
      "w\n",
      "hw;xi+\n",
      "b= 0hw;xi+\n",
      "b= 1.x0\n",
      "ar\n",
      "problem, we obtain the objective\n",
      "max\n",
      "w;b;rr|{z}\n",
      "margin\n",
      "subject to yn(hw;xni+b)>r|{z}\n",
      "data ﬁtting;kwk= 1|{z}\n",
      "normalization; r> 0;(12.10)\n",
      "which says that we want to maximize the margin rwhile ensuring that\n",
      "the data lies on the correct side of the hyperplane.\n",
      "Remark. The concept of the margin turns out to be highly pervasive in ma-\n",
      "chine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis\n",
      "to show that when the margin is large, the “complexity” of the function\n",
      "class is low, and hence learning is possible (Vapnik, 2000). It turns out\n",
      "that the concept is useful for various different approaches for theoret-\n",
      "ically analyzing generalization error (Steinwart and Christmann, 2008;\n",
      "Shalev-Shwartz and Ben-David, 2014). }\n",
      "12.2.2 Traditional Derivation of the Margin\n",
      "In the previous section, we derived (12.10) by making the observation that\n",
      "we are only interested in the direction of wand not its length, leading to\n",
      "the assumption that kwk= 1. In this section, we derive the margin max-\n",
      "imization problem by making a different assumption. Instead of choosing\n",
      "that the parameter vector is normalized, we choose a scale for the data.\n",
      "We choose this scale such that the value of the predictor hw;xi+bis1at\n",
      "the closest example. Let us also denote the example in the dataset that is Recall that we\n",
      "currently consider\n",
      "linearly separable\n",
      "data.closest to the hyperplane by xa.\n",
      "Figure 12.5 is identical to Figure 12.4, except that now we rescaled the\n",
      "axes, such that the example xalies exactly on the margin, i.e., hw;xai+\n",
      "b= 1. Sincex0\n",
      "ais the orthogonal projection of xaonto the hyperplane, it\n",
      "must by deﬁnition lie on the hyperplane, i.e.,\n",
      "hw;x0\n",
      "ai+b= 0: (12.11)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.2 Primal Support Vector Machine 377\n",
      "By substituting (12.8) into (12.11), we obtain\n",
      "\u001c\n",
      "w;xa\u0000rw\n",
      "kwk\u001d\n",
      "+b= 0: (12.12)\n",
      "Exploiting the bilinearity of the inner product (see Section 3.2), we get\n",
      "hw;xai+b\u0000rhw;wi\n",
      "kwk= 0: (12.13)\n",
      "Observe that the ﬁrst term is 1by our assumption of scale, i.e., hw;xai+\n",
      "b= 1. From (3.16) in Section 3.1, we know that hw;wi=kwk2. Hence,\n",
      "the second term reduces to rkwk. Using these simpliﬁcations, we obtain\n",
      "r=1\n",
      "kwk: (12.14)\n",
      "This means we derived the distance rin terms of the normal vector w\n",
      "of the hyperplane. At ﬁrst glance, this equation is counterintuitive as we We can also think of\n",
      "the distance as the\n",
      "projection error that\n",
      "incurs when\n",
      "projectingxaonto\n",
      "the hyperplane.seem to have derived the distance from the hyperplane in terms of the\n",
      "length of the vector w, but we do not yet know this vector. One way to\n",
      "think about it is to consider the distance rto be a temporary variable\n",
      "that we only use for this derivation. Therefore, for the rest of this section\n",
      "we will denote the distance to the hyperplane by1\n",
      "kwk. In Section 12.2.3,\n",
      "we will see that the choice that the margin equals 1is equivalent to our\n",
      "previous assumption of kwk= 1in Section 12.2.1.\n",
      "Similar to the argument to obtain (12.9), we want the positive and\n",
      "negative examples to be at least 1away from the hyperplane, which yields\n",
      "the condition\n",
      "yn(hw;xni+b)>1: (12.15)\n",
      "Combining the margin maximization with the fact that examples need to\n",
      "be on the correct side of the hyperplane (based on their labels) gives us\n",
      "max\n",
      "w;b1\n",
      "kwk(12.16)\n",
      "subject toyn(hw;xni+b)>1 for all n= 1;:::;N: (12.17)\n",
      "Instead of maximizing the reciprocal of the norm as in (12.16), we often\n",
      "minimize the squared norm. We also often include a constant1\n",
      "2that does The squared norm\n",
      "results in a convex\n",
      "quadratic\n",
      "programming\n",
      "problem for the\n",
      "SVM (Section 12.5).not affect the optimal w;bbut yields a tidier form when we compute the\n",
      "gradient. Then, our objective becomes\n",
      "min\n",
      "w;b1\n",
      "2kwk2(12.18)\n",
      "subject toyn(hw;xni+b)>1 for all n= 1;:::;N: (12.19)\n",
      "Equation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM\n",
      "expression “hard” is because the formulation does not allow for any vi-\n",
      "olations of the margin condition. We will see in Section 12.2.4 that this\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "378 Classiﬁcation with Support Vector Machines\n",
      "“hard” condition can be relaxed to accommodate violations if the data is\n",
      "not linearly separable.\n",
      "12.2.3 Why We Can Set the Margin to 1\n",
      "In Section 12.2.1, we argued that we would like to maximize some value\n",
      "r, which represents the distance of the closest example to the hyperplane.\n",
      "In Section 12.2.2, we scaled the data such that the closest example is of\n",
      "distance 1to the hyperplane. In this section, we relate the two derivations,\n",
      "and show that they are equivalent.\n",
      "Theorem 12.1. Maximizing the margin r, where we consider normalized\n",
      "weights as in (12.10) ,\n",
      "max\n",
      "w;b;rr|{z}\n",
      "margin\n",
      "subject to yn(hw;xni+b)>r|{z}\n",
      "data ﬁtting;kwk= 1|{z}\n",
      "normalization; r> 0;(12.20)\n",
      "is equivalent to scaling the data, such that the margin is unity:\n",
      "min\n",
      "w;b1\n",
      "2kwk2\n",
      "|{z}\n",
      "margin\n",
      "subject to yn(hw;xni+b)>1|{z}\n",
      "data ﬁtting:(12.21)\n",
      "Proof Consider (12.20). Since the square is a strictly monotonic trans-\n",
      "formation for non-negative arguments, the maximum stays the same if we\n",
      "considerr2in the objective. Since kwk= 1 we can reparametrize the\n",
      "equation with a new weight vector w0that is not normalized by explicitly\n",
      "usingw0\n",
      "kw0k. We obtain\n",
      "max\n",
      "w0;b;rr2\n",
      "subject to yn\u0012\u001cw0\n",
      "kw0k;xn\u001d\n",
      "+b\u0013\n",
      ">r; r> 0:(12.22)\n",
      "Equation (12.22) explicitly states that the distance ris positive. Therefore,\n",
      "we can divide the ﬁrst constraint by r, which yields Note thatr>0\n",
      "because we\n",
      "assumed linear\n",
      "separability, and\n",
      "hence there is no\n",
      "issue to divide by r.max\n",
      "w0;b;rr2\n",
      "subject to yn0\n",
      "BBB@*\n",
      "w0\n",
      "kw0kr|{z}\n",
      "w00;xn+\n",
      "+b\n",
      "r|{z}\n",
      "b001\n",
      "CCCA>1; r> 0(12.23)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.2 Primal Support Vector Machine 379\n",
      "Figure 12.6\n",
      "(a) Linearly\n",
      "separable and\n",
      "(b) non-linearly\n",
      "separable data.\n",
      "x(1)x(2)\n",
      "(a) Linearly separable data, with a large\n",
      "margin\n",
      "x(1)x(2)(b) Non-linearly separable data\n",
      "renaming the parameters to w00andb00. Sincew00=w0\n",
      "kw0kr, rearranging for\n",
      "rgives\n",
      "w000k=\n",
      "=10kr\n",
      "w0\n",
      "=10k\n",
      "r: (12.24)\n",
      "By substituting this result into (12.23), we obtain\n",
      "max\n",
      "w00;b001\n",
      "kw00k2\n",
      "subject to yn(hw00;xni+b00)>1:(12.25)\n",
      "The ﬁnal step is to observe that maximizing1\n",
      "kw00k2yields the same solution\n",
      "as minimizing1\n",
      "2kw00k2, which concludes the proof of Theorem 12.1.\n",
      "12.2.4 Soft Margin SVM: Geometric View\n",
      "In the case where data is not linearly separable, we may wish to allow\n",
      "some examples to fall within the margin region, or even to be on the\n",
      "wrong side of the hyperplane as illustrated in Figure 12.6.\n",
      "The model that allows for some classiﬁcation errors is called the soft soft margin SVM\n",
      "margin SVM . In this section, we derive the resulting optimization problem\n",
      "using geometric arguments. In Section 12.2.5, we will derive an equiv-\n",
      "alent optimization problem using the idea of a loss function. Using La-\n",
      "grange multipliers (Section 7.2), we will derive the dual optimization\n",
      "problem of the SVM in Section 12.3. This dual optimization problem al-\n",
      "lows us to observe a third interpretation of the SVM: as a hyperplane that\n",
      "bisects the line between convex hulls corresponding to the positive and\n",
      "negative data examples (Section 12.3.2).\n",
      "The key geometric idea is to introduce a slack variable \u0018ncorresponding slack variable\n",
      "to each example–label pair (xn;yn)that allows a particular example to be\n",
      "within the margin or even on the wrong side of the hyperplane (refer to\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "380 Classiﬁcation with Support Vector Machines\n",
      "Figure 12.7 Soft\n",
      "margin SVM allows\n",
      "examples to be\n",
      "within the margin or\n",
      "on the wrong side of\n",
      "the hyperplane. The\n",
      "slack variable \u0018\n",
      "measures the\n",
      "distance of a\n",
      "positive example\n",
      "x+to the positive\n",
      "margin hyperplane\n",
      "hw;xi+b= 1\n",
      "whenx+is on the\n",
      "wrong side..x+w\n",
      "hw;xi+\n",
      "b= 0hw;xi+\n",
      "b= 1.\n",
      "\u0018\n",
      "Figure 12.7). We subtract the value of \u0018nfrom the margin, constraining\n",
      "\u0018nto be non-negative. To encourage correct classiﬁcation of the samples,\n",
      "we add\u0018nto the objective\n",
      "min\n",
      "w;b;\u00181\n",
      "2kwk2+CNX\n",
      "n=1\u0018n (12.26a)\n",
      "subject to yn(hw;xni+b)>1\u0000\u0018n (12.26b)\n",
      "\u0018n>0 (12.26c)\n",
      "forn= 1;:::;N . In contrast to the optimization problem (12.18) for the\n",
      "hard margin SVM, this one is called the soft margin SVM . The parameter soft margin SVM\n",
      "C > 0trades off the size of the margin and the total amount of slack that\n",
      "we have. This parameter is called the regularization parameter since, as regularization\n",
      "parameter we will see in the following section, the margin term in the objective func-\n",
      "tion (12.26a) is a regularization term. The margin term kwk2is called\n",
      "theregularizer , and in many books on numerical optimization, the reg- regularizer\n",
      "ularization parameter is multiplied with this term (Section 8.2.3). This\n",
      "is in contrast to our formulation in this section. Here a large value of C\n",
      "implies low regularization, as we give the slack variables larger weight,\n",
      "hence giving more priority to examples that do not lie on the correct side\n",
      "of the margin. There are\n",
      "alternative\n",
      "parametrizations of\n",
      "this regularization,\n",
      "which is\n",
      "why (12.26a) is also\n",
      "often referred to as\n",
      "theC-SVM.Remark. In the formulation of the soft margin SVM (12.26a) wis reg-\n",
      "ularized, but bis not regularized. We can see this by observing that the\n",
      "regularization term does not contain b. The unregularized term bcom-\n",
      "plicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\n",
      "and decreases computational efﬁciency (Fan et al., 2008). }\n",
      "12.2.5 Soft Margin SVM: Loss Function View\n",
      "Let us consider a different approach for deriving the SVM, following the\n",
      "principle of empirical risk minimization (Section 8.2). For the SVM, we\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.2 Primal Support Vector Machine 381\n",
      "choose hyperplanes as the hypothesis class, that is\n",
      "f(x) =hw;xi+b: (12.27)\n",
      "We will see in this section that the margin corresponds to the regulariza-\n",
      "tion term. The remaining question is, what is the loss function ? In con- loss function\n",
      "trast to Chapter 9, where we consider regression problems (the output\n",
      "of the predictor is a real number), in this chapter, we consider binary\n",
      "classiﬁcation problems (the output of the predictor is one of two labels\n",
      "f+1;\u00001g). Therefore, the error/loss function for each single example–\n",
      "label pair needs to be appropriate for binary classiﬁcation. For example,\n",
      "the squared loss that is used for regression (9.10b) is not suitable for bi-\n",
      "nary classiﬁcation.\n",
      "Remark. The ideal loss function between binary labels is to count the num-\n",
      "ber of mismatches between the prediction and the label. This means that\n",
      "for a predictor fapplied to an example xn, we compare the output f(xn)\n",
      "with the label yn. We deﬁne the loss to be zero if they match, and one if\n",
      "they do not match. This is denoted by 1(f(xn)6=yn)and is called the\n",
      "zero-one loss . Unfortunately, the zero-one loss results in a combinatorial zero-one loss\n",
      "optimization problem for ﬁnding the best parameters w;b. Combinatorial\n",
      "optimization problems (in contrast to continuous optimization problems\n",
      "discussed in Chapter 7) are in general more challenging to solve. }\n",
      "What is the loss function corresponding to the SVM? Consider the error\n",
      "between the output of a predictor f(xn)and the label yn. The loss de-\n",
      "scribes the error that is made on the training data. An equivalent way to\n",
      "derive (12.26a) is to use the hinge loss hinge loss\n",
      "`(t) = maxf0;1\u0000tgwheret=yf(x) =y(hw;xi+b):(12.28)\n",
      "Iff(x)is on the correct side (based on the corresponding label y) of the\n",
      "hyperplane, and further than distance 1, this means that t>1and the\n",
      "hinge loss returns a value of zero. If f(x)is on the correct side but too\n",
      "close to the hyperplane ( 0<t< 1), the example xis within the margin,\n",
      "and the hinge loss returns a positive value. When the example is on the\n",
      "wrong side of the hyperplane ( t<0), the hinge loss returns an even larger\n",
      "value, which increases linearly. In other words, we pay a penalty once we\n",
      "are closer than the margin to the hyperplane, even if the prediction is\n",
      "correct, and the penalty increases linearly. An alternative way to express\n",
      "the hinge loss is by considering it as two linear pieces\n",
      "`(t) =(\n",
      "0 ift>1\n",
      "1\u0000tift<1; (12.29)\n",
      "as illustrated in Figure 12.8. The loss corresponding to the hard margin\n",
      "SVM 12.18 is deﬁned as\n",
      "`(t) =(\n",
      "0 ift>1\n",
      "1ift<1: (12.30)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "382 Classiﬁcation with Support Vector Machines\n",
      "Figure 12.8 The\n",
      "hinge loss is a\n",
      "convex upper bound\n",
      "of zero-one loss.\n",
      "−2 0 2\n",
      "t024max{0,1−t}Zero-one loss\n",
      "Hinge loss\n",
      "This loss can be interpreted as never allowing any examples inside the\n",
      "margin.\n",
      "For a given training set f(x1;y1);:::; (xN;yN)g, we seek to minimize\n",
      "the total loss, while regularizing the objective with `2-regularization (see\n",
      "Section 8.2.3). Using the hinge loss (12.28) gives us the unconstrained\n",
      "optimization problem\n",
      "min\n",
      "w;b1\n",
      "2kwk2\n",
      "|{z}\n",
      "regularizer+CNX\n",
      "n=1maxf0;1\u0000yn(hw;xni+b)g\n",
      "| {z }\n",
      "error term: (12.31)\n",
      "The ﬁrst term in (12.31) is called the regularization term or the regularizer regularizer\n",
      "(see Section 8.2.3), and the second term is called the loss term or the error loss term\n",
      "error termterm. Recall from Section 12.2.4 that the term1\n",
      "2kwk2arises directly from\n",
      "the margin. In other words, margin maximization can be interpreted as\n",
      "regularization . regularization\n",
      "In principle, the unconstrained optimization problem in (12.31) can\n",
      "be directly solved with (sub-)gradient descent methods as described in\n",
      "Section 7.1. To see that (12.31) and (12.26a) are equivalent, observe that\n",
      "the hinge loss (12.28) essentially consists of two linear parts, as expressed\n",
      "in (12.29). Consider the hinge loss for a single example-label pair (12.28).\n",
      "We can equivalently replace minimization of the hinge loss over twith a\n",
      "minimization of a slack variable \u0018with two constraints. In equation form,\n",
      "min\n",
      "tmaxf0;1\u0000tg (12.32)\n",
      "is equivalent to\n",
      "min\n",
      "\u0018;t\u0018\n",
      "subject to \u0018>0; \u0018>1\u0000t:(12.33)\n",
      "By substituting this expression into (12.31) and rearranging one of the\n",
      "constraints, we obtain exactly the soft margin SVM (12.26a).\n",
      "Remark. Let us contrast our choice of the loss function in this section to the\n",
      "loss function for linear regression in Chapter 9. Recall from Section 9.2.1\n",
      "that for ﬁnding maximum likelihood estimators, we usually minimize the\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.3 Dual Support Vector Machine 383\n",
      "negative log-likelihood. Furthermore, since the likelihood term for linear\n",
      "regression with Gaussian noise is Gaussian, the negative log-likelihood for\n",
      "each example is a squared error function. The squared error function is the\n",
      "loss function that is minimized when looking for the maximum likelihood\n",
      "solution. }\n",
      "12.3 Dual Support Vector Machine\n",
      "The description of the SVM in the previous sections, in terms of the vari-\n",
      "ableswandb, is known as the primal SVM. Recall that we consider inputs\n",
      "x2RDwithDfeatures. Since wis of the same dimension as x, this\n",
      "means that the number of parameters (the dimension of w) of the opti-\n",
      "mization problem grows linearly with the number of features.\n",
      "In the following, we consider an equivalent optimization problem (the\n",
      "so-called dual view), which is independent of the number of features. In-\n",
      "stead, the number of parameters increases with the number of examples\n",
      "in the training set. We saw a similar idea appear in Chapter 10, where we\n",
      "expressed the learning problem in a way that does not scale with the num-\n",
      "ber of features. This is useful for problems where we have more features\n",
      "than the number of examples in the training dataset. The dual SVM also\n",
      "has the additional advantage that it easily allows kernels to be applied,\n",
      "as we shall see at the end of this chapter. The word “dual” appears often\n",
      "in mathematical literature, and in this particular case it refers to convex\n",
      "duality. The following subsections are essentially an application of convex\n",
      "duality, which we discussed in Section 7.2.\n",
      "12.3.1 Convex Duality via Lagrange Multipliers\n",
      "Recall the primal soft margin SVM (12.26a). We call the variables w,b,\n",
      "and\u0018corresponding to the primal SVM the primal variables. We use \u000bn> In Chapter 7, we\n",
      "used\u0015as Lagrange\n",
      "multipliers. In this\n",
      "section, we follow\n",
      "the notation\n",
      "commonly chosen in\n",
      "SVM literature, and\n",
      ".0as the Lagrange multiplier corresponding to the constraint (12.26b) that\n",
      "n>0as the Lagrange multi-d correctly and \n",
      "plier corresponding to the non-negativity constraint of the slack variable;\n",
      "see (12.26c). The Lagrangian is then given by\n",
      " ) =1;\u0018;\u000b;\n",
      "2kwk2+CNX\n",
      "n=1\u0018n (12.34)\n",
      "\u0000NX\n",
      "n=1\u000bn(yn(hw;xni+b)\u00001 +\u0018n)\n",
      "| {z }\n",
      "constraint (12.26b)\u0000NX\n",
      "n\u0018n\n",
      "|{z}\n",
      "constraint (12.26c):\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "384 Classiﬁcation with Support Vector Machines\n",
      "By differentiating the Lagrangian (12.34) with respect to the three primal\n",
      "variablesw,b, and\u0018respectively, we obtain\n",
      "@L\n",
      "@w=w>\u0000NX\n",
      "n=1\u000bnynxn>; (12.35)\n",
      "@L\n",
      "@b=\u0000NX\n",
      "n=1\u000bnyn; (12.36)\n",
      "@L\n",
      "n: (12.37)\n",
      "We now ﬁnd the maximum of the Lagrangian by setting each of these\n",
      "partial derivatives to zero. By setting (12.35) to zero, we ﬁnd\n",
      "w=NX\n",
      "n=1\u000bnynxn; (12.38)\n",
      "which is a particular instance of the representer theorem (Kimeldorf and representer theorem\n",
      "Wahba, 1970). Equation (12.38) states that the optimal weight vector in The representer\n",
      "theorem is actually\n",
      "a collection of\n",
      "theorems saying\n",
      "that the solution of\n",
      "minimizing\n",
      "empirical risk lies in\n",
      "the subspace\n",
      "(Section 2.4.3)\n",
      "deﬁned by the\n",
      "examples.the primal is a linear combination of the examples xn. Recall from Sec-\n",
      "tion 2.6.1 that this means that the solution of the optimization problem\n",
      "lies in the span of training data. Additionally, the constraint obtained by\n",
      "setting (12.36) to zero implies that the optimal weight vector is an afﬁne\n",
      "combination of the examples. The representer theorem turns out to hold\n",
      "for very general settings of regularized empirical risk minimization (Hof-\n",
      "mann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\n",
      "general versions (Sch ¨olkopf et al., 2001), and necessary and sufﬁcient\n",
      "conditions on its existence can be found in Yu et al. (2013).\n",
      "Remark. The representer theorem (12.38) also provides an explanation\n",
      "of the name “support vector machine.” The examples xn, for which the\n",
      "corresponding parameters \u000bn= 0, do not contribute to the solution wat\n",
      "all. The other examples, where \u000bn>0, are called support vectors since support vector\n",
      "they “support” the hyperplane. }\n",
      "By substituting the expression for winto the Lagrangian (12.34), we\n",
      "obtain the dual\n",
      " ) =1;\n",
      "2NX\n",
      "i=1NX\n",
      "j=1yiyj\u000bi\u000bjhxi;xji\u0000NX\n",
      "i=1yi\u000bi*NX\n",
      "j=1yj\u000bjxj;xi+\n",
      "+CNX\n",
      "i=1\u0018i\u0000bNX\n",
      "i=1yi\u000bi+NX\n",
      "i=1\u000bi\u0000NX\n",
      "i=1\u000bi\u0018i\u0000NX\n",
      "i\u0018i:\n",
      "(12.39)\n",
      "Note that there are no longer any terms involving the primal variable w.\n",
      "By setting (12.36) to zero, we obtainPN\n",
      "n=1yn\u000bn= 0. Therefore, the term\n",
      "involvingbalso vanishes. Recall that inner products are symmetric and\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.3 Dual Support Vector Machine 385\n",
      "bilinear (see Section 3.2). Therefore, the ﬁrst two terms in (12.39) are\n",
      "over the same objects. These terms (colored blue) can be simpliﬁed, and\n",
      "we obtain the Lagrangian\n",
      " ) =\u00001\n",
      "2NX\n",
      "i=1NX\n",
      "j=1yiyj\u000bi\u000bjhxi;xji+NX\n",
      "i=1\u000bi+NX\n",
      "i)\u0018i:\u0000\u000bi\u0000\n",
      "(12.40)\n",
      "The last term in this equation is a collection of all terms that contain slack\n",
      "variables\u0018i. By setting (12.37) to zero, we see that the last term in (12.40)\n",
      "is also zero. Furthermore, by using the same equation and recalling that\n",
      "iare non-negative, we conclude that \u000bi6C.\n",
      "We now obtain the dual optimization problem of the SVM, which is ex-\n",
      "pressed exclusively in terms of the Lagrange multipliers \u000bi. Recall from\n",
      "Lagrangian duality (Deﬁnition 7.1) that we maximize the dual problem.\n",
      "This is equivalent to minimizing the negative dual problem, such that we\n",
      "end up with the dual SVM dual SVM\n",
      "min\n",
      "\u000b1\n",
      "2NX\n",
      "i=1NX\n",
      "j=1yiyj\u000bi\u000bjhxi;xji\u0000NX\n",
      "i=1\u000bi\n",
      "subject toNX\n",
      "i=1yi\u000bi= 0\n",
      "06\u000bi6Cfor alli= 1;:::;N:(12.41)\n",
      "The equality constraint in (12.41) is obtained from setting (12.36) to\n",
      "zero. The inequality constraint \u000bi>0is the condition imposed on La-\n",
      "grange multipliers of inequality constraints (Section 7.2). The inequality\n",
      "constraint\u000bi6Cis discussed in the previous paragraph.\n",
      "The set of inequality constraints in the SVM are called “box constraints”\n",
      "because they limit the vector \u000b= [\u000b1;\u0001\u0001\u0001;\u000bN]>2RNof Lagrange mul-\n",
      "tipliers to be inside the box deﬁned by 0andCon each axis. These\n",
      "axis-aligned boxes are particularly efﬁcient to implement in numerical\n",
      "solvers (Dost ´al, 2009, chapter 5). It turns out that\n",
      "examples that lie\n",
      "exactly on the\n",
      "margin are\n",
      "examples whose\n",
      "dual parameters lie\n",
      "strictly inside the\n",
      "box constraints,\n",
      "0<\u000bi<C. This is\n",
      "derived using the\n",
      "Karush Kuhn Tucker\n",
      "conditions, for\n",
      "example in\n",
      "Sch¨olkopf and\n",
      "Smola (2002).Once we obtain the dual parameters \u000b, we can recover the primal pa-\n",
      "rameterswby using the representer theorem (12.38). Let us call the op-\n",
      "timal primal parameter w\u0003. However, there remains the question on how\n",
      "to obtain the parameter b\u0003. Consider an example xnthat lies exactly on\n",
      "the margin’s boundary, i.e., hw\u0003;xni+b=yn. Recall that ynis either +1\n",
      "or\u00001. Therefore, the only unknown is b, which can be computed by\n",
      "b\u0003=yn\u0000hw\u0003;xni: (12.42)\n",
      "Remark. In principle, there may be no examples that lie exactly on the\n",
      "margin. In this case, we should compute jyn\u0000hw\u0003;xnijfor all support\n",
      "vectors and take the median value of this absolute value difference to be\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "386 Classiﬁcation with Support Vector Machines\n",
      "Figure 12.9 Convex\n",
      "hulls. (a) Convex\n",
      "hull of points, some\n",
      "of which lie within\n",
      "the boundary;\n",
      "(b) convex hulls\n",
      "around positive and\n",
      "negative examples.\n",
      "(a) Convex hull.\n",
      "c\n",
      "d (b) Convex hulls around positive (blue) and\n",
      "negative (orange) examples. The distance be-\n",
      "tween the two convex sets is the length of the\n",
      "difference vector c\u0000d.\n",
      "the value of b\u0003. A derivation of this can be found in http://fouryears.\n",
      "eu/2012/06/07/the-svm-bias-term-conspiracy/ .}\n",
      "12.3.2 Dual SVM: Convex Hull View\n",
      "Another approach to obtain the dual SVM is to consider an alternative\n",
      "geometric argument. Consider the set of examples xnwith the same label.\n",
      "We would like to build a convex set that contains all the examples such\n",
      "that it is the smallest possible set. This is called the convex hull and is\n",
      "illustrated in Figure 12.9.\n",
      "Let us ﬁrst build some intuition about a convex combination of points.\n",
      "Consider two points x1andx2and corresponding non-negative weights\n",
      "\u000b1;\u000b2>0such that\u000b1+\u000b2= 1. The equation \u000b1x1+\u000b2x2describes each\n",
      "point on a line between x1andx2. Consider what happens when we add\n",
      "a third point x3along with a weight \u000b3>0such thatP3\n",
      "n=1\u000bn= 1.\n",
      "The convex combination of these three points x1;x2;x3spans a two-\n",
      "dimensional area. The convex hull of this area is the triangle formed by convex hull\n",
      "the edges corresponding to each pair of of points. As we add more points,\n",
      "and the number of points becomes greater than the number of dimen-\n",
      "sions, some of the points will be inside the convex hull, as we can see in\n",
      "Figure 12.9(a).\n",
      "In general, building a convex convex hull can be done by introducing\n",
      "non-negative weights \u000bn>0corresponding to each example xn. Then\n",
      "the convex hull can be described as the set\n",
      "conv (X) =(NX\n",
      "n=1\u000bnxn)\n",
      "withNX\n",
      "n=1\u000bn= 1 and\u000bn>0;(12.43)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.3 Dual Support Vector Machine 387\n",
      "for alln= 1;:::;N . If the two clouds of points corresponding to the\n",
      "positive and negative classes are separated, then the convex hulls do not\n",
      "overlap. Given the training data (x1;y1);:::; (xN;yN), we form two con-\n",
      "vex hulls, corresponding to the positive and negative classes respectively.\n",
      "We pick a point c, which is in the convex hull of the set of positive exam-\n",
      "ples, and is closest to the negative class distribution. Similarly, we pick a\n",
      "pointdin the convex hull of the set of negative examples and is closest to\n",
      "the positive class distribution; see Figure 12.9(b). We deﬁne a difference\n",
      "vector between dandcas\n",
      "w:=c\u0000d: (12.44)\n",
      "Picking the points canddas in the preceding cases, and requiring them\n",
      "to be closest to each other is equivalent to minimizing the length/norm of\n",
      "w, so that we end up with the corresponding optimization problem\n",
      "arg min\n",
      "wkwk= arg min\n",
      "w1\n",
      "2kwk2: (12.45)\n",
      "Sincecmust be in the positive convex hull, it can be expressed as a convex\n",
      "combination of the positive examples, i.e., for non-negative coefﬁcients\n",
      "\u000b+\n",
      "n\n",
      "c=X\n",
      "n:yn=+1\u000b+\n",
      "nxn: (12.46)\n",
      "In (12.46), we use the notation n:yn= +1 to indicate the set of indices\n",
      "nfor whichyn= +1 . Similarly, for the examples with negative labels, we\n",
      "obtain\n",
      "d=X\n",
      "n:yn=\u00001\u000b\u0000\n",
      "nxn: (12.47)\n",
      "By substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the\n",
      "objective\n",
      "min\n",
      "\u000b1\n",
      "X\n",
      "n:yn=+1\u000b+\n",
      "nxn\u0000X\n",
      "n:yn=\u00001\u000b\u0000\n",
      "2xn\n",
      ": (12.48)\n",
      "Let\u000bbe the set of all coefﬁcients, i.e., the concatenation of \u000b+and\u000b\u0000.\n",
      "Recall that we require that for each convex hull that their coefﬁcients sum\n",
      "to one,\n",
      "X\n",
      "n:yn=+1\u000b+\n",
      "n= 1 andX\n",
      "n:yn=\u00001\u000b\u0000\n",
      "n= 1: (12.49)\n",
      "This implies the constraint\n",
      "NX\n",
      "n=1yn\u000bn= 0: (12.50)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "388 Classiﬁcation with Support Vector Machines\n",
      "This result can be seen by multiplying out the individual classes\n",
      "NX\n",
      "n=1yn\u000bn=X\n",
      "n:yn=+1(+1)\u000b+\n",
      "n+X\n",
      "n:yn=\u00001(\u00001)\u000b\u0000\n",
      "n (12.51a)\n",
      "=X\n",
      "n:yn=+1\u000b+\n",
      "n\u0000X\n",
      "n:yn=\u00001\u000b\u0000\n",
      "n= 1\u00001 = 0: (12.51b)\n",
      "The objective function (12.48) and the constraint (12.50), along with the\n",
      "assumption that \u000b>0, give us a constrained (convex) optimization prob-\n",
      "lem. This optimization problem can be shown to be the same as that of\n",
      "the dual hard margin SVM (Bennett and Bredensteiner, 2000a).\n",
      "Remark. To obtain the soft margin dual, we consider the reduced hull. The\n",
      "reduced hull is similar to the convex hull but has an upper bound to the reduced hull\n",
      "size of the coefﬁcients \u000b. The maximum possible value of the elements\n",
      "of\u000brestricts the size that the convex hull can take. In other words, the\n",
      "bound on\u000bshrinks the convex hull to a smaller volume (Bennett and\n",
      "Bredensteiner, 2000b). }\n",
      "12.4 Kernels\n",
      "Consider the formulation of the dual SVM (12.41). Notice that the in-\n",
      "ner product in the objective occurs only between examples xiandxj.\n",
      "There are no inner products between the examples and the parameters.\n",
      "Therefore, if we consider a set of features \u001e(xi)to representxi, the only\n",
      "change in the dual SVM will be to replace the inner product. This mod-\n",
      "ularity, where the choice of the classiﬁcation method (the SVM) and the\n",
      "choice of the feature representation \u001e(x)can be considered separately,\n",
      "provides ﬂexibility for us to explore the two problems independently. In\n",
      "this section, we discuss the representation \u001e(x)and brieﬂy introduce the\n",
      "idea of kernels, but do not go into the technical details.\n",
      "Since\u001e(x)could be a non-linear function, we can use the SVM (which\n",
      "assumes a linear classiﬁer) to construct classiﬁers that are nonlinear in\n",
      "the examples xn. This provides a second avenue, in addition to the soft\n",
      "margin, for users to deal with a dataset that is not linearly separable. It\n",
      "turns out that there are many algorithms and statistical methods that have\n",
      "this property that we observed in the dual SVM: the only inner products\n",
      "are those that occur between examples. Instead of explicitly deﬁning a\n",
      "non-linear feature map \u001e(\u0001)and computing the resulting inner product\n",
      "between examples xiandxj, we deﬁne a similarity function k(xi;xj)be-\n",
      "tweenxiandxj. For a certain class of similarity functions, called kernels , kernel\n",
      "the similarity function implicitly deﬁnes a non-linear feature map \u001e(\u0001).\n",
      "Kernels are by deﬁnition functions k:X\u0002X! Rfor which there exists The inputsXof the\n",
      "kernel function can\n",
      "be very general and\n",
      "are not necessarily\n",
      "restricted to RD.a Hilbert spaceHand\u001e:X!H a feature map such that\n",
      "k(xi;xj) =h\u001e(xi);\u001e(xj)iH: (12.52)\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.4 Kernels 389\n",
      "Figure 12.10 SVM\n",
      "with different\n",
      "kernels. Note that\n",
      "while the decision\n",
      "boundary is\n",
      "nonlinear, the\n",
      "underlying problem\n",
      "being solved is for a\n",
      "linear separating\n",
      "hyperplane (albeit\n",
      "with a nonlinear\n",
      "kernel).\n",
      "First featureSecond feature\n",
      "(a) SVM with linear kernel\n",
      "First featureSecond feature (b) SVM with RBF kernel\n",
      "First featureSecond feature\n",
      "(c) SVM with polynomial (degree 2) kernel\n",
      "First featureSecond feature (d) SVM with polynomial (degree 3) kernel\n",
      "There is a unique reproducing kernel Hilbert space associated with every\n",
      "kernelk(Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this\n",
      "unique association, \u001e(x) =k(\u0001;x)is called the canonical feature map .canonical feature\n",
      "map The generalization from an inner product to a kernel function (12.52) is\n",
      "known as the kernel trick (Sch¨olkopf and Smola, 2002; Shawe-Taylor and kernel trick\n",
      "Cristianini, 2004), as it hides away the explicit non-linear feature map.\n",
      "The matrixK2RN\u0002N, resulting from the inner products or the appli-\n",
      "cation ofk(\u0001;\u0001)to a dataset, is called the Gram matrix , and is often just Gram matrix\n",
      "referred to as the kernel matrix . Kernels must be symmetric and positive kernel matrix\n",
      "semideﬁnite functions so that every kernel matrix Kis symmetric and\n",
      "positive semideﬁnite (Section 3.2.3):\n",
      "8z2RN:z>Kz>0: (12.53)\n",
      "Some popular examples of kernels for multivariate real-valued data xi2\n",
      "RDare the polynomial kernel, the Gaussian radial basis function kernel,\n",
      "and the rational quadratic kernel (Sch ¨olkopf and Smola, 2002; Rasmussen\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "390 Classiﬁcation with Support Vector Machines\n",
      "and Williams, 2006). Figure 12.10 illustrates the effect of different kernels\n",
      "on separating hyperplanes on an example dataset. Note that we are still\n",
      "solving for hyperplanes, that is, the hypothesis class of functions are still\n",
      "linear. The non-linear surfaces are due to the kernel function.\n",
      "Remark. Unfortunately for the ﬂedgling machine learner, there are mul-\n",
      "tiple meanings of the word “kernel.” In this chapter, the word “kernel”\n",
      "comes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-\n",
      "szajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-\n",
      "ear algebra (Section 2.7.3), where the kernel is another word for the null\n",
      "space. The third common use of the word “kernel” in machine learning is\n",
      "the smoothing kernel in kernel density estimation (Section 11.5). }\n",
      "Since the explicit representation \u001e(x)is mathematically equivalent to\n",
      "the kernel representation k(xi;xj), a practitioner will often design the\n",
      "kernel function such that it can be computed more efﬁciently than the\n",
      "inner product between explicit feature maps. For example, consider the\n",
      "polynomial kernel (Sch ¨olkopf and Smola, 2002), where the number of\n",
      "terms in the explicit expansion grows very quickly (even for polynomials\n",
      "of low degree) when the input dimension is large. The kernel function\n",
      "only requires one multiplication per input dimension, which can provide\n",
      "signiﬁcant computational savings. Another example is the Gaussian ra-\n",
      "dial basis function kernel (Sch ¨olkopf and Smola, 2002; Rasmussen and\n",
      "Williams, 2006), where the corresponding feature space is inﬁnite dimen-\n",
      "sional. In this case, we cannot explicitly represent the feature space but\n",
      "can still compute similarities between a pair of examples using the kernel. The choice of\n",
      "kernel, as well as\n",
      "the parameters of\n",
      "the kernel, is often\n",
      "chosen using nested\n",
      "cross-validation\n",
      "(Section 8.6.1).Another useful aspect of the kernel trick is that there is no need for\n",
      "the original data to be already represented as multivariate real-valued\n",
      "data. Note that the inner product is deﬁned on the output of the function\n",
      "\u001e(\u0001), but does not restrict the input to real numbers. Hence, the function\n",
      "\u001e(\u0001)and the kernel function k(\u0001;\u0001)can be deﬁned on any object, e.g.,\n",
      "sets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;\n",
      "G¨artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan\n",
      "et al., 2010).\n",
      "12.5 Numerical Solution\n",
      "We conclude our discussion of SVMs by looking at how to express the\n",
      "problems derived in this chapter in terms of the concepts presented in\n",
      "Chapter 7. We consider two different approaches for ﬁnding the optimal\n",
      "solution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-\n",
      "press this as an unconstrained optimization problem. Then we express the\n",
      "constrained versions of the primal and dual SVMs as quadratic programs\n",
      "in standard form 7.3.2.\n",
      "Consider the loss function view of the SVM (12.31). This is a convex\n",
      "unconstrained optimization problem, but the hinge loss (12.28) is not dif-\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.5 Numerical Solution 391\n",
      "ferentiable. Therefore, we apply a subgradient approach for solving it.\n",
      "However, the hinge loss is differentiable almost everywhere, except for\n",
      "one single point at the hinge t= 1. At this point, the gradient is a set of\n",
      "possible values that lie between 0and\u00001. Therefore, the subgradient gof\n",
      "the hinge loss is given by\n",
      "g(t) =8\n",
      "><\n",
      ">:\u00001t<1\n",
      "[\u00001;0]t= 1\n",
      "0t>1: (12.54)\n",
      "Using this subgradient, we can apply the optimization methods presented\n",
      "in Section 7.1.\n",
      "Both the primal and the dual SVM result in a convex quadratic pro-\n",
      "gramming problem (constrained optimization). Note that the primal SVM\n",
      "in (12.26a) has optimization variables that have the size of the dimen-\n",
      "sionDof the input examples. The dual SVM in (12.41) has optimization\n",
      "variables that have the size of the number Nof examples.\n",
      "To express the primal SVM in the standard form (7.45) for quadratic\n",
      "programming, let us assume that we use the dot product (3.5) as the\n",
      "inner product. We rearrange the equation for the primal SVM (12.26a), Recall from\n",
      "Section 3.2 that we\n",
      "use the phrase dot\n",
      "product to mean the\n",
      "inner product on\n",
      "Euclidean vector\n",
      "space.such that the optimization variables are all on the right and the inequality\n",
      "of the constraint matches the standard form. This yields the optimization\n",
      "min\n",
      "w;b;\u00181\n",
      "2kwk2+CNX\n",
      "n=1\u0018n\n",
      "subject to\u0000ynx>\n",
      "nw\u0000ynb\u0000\u0018n6\u00001\n",
      "\u0000\u0018n60(12.55)\n",
      "n= 1;:::;N . By concatenating the variables w;b;xninto a single vector,\n",
      "and carefully collecting the terms, we obtain the following matrix form of\n",
      "the soft margin SVM:\n",
      "min\n",
      "w;b;\u00181\n",
      "22\n",
      "4w\n",
      "b\n",
      "\u00183\n",
      "5>\u0014ID 0D;N+1\n",
      "0N+1;D0N+1;N+1\u00152\n",
      "4w\n",
      "b\n",
      "\u00183\n",
      "5+\u00020D+1;1C1N;1\u0003>2\n",
      "4w\n",
      "b\n",
      "\u00183\n",
      "5\n",
      "subject to\u0014\u0000YX\u0000y\u0000IN\n",
      "0N;D+1\u0000IN\u00152\n",
      "4w\n",
      "b\n",
      "\u00183\n",
      "56\u0014\u00001N;1\n",
      "0N;1\u0015\n",
      ":\n",
      "(12.56)\n",
      "In the preceding optimization problem, the minimization is over the pa-\n",
      "rameters [w>;b;\u0018>]>2RD+1+N, and we use the notation: Imto rep-\n",
      "resent the identity matrix of size m\u0002m,0m;nto represent the matrix\n",
      "of zeros of size m\u0002n, and 1m;nto represent the matrix of ones of size\n",
      "m\u0002n. In addition, yis the vector of labels [y1;\u0001\u0001\u0001;yN]>,Y= diag(y)\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "392 Classiﬁcation with Support Vector Machines\n",
      "is anNbyNmatrix where the elements of the diagonal are from y, and\n",
      "X2RN\u0002Dis the matrix obtained by concatenating all the examples.\n",
      "We can similarly perform a collection of terms for the dual version of the\n",
      "SVM (12.41). To express the dual SVM in standard form, we ﬁrst have to\n",
      "express the kernel matrix Ksuch that each entry is Kij=k(xi;xj). If we\n",
      "have an explicit feature representation xithen we deﬁne Kij=hxi;xji.\n",
      "For convenience of notation we introduce a matrix with zeros everywhere\n",
      "except on the diagonal, where we store the labels, that is, Y= diag(y).\n",
      "The dual SVM can be written as\n",
      "min\n",
      "\u000b1\n",
      "2\u000b>YKY\u000b\u00001>\n",
      "N;1\u000b\n",
      "subject to2\n",
      "664y>\n",
      "\u0000y>\n",
      "\u0000IN\n",
      "IN3\n",
      "775\u000b6\u00140N+2;1\n",
      "C1N;1\u0015\n",
      ":(12.57)\n",
      "Remark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms\n",
      "of the constraints to be inequality constraints. We will express the dual\n",
      "SVM’s equality constraint as two inequality constraints, i.e.,\n",
      "Ax=bis replaced by Ax6bandAx>b: (12.58)\n",
      "Particular software implementations of convex optimization methods may\n",
      "provide the ability to express equality constraints. }\n",
      "Since there are many different possible views of the SVM, there are\n",
      "many approaches for solving the resulting optimization problem. The ap-\n",
      "proach presented here, expressing the SVM problem in standard convex\n",
      "optimization form, is not often used in practice. The two main implemen-\n",
      "tations of SVM solvers are Chang and Lin (2011) (which is open source)\n",
      "and Joachims (1999). Since SVMs have a clear and well-deﬁned optimiza-\n",
      "tion problem, many approaches based on numerical optimization tech-\n",
      "niques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and\n",
      "Sun, 2011).\n",
      "12.6 Further Reading\n",
      "The SVM is one of many approaches for studying binary classiﬁcation.\n",
      "Other approaches include the perceptron, logistic regression, Fisher dis-\n",
      "criminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\n",
      "Murphy, 2012). A short tutorial on SVMs and kernels on discrete se-\n",
      "quences can be found in Ben-Hur et al. (2008). The development of SVMs\n",
      "is closely linked to empirical risk minimization, discussed in Section 8.2.\n",
      "Hence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-\n",
      "wart and Christmann, 2008). The book about kernel methods (Sch ¨olkopf\n",
      "and Smola, 2002) includes many details of support vector machines and\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "12.6 Further Reading 393\n",
      "how to optimize them. A broader book about kernel methods (Shawe-\n",
      "Taylor and Cristianini, 2004) also includes many linear algebra approaches\n",
      "for different machine learning problems.\n",
      "An alternative derivation of the dual SVM can be obtained using the\n",
      "idea of the Legendre–Fenchel transform (Section 7.3.3). The derivation\n",
      "considers each term of the unconstrained formulation of the SVM (12.31)\n",
      "separately and calculates their convex conjugates (Rifkin and Lippert,\n",
      "2007). Readers interested in the functional analysis view (also the reg-\n",
      "ularization methods view) of SVMs are referred to the work by Wahba\n",
      "(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,\n",
      "1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic ground-\n",
      "ing in linear operators (Akhiezer and Glazman, 1993). The idea of kernels\n",
      "have been generalized to Banach spaces (Zhang et al., 2009) and Kre ˘ın\n",
      "spaces (Ong et al., 2004; Loosli et al., 2016).\n",
      "Observe that the hinge loss has three equivalent representations, as\n",
      "shown in (12.28) and (12.29), as well as the constrained optimization\n",
      "problem in (12.33). The formulation (12.28) is often used when compar-\n",
      "ing the SVM loss function with other loss functions (Steinwart, 2007).\n",
      "The two-piece formulation (12.29) is convenient for computing subgra-\n",
      "dients, as each piece is linear. The third formulation (12.33), as seen\n",
      "in Section 12.5, enables the use of convex quadratic programming (Sec-\n",
      "tion 7.3.2) tools.\n",
      "Since binary classiﬁcation is a well-studied task in machine learning,\n",
      "other words are also sometimes used, such as discrimination, separation,\n",
      "and decision. Furthermore, there are three quantities that can be the out-\n",
      "put of a binary classiﬁer. First is the output of the linear function itself\n",
      "(often called the score), which can take any real value. This output can be\n",
      "used for ranking the examples, and binary classiﬁcation can be thought\n",
      "of as picking a threshold on the ranked examples (Shawe-Taylor and Cris-\n",
      "tianini, 2004). The second quantity that is often considered the output\n",
      "of a binary classiﬁer is the output determined after it is passed through\n",
      "a non-linear function to constrain its value to a bounded range, for ex-\n",
      "ample in the interval [0;1]. A common non-linear function is the sigmoid\n",
      "function (Bishop, 2006). When the non-linearity results in well-calibrated\n",
      "probabilities (Gneiting and Raftery, 2007; Reid and Williamson, 2011),\n",
      "this is called class probability estimation. The third output of a binary\n",
      "classiﬁer is the ﬁnal binary decision f+1;\u00001g, which is the one most com-\n",
      "monly assumed to be the output of the classiﬁer.\n",
      "The SVM is a binary classiﬁer that does not naturally lend itself to a\n",
      "probabilistic interpretation. There are several approaches for converting\n",
      "the raw output of the linear function (the score) into a calibrated class\n",
      "probability estimate ( P(Y= 1jX=x)) that involve an additional cal-\n",
      "ibration step (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007).\n",
      "From the training perspective, there are many related probabilistic ap-\n",
      "proaches. We mentioned at the end of Section 12.2.5 that there is a re-\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "394 Classiﬁcation with Support Vector Machines\n",
      "lationship between loss function and the likelihood (also compare Sec-\n",
      "tions 8.2 and 8.3). The maximum likelihood approach corresponding to\n",
      "a well-calibrated transformation during training is called logistic regres-\n",
      "sion, which comes from a class of methods called generalized linear mod-\n",
      "els. Details of logistic regression from this point of view can be found in\n",
      "Agresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).\n",
      "Naturally, one could take a more Bayesian view of the classiﬁer output by\n",
      "estimating a posterior distribution using Bayesian logistic regression. The\n",
      "Bayesian view also includes the speciﬁcation of the prior, which includes\n",
      "design choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-\n",
      "ditionally, one could consider latent functions as priors, which results in\n",
      "Gaussian process classiﬁcation (Rasmussen and Williams, 2006, chapter\n",
      "3).\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "References\n",
      "Abel, Niels H. 1826. D´emonstration de l’Impossibilit ´e de la R ´esolution Alg ´ebrique des\n",
      "´Equations G ´en´erales qui Passent le Quatri `eme Degr ´e. Grøndahl and Søn.\n",
      "Adhikari, Ani, and DeNero, John. 2018. Computational and Inferential Thinking: The\n",
      "Foundations of Data Science . Gitbooks.\n",
      "Agarwal, Arvind, and Daum ´e III, Hal. 2010. A Geometric View of Conjugate Priors.\n",
      "Machine Learning ,81(1), 99–113.\n",
      "Agresti, A. 2002. Categorical Data Analysis . Wiley.\n",
      "Akaike, Hirotugu. 1974. A New Look at the Statistical Model Identiﬁcation. IEEE\n",
      "Transactions on Automatic Control ,19(6), 716–723.\n",
      "Akhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert\n",
      "Space . Dover Publications.\n",
      "Alpaydin, Ethem. 2010. Introduction to Machine Learning . MIT Press.\n",
      "Amari, Shun-ichi. 2016. Information Geometry and Its Applications . Springer.\n",
      "Argyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer\n",
      "Theorems. In: Proceedings of the International Conference on Machine Learning .\n",
      "Aronszajn, Nachman. 1950. Theory of Reproducing Kernels. Transactions of the Amer-\n",
      "ican Mathematical Society ,68, 337–404.\n",
      "Axler, Sheldon. 2015. Linear Algebra Done Right . Springer.\n",
      "Bakir, G ¨okhan, Hofmann, Thomas, Sch ¨olkopf, Bernhard, Smola, Alexander J., Taskar,\n",
      "Ben, and Vishwanathan, S. V. N. (eds). 2007. Predicting Structured Data . MIT Press.\n",
      "Barber, David. 2012. Bayesian Reasoning and Machine Learning . Cambridge University\n",
      "Press.\n",
      "Barndorff-Nielsen, Ole. 2014. Information and Exponential Families: In Statistical The-\n",
      "ory. Wiley.\n",
      "Bartholomew, David, Knott, Martin, and Moustaki, Irini. 2011. Latent Variable Models\n",
      "and Factor Analysis: A Uniﬁed Approach . Wiley.\n",
      "Baydin, Atılım G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M.\n",
      "2018. Automatic Differentiation in Machine Learning: A Survey. Journal of Machine\n",
      "Learning Research ,18, 1–43.\n",
      "Beck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgra-\n",
      "dient Methods for Convex Optimization. Operations Research Letters ,31(3), 167–\n",
      "175.\n",
      "Belabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine\n",
      "Learning and New Strategies for Very Large Datasets. Proceedings of the National\n",
      "Academy of Sciences , 0810600105.\n",
      "Belkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality\n",
      "Reduction and Data Representation. Neural Computation ,15(6), 1373–1396.\n",
      "Ben-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S ¨oren, Sch ¨olkopf, Bernhard, and R ¨atsch,\n",
      "Gunnar. 2008. Support Vector Machines and Kernels for Computational Biology.\n",
      "PLoS Computational Biology ,4(10), e1000173.\n",
      "395\n",
      "This material is published by Cambridge University Press as Mathematics for Machine Learning by\n",
      "Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\n",
      "and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\n",
      "©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\n",
      "396 References\n",
      "Bennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM\n",
      "Classiﬁers. In: Proceedings of the International Conference on Machine Learning .\n",
      "Bennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pages\n",
      "132–145 of: Geometry at Work . Mathematical Association of America.\n",
      "Berlinet, Alain, and Thomas-Agnan, Christine. 2004. Reproducing Kernel Hilbert Spaces\n",
      "in Probability and Statistics . Springer.\n",
      "Bertsekas, Dimitri P. 1999. Nonlinear Programming . Athena Scientiﬁc.\n",
      "Bertsekas, Dimitri P. 2009. Convex Optimization Theory . Athena Scientiﬁc.\n",
      "Bickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and\n",
      "Selected Topics . Vol. 1. Prentice Hall.\n",
      "Bickson, Danny, Dolev, Danny, Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007.\n",
      "Linear Detection via Belief Propagation. In: Proceedings of the Annual Allerton Con-\n",
      "ference on Communication, Control, and Computing .\n",
      "Billingsley, Patrick. 1995. Probability and Measure . Wiley.\n",
      "Bishop, Christopher M. 1995. Neural Networks for Pattern Recognition . Clarendon\n",
      "Press.\n",
      "Bishop, Christopher M. 1999. Bayesian PCA. In: Advances in Neural Information Pro-\n",
      "cessing Systems .\n",
      "Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning . Springer.\n",
      "Blei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A\n",
      "Review for Statisticians. Journal of the American Statistical Association ,112(518),\n",
      "859–877.\n",
      "Blum, Arvim, and Hardt, Moritz. 2015. The Ladder: A Reliable Leaderboard for Ma-\n",
      "chine Learning Competitions. In: International Conference on Machine Learning .\n",
      "Bonnans, J. Fr ´ed´eric, Gilbert, J. Charles, Lemar ´echal, Claude, and Sagastiz ´abal, Clau-\n",
      "dia A. 2006. Numerical Optimization: Theoretical and Practical Aspects . Springer.\n",
      "Borwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear\n",
      "Optimization . 2nd edn. Canadian Mathematical Society.\n",
      "Bottou, L ´eon. 1998. Online Algorithms and Stochastic Approximations. Pages 9–42\n",
      "of:Online Learning and Neural Networks . Cambridge University Press.\n",
      "Bottou, L ´eon, Curtis, Frank E., and Nocedal, Jorge. 2018. Optimization Methods for\n",
      "Large-Scale Machine Learning. SIAM Review ,60(2), 223–311.\n",
      "Boucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration In-\n",
      "equalities: A Nonasymptotic Theory of Independence. Oxford University Press.\n",
      "Boyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization . Cambridge\n",
      "University Press.\n",
      "Boyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Alge-\n",
      "bra. Cambridge University Press.\n",
      "Brochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian\n",
      "Optimization of Expensive Cost Functions, with Application to Active User Modeling\n",
      "and Hierarchical Reinforcement Learning . Tech. rept. TR-2009-023. Department of\n",
      "Computer Science, University of British Columbia.\n",
      "Brooks, Steve, Gelman, Andrew, Jones, Galin L., and Meng, Xiao-Li (eds). 2011. Hand-\n",
      "book of Markov Chain Monte Carlo . Chapman and Hall/CRC.\n",
      "Brown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Ap-\n",
      "plications in Statistical Decision Theory . Institute of Mathematical Statistics.\n",
      "Bryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation\n",
      "Processes. In: Proceedings of the Harvard University Symposium on Digital Computers\n",
      "and Their Applications .\n",
      "Bubeck, S ´ebastien. 2015. Convex Optimization: Algorithms and Complexity. Founda-\n",
      "tions and Trends in Machine Learning ,8(3-4), 231–357.\n",
      "B¨uhlmann, Peter, and Van De Geer, Sara. 2011. Statistics for High-Dimensional Data .\n",
      "Springer.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "References 397\n",
      "Burges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and\n",
      "Trends in Machine Learning ,2(4), 275–365.\n",
      "Carroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in\n",
      "Multidimensional Scaling via an N-Way Generalization of “Eckart-Young” Decom-\n",
      "position. Psychometrika ,35(3), 283–319.\n",
      "Casella, George, and Berger, Roger L. 2002. Statistical Inference . Duxbury.\n",
      "C ¸inlar, Erhan. 2011. Probability and Stochastics . Springer.\n",
      "Chang, Chih-Chung, and Lin, Chih-Jen. 2011. LIBSVM: A Library for Support Vector\n",
      "Machines. ACM Transactions on Intelligent Systems and Technology ,2, 27:1–27:27.\n",
      "Cheeseman, Peter. 1985. In Defense of Probability. In: Proceedings of the International\n",
      "Joint Conference on Artiﬁcial Intelligence .\n",
      "Chollet, Francois, and Allaire, J. J. 2018. Deep Learning with R . Manning Publications.\n",
      "Codd, Edgar F. 1990. The Relational Model for Database Management . Addison-Wesley\n",
      "Longman Publishing.\n",
      "Cunningham, John P., and Ghahramani, Zoubin. 2015. Linear Dimensionality Reduc-\n",
      "tion: Survey, Insights, and Generalizations. Journal of Machine Learning Research ,\n",
      "16, 2859–2900.\n",
      "Datta, Biswa N. 2010. Numerical Linear Algebra and Applications . SIAM.\n",
      "Davidson, Anthony C., and Hinkley, David V. 1997. Bootstrap Methods and Their Appli-\n",
      "cation . Cambridge University Press.\n",
      "Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, and Chen, et al. 2012. Large Scale\n",
      "Distributed Deep Networks. In: Advances in Neural Information Processing Systems .\n",
      "Deisenroth, Marc P., and Mohamed, Shakir. 2012. Expectation Propagation in Gaus-\n",
      "sian Process Dynamical Systems. Pages 2618–2626 of: Advances in Neural Informa-\n",
      "tion Processing Systems .\n",
      "Deisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian\n",
      "Filtering and Smoothing: Explaining Current and Deriving New Algorithms. In:\n",
      "Proceedings of the American Control Conference .\n",
      "Deisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes\n",
      "for Data-Efﬁcient Learning in Robotics and Control. IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence ,37(2), 408–423.\n",
      "Dempster, Arthur P., Laird, Nan M., and Rubin, Donald B. 1977. Maximum Likelihood\n",
      "from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society ,\n",
      "39(1), 1–38.\n",
      "Deng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and\n",
      "Hinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep\n",
      "Auto-Encoder. In: Proceedings of Interspeech .\n",
      "Devroye, Luc. 1986. Non-Uniform Random Variate Generation . Springer.\n",
      "Donoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear\n",
      "Embedding Techniques for High-Dimensional Data. Proceedings of the National\n",
      "Academy of Sciences ,100(10), 5591–5596.\n",
      "Dost´al, Zden ˘ek. 2009. Optimal Quadratic Programming Algorithms: With Applications\n",
      "to Variational Inequalities . Springer.\n",
      "Douven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy . Meta-\n",
      "physics Research Lab, Stanford University.\n",
      "Downey, Allen B. 2014. Think Stats: Exploratory Data Analysis . 2nd edn. O’Reilly\n",
      "Media.\n",
      "Dreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of\n",
      "Mathematical Analysis and Applications ,5(1), 30–45.\n",
      "Drumm, Volker, and Weil, Wolfgang. 2001. Lineare Algebra und Analytische Geometrie .\n",
      "Lecture Notes, Universit ¨at Karlsruhe (TH).\n",
      "Dudley, Richard M. 2002. Real Analysis and Probability . Cambridge University Press.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "398 References\n",
      "Eaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach . Institute of\n",
      "Mathematical Statistics Lecture Notes.\n",
      "Eckart, Carl, and Young, Gale. 1936. The Approximation of One Matrix by Another of\n",
      "Lower Rank. Psychometrika ,1(3), 211–218.\n",
      "Efron, Bradley, and Hastie, Trevor. 2016. Computer Age Statistical Inference: Algorithms,\n",
      "Evidence and Data Science . Cambridge University Press.\n",
      "Efron, Bradley, and Tibshirani, Robert J. 1993. An Introduction to the Bootstrap . Chap-\n",
      "man and Hall/CRC.\n",
      "Elliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Func-\n",
      "tional Programming .\n",
      "Evgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical\n",
      "Learning Theory: A Primer. International Journal of Computer Vision ,38(1), 9–13.\n",
      "Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen.\n",
      "2008. LIBLINEAR: A Library for Large Linear Classiﬁcation. Journal of Machine\n",
      "Learning Research ,9, 1871–1874.\n",
      "Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. 2014. Distributed Variational\n",
      "Inference in Sparse Gaussian Process Regression and Latent Variable Models. In:\n",
      "Advances in Neural Information Processing Systems .\n",
      "G¨artner, Thomas. 2008. Kernels for Structured Data . World Scientiﬁc.\n",
      "Gavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Singular\n",
      "Values is 4p\n",
      "3.IEEE Transactions on Information Theory ,60(8), 5040–5053.\n",
      "Gelman, Andrew, Carlin, John B., Stern, Hal S., and Rubin, Donald B. 2004. Bayesian\n",
      "Data Analysis . Chapman and Hall/CRC.\n",
      "Gentle, James E. 2004. Random Number Generation and Monte Carlo Methods .\n",
      "Springer.\n",
      "Ghahramani, Zoubin. 2015. Probabilistic Machine Learning and Artiﬁcial Intelligence.\n",
      "Nature ,521, 452–459.\n",
      "Ghahramani, Zoubin, and Roweis, Sam T. 1999. Learning Nonlinear Dynamical Sys-\n",
      "tems Using an EM Algorithm. In: Advances in Neural Information Processing Systems .\n",
      "MIT Press.\n",
      "Gilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain\n",
      "Monte Carlo in Practice . Chapman and Hall/CRC.\n",
      "Gneiting, Tilmann, and Raftery, Adrian E. 2007. Strictly Proper Scoring Rules, Pre-\n",
      "diction, and Estimation. Journal of the American Statistical Association ,102(477),\n",
      "359–378.\n",
      "Goh, Gabriel. 2017. Why Momentum Really Works. Distill .\n",
      "Gohberg, Israel, Goldberg, Seymour, and Krupnik, Nahum. 2012. Traces and Determi-\n",
      "nants of Linear Operators . Birkh ¨auser.\n",
      "Golan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to\n",
      "Know . Springer.\n",
      "Golub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations . JHU Press.\n",
      "Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning . MIT\n",
      "Press.\n",
      "Graepel, Thore, Candela, Joaquin Qui ˜nonero-Candela, Borchert, Thomas, and Her-\n",
      "brich, Ralf. 2010. Web-Scale Bayesian Click-through Rate Prediction for Sponsored\n",
      "Search Advertising in Microsoft’s Bing Search Engine. In: Proceedings of the Interna-\n",
      "tional Conference on Machine Learning .\n",
      "Griewank, Andreas, and Walther, Andrea. 2003. Introduction to Automatic Differenti-\n",
      "ation. In: Proceedings in Applied Mathematics and Mechanics .\n",
      "Griewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and\n",
      "Techniques of Algorithmic Differentiation . SIAM.\n",
      "Grimmett, Geoffrey R., and Welsh, Dominic. 2014. Probability: An Introduction . Oxford\n",
      "University Press.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "References 399\n",
      "Grinstead, Charles M., and Snell, J. Laurie. 1997. Introduction to Probability . American\n",
      "Mathematical Society.\n",
      "Hacking, Ian. 2001. Probability and Inductive Logic . Cambridge University Press.\n",
      "Hall, Peter. 1992. The Bootstrap and Edgeworth Expansion . Springer.\n",
      "Hallin, Marc, Paindaveine, Davy, and ˇSiman, Miroslav. 2010. Multivariate Quan-\n",
      "tiles and Multiple-Output Regression Quantiles: From `1Optimization to Halfspace\n",
      "Depth. Annals of Statistics ,38, 635–669.\n",
      "Hasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a\n",
      "Panorama of Recent Developments . Cambridge University Press.\n",
      "Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Sta-\n",
      "tistical Learning – Data Mining, Inference, and Prediction . Springer.\n",
      "Hausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller,\n",
      "Martin. 2018. Learning an Embedding Space for Transferable Robot Skills. In:\n",
      "Proceedings of the International Conference on Learning Representations .\n",
      "Hazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and\n",
      "Trends in Optimization ,2(3–4), 157–325.\n",
      "Hensman, James, Fusi, Nicol `o, and Lawrence, Neil D. 2013. Gaussian Processes for\n",
      "Big Data. In: Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence .\n",
      "Herbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian\n",
      "Skill Rating System. In: Advances in Neural Information Processing Systems .\n",
      "Hiriart-Urruty, Jean-Baptiste, and Lemar ´echal, Claude. 2001. Fundamentals of Convex\n",
      "Analysis . Springer.\n",
      "Hoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for\n",
      "Latent Dirichlet Allocation. Advances in Neural Information Processing Systems .\n",
      "Hoffman, Matthew D., Blei, David M., Wang, Chong, and Paisley, John. 2013. Stochas-\n",
      "tic Variational Inference. Journal of Machine Learning Research ,14(1), 1303–1347.\n",
      "Hofmann, Thomas, Sch ¨olkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Meth-\n",
      "ods in Machine Learning. Annals of Statistics ,36(3), 1171–1220.\n",
      "Hogben, Leslie. 2013. Handbook of Linear Algebra . Chapman and Hall/CRC.\n",
      "Horn, Roger A., and Johnson, Charles R. 2013. Matrix Analysis . Cambridge University\n",
      "Press.\n",
      "Hotelling, Harold. 1933. Analysis of a Complex of Statistical Variables into Principal\n",
      "Components. Journal of Educational Psychology ,24, 417–441.\n",
      "Hyvarinen, Aapo, Oja, Erkki, and Karhunen, Juha. 2001. Independent Component Anal-\n",
      "ysis. Wiley.\n",
      "Imbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social\n",
      "and Biomedical Sciences . Cambridge University Press.\n",
      "Jacod, Jean, and Protter, Philip. 2004. Probability Essentials . Springer.\n",
      "Jaynes, Edwin T. 2003. Probability Theory: The Logic of Science . Cambridge University\n",
      "Press.\n",
      "Jefferys, William H., and Berger, James O. 1992. Ockham’s Razor and Bayesian Anal-\n",
      "ysis. American Scientist ,80, 64–72.\n",
      "Jeffreys, Harold. 1961. Theory of Probability . Oxford University Press.\n",
      "Jimenez Rezende, Danilo, and Mohamed, Shakir. 2015. Variational Inference with Nor-\n",
      "malizing Flows. In: Proceedings of the International Conference on Machine Learning .\n",
      "Jimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic\n",
      "Backpropagation and Approximate Inference in Deep Generative Models. In: Pro-\n",
      "ceedings of the International Conference on Machine Learning .\n",
      "Joachims, Thorsten. 1999. Advances in Kernel Methods – Support Vector Learning . MIT\n",
      "Press. Chap. Making Large-Scale SVM Learning Practical, pages 169–184.\n",
      "Jordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., and Saul, Lawrence K.\n",
      "1999. An Introduction to Variational Methods for Graphical Models. Machine Learn-\n",
      "ing,37, 183–233.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "400 References\n",
      "Julier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter\n",
      "to Nonlinear Systems. In: Proceedings of AeroSense Symposium on Aerospace/Defense\n",
      "Sensing, Simulation and Controls .\n",
      "Kaiser, Marcus, and Hilgetag, Claus C. 2006. Nonoptimal Component Placement, but\n",
      "Short Processing Paths, Due to Long-Distance Projections in Neural Systems. PLoS\n",
      "Computational Biology ,2(7), e95.\n",
      "Kalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. Col-\n",
      "lege Mathematics Journal ,27(1), 2–23.\n",
      "Kalman, Rudolf E. 1960. A New Approach to Linear Filtering and Prediction Problems.\n",
      "Transactions of the ASME – Journal of Basic Engineering ,82(Series D), 35–45.\n",
      "Kamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efﬁcient Reinforcement Learning\n",
      "with Probabilistic Model Predictive Control. In: Proceedings of the International\n",
      "Conference on Artiﬁcial Intelligence and Statistics .\n",
      "Katz, Victor J. 2004. A History of Mathematics . Pearson/Addison-Wesley.\n",
      "Kelley, Henry J. 1960. Gradient Theory of Optimal Flight Paths. Ars Journal ,30(10),\n",
      "947–954.\n",
      "Kimeldorf, George S., and Wahba, Grace. 1970. A Correspondence between Bayesian\n",
      "Estimation on Stochastic Processes and Smoothing by Splines. Annals of Mathemat-\n",
      "ical Statistics ,41(2), 495–502.\n",
      "Kingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:\n",
      "Proceedings of the International Conference on Learning Representations .\n",
      "Kittler, Josef, and F ¨oglein, Janos. 1984. Contextual Classiﬁcation of Multispectral Pixel\n",
      "Data. Image and Vision Computing ,2(1), 13–29.\n",
      "Kolda, Tamara G., and Bader, Brett W. 2009. Tensor Decompositions and Applications.\n",
      "SIAM Review ,51(3), 455–500.\n",
      "Koller, Daphne, and Friedman, Nir. 2009. Probabilistic Graphical Models . MIT Press.\n",
      "Kong, Linglong, and Mizera, Ivan. 2012. Quantile Tomography: Using Quantiles with\n",
      "Multivariate Data. Statistica Sinica ,22, 1598–1610.\n",
      "Lang, Serge. 1987. Linear Algebra . Springer.\n",
      "Lawrence, Neil D. 2005. Probabilistic Non-Linear Principal Component Analysis with\n",
      "Gaussian Process Latent Variable Models. Journal of Machine Learning Research ,\n",
      "6(Nov.), 1783–1816.\n",
      "Leemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution\n",
      "Relationships. American Statistician ,62(1), 45–53.\n",
      "Lehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses .\n",
      "Springer.\n",
      "Lehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation . Springer.\n",
      "Liesen, J ¨org, and Mehrmann, Volker. 2015. Linear Algebra . Springer.\n",
      "Lin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt’s Probabilistic\n",
      "Outputs for Support Vector Machines. Machine Learning ,68, 267–276.\n",
      "Ljung, Lennart. 1999. System Identiﬁcation: Theory for the User . Prentice Hall.\n",
      "Loosli, Ga ¨elle, Canu, St ´ephane, and Ong, Cheng Soon. 2016. Learning SVM in Kre ˘ın\n",
      "Spaces. IEEE Transactions of Pattern Analysis and Machine Intelligence ,38(6), 1204–\n",
      "1216.\n",
      "Luenberger, David G. 1969. Optimization by Vector Space Methods . Wiley.\n",
      "MacKay, David J. C. 1992. Bayesian Interpolation. Neural Computation ,4, 415–447.\n",
      "MacKay, David J. C. 1998. Introduction to Gaussian Processes. Pages 133–165 of:\n",
      "Bishop, C. M. (ed), Neural Networks and Machine Learning . Springer.\n",
      "MacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms .\n",
      "Cambridge University Press.\n",
      "Magnus, Jan R., and Neudecker, Heinz. 2007. Matrix Differential Calculus with Appli-\n",
      "cations in Statistics and Econometrics . Wiley.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "References 401\n",
      "Manton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing\n",
      "Kernel Hilbert Spaces. Foundations and Trends in Signal Processing ,8(1–2), 1–126.\n",
      "Markovsky, Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Appli-\n",
      "cations . Springer.\n",
      "Maybeck, Peter S. 1979. Stochastic Models, Estimation, and Control . Academic Press.\n",
      "McCullagh, Peter, and Nelder, John A. 1989. Generalized Linear Models . CRC Press.\n",
      "McEliece, Robert J., MacKay, David J. C., and Cheng, Jung-Fu. 1998. Turbo Decoding\n",
      "as an Instance of Pearl’s “Belief Propagation” Algorithm. IEEE Journal on Selected\n",
      "Areas in Communications ,16(2), 140–152.\n",
      "Mika, Sebastian, R ¨atsch, Gunnar, Weston, Jason, Sch ¨olkopf, Bernhard, and M ¨uller,\n",
      "Klaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41–48 of:\n",
      "Proceedings of the Workshop on Neural Networks for Signal Processing .\n",
      "Minka, Thomas P. 2001a. A Family of Algorithms for Approximate Bayesian Inference .\n",
      "Ph.D. thesis, Massachusetts Institute of Technology.\n",
      "Minka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in\n",
      "Neural Information Processing Systems .\n",
      "Mitchell, Tom. 1997. Machine Learning . McGraw-Hill.\n",
      "Mnih, Volodymyr, Kavukcuoglu, Koray, and Silver, David, et al. 2015. Human-Level\n",
      "Control through Deep Reinforcement Learning. Nature ,518, 529–533.\n",
      "Moonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms,\n",
      "Architectures and Applications . Elsevier.\n",
      "Moustaki, Irini, Knott, Martin, and Bartholomew, David J. 2015. Latent-Variable Mod-\n",
      "eling. American Cancer Society. Pages 1–10.\n",
      "M¨uller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with\n",
      "Python: A Guide for Data Scientists . O’Reilly Publishing.\n",
      "Murphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective . MIT Press.\n",
      "Neal, Radford M. 1996. Bayesian Learning for Neural Networks . Ph.D. thesis, Depart-\n",
      "ment of Computer Science, University of Toronto.\n",
      "Neal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that\n",
      "Justiﬁes Incremental, Sparse, and Other Variants. Pages 355–368 of: Learning in\n",
      "Graphical Models . MIT Press.\n",
      "Nelsen, Roger. 2006. An Introduction to Copulas . Springer.\n",
      "Nesterov, Yuri. 2018. Lectures on Convex Optimization . Springer.\n",
      "Neumaier, Arnold. 1998. Solving Ill-Conditioned and Singular Linear Systems: A Tu-\n",
      "torial on Regularization. SIAM Review ,40, 636–666.\n",
      "Nocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization . Springer.\n",
      "Nowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H.\n",
      "(eds). 2014. Advanced Structured Prediction . MIT Press.\n",
      "O’Hagan, Anthony. 1991. Bayes-Hermite Quadrature. Journal of Statistical Planning\n",
      "and Inference ,29, 245–260.\n",
      "Ong, Cheng Soon, Mary, Xavier, Canu, St ´ephane, and Smola, Alexander J. 2004. Learn-\n",
      "ing with Non-Positive Kernels. In: Proceedings of the International Conference on\n",
      "Machine Learning .\n",
      "Ormoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001.\n",
      "Learning and Tracking Cyclic Human Motion. In: Advances in Neural Information\n",
      "Processing Systems .\n",
      "Page, Lawrence, Brin, Sergey, Motwani, Rajeev, and Winograd, Terry. 1999. The\n",
      "PageRank Citation Ranking: Bringing Order to the Web . Tech. rept. Stanford Info-\n",
      "Lab.\n",
      "Paquet, Ulrich. 2008. Bayesian Inference for Latent Variable Models . Ph.D. thesis, Uni-\n",
      "versity of Cambridge.\n",
      "Parzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode.\n",
      "Annals of Mathematical Statistics ,33(3), 1065–1076.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "402 References\n",
      "Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\n",
      "Inference . Morgan Kaufmann.\n",
      "Pearl, Judea. 2009. Causality: Models, Reasoning and Inference . 2nd edn. Cambridge\n",
      "University Press.\n",
      "Pearson, Karl. 1895. Contributions to the Mathematical Theory of Evolution. II. Skew\n",
      "Variation in Homogeneous Material. Philosophical Transactions of the Royal Society\n",
      "A: Mathematical, Physical and Engineering Sciences ,186, 343–414.\n",
      "Pearson, Karl. 1901. On Lines and Planes of Closest Fit to Systems of Points in Space.\n",
      "Philosophical Magazine ,2(11), 559–572.\n",
      "Peters, Jonas, Janzing, Dominik, and Sch ¨olkopf, Bernhard. 2017. Elements of Causal\n",
      "Inference: Foundations and Learning Algorithms . MIT Press.\n",
      "Petersen, Kaare B., and Pedersen, Michael S. 2012. The Matrix Cookbook . Tech. rept.\n",
      "Technical University of Denmark.\n",
      "Platt, John C. 2000. Probabilistic Outputs for Support Vector Machines and Compar-\n",
      "isons to Regularized Likelihood Methods. In: Advances in Large Margin Classiﬁers .\n",
      "Pollard, David. 2002. A User’s Guide to Measure Theoretic Probability . Cambridge\n",
      "University Press.\n",
      "Polyak, Roman A. 2016. The Legendre Transformation in Modern Optimization. Pages\n",
      "437–507 of: Goldengorin, B. (ed), Optimization and Its Applications in Control and\n",
      "Data Sciences . Springer.\n",
      "Press, William H., Teukolsky, Saul A., Vetterling, William T., and Flannery, Brian P.\n",
      "2007. Numerical Recipes: The Art of Scientiﬁc Computing . Cambridge University\n",
      "Press.\n",
      "Proschan, Michael A., and Presnell, Brett. 1998. Expect the Unexpected from Condi-\n",
      "tional Expectation. American Statistician ,52(3), 248–252.\n",
      "Raschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine\n",
      "Learning and Deep Learning with Python, scikit-learn, and TensorFlow . Packt Publish-\n",
      "ing.\n",
      "Rasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam’s Razor. In: Advances in\n",
      "Neural Information Processing Systems .\n",
      "Rasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-\n",
      "vances in Neural Information Processing Systems .\n",
      "Rasmussen, Carl E., and Williams, Christopher K. I. 2006. Gaussian Processes for Ma-\n",
      "chine Learning . MIT Press.\n",
      "Reid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for\n",
      "Binary Experiments. Journal of Machine Learning Research ,12, 731–817.\n",
      "Rifkin, Ryan M., and Lippert, Ross A. 2007. Value Regularization and Fenchel Duality.\n",
      "Journal of Machine Learning Research ,8, 441–479.\n",
      "Rockafellar, Ralph T. 1970. Convex Analysis . Princeton University Press.\n",
      "Rogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning . Chap-\n",
      "man and Hall/CRC.\n",
      "Rosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal\n",
      "Inference . Harvard University Press.\n",
      "Rosenblatt, Murray. 1956. Remarks on Some Nonparametric Estimates of a Density\n",
      "Function. Annals of Mathematical Statistics ,27(3), 832–837.\n",
      "Roweis, Sam T. 1998. EM Algorithms for PCA and SPCA. Pages 626–632 of: Advances\n",
      "in Neural Information Processing Systems .\n",
      "Roweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gaussian\n",
      "Models. Neural Computation ,11(2), 305–345.\n",
      "Roy, Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for\n",
      "Statistics . Chapman and Hall/CRC.\n",
      "Rubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo\n",
      "Method . Wiley.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "References 403\n",
      "Rufﬁni, Paolo. 1799. Teoria Generale delle Equazioni, in cui si Dimostra Impossibile la\n",
      "Soluzione Algebraica delle Equazioni Generali di Grado Superiore al Quarto . Stampe-\n",
      "ria di S. Tommaso d’Aquino.\n",
      "Rumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning\n",
      "Representations by Back-Propagating Errors. Nature ,323(6088), 533–536.\n",
      "Sæmundsson, Steind ´or, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein-\n",
      "forcement Learning with Latent Variable Gaussian Processes. In: Proceedings of the\n",
      "Conference on Uncertainty in Artiﬁcial Intelligence .\n",
      "Saitoh, Saburou. 1988. Theory of Reproducing Kernels and its Applications . Longman\n",
      "Scientiﬁc and Technical.\n",
      "S¨arkk¨a, Simo. 2013. Bayesian Filtering and Smoothing . Cambridge University Press.\n",
      "Sch¨olkopf, Bernhard, and Smola, Alexander J. 2002. Learning with Kernels – Support\n",
      "Vector Machines, Regularization, Optimization, and Beyond . MIT Press.\n",
      "Sch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1997. Kernel\n",
      "Principal Component Analysis. In: Proceedings of the International Conference on\n",
      "Artiﬁcial Neural Networks .\n",
      "Sch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1998. Nonlinear\n",
      "Component Analysis as a Kernel Eigenvalue Problem. Neural Computation ,10(5),\n",
      "1299–1319.\n",
      "Sch¨olkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized\n",
      "Representer Theorem. In: Proceedings of the International Conference on Computa-\n",
      "tional Learning Theory .\n",
      "Schwartz, Laurent. 1964. Sous Espaces Hilbertiens d’Espaces Vectoriels Topologiques\n",
      "et Noyaux Associ ´es.Journal d’Analyse Math ´ematique ,13, 115–256.\n",
      "Schwarz, Gideon E. 1978. Estimating the Dimension of a Model. Annals of Statistics ,\n",
      "6(2), 461–464.\n",
      "Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams, Ryan P., and De Freitas, Nando.\n",
      "2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.\n",
      "Proceedings of the IEEE ,104(1), 148–175.\n",
      "Shalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning:\n",
      "From Theory to Algorithms . Cambridge University Press.\n",
      "Shawe-Taylor, John, and Cristianini, Nello. 2004. Kernel Methods for Pattern Analysis .\n",
      "Cambridge University Press.\n",
      "Shawe-Taylor, John, and Sun, Shiliang. 2011. A Review of Optimization Methodologies\n",
      "in Support Vector Machines. Neurocomputing ,74(17), 3609–3618.\n",
      "Shental, Ori, Siegel, Paul H., Wolf, Jack K., Bickson, Danny, and Dolev, Danny. 2008.\n",
      "Gaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863–\n",
      "1867 of: Proceedings of the International Symposium on Information Theory .\n",
      "Shewchuk, Jonathan R. 1994. An Introduction to the Conjugate Gradient Method with-\n",
      "out the Agonizing Pain .\n",
      "Shi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation.\n",
      "IEEE Transactions on Pattern Analysis and Machine Intelligence ,22(8), 888–905.\n",
      "Shi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J.,\n",
      "and Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of\n",
      "Machine Learning Research , 2615–2637.\n",
      "Shiryayev, Albert N. 1984. Probability . Springer.\n",
      "Shor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions . Springer.\n",
      "Shotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton-\n",
      "Boost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recog-\n",
      "nition and Segmentation. In: Proceedings of the European Conference on Computer\n",
      "Vision .\n",
      "Smith, Adrian F. M., and Spiegelhalter, David. 1980. Bayes Factors and Choice Criteria\n",
      "for Linear Models. Journal of the Royal Statistical Society B ,42(2), 213–220.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "404 References\n",
      "Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op-\n",
      "timization of Machine Learning Algorithms. In: Advances in Neural Information\n",
      "Processing Systems .\n",
      "Spearman, Charles. 1904. “General Intelligence,” Objectively Determined and Mea-\n",
      "sured. American Journal of Psychology ,15(2), 201–292.\n",
      "Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Sch ¨olkopf, Bernhard,\n",
      "and Lanckriet, Gert R. G. 2010. Hilbert Space Embeddings and Metrics on Proba-\n",
      "bility Measures. Journal of Machine Learning Research ,11, 1517–1561.\n",
      "Steinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks.\n",
      "Constructive Approximation ,26, 225–287.\n",
      "Steinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines . Springer.\n",
      "Stoer, Josef, and Burlirsch, Roland. 2002. Introduction to Numerical Analysis . Springer.\n",
      "Strang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. The American\n",
      "Mathematical Monthly ,100(9), 848–855.\n",
      "Strang, Gilbert. 2003. Introduction to Linear Algebra . Wellesley-Cambridge Press.\n",
      "Stray, Jonathan. 2016. The Curious Journalist’s Guide to Data . Tow Center for Digital\n",
      "Journalism at Columbia’s Graduate School of Journalism.\n",
      "Strogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.\n",
      "Notices of the American Mathematical Society ,61(3), 286–291.\n",
      "Sucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level\n",
      "Vision. Image and Vision Computing ,12(1), 42–60.\n",
      "Szeliski, Richard, Zabih, Ramin, and Scharstein, Daniel, et al. 2008. A Compar-\n",
      "ative Study of Energy Minimization Methods for Markov Random Fields with\n",
      "Smoothness-Based Priors. IEEE Transactions on Pattern Analysis and Machine In-\n",
      "telligence ,30(6), 1068–1080.\n",
      "Tandra, Haryono. 2014. The Relationship between the Change of Variable Theorem\n",
      "and the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of\n",
      "Mathematics ,17(2), 76–83.\n",
      "Tenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geometric\n",
      "Framework for Nonlinear Dimensionality Reduction. Science ,290(5500), 2319–\n",
      "2323.\n",
      "Tibshirani, Robert. 1996. Regression Selection and Shrinkage via the Lasso. Journal\n",
      "of the Royal Statistical Society B ,58(1), 267–288.\n",
      "Tipping, Michael E., and Bishop, Christopher M. 1999. Probabilistic Principal Compo-\n",
      "nent Analysis. Journal of the Royal Statistical Society: Series B ,61(3), 611–622.\n",
      "Titsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent\n",
      "Variable Model. In: Proceedings of the International Conference on Artiﬁcial Intelli-\n",
      "gence and Statistics .\n",
      "Toussaint, Marc. 2012. Some Notes on Gradient Descent . https://ipvs.informatik.uni-\n",
      "stuttgart.de/mlr/marc/notes/gradientDescent.pdf.\n",
      "Trefethen, Lloyd N., and Bau III, David. 1997. Numerical Linear Algebra . SIAM.\n",
      "Tucker, Ledyard R. 1966. Some Mathematical Notes on Three-Mode Factor Analysis.\n",
      "Psychometrika ,31(3), 279–311.\n",
      "Vapnik, Vladimir N. 1998. Statistical Learning Theory . Wiley.\n",
      "Vapnik, Vladimir N. 1999. An Overview of Statistical Learning Theory. IEEE Transac-\n",
      "tions on Neural Networks ,10(5), 988–999.\n",
      "Vapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory . Springer.\n",
      "Vishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt,\n",
      "Karsten M. 2010. Graph Kernels. Journal of Machine Learning Research ,11, 1201–\n",
      "1242.\n",
      "von Luxburg, Ulrike, and Sch ¨olkopf, Bernhard. 2011. Statistical Learning Theory:\n",
      "Models, Concepts, and Results. Pages 651–706 of: D. M. Gabbay, S. Hartmann,\n",
      "J. Woods (ed), Handbook of the History of Logic , vol. 10. Elsevier.\n",
      "Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .\n",
      "References 405\n",
      "Wahba, Grace. 1990. Spline Models for Observational Data . Society for Industrial and\n",
      "Applied Mathematics.\n",
      "Walpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Ye, Keying. 2011.\n",
      "Probability and Statistics for Engineers and Scientists . Prentice Hall.\n",
      "Wasserman, Larry. 2004. All of Statistics . Springer.\n",
      "Wasserman, Larry. 2007. All of Nonparametric Statistics . Springer.\n",
      "Whittle, Peter. 2000. Probability via Expectation . Springer.\n",
      "Wickham, Hadley. 2014. Tidy Data. Journal of Statistical Software ,59, 1–23.\n",
      "Williams, Christopher K. I. 1997. Computing with Inﬁnite Networks. In: Advances in\n",
      "Neural Information Processing Systems .\n",
      "Yu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv ´ari, Csaba. 2013. Charac-\n",
      "terizing the Representer Theorem. In: Proceedings of the International Conference on\n",
      "Machine Learning .\n",
      "Zadrozny, Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti-\n",
      "mates from Decision Trees and Naive Bayesian Classiﬁers. In: Proceedings of the\n",
      "International Conference on Machine Learning .\n",
      "Zhang, Haizhang, Xu, Yuesheng, and Zhang, Jun. 2009. Reproducing Kernel Banach\n",
      "Spaces for Machine Learning. Journal of Machine Learning Research ,10, 2741–2775.\n",
      "Zia, Royce K. P., Redish, Edward F., and McKay, Susan R. 2009. Making Sense of the\n",
      "Legendre Transform. American Journal of Physics ,77(614), 614–622.\n",
      "©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c1efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../dat/parsed_books/mml-book.txt\", \"a\")\n",
    "f.write(text)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('LD-inference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b3dd287303ef379cd3bfc1446cef400144e42385eabfb9b315f60147448249f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

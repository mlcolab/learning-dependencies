{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('all')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download(\"omw-1.4\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859754\n",
      "817842\n"
     ]
    }
   ],
   "source": [
    "with open (\"../dat/parsed_books/mml-book.txt\", encoding = \"utf8\") as f:\n",
    "    text = f.readlines()\n",
    "text = \"\".join(text)\n",
    "print(len(text))\n",
    "text = text.replace('©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).',\" \")\n",
    "text = text.replace(\"\\n©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com .\",\" \")\n",
    "text = text.replace('Draft (2022-01-11) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .',\" \")\n",
    "text = text.replace(\"This material is published by Cambridge University Press as Mathematics for Machine Learning by\\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\\nand download for personal use only. Not for re-distribution, re-sale, or use in derivative works.\",\" \")\n",
    "print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467726"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part1 = text.split(\"\\nPart II\\nCentral Machine Learning Problems\")[0]\n",
    "len(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter: \n",
      "1\n",
      "Introduction and Motivation\n",
      "\n",
      "chapter: \n",
      "2\n",
      "Linear Algebra\n",
      "\n",
      "chapter: \n",
      "3\n",
      "Analytic Geometry\n",
      "\n",
      "chapter: \n",
      "4\n",
      "Matrix Decompositions\n",
      "\n",
      "chapter: \n",
      "5\n",
      "Vector Calculus\n",
      "\n",
      "chapter: \n",
      "6\n",
      "Probability and Distributions\n",
      "\n",
      "chapter: \n",
      "7\n",
      "Continuous Optimization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chapters = [\"\\n1\\nIntroduction and Motivation\\n\",\"\\n2\\nLinear Algebra\\n\",\"\\n3\\nAnalytic Geometry\\n\",\"\\n4\\nMatrix Decompositions\\n\",\n",
    "            \"\\n5\\nVector Calculus\\n\",\"\\n6\\nProbability and Distributions\\n\",\"\\n7\\nContinuous Optimization\\n\"]\n",
    "\n",
    "chapters_dict ={}\n",
    "rest_chapters = part1 ## .split(chapters[0])[1]\n",
    "\n",
    "# Chapter 0 (not in index): intro\n",
    "# chapter 1: ...\n",
    "# ...\n",
    "# Chapter n : ....\n",
    "for idx,chapter_name in enumerate(chapters):\n",
    "    text_splitted = rest_chapters.split(chapter_name)\n",
    "    if idx ==0:\n",
    "        pass\n",
    "    else:\n",
    "        chapters_dict[chapters[idx-1][3:-1]] = text_splitted[0]\n",
    "    print(\"chapter: \"+ chapter_name)\n",
    "    rest_chapters = text_splitted[1]\n",
    "chapters_dict[chapters[idx][3:-1]] = text_splitted[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Introduction and Motivation</th>\n",
       "      <td>Machine learning is about designing algorithms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Algebra</th>\n",
       "      <td>When formalizing intuitive concepts, a common ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analytic Geometry</th>\n",
       "      <td>In Chapter 2, we studied vectors, vector space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matrix Decompositions</th>\n",
       "      <td>In Chapters 2 and 3, we studied ways to manipu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vector Calculus</th>\n",
       "      <td>Many algorithms in machine learning optimize a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Probability and Distributions</th>\n",
       "      <td>Probability, loosely speaking, concerns the st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continuous Optimization</th>\n",
       "      <td>Since machine learning algorithms are implemen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            text\n",
       "Introduction and Motivation    Machine learning is about designing algorithms...\n",
       "Linear Algebra                 When formalizing intuitive concepts, a common ...\n",
       "Analytic Geometry              In Chapter 2, we studied vectors, vector space...\n",
       "Matrix Decompositions          In Chapters 2 and 3, we studied ways to manipu...\n",
       "Vector Calculus                Many algorithms in machine learning optimize a...\n",
       "Probability and Distributions  Probability, loosely speaking, concerns the st...\n",
       "Continuous Optimization        Since machine learning algorithms are implemen..."
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict(chapters_dict, orient='index',columns=[\"text\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(text,delete_stopwords=True):\n",
    "    #text = re.sub(\"\\n\",\" \",text)\n",
    "    #text = re.sub(\"\\d+\", \"\", text)\n",
    "\n",
    "    # words only\n",
    "    regex=u\"[A-Za-z]+\"\n",
    "    regex = re.compile(regex)\n",
    "    text = \" \".join(regex.findall(text))\n",
    "    # lower\n",
    "    text = text.lower()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    # remove stop words + lemmatization\n",
    "    if delete_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = \" \".join([lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words])\n",
    "    else:\n",
    "        text = \" \".join([lemmatizer.lemmatize(w) for w in word_tokens])  \n",
    "    return text\n",
    "\n",
    "data[\"text_prep\"] = data.text.apply(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 65421)\n",
      "Introduction and Motivation: \n",
      "  learning, data, machine, machine learning, model, book, pillar, vector, concept, chapter, part, mathematical concept, foundation, mathematical, unseen, predictor, label, read, training, two, regression, way, learning system, machine learning system, read book, part ii, way read book, way read, pillar machine, two way read, four pillar machine, four pillar, pillar machine learning, data vector, unseen data, parameter, nd, well, input, density, system, algorithm, mean, estimation, two way, motivation, ing, classi cation, machine learning algorithm, goal\n",
      "Linear Algebra: \n",
      "  vector, linear, matrix, column, basis, space, equation, mapping, subspace, system linear, pivot, system linear equation, linear equation, system, vector space, de, row, linearly, set, coordinate, solution, pivot column, linear mapping, ne, element, rn, linear algebra, transformation, respect, echelon form, echelon, algebra, form, row echelon form, row echelon, af, af ne, transformation matrix, group, inverse, following, example, two, consider, rm, linearly independent, base, equation system, multiplication, combination\n",
      "Analytic Geometry: \n",
      "  projection, vector, product, inner, inner product, orthogonal, basis, subspace, rotation, de, angle, space, hx, norm, matrix, dimensional, onto, orthogonal projection, distance, figure, co, nite, basis vector, projection onto, positive, length, projection matrix, hx yi, yi, de nite, vector space, dot, positive de, sin, two, dot product, product inner product, positive de nite, dimensional subspace, linear, span, orthonormal, de nition, nition, rotate, symmetric, product inner, chapter, dot product inner, analytic geometry\n",
      "Matrix Decompositions: \n",
      "  matrix, svd, eigenvalue, eigenvectors, singular, decomposition, vector, singular value, movie, determinant, det, basis, rank, value, rn, singular vector, eigendecomposition, diagonal, theorem, right singular, mapping, de, spectral, square, rank approximation, right, matrix decomposition, column, cholesky, right singular vector, approximation, linear, trace, det det, orthogonal, section, eigenvector, characteristic, square matrix, tr, left, linear mapping, figure, diagonal matrix, rating, eigenvalue eigenvectors, change, positive, matrix rn, ba\n",
      "Vector Calculus: \n",
      "  derivative, partial, function, taylor, gradient, partial derivative, rule, chain rule, chain, taylor series, vector, series, fk, df, jacobian, differentiation, compute, matrix, de, fi, respect, tensor, xn, dx, fk fk, series expansion, order, polynomial, taylor series expansion, exp, rn, fk fk fk, expansion, yf, taylor polynomial, compute gradient, automatic differentiation, xf, sin, vector calculus, figure, automatic, obtain, variable, section, fm, compute derivative, linear, calculus, df dx\n",
      "Probability and Distributions: \n",
      "  random, random variable, distribution, variable, probability, covariance, variance, gaussian, prior, statistic, mean, discrete, function, de, exponential, family, exponential family, section, two, cov, beta, example, event, rule, space, density, probability distribution, marginal, sample, value, conditional, bernoulli, continuous random, consider, gaussian distribution, posterior, continuous, theorem, continuous random variable, univariate, cdf, xjy, outcome, multivariate, de nition, nition, state, two random, sum, covariance matrix\n",
      "Continuous Optimization: \n",
      "  convex, function, gradient, optimization, descent, gradient descent, convex function, min, optimization problem, dual, problem, minimum, convex set, convex optimization, set, constraint, function convex, legendre, convex conjugate, continuous optimization, multiplier, lagrange, lagrangian, duality, subject, de, differentiable, max, step size, point, value, two, conjugate, consider, objective function, lagrange multiplier, step, constrained, primal, negative, inequality, size, objective, linear program, example, program, line, figure, legendre fenchel, momentum\n"
     ]
    }
   ],
   "source": [
    "# !!! tf-idf \n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf.fit_transform(data.text_prep.values)\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "vocab = {v:k for k,v in tfidf.vocabulary_.items()}\n",
    "\n",
    "data[\"tfidf_topwords\"]=None\n",
    "for n,chapter in enumerate(data.index):\n",
    "    s = \"\"\n",
    "    for idx in np.argsort(tfidf_matrix.A[n,:])[::-1][:50]:\n",
    "        s += \", \"+vocab[idx]\n",
    "    data.loc[chapter,\"tfidf_topwords\"] = s[2:]\n",
    "\n",
    "    print(chapter+\": \\n \", data.loc[chapter,\"tfidf_topwords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIMPLE BAG OF WORDS (COUNTVECTORIZER) + POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 404),\n",
       " ('vector', 299),\n",
       " ('b', 278),\n",
       " ('linear', 240),\n",
       " ('matrix', 181),\n",
       " ('c', 150),\n",
       " ('v', 135),\n",
       " ('space', 116),\n",
       " ('column', 115),\n",
       " ('equation', 112),\n",
       " ('r', 108),\n",
       " ('basis', 104),\n",
       " ('system', 103),\n",
       " ('mapping', 97),\n",
       " ('n', 90)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(data.text_prep['Linear Algebra'].split(\" \")).most_common()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging \n",
    "data[\"text_prep_stopwords_incl\"] = data.text.apply(prep,delete_stopwords=False)\n",
    "\n",
    "def filter_by_POS(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    text = [p[0] for p in pos if p[1] in [\"VBG\",\"RB\",\"RBR\",\"RBS\",\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNS\"]]\n",
    "    return \" \".join(text)\n",
    "\n",
    "data[\"text_nouns_adj_adv\"] = data[\"text_prep_stopwords_incl\"].apply(filter_by_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning designing algorithm automatically valuable information data emphasis here automatic i machine learning general purpose methodology many datasets producing something mean ingful core machine learning data model learning machine learning inherently data driven data core data machine learning goal machine learning general purpose methodology valuable pattern data ideally much domain speci c expertise example large corpus document e g book many library machine learning method automatically nd relevant topic document hoffman al goal mod typically process data similar dataset example regression setting model function input real output model data formance task improves data account goal good model well yet unseen data future learning learning way automatically pattern structure data optimizing parameter model machine learning ha many success story software readily available rich exible machine learning system mathematical foundation machine learn ing important order fundamental principle more complicated machine learning system understand ing principle creating new machine learning solution understanding debugging existing approach learning inherent assumption limitation methodology work ing introduction motivation finding word intuition challenge regularly machine learning concept word slippery particular component machine learning system different mathematical concept example word algorithm least different sens con text machine learning rst sense phrase machine learning algorithm system prediction put data predictor second sense predictor exact same phrase machine learning algorithm system internal parameter predictor well future unseen input data here adapta tion training system training book not issue ambiguity light upfront depending context same expression different thing however context suf ciently clear level ambiguity rst part book mathematical concept foundation main component machine learning system data model learning y component here again chapter necessary mathematical concept not data numerical often useful data number format book data ha already appropriately numerical representation suitable read ing computer program data vector data vector illustration subtle word least different way vector vector array number computer science view vector arrow direction magni physic view vector object addition scaling mathematical view amodel typically process generating data sim model ilar dataset hand therefore good model also simpli ed version real unknown data generating process capturing aspect relevant modeling data extracting hidden pattern good model then real world performing real world experi ments now crux matter learning component learning machine learning dataset suitable model training model mean data available pa rameters model respect utility function well model training data most training method approach analogous climbing hill analogy peak hill maximum way book performance measure however practice interested model well unseen data performing well data already training data only good way data however not well data practical application often machine learning system situation not let u main concept machine learning book data vector appropriate model using probabilistic opti mization view available data using numerical optimization method aim model well data not training way book strategy understanding mathematics machine learning building concept foundational more ad often preferred approach more technical eld mathematics strategy advantage reader time able previously concept unfor tunately practitioner many foundational concept not particularly interesting lack motivation most foundational nitions quickly top drilling practical need more basic require ments goal driven approach advantage reader time particular concept clear path knowledge downside strat egy knowledge potentially shaky foundation reader set word not way understanding book modular way foundational mathematical concept application so book way book part part math ematical foundation part ii concept part i set fundamental machine learning problem pillar machine learning figure regression dimensionality reduction density estimation classi cation chapter part i mostly previous possible chapter work backward necessary chapter part ii only loosely order many pointer forward backward introduction motivation foundation pillar machine learning classi cation density estimation regression dimensionality reduction machine learning vector calculus probability distribution optimization analytic geometry matrix decomposition algebra part book mathematical concept machine learning algorithm course more way book most reader using combination top down time building basic mathematical skill attempting more com plex concept also choosing topic application machine learning part i mathematics pillar machine learning book figure solid mathematical foundation part i numerical data vector table such data matrix study vector matrix linear algebra chapter collection vector matrix linear algebra also there vector representing object real world statement similarity idea vector similar similar output machine learning algorithm predictor idea similarity tween vector operation vector input numerical value representing similarity con struction similarity distance central analytic geometry analytic geometry chapter chapter fundamental concept matri ce matrix decomposition operation matrix extremely matrix decomposition useful machine learning intuitive interpretation data more ef cient learning often data noisy observation true underly ing signal applying machine learning signal noise u language quantify ing noise mean often also predictor way book u sort uncertainty e g con dence value prediction particular test data point quanti cation uncertainty realm probability theory probability theory chapter machine learning model typically parameter performance measure many optimization technique re concept gradient direction solution chapter vector calculus vector calculus concept gradient subsequently chapter optimization maximum minimum function optimization part ii machine learning second part book pillar machine learning figure mathematical concept rst part book foundation pillar broadly speaking chapter dif culty ascending order chapter component machine learning data model parameter estimation mathematical fashion addition guideline building experimental set ups overly optimistic evaluation machine learning sys tems goal predictor well unseen data chapter close look linear regression linear regression objective function map input rdto corresponding function value y r label respective input classical model tting parameter esti mation maximum likelihood maximum posteriori estimation well bayesian linear regression parameter instead optimizing chapter focus dimensionality reduction second pillar fig dimensionality reduction ure using principal component analysis key objective dimen sionality reduction compact lower dimensional representation high dimensional data rd often easier original data regression dimensionality reduction only con modeling data label data pointx chapter third pillar density estimation density estimation objective density estimation probability distribution dataset gaussian mixture model purpose iterative scheme parameter model dimensionality reduction label data point x rd however not low dimensional representation data instead interested density model data chapter book depth discussion fourth introduction motivation pillar classi cation classi cation context support classi cation vector machine similar chapter corresponding label y however regression label real label classi cation integer special care exercise feedback exercise part i mostly pen paper part ii programming tutorial jupyter notebook property machine learning book cambridge university press strongly aim education learning making book freely available download http mml book com tutorial erratum additional material mistake feedback using preceding url'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text_nouns_adj_adv\"][\"Introduction and Motivation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 25348)\n",
      "Introduction and Motivation: \n",
      "  learning, data, machine, machine learning, model, book, vector, concept, chapter, part, mathematical, way, well, pillar, regression, system, not, matrix, reduction, parameter, cation, estimation, foundation, training, more, label, often, classi cation, ing, many, dimensionality, real, mathematical concept, predictor, density, component, dimensionality reduction, classi, algorithm, using, density estimation, input, function, numerical, goal, learning system, optimization, word, linear, however\n",
      "Linear Algebra: \n",
      "  vector, linear, matrix, space, column, equation, basis, system, mapping, solution, vector space, linear equation, subspace, set, respect, row, system linear, transformation, form, coordinate, element, example, linearly, rn, pivot, following, linear mapping, not, inverse, then, af, algebra, rm, ne, remark, operation, multiplication, linear algebra, only, combination, group, pivot column, figure, base, independent, af ne, echelon, echelon form, row echelon, general\n",
      "Analytic Geometry: \n",
      "  vector, product, inner, inner product, projection, orthogonal, basis, space, dimensional, matrix, subspace, angle, rotation, figure, norm, dot, vector space, distance, co, linear, length, dot product, positive, hx, orthogonal projection, basis vector, chapter, then, nition, symmetric, using, also, not, example, yi, function, sin, dimensional subspace, section, geometry, mapping, rn, hx yi, coordinate, vector orthogonal, projection matrix, euclidean, only, span, nite\n",
      "Matrix Decompositions: \n",
      "  matrix, vector, eigenvalue, svd, singular, value, decomposition, basis, determinant, eigenvectors, rn, rank, singular value, diagonal, square, linear, mapping, movie, right, theorem, section, column, only, not, figure, then, change, orthogonal, singular vector, det, example, approximation, eigendecomposition, same, linear mapping, symmetric, space, right singular, transformation, positive, matrix decomposition, spectral, cholesky, data, using, rm, number, trace, rank approximation, eigenvector\n",
      "Vector Calculus: \n",
      "  function, derivative, gradient, vector, partial, rule, partial derivative, taylor, respect, series, matrix, chain, chain rule, order, taylor series, differentiation, figure, dx, polynomial, jacobian, linear, section, variable, rn, example, df, fk, tensor, not, case, sin, using, calculus, term, nition, fi, product, vector calculus, parameter, rm, model, dimension, series expansion, rst, following, co, fk fk, expansion, chapter, rd\n",
      "Probability and Distributions: \n",
      "  distribution, random, variable, random variable, probability, gaussian, function, mean, covariance, section, example, value, space, variance, statistic, probability distribution, discrete, continuous, not, family, density, rule, prior, nition, also, exponential, state, more, sample, transformation, matrix, learning, event, exponential family, sum, multivariate, theorem, often, likelihood, product, univariate, inverse, chapter, case, then, beta, figure, conjugate, machine, gaussian distribution\n",
      "Continuous Optimization: \n",
      "  function, convex, gradient, optimization, problem, descent, gradient descent, convex function, min, point, value, minimum, constraint, step, example, optimization problem, figure, set, dual, linear, conjugate, size, section, differentiable, learning, ax, line, nition, program, method, objective, chapter, convex optimization, not, function convex, using, lagrangian, continuous optimization, continuous, convex conjugate, objective function, lagrange, negative, max, inequality, subject, machine, variable, multiplier, step size\n"
     ]
    }
   ],
   "source": [
    "countvect = CountVectorizer(analyzer='word',ngram_range=(1, 2))\n",
    "countvect_matrix = countvect.fit_transform(data.text_nouns_adj_adv.values)\n",
    "print(countvect_matrix.shape)\n",
    "\n",
    "vocab = {v:k for k,v in countvect.vocabulary_.items()}\n",
    "\n",
    "data[\"countvect_topwords\"]=None\n",
    "for n,chapter in enumerate(data.index):\n",
    "    s = \"\"\n",
    "    for idx in np.argsort(countvect_matrix.A[n,:])[::-1][:50]:\n",
    "        s += \", \"+vocab[idx]\n",
    "    data.loc[chapter,\"countvect_topwords\"] = s[2:]\n",
    "\n",
    "    print(chapter+\": \\n \", data.loc[chapter,\"countvect_topwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea183bb76f01b1c0f19d4faefaf72022d209702e86a95c61a348be375d9bcd4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

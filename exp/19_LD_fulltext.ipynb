{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is designed to extract dependencies for all index entries based on common occurences of other entities around one entity's page of introduction. Hence, we need a mapping from page to character offset and a data structure that is able to capture the concepts that are common across books. In a second step, maybe a Gaussian or asymmetric weighting of distance is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import data_layer\n",
    "#from importlib import reload\n",
    "#data_layer = reload(data_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPS = 5\n",
    "MIN_PAGERANK = 0.00005\n",
    "MIN_SPAN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_indices, wiki_concepts = data_layer.read_index_and_wiki_concepts()\n",
    "page_offsets = data_layer.get_page_offsets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book, index in list(book_indices.items()):\n",
    "    #print(book)\n",
    "    mentions_by_offset, offset_index = data_layer.get_mentions_by_offset(book, min_pr=0.00005)\n",
    "    def get_mentions_around(index_page):\n",
    "        page = index_page + data_layer.page_correction.get(book)\n",
    "        print(f\"1-indexed pdf page {page}\")\n",
    "        if page<2: return []\n",
    "        page_slice = slice(page-2, page)\n",
    "        #print(page_slice)\n",
    "        intro_offsets = page_offsets[book.replace(\".csv\",\"\")][page_slice] # offsets of the two pages\n",
    "        #print(intro_offsets)\n",
    "        after_page_start = (offset_index > intro_offsets[0])\n",
    "        before_page_end = (offset_index < intro_offsets[1])\n",
    "        is_same_page = after_page_start & before_page_end\n",
    "        closest = offset_index[is_same_page]\n",
    "        return list(map(lambda x: mentions_by_offset[x]['title'], list(closest)))\n",
    "    \n",
    "    #print(len(page_offsets[book.replace(\".csv\",\"\")])-data_layer.page_correction.get(book)-1)\n",
    "\n",
    "    for i in range(1,len(page_offsets[book.replace(\".csv\",\"\")])-data_layer.page_correction.get(book)):\n",
    "        print(i)\n",
    "        print(book + \" book page \" + str(i))\n",
    "        print(get_mentions_around(i))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_deps = {concept: [] for concept in wiki_concepts}\n",
    "\n",
    "for book, index in book_indices.items():\n",
    "    mentions_by_offset, offset_index = data_layer.get_mentions_by_offset(book, min_pr=MIN_PAGERANK, min_span=MIN_SPAN)\n",
    "    def get_mentions_around(page):\n",
    "        page = page + data_layer.page_correction.get(book)\n",
    "        if page<2: return []\n",
    "        page_slice = slice(page-2, page)\n",
    "        intro_offsets = page_offsets[book.replace(\".csv\",\"\")][page_slice]\n",
    "        after_page_start = (offset_index > intro_offsets[0])\n",
    "        before_page_end = (offset_index < intro_offsets[1])\n",
    "        is_same_page = after_page_start & before_page_end\n",
    "        closest = offset_index[is_same_page]\n",
    "        return [(mentions_by_offset[x]['title'], mentions_by_offset[x]['pr']) for x in closest]\n",
    "\n",
    "    for concept, page in zip(index.wiki_concept, index.first_page):\n",
    "        potential_deps[concept].append(get_mentions_around(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_articles = []\n",
    "for concept, deps in potential_deps.items():\n",
    "    \n",
    "    titles = [[dep[0] for dep in occs] for occs in deps]\n",
    "    title_counter = Counter(sum(titles,[]))\n",
    "    \n",
    "    page_ranks = {occ[0]: occ[1] for occ in sum(deps, [])}\n",
    "    unique_titles = [list(set(book_titles)) for book_titles in titles]\n",
    "    unique_title_counter = Counter(sum(unique_titles,[]))\n",
    "    \n",
    "    concepts = [(unique_title_counter[title], # number of books with appearance,\n",
    "                 title_counter[title]/unique_title_counter[title], # average number of appeareances\n",
    "                 page_ranks[title],\n",
    "                 title) for title in title_counter.keys() if title != concept]\n",
    "    concepts.sort(reverse=True) # rank from first to third tupel element\n",
    "    dep_articles.append([d for d in concepts[:min(MAX_DEPS, len(concepts))]])\n",
    "    #if concept.find(\"Cross\") != -1:\n",
    "    #    print(concept)\n",
    "    #    print(unique_title_counter.most_common())\n",
    "    #    print(dep_articles[-1])\n",
    "\n",
    "\n",
    "df_concepts = pd.DataFrame({'concept': potential_deps.keys(), 'dep_articles': dep_articles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concepts.to_json(\"../dat/textbooks/de3.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
